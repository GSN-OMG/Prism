{
  "finished_at": "2026-01-20T04:54:19.801450Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "d638649b94e417d3",
    "tag": "rest_pr_files_pr2254_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2254/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "33038",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:54:19 GMT",
      "etag": "\"f7c64b8f9aca37e0a59e87845e3d7ab8ef2a478150b4c1fada17614a9aa542a8\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Fri, 16 Jan 2026 15:55:19 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DB06:37BFB7:160CA9E:1F172B6:696F0A7B",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4986",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "14",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 28,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2F_run_impl.py",
        "changes": 32,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F_run_impl.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 4,
        "filename": "src/agents/_run_impl.py",
        "patch": "@@ -244,7 +244,8 @@ class SingleStepResult:\n     \"\"\"Items generated before the current step.\"\"\"\n \n     new_step_items: list[RunItem]\n-    \"\"\"Items generated during this current step.\"\"\"\n+    \"\"\"Items generated during this current step. May be filtered during handoffs to avoid\n+    duplication in model input.\"\"\"\n \n     next_step: NextStepHandoff | NextStepFinalOutput | NextStepRunAgain\n     \"\"\"The next step to take.\"\"\"\n@@ -255,11 +256,18 @@ class SingleStepResult:\n     tool_output_guardrail_results: list[ToolOutputGuardrailResult]\n     \"\"\"Tool output guardrail results from this step.\"\"\"\n \n+    session_step_items: list[RunItem] | None = None\n+    \"\"\"Full unfiltered items for session history. When set, these are used instead of\n+    new_step_items for session saving and generated_items property.\"\"\"\n+\n     @property\n     def generated_items(self) -> list[RunItem]:\n         \"\"\"Items generated during the agent run (i.e. everything generated after\n-        `original_input`).\"\"\"\n-        return self.pre_step_items + self.new_step_items\n+        `original_input`). Uses session_step_items when available for full observability.\"\"\"\n+        items = (\n+            self.session_step_items if self.session_step_items is not None else self.new_step_items\n+        )\n+        return self.pre_step_items + items\n \n \n def get_model_tracing_impl(\n@@ -1286,6 +1294,12 @@ async def execute_handoffs(\n                 )\n                 pre_step_items = list(filtered.pre_handoff_items)\n                 new_step_items = list(filtered.new_items)\n+                # For custom input filters, use input_items if available, otherwise new_items\n+                if filtered.input_items is not None:\n+                    session_step_items = list(filtered.new_items)\n+                    new_step_items = list(filtered.input_items)\n+                else:\n+                    session_step_items = None\n             elif should_nest_history and handoff_input_data is not None:\n                 nested = nest_handoff_history(\n                     handoff_input_data,\n@@ -1297,7 +1311,16 @@ async def execute_handoffs(\n                     else list(nested.input_history)\n                 )\n                 pre_step_items = list(nested.pre_handoff_items)\n-                new_step_items = list(nested.new_items)\n+                # Keep full new_items for session history.\n+                session_step_items = list(nested.new_items)\n+                # Use input_items (filtered) for model input if available.\n+                if nested.input_items is not None:\n+                    new_step_items = list(nested.input_items)\n+                else:\n+                    new_step_items = session_step_items\n+            else:\n+                # No filtering or nesting - session_step_items not needed\n+                session_step_items = None\n \n         return SingleStepResult(\n             original_input=original_input,\n@@ -1307,6 +1330,7 @@ async def execute_handoffs(\n             next_step=NextStepHandoff(new_agent),\n             tool_input_guardrail_results=[],\n             tool_output_guardrail_results=[],\n+            session_step_items=session_step_items,\n         )\n \n     @classmethod",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2F_run_impl.py",
        "sha": "7815b0f23a495e02629b7d25b781574db163fc00",
        "status": "modified"
      },
      {
        "additions": 7,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2Fhandoffs%2F__init__.py",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fhandoffs%2F__init__.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 0,
        "filename": "src/agents/handoffs/__init__.py",
        "patch": "@@ -62,6 +62,13 @@ class HandoffInputData:\n     later on, it is optional for backwards compatibility.\n     \"\"\"\n \n+    input_items: tuple[RunItem, ...] | None = None\n+    \"\"\"\n+    Items to include in the next agent's input. When set, these items are used instead of\n+    new_items for building the input to the next agent. This allows filtering duplicates\n+    from agent input while preserving all items in new_items for session history.\n+    \"\"\"\n+\n     def clone(self, **kwargs: Any) -> HandoffInputData:\n         \"\"\"\n         Make a copy of the handoff input data, with the given arguments changed. For example, you",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2Fhandoffs%2F__init__.py",
        "sha": "11372dde0e1c2752ec2551fe9c38cc59d949ec85",
        "status": "modified"
      },
      {
        "additions": 45,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2Fhandoffs%2Fhistory.py",
        "changes": 58,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fhandoffs%2Fhistory.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 13,
        "filename": "src/agents/handoffs/history.py",
        "patch": "@@ -26,6 +26,13 @@\n _conversation_history_start = _DEFAULT_CONVERSATION_HISTORY_START\n _conversation_history_end = _DEFAULT_CONVERSATION_HISTORY_END\n \n+# Item types that are summarized in the conversation history.\n+# They should not be forwarded verbatim to the next agent to avoid duplication.\n+_SUMMARY_ONLY_INPUT_TYPES = {\n+    \"function_call\",\n+    \"function_call_output\",\n+}\n+\n \n def set_conversation_history_wrappers(\n     *,\n@@ -67,23 +74,34 @@ def nest_handoff_history(\n \n     normalized_history = _normalize_input_history(handoff_input_data.input_history)\n     flattened_history = _flatten_nested_history_messages(normalized_history)\n-    pre_items_as_inputs = [\n-        _run_item_to_plain_input(item) for item in handoff_input_data.pre_handoff_items\n-    ]\n-    new_items_as_inputs = [_run_item_to_plain_input(item) for item in handoff_input_data.new_items]\n+\n+    # Convert items to plain inputs for the transcript summary.\n+    pre_items_as_inputs: list[TResponseInputItem] = []\n+    filtered_pre_items: list[RunItem] = []\n+    for run_item in handoff_input_data.pre_handoff_items:\n+        plain_input = _run_item_to_plain_input(run_item)\n+        pre_items_as_inputs.append(plain_input)\n+        if _should_forward_pre_item(plain_input):\n+            filtered_pre_items.append(run_item)\n+\n+    new_items_as_inputs: list[TResponseInputItem] = []\n+    filtered_input_items: list[RunItem] = []\n+    for run_item in handoff_input_data.new_items:\n+        plain_input = _run_item_to_plain_input(run_item)\n+        new_items_as_inputs.append(plain_input)\n+        if _should_forward_new_item(plain_input):\n+            filtered_input_items.append(run_item)\n+\n     transcript = flattened_history + pre_items_as_inputs + new_items_as_inputs\n \n     mapper = history_mapper or default_handoff_history_mapper\n     history_items = mapper(transcript)\n-    filtered_pre_items = tuple(\n-        item\n-        for item in handoff_input_data.pre_handoff_items\n-        if _get_run_item_role(item) != \"assistant\"\n-    )\n \n     return handoff_input_data.clone(\n         input_history=tuple(deepcopy(item) for item in history_items),\n-        pre_handoff_items=filtered_pre_items,\n+        pre_handoff_items=tuple(filtered_pre_items),\n+        # new_items stays unchanged for session history.\n+        input_items=tuple(filtered_input_items),\n     )\n \n \n@@ -231,6 +249,20 @@ def _split_role_and_name(role_text: str) -> tuple[str, str | None]:\n     return (role_text or \"developer\", None)\n \n \n-def _get_run_item_role(run_item: RunItem) -> str | None:\n-    role_candidate = run_item.to_input_item().get(\"role\")\n-    return role_candidate if isinstance(role_candidate, str) else None\n+def _should_forward_pre_item(input_item: TResponseInputItem) -> bool:\n+    \"\"\"Return False when the previous transcript item is represented in the summary.\"\"\"\n+    role_candidate = input_item.get(\"role\")\n+    if isinstance(role_candidate, str) and role_candidate == \"assistant\":\n+        return False\n+    type_candidate = input_item.get(\"type\")\n+    return not (isinstance(type_candidate, str) and type_candidate in _SUMMARY_ONLY_INPUT_TYPES)\n+\n+\n+def _should_forward_new_item(input_item: TResponseInputItem) -> bool:\n+    \"\"\"Return False for tool or side-effect items that the summary already covers.\"\"\"\n+    # Items with a role should always be forwarded.\n+    role_candidate = input_item.get(\"role\")\n+    if isinstance(role_candidate, str) and role_candidate:\n+        return True\n+    type_candidate = input_item.get(\"type\")\n+    return not (isinstance(type_candidate, str) and type_candidate in _SUMMARY_ONLY_INPUT_TYPES)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2Fhandoffs%2Fhistory.py",
        "sha": "9dd164cac51c62517ee989e617a28a6274f339d1",
        "status": "modified"
      },
      {
        "additions": 58,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2Frun.py",
        "changes": 72,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frun.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 14,
        "filename": "src/agents/run.py",
        "patch": "@@ -583,7 +583,8 @@ async def run(\n         ):\n             current_turn = 0\n             original_input: str | list[TResponseInputItem] = _copy_str_or_list(prepared_input)\n-            generated_items: list[RunItem] = []\n+            generated_items: list[RunItem] = []  # For model input (may be filtered on handoffs)\n+            session_items: list[RunItem] = []  # For observability (always unfiltered)\n             model_responses: list[ModelResponse] = []\n \n             context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(\n@@ -705,7 +706,15 @@ async def run(\n \n                     model_responses.append(turn_result.model_response)\n                     original_input = turn_result.original_input\n-                    generated_items = turn_result.generated_items\n+                    # For model input, use new_step_items (filtered on handoffs)\n+                    generated_items = turn_result.pre_step_items + turn_result.new_step_items\n+                    # Accumulate unfiltered items for observability\n+                    session_items_for_turn = (\n+                        turn_result.session_step_items\n+                        if turn_result.session_step_items is not None\n+                        else turn_result.new_step_items\n+                    )\n+                    session_items.extend(session_items_for_turn)\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -725,7 +734,7 @@ async def run(\n                             )\n                             result = RunResult(\n                                 input=original_input,\n-                                new_items=generated_items,\n+                                new_items=session_items,  # Use unfiltered items for observability\n                                 raw_responses=model_responses,\n                                 final_output=turn_result.next_step.output,\n                                 _last_agent=current_agent,\n@@ -740,7 +749,11 @@ async def run(\n                                 for guardrail_result in input_guardrail_results\n                             ):\n                                 await self._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                 )\n \n                             return result\n@@ -752,7 +765,11 @@ async def run(\n                                     for guardrail_result in input_guardrail_results\n                                 ):\n                                     await self._save_result_to_session(\n-                                        session, [], turn_result.new_step_items\n+                                        session,\n+                                        [],\n+                                        turn_result.session_step_items\n+                                        if turn_result.session_step_items is not None\n+                                        else turn_result.new_step_items,\n                                     )\n                             current_agent = cast(Agent[TContext], turn_result.next_step.new_agent)\n                             current_span.finish(reset_current=True)\n@@ -764,7 +781,11 @@ async def run(\n                                 for guardrail_result in input_guardrail_results\n                             ):\n                                 await self._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                 )\n                         else:\n                             raise AgentsException(\n@@ -780,7 +801,7 @@ async def run(\n             except AgentsException as exc:\n                 exc.run_data = RunErrorDetails(\n                     input=original_input,\n-                    new_items=generated_items,\n+                    new_items=session_items,  # Use unfiltered items for observability\n                     raw_responses=model_responses,\n                     last_agent=current_agent,\n                     context_wrapper=context_wrapper,\n@@ -1218,7 +1239,13 @@ async def _start_streaming(\n                         turn_result.model_response\n                     ]\n                     streamed_result.input = turn_result.original_input\n-                    streamed_result.new_items = turn_result.generated_items\n+                    # Accumulate unfiltered items for observability\n+                    session_items_for_turn = (\n+                        turn_result.session_step_items\n+                        if turn_result.session_step_items is not None\n+                        else turn_result.new_step_items\n+                    )\n+                    streamed_result.new_items.extend(session_items_for_turn)\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -1234,7 +1261,11 @@ async def _start_streaming(\n                             )\n                             if should_skip_session_save is False:\n                                 await AgentRunner._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                 )\n \n                         current_agent = turn_result.next_step.new_agent\n@@ -1280,7 +1311,11 @@ async def _start_streaming(\n                             )\n                             if should_skip_session_save is False:\n                                 await AgentRunner._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                 )\n \n                         streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n@@ -1293,7 +1328,11 @@ async def _start_streaming(\n                             )\n                             if should_skip_session_save is False:\n                                 await AgentRunner._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                 )\n \n                         # Check for soft cancel after turn completion\n@@ -1745,10 +1784,15 @@ async def _get_single_step_result_from_streamed_response(\n             context_wrapper=context_wrapper,\n             run_config=run_config,\n         )\n+        # Use session_step_items (unfiltered) if available for streaming observability,\n+        # otherwise fall back to new_step_items.\n+        streaming_items = (\n+            single_step_result.session_step_items\n+            if single_step_result.session_step_items is not None\n+            else single_step_result.new_step_items\n+        )\n         new_step_items = [\n-            item\n-            for item in single_step_result.new_step_items\n-            if item not in new_items_processed_response\n+            item for item in streaming_items if item not in new_items_processed_response\n         ]\n         RunImpl.stream_step_items_to_queue(new_step_items, event_queue)\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/src%2Fagents%2Frun.py",
        "sha": "839eea52e33cfe5f2a7fbe31583b3a2b2c2cd3ad",
        "status": "modified"
      },
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/tests%2Ftest_agent_runner_streamed.py",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_agent_runner_streamed.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 1,
        "filename": "tests/test_agent_runner_streamed.py",
        "patch": "@@ -737,7 +737,8 @@ async def test_streaming_events():\n         \"tool_call_output\": 2,\n         \"message\": 2,  # get_text_message(\"a_message\") + get_final_output_message(...)\n         \"handoff\": 1,  # get_handoff_tool_call(agent_1)\n-        \"handoff_output\": 1,  # handoff_output_item\n+        # handoff_output is summarized in conversation history, not duplicated as raw item\n+        \"handoff_output\": 0,\n     }\n \n     total_expected_item_count = sum(expected_item_type_map.values())",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/tests%2Ftest_agent_runner_streamed.py",
        "sha": "a520140a7c9c8cdd47a8c4920863c7145c67d868",
        "status": "modified"
      },
      {
        "additions": 276,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/tests%2Ftest_handoff_history_duplication.py",
        "changes": 276,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_handoff_history_duplication.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 0,
        "filename": "tests/test_handoff_history_duplication.py",
        "patch": "@@ -0,0 +1,276 @@\n+\"\"\"Tests for handoff history duplication fix (Issue #2171).\n+\n+These tests verify that when nest_handoff_history is enabled,\n+function_call and function_call_output items are NOT duplicated\n+in the input sent to the next agent.\n+\"\"\"\n+\n+from typing import Any, cast\n+\n+from openai.types.responses import (\n+    ResponseFunctionToolCall,\n+    ResponseOutputMessage,\n+    ResponseOutputText,\n+)\n+\n+from agents import Agent\n+from agents.handoffs import HandoffInputData, nest_handoff_history\n+from agents.items import (\n+    HandoffCallItem,\n+    HandoffOutputItem,\n+    MessageOutputItem,\n+    ToolCallItem,\n+    ToolCallOutputItem,\n+)\n+\n+\n+def _create_mock_agent() -> Agent:\n+    \"\"\"Create a mock agent for testing.\"\"\"\n+    return Agent(name=\"test_agent\")\n+\n+\n+def _create_tool_call_item(agent: Agent) -> ToolCallItem:\n+    \"\"\"Create a mock ToolCallItem.\"\"\"\n+    raw_item = ResponseFunctionToolCall(\n+        id=\"call_tool_123\",\n+        call_id=\"call_tool_123\",\n+        name=\"get_weather\",\n+        arguments='{\"city\": \"London\"}',\n+        type=\"function_call\",\n+    )\n+    return ToolCallItem(agent=agent, raw_item=raw_item, type=\"tool_call_item\")\n+\n+\n+def _create_tool_output_item(agent: Agent) -> ToolCallOutputItem:\n+    \"\"\"Create a mock ToolCallOutputItem.\"\"\"\n+    raw_item = {\n+        \"type\": \"function_call_output\",\n+        \"call_id\": \"call_tool_123\",\n+        \"output\": \"Sunny, 22°C\",\n+    }\n+    return ToolCallOutputItem(\n+        agent=agent,\n+        raw_item=raw_item,\n+        output=\"Sunny, 22°C\",\n+        type=\"tool_call_output_item\",\n+    )\n+\n+\n+def _create_handoff_call_item(agent: Agent) -> HandoffCallItem:\n+    \"\"\"Create a mock HandoffCallItem.\"\"\"\n+    raw_item = ResponseFunctionToolCall(\n+        id=\"call_handoff_456\",\n+        call_id=\"call_handoff_456\",\n+        name=\"transfer_to_agent_b\",\n+        arguments=\"{}\",\n+        type=\"function_call\",\n+    )\n+    return HandoffCallItem(agent=agent, raw_item=raw_item, type=\"handoff_call_item\")\n+\n+\n+def _create_handoff_output_item(agent: Agent[Any]) -> HandoffOutputItem:\n+    \"\"\"Create a mock HandoffOutputItem.\"\"\"\n+    raw_item: dict[str, str] = {\n+        \"type\": \"function_call_output\",\n+        \"call_id\": \"call_handoff_456\",\n+        \"output\": '{\"assistant\": \"agent_b\"}',\n+    }\n+    return HandoffOutputItem(\n+        agent=agent,\n+        raw_item=cast(Any, raw_item),\n+        source_agent=agent,\n+        target_agent=agent,\n+        type=\"handoff_output_item\",\n+    )\n+\n+\n+def _create_message_item(agent: Agent) -> MessageOutputItem:\n+    \"\"\"Create a mock MessageOutputItem.\"\"\"\n+    raw_item = ResponseOutputMessage(\n+        id=\"msg_123\",\n+        content=[ResponseOutputText(text=\"Hello!\", type=\"output_text\", annotations=[])],\n+        role=\"assistant\",\n+        status=\"completed\",\n+        type=\"message\",\n+    )\n+    return MessageOutputItem(agent=agent, raw_item=raw_item, type=\"message_output_item\")\n+\n+\n+class TestHandoffHistoryDuplicationFix:\n+    \"\"\"Tests for Issue #2171: nest_handoff_history duplication fix.\"\"\"\n+\n+    def test_pre_handoff_tool_items_are_filtered(self):\n+        \"\"\"Verify ToolCallItem and ToolCallOutputItem in pre_handoff_items are filtered.\n+\n+        These items should NOT appear in the filtered output because they are\n+        already included in the summary message.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(\n+                _create_tool_call_item(agent),\n+                _create_tool_output_item(agent),\n+            ),\n+            new_items=(),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # pre_handoff_items should be empty (tool items filtered)\n+        assert len(nested.pre_handoff_items) == 0, (\n+            \"ToolCallItem and ToolCallOutputItem should be filtered from pre_handoff_items\"\n+        )\n+\n+        # Summary should contain the conversation\n+        assert len(nested.input_history) == 1\n+        first_item = nested.input_history[0]\n+        assert isinstance(first_item, dict)\n+        assert \"<CONVERSATION HISTORY>\" in str(first_item.get(\"content\", \"\"))\n+\n+    def test_new_items_handoff_output_is_filtered_for_input(self):\n+        \"\"\"Verify HandoffOutputItem in new_items is filtered from input_items.\n+\n+        The HandoffOutputItem is a function_call_output which would be duplicated.\n+        It should be filtered from input_items but preserved in new_items.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(),\n+            new_items=(\n+                _create_handoff_call_item(agent),\n+                _create_handoff_output_item(agent),\n+            ),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # new_items should still have both items (for session history)\n+        assert len(nested.new_items) == 2, \"new_items should preserve all items for session history\"\n+\n+        # input_items should be populated and filtered\n+        assert nested.input_items is not None, \"input_items should be populated\"\n+\n+        # input_items should NOT contain HandoffOutputItem (it's function_call_output)\n+        has_handoff_output = any(isinstance(item, HandoffOutputItem) for item in nested.input_items)\n+        assert not has_handoff_output, \"HandoffOutputItem should be filtered from input_items\"\n+\n+    def test_message_items_are_preserved_in_new_items(self):\n+        \"\"\"Verify MessageOutputItem in new_items is preserved.\n+\n+        Message items have a 'role' and should NOT be filtered from input_items.\n+        Note: pre_handoff_items are converted to summary text regardless of type.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(),  # pre_handoff items go into summary\n+            new_items=(_create_message_item(agent),),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # Message items should be preserved in new_items\n+        assert len(nested.new_items) == 1, \"MessageOutputItem should be preserved in new_items\"\n+        # And in input_items (since it has a role)\n+        assert nested.input_items is not None\n+        assert len(nested.input_items) == 1, \"MessageOutputItem should be preserved in input_items\"\n+        assert isinstance(nested.input_items[0], MessageOutputItem)\n+\n+    def test_summary_contains_filtered_items_as_text(self):\n+        \"\"\"Verify the summary message contains the filtered tool items as text.\n+\n+        This ensures observability - the items are not lost, just converted to text.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(\n+                _create_tool_call_item(agent),\n+                _create_tool_output_item(agent),\n+            ),\n+            new_items=(),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        first_item = nested.input_history[0]\n+        assert isinstance(first_item, dict)\n+        summary = str(first_item.get(\"content\", \"\"))\n+\n+        # Summary should contain function_call reference\n+        assert \"function_call\" in summary or \"get_weather\" in summary, (\n+            \"Summary should contain the tool call that was filtered\"\n+        )\n+\n+    def test_input_items_field_exists_after_nesting(self):\n+        \"\"\"Verify the input_items field is populated after nest_handoff_history.\n+\n+        This is the key field that separates model input from session history.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(),\n+            new_items=(_create_handoff_call_item(agent),),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        assert nested.input_items is not None, (\n+            \"input_items should be populated after nest_handoff_history\"\n+        )\n+\n+    def test_full_handoff_scenario_no_duplication(self):\n+        \"\"\"Full end-to-end test of the handoff scenario from Issue #2171.\n+\n+        Simulates: User -> Agent does tool call -> Agent hands off to next agent\n+        Verifies: Next agent receives summary only, no duplicate raw items.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        # Full scenario: tool call in pre_handoff, handoff in new_items\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"What's the weather?\"},),\n+            pre_handoff_items=(\n+                _create_tool_call_item(agent),  # function_call\n+                _create_tool_output_item(agent),  # function_call_output\n+            ),\n+            new_items=(\n+                _create_message_item(agent),  # assistant message\n+                _create_handoff_call_item(agent),  # function_call (handoff)\n+                _create_handoff_output_item(agent),  # function_call_output (handoff)\n+            ),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # Count what would be sent to the model\n+        total_model_items = (\n+            len(nested.input_history)  # Summary\n+            + len(nested.pre_handoff_items)  # Filtered pre-handoff\n+            + len(nested.input_items or [])  # Filtered new items\n+        )\n+\n+        # Before fix: would have 6+ items (summary + raw tool items)\n+        # After fix: should have ~2 items (summary + message)\n+        assert total_model_items <= 3, (\n+            f\"Model should receive at most 3 items (summary + messages), got {total_model_items}\"\n+        )\n+\n+        # Verify no raw function_call_output items in model input\n+        all_input_items = list(nested.pre_handoff_items) + list(nested.input_items or [])\n+        function_call_outputs = [\n+            item\n+            for item in all_input_items\n+            if isinstance(item, (ToolCallOutputItem, HandoffOutputItem))\n+        ]\n+        assert len(function_call_outputs) == 0, (\n+            \"No function_call_output items should be in model input\"\n+        )",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/tests%2Ftest_handoff_history_duplication.py",
        "sha": "617c7ef71064538eb99214d3c5133b00f08c0387",
        "status": "added"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b03063d3ff76a98f507f3d7a91d7478816e25e25/tests%2Ftest_soft_cancel.py",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_soft_cancel.py?ref=b03063d3ff76a98f507f3d7a91d7478816e25e25",
        "deletions": 1,
        "filename": "tests/test_soft_cancel.py",
        "patch": "@@ -421,7 +421,7 @@ async def on_invoke_handoff(context, data):\n \n     handoff_seen = False\n     async for event in result.stream_events():\n-        if event.type == \"run_item_stream_event\" and event.name == \"handoff_occured\":\n+        if event.type == \"run_item_stream_event\" and event.name == \"handoff_requested\":\n             handoff_seen = True\n             # Cancel right after handoff\n             result.cancel(mode=\"after_turn\")",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b03063d3ff76a98f507f3d7a91d7478816e25e25/tests%2Ftest_soft_cancel.py",
        "sha": "ddb51f8f17ff90dbfc544ca7174f761defe353aa",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:54:19.353313Z"
}
