{
  "finished_at": "2026-01-20T04:58:31.283564Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "72bd8b4569130ac0",
    "tag": "rest_pr_files_pr2337_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2337/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "14686",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:58:31 GMT",
      "etag": "\"c14b477b8ce0cec5ff436498b45c87d5cf7a111562efc69f65bed626ddfe1ebf\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Tue, 20 Jan 2026 01:46:32 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DD45:7699F:16673CB:1F72CFA:696F0B76",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4925",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "75",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b308b97e8796b7b5cf56647d6e09b8a890b07d08/src%2Fagents%2F_run_impl.py",
        "changes": 4,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F_run_impl.py?ref=b308b97e8796b7b5cf56647d6e09b8a890b07d08",
        "deletions": 2,
        "filename": "src/agents/_run_impl.py",
        "patch": "@@ -970,7 +970,7 @@ async def run_single_tool(\n \n                     if rejected_message is not None:\n                         # Input guardrail rejected the tool call\n-                        final_result = rejected_message\n+                        result = rejected_message\n                     else:\n                         # 2) Actually run the tool\n                         real_result = await cls._execute_tool_with_hooks(\n@@ -1001,7 +1001,7 @@ async def run_single_tool(\n                                 else _coro.noop_coroutine()\n                             ),\n                         )\n-                    result = final_result\n+                        result = final_result\n                 except Exception as e:\n                     _error_tracing.attach_error_to_current_span(\n                         SpanError(",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b308b97e8796b7b5cf56647d6e09b8a890b07d08/src%2Fagents%2F_run_impl.py",
        "sha": "7a18aded205cea4bf0318ee165ba289d90db3d7d",
        "status": "modified"
      },
      {
        "additions": 34,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b308b97e8796b7b5cf56647d6e09b8a890b07d08/src%2Fagents%2Ftool.py",
        "changes": 36,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Ftool.py?ref=b308b97e8796b7b5cf56647d6e09b8a890b07d08",
        "deletions": 2,
        "filename": "src/agents/tool.py",
        "patch": "@@ -668,8 +668,32 @@ def type(self) -> str:\n \"\"\"A tool that can be used in an agent.\"\"\"\n \n \n+def _extract_json_decode_error(error: BaseException) -> json.JSONDecodeError | None:\n+    current: BaseException | None = error\n+    while current is not None:\n+        if isinstance(current, json.JSONDecodeError):\n+            return current\n+        current = current.__cause__ or current.__context__\n+    return None\n+\n+\n+def _extract_tool_argument_json_error(error: Exception) -> json.JSONDecodeError | None:\n+    if not isinstance(error, ModelBehaviorError):\n+        return None\n+    if not str(error).startswith(\"Invalid JSON input for tool\"):\n+        return None\n+    return _extract_json_decode_error(error)\n+\n+\n def default_tool_error_function(ctx: RunContextWrapper[Any], error: Exception) -> str:\n     \"\"\"The default tool error function, which just returns a generic error message.\"\"\"\n+    json_decode_error = _extract_tool_argument_json_error(error)\n+    if json_decode_error is not None:\n+        return (\n+            \"An error occurred while parsing tool arguments. \"\n+            \"Please try again with valid JSON. \"\n+            f\"Error: {json_decode_error}\"\n+        )\n     return f\"An error occurred while running the tool. Please try again. Error: {str(error)}\"\n \n \n@@ -828,12 +852,20 @@ async def _on_invoke_tool(ctx: ToolContext[Any], input: str) -> Any:\n                 if inspect.isawaitable(result):\n                     return await result\n \n+                json_decode_error = _extract_tool_argument_json_error(e)\n+                if json_decode_error is not None:\n+                    span_error_message = \"Error running tool\"\n+                    span_error_detail = str(json_decode_error)\n+                else:\n+                    span_error_message = \"Error running tool (non-fatal)\"\n+                    span_error_detail = str(e)\n+\n                 _error_tracing.attach_error_to_current_span(\n                     SpanError(\n-                        message=\"Error running tool (non-fatal)\",\n+                        message=span_error_message,\n                         data={\n                             \"tool_name\": schema.name,\n-                            \"error\": str(e),\n+                            \"error\": span_error_detail,\n                         },\n                     )\n                 )",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b308b97e8796b7b5cf56647d6e09b8a890b07d08/src%2Fagents%2Ftool.py",
        "sha": "b73b6803b5eb0497ebd2a9b59bdc50f641dad780",
        "status": "modified"
      },
      {
        "additions": 61,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b308b97e8796b7b5cf56647d6e09b8a890b07d08/tests%2Ftest_run_step_execution.py",
        "changes": 61,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_run_step_execution.py?ref=b308b97e8796b7b5cf56647d6e09b8a890b07d08",
        "deletions": 0,
        "filename": "tests/test_run_step_execution.py",
        "patch": "@@ -9,13 +9,16 @@\n from agents import (\n     Agent,\n     MessageOutputItem,\n+    ModelBehaviorError,\n     ModelResponse,\n     RunConfig,\n     RunContextWrapper,\n     RunHooks,\n     RunItem,\n     ToolCallItem,\n     ToolCallOutputItem,\n+    ToolGuardrailFunctionOutput,\n+    ToolInputGuardrail,\n     TResponseInputItem,\n     Usage,\n )\n@@ -288,6 +291,64 @@ async def test_multiple_final_output_leads_to_final_output_next_step():\n     assert result.next_step.output == Foo(bar=\"456\")\n \n \n+@pytest.mark.asyncio\n+async def test_input_guardrail_runs_on_invalid_json():\n+    guardrail_calls: list[str] = []\n+\n+    def guardrail(data) -> ToolGuardrailFunctionOutput:\n+        guardrail_calls.append(data.context.tool_arguments)\n+        return ToolGuardrailFunctionOutput.allow(output_info=\"checked\")\n+\n+    guardrail_obj: ToolInputGuardrail[Any] = ToolInputGuardrail(guardrail_function=guardrail)\n+\n+    def _echo(value: str) -> str:\n+        return value\n+\n+    tool = function_tool(\n+        _echo,\n+        name_override=\"guarded\",\n+        tool_input_guardrails=[guardrail_obj],\n+    )\n+    agent = Agent(name=\"test\", tools=[tool])\n+    response = ModelResponse(\n+        output=[get_function_tool_call(\"guarded\", \"bad_json\")],\n+        usage=Usage(),\n+        response_id=None,\n+    )\n+\n+    result = await get_execute_result(agent, response)\n+\n+    assert guardrail_calls == [\"bad_json\"]\n+    assert result.tool_input_guardrail_results\n+    assert result.tool_input_guardrail_results[0].output.output_info == \"checked\"\n+\n+    output_item = next(\n+        item for item in result.generated_items if isinstance(item, ToolCallOutputItem)\n+    )\n+    assert \"An error occurred while parsing tool arguments\" in str(output_item.output)\n+\n+\n+@pytest.mark.asyncio\n+async def test_invalid_json_raises_with_failure_error_function_none():\n+    def _echo(value: str) -> str:\n+        return value\n+\n+    tool = function_tool(\n+        _echo,\n+        name_override=\"guarded\",\n+        failure_error_function=None,\n+    )\n+    agent = Agent(name=\"test\", tools=[tool])\n+    response = ModelResponse(\n+        output=[get_function_tool_call(\"guarded\", \"bad_json\")],\n+        usage=Usage(),\n+        response_id=None,\n+    )\n+\n+    with pytest.raises(ModelBehaviorError, match=\"Invalid JSON input for tool\"):\n+        await get_execute_result(agent, response)\n+\n+\n # === Helpers ===\n \n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b308b97e8796b7b5cf56647d6e09b8a890b07d08/tests%2Ftest_run_step_execution.py",
        "sha": "8e0860e0e598d320e4a074c1bf7e2dc4eada7627",
        "status": "modified"
      },
      {
        "additions": 23,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b308b97e8796b7b5cf56647d6e09b8a890b07d08/tests%2Ftest_tracing_errors.py",
        "changes": 31,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_tracing_errors.py?ref=b308b97e8796b7b5cf56647d6e09b8a890b07d08",
        "deletions": 8,
        "filename": "tests/test_tracing_errors.py",
        "patch": "@@ -13,7 +13,6 @@\n     InputGuardrail,\n     InputGuardrailTripwireTriggered,\n     MaxTurnsExceeded,\n-    ModelBehaviorError,\n     RunContextWrapper,\n     Runner,\n     TResponseInputItem,\n@@ -140,15 +139,22 @@ async def test_tool_call_error():\n     agent = Agent(\n         name=\"test_agent\",\n         model=model,\n-        tools=[get_function_tool(\"foo\", \"tool_result\", hide_errors=True)],\n+        tools=[get_function_tool(\"foo\", \"tool_result\")],\n     )\n \n-    model.set_next_output(\n-        [get_text_message(\"a_message\"), get_function_tool_call(\"foo\", \"bad_json\")],\n+    model.add_multiple_turn_outputs(\n+        [\n+            [get_text_message(\"a_message\"), get_function_tool_call(\"foo\", \"bad_json\")],\n+            [get_text_message(\"done\")],\n+        ]\n     )\n \n-    with pytest.raises(ModelBehaviorError):\n-        await Runner.run(agent, input=\"first_test\")\n+    result = await Runner.run(agent, input=\"first_test\")\n+\n+    tool_outputs = [item for item in result.new_items if item.type == \"tool_call_output_item\"]\n+    assert tool_outputs, \"Expected a tool output item for invalid JSON\"\n+    assert \"An error occurred while parsing tool arguments\" in str(tool_outputs[0].output)\n+    assert \"valid JSON\" in str(tool_outputs[0].output)\n \n     assert fetch_normalized_spans() == snapshot(\n         [\n@@ -171,11 +177,20 @@ async def test_tool_call_error():\n                                     \"message\": \"Error running tool\",\n                                     \"data\": {\n                                         \"tool_name\": \"foo\",\n-                                        \"error\": \"Invalid JSON input for tool foo: bad_json\",\n+                                        \"error\": \"Expecting value: line 1 column 1 (char 0)\",\n                                     },\n                                 },\n-                                \"data\": {\"name\": \"foo\", \"input\": \"bad_json\"},\n+                                \"data\": {\n+                                    \"name\": \"foo\",\n+                                    \"input\": \"bad_json\",\n+                                    \"output\": (\n+                                        \"An error occurred while parsing tool arguments. \"\n+                                        \"Please try again with valid JSON. Error: Expecting \"\n+                                        \"value: line 1 column 1 (char 0)\"\n+                                    ),\n+                                },\n                             },\n+                            {\"type\": \"generation\"},\n                         ],\n                     }\n                 ],",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b308b97e8796b7b5cf56647d6e09b8a890b07d08/tests%2Ftest_tracing_errors.py",
        "sha": "6149afc79fb3e65c6925eda627f3948888f97c24",
        "status": "modified"
      },
      {
        "additions": 25,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/b308b97e8796b7b5cf56647d6e09b8a890b07d08/tests%2Ftest_tracing_errors_streamed.py",
        "changes": 35,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_tracing_errors_streamed.py?ref=b308b97e8796b7b5cf56647d6e09b8a890b07d08",
        "deletions": 10,
        "filename": "tests/test_tracing_errors_streamed.py",
        "patch": "@@ -14,7 +14,6 @@\n     InputGuardrail,\n     InputGuardrailTripwireTriggered,\n     MaxTurnsExceeded,\n-    ModelBehaviorError,\n     OutputGuardrail,\n     OutputGuardrailTripwireTriggered,\n     RunContextWrapper,\n@@ -149,17 +148,24 @@ async def test_tool_call_error():\n     agent = Agent(\n         name=\"test_agent\",\n         model=model,\n-        tools=[get_function_tool(\"foo\", \"tool_result\", hide_errors=True)],\n+        tools=[get_function_tool(\"foo\", \"tool_result\")],\n     )\n \n-    model.set_next_output(\n-        [get_text_message(\"a_message\"), get_function_tool_call(\"foo\", \"bad_json\")],\n+    model.add_multiple_turn_outputs(\n+        [\n+            [get_text_message(\"a_message\"), get_function_tool_call(\"foo\", \"bad_json\")],\n+            [get_text_message(\"done\")],\n+        ]\n     )\n \n-    with pytest.raises(ModelBehaviorError):\n-        result = Runner.run_streamed(agent, input=\"first_test\")\n-        async for _ in result.stream_events():\n-            pass\n+    result = Runner.run_streamed(agent, input=\"first_test\")\n+    async for _ in result.stream_events():\n+        pass\n+\n+    tool_outputs = [item for item in result.new_items if item.type == \"tool_call_output_item\"]\n+    assert tool_outputs, \"Expected a tool output item for invalid JSON\"\n+    assert \"An error occurred while parsing tool arguments\" in str(tool_outputs[0].output)\n+    assert \"valid JSON\" in str(tool_outputs[0].output)\n \n     assert fetch_normalized_spans() == snapshot(\n         [\n@@ -182,11 +188,20 @@ async def test_tool_call_error():\n                                     \"message\": \"Error running tool\",\n                                     \"data\": {\n                                         \"tool_name\": \"foo\",\n-                                        \"error\": \"Invalid JSON input for tool foo: bad_json\",\n+                                        \"error\": \"Expecting value: line 1 column 1 (char 0)\",\n                                     },\n                                 },\n-                                \"data\": {\"name\": \"foo\", \"input\": \"bad_json\"},\n+                                \"data\": {\n+                                    \"name\": \"foo\",\n+                                    \"input\": \"bad_json\",\n+                                    \"output\": (\n+                                        \"An error occurred while parsing tool arguments. \"\n+                                        \"Please try again with valid JSON. Error: Expecting \"\n+                                        \"value: line 1 column 1 (char 0)\"\n+                                    ),\n+                                },\n                             },\n+                            {\"type\": \"generation\"},\n                         ],\n                     }\n                 ],",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/b308b97e8796b7b5cf56647d6e09b8a890b07d08/tests%2Ftest_tracing_errors_streamed.py",
        "sha": "5b66cb968b6c86bdabe335dcd49c61a52527d6f3",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:58:30.600683Z"
}
