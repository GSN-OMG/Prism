{
  "finished_at": "2026-01-20T04:53:24.570967Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "52708df8b955398e",
    "tag": "rest_pr_files_pr2157_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2157/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "77551",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:53:24 GMT",
      "etag": "\"b3fc2ce59f727c8f8c30f56507bf9156c3e6df0d02b32f64174394e8998f45f1\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Fri, 16 Jan 2026 01:32:43 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DA95:29AB24:15F3E27:1EFDD0F:696F0A43",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4996",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "4",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 4,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/docs%2Fhandoffs.md",
        "changes": 4,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fhandoffs.md?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 0,
        "filename": "docs/handoffs.md",
        "patch": "@@ -82,6 +82,10 @@ handoff_obj = handoff(\n \n When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an [`input_filter`][agents.handoffs.Handoff.input_filter]. An input filter is a function that receives the existing input via a [`HandoffInputData`][agents.handoffs.HandoffInputData], and must return a new `HandoffInputData`.\n \n+By default the runner now collapses the prior transcript into a single assistant summary message (see [`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]). The summary appears inside a `<CONVERSATION HISTORY>` block that keeps appending new turns when multiple handoffs happen during the same run. You can provide your own mapping function via [`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper] to replace the generated message without writing a full `input_filter`. That default only applies when neither the handoff nor the run supplies an explicit `input_filter`, so existing code that already customizes the payload (including the examples in this repository) keeps its current behavior without changes. You can override the nesting behaviour for a single handoff by passing `nest_handoff_history=True` or `False` to [`handoff(...)`][agents.handoffs.handoff], which sets [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history]. If you just need to change the wrapper text for the generated summary, call [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] (and optionally [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]) before running your agents.\n+\n+If you are using server-managed conversations (`conversation_id` or `previous_response_id`), the SDK keeps the server's transcript authoritative: handoff input filters are rejected and nested history is disabled to avoid desynchronizing the stored conversation.\n+\n There are some common patterns (for example removing all tool calls from the history), which are implemented for you in [`agents.extensions.handoff_filters`][]\n \n ```python",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/docs%2Fhandoffs.md",
        "sha": "f7be19a5d4fb22cd053dde30c86b6edf4835763a",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/docs%2Frelease.md",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Frelease.md?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 0,
        "filename": "docs/release.md",
        "patch": "@@ -25,6 +25,9 @@ This version doesnâ€™t introduce any visible breaking changes, but it includes n\n \n - Added support for `RealtimeRunner` to handle [SIP protocol connections](https://platform.openai.com/docs/guides/realtime-sip)\n - Significantly revised the internal logic of `Runner#run_sync` for Python 3.14 compatibility\n+- By default handoff history is now packaged into a single assistant message instead of exposing the raw user/assistant turns, giving downstream agents a concise, predictable recap\n+- The existing single-message handoff transcript now starts with \"For context, here is the conversation so far between the user and the previous agent:\" before the `<CONVERSATION HISTORY>` block, so downstream agents get a clearly labeled recap\n+- The existing single-message handoff transcript now starts with \"For context, here is the conversation so far between the user and the previous agent:\" before the `<CONVERSATION HISTORY>` block, so downstream agents get a clearly labeled recap\n \n ### 0.4.0\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/docs%2Frelease.md",
        "sha": "93161a7c855341bb2f06ba41fa259f39eed7018e",
        "status": "modified"
      },
      {
        "additions": 5,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/docs%2Frunning_agents.md",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Frunning_agents.md?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 1,
        "filename": "docs/running_agents.md",
        "patch": "@@ -51,11 +51,15 @@ The `run_config` parameter lets you configure some global settings for the agent\n -   [`model_settings`][agents.run.RunConfig.model_settings]: Overrides agent-specific settings. For example, you can set a global `temperature` or `top_p`.\n -   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: A list of input or output guardrails to include on all runs.\n -   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] for more details.\n+-   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]: When `True` (the default) the runner collapses the prior transcript into a single assistant message before invoking the next agent. The helper places the content inside a `<CONVERSATION HISTORY>` block that keeps appending new turns as subsequent handoffs occur. Set this to `False` or provide a custom handoff filter if you prefer to pass through the raw transcript. All [`Runner` methods](agents.run.Runner) automatically create a `RunConfig` when you do not pass one, so the quickstarts and examples pick up this default automatically, and any explicit [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] callbacks continue to override it. Individual handoffs can override this setting via [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history].\n+-   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]: Optional callable that receives the normalized transcript (history + handoff items) whenever `nest_handoff_history` is `True`. It must return the exact list of input items to forward to the next agent, allowing you to replace the built-in summary without writing a full handoff filter.\n -   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: Allows you to disable [tracing](tracing.md) for the entire run.\n -   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.\n -   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting `workflow_name`. The group ID is an optional field that lets you link traces across multiple runs.\n -   [`trace_metadata`][agents.run.RunConfig.trace_metadata]: Metadata to include on all traces.\n \n+By default, the SDK now nests prior turns inside a single assistant summary message whenever an agent hands off to another agent. This reduces repeated assistant messages and keeps the full transcript inside a single block that new agents can scan quickly. If you'd like to return to the legacy behavior, pass `RunConfig(nest_handoff_history=False)` or supply a `handoff_input_filter` (or `handoff_history_mapper`) that forwards the conversation exactly as you need. You can also opt out (or in) for a specific handoff by setting `handoff(..., nest_handoff_history=False)` or `True`. To change the wrapper text used in the generated summary without writing a custom mapper, call [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] (and [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers] to restore the defaults).\n+\n ## Conversations/chat threads\n \n Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:\n@@ -200,4 +204,4 @@ The SDK raises exceptions in certain cases. The full list is in [`agents.excepti\n     -   Malformed JSON: When the model provides a malformed JSON structure for tool calls or in its direct output, especially if a specific `output_type` is defined.\n     -   Unexpected tool-related failures: When the model fails to use tools in an expected manner\n -   [`UserError`][agents.exceptions.UserError]: This exception is raised when you (the person writing code using the SDK) make an error while using the SDK. This typically results from incorrect code implementation, invalid configuration, or misuse of the SDK's API.\n--   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: This exception is raised when the conditions of an input guardrail or output guardrail are met, respectively. Input guardrails check incoming messages before processing, while output guardrails check the agent's final response before delivery.\n\\ No newline at end of file\n+-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: This exception is raised when the conditions of an input guardrail or output guardrail are met, respectively. Input guardrails check incoming messages before processing, while output guardrails check the agent's final response before delivery.",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/docs%2Frunning_agents.md",
        "sha": "fb3e9aa47143d6c0cc11813bcb596a4405b1b1b7",
        "status": "modified"
      },
      {
        "additions": 16,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2F__init__.py",
        "changes": 17,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F__init__.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 1,
        "filename": "src/agents/__init__.py",
        "patch": "@@ -34,7 +34,17 @@\n     input_guardrail,\n     output_guardrail,\n )\n-from .handoffs import Handoff, HandoffInputData, HandoffInputFilter, handoff\n+from .handoffs import (\n+    Handoff,\n+    HandoffInputData,\n+    HandoffInputFilter,\n+    default_handoff_history_mapper,\n+    get_conversation_history_wrappers,\n+    handoff,\n+    nest_handoff_history,\n+    reset_conversation_history_wrappers,\n+    set_conversation_history_wrappers,\n+)\n from .items import (\n     HandoffCallItem,\n     HandoffOutputItem,\n@@ -191,6 +201,11 @@ def enable_verbose_stdout_logging():\n     \"StopAtTools\",\n     \"ToolsToFinalOutputFunction\",\n     \"ToolsToFinalOutputResult\",\n+    \"default_handoff_history_mapper\",\n+    \"get_conversation_history_wrappers\",\n+    \"nest_handoff_history\",\n+    \"reset_conversation_history_wrappers\",\n+    \"set_conversation_history_wrappers\",\n     \"Runner\",\n     \"run_demo_loop\",\n     \"Model\",",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2F__init__.py",
        "sha": "8488cd540d68f4670189c00d10f9da626d3b7528",
        "status": "modified"
      },
      {
        "additions": 56,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2F_run_impl.py",
        "changes": 59,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F_run_impl.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 3,
        "filename": "src/agents/_run_impl.py",
        "patch": "@@ -52,7 +52,7 @@\n     UserError,\n )\n from .guardrail import InputGuardrail, InputGuardrailResult, OutputGuardrail, OutputGuardrailResult\n-from .handoffs import Handoff, HandoffInputData\n+from .handoffs import Handoff, HandoffInputData, nest_handoff_history\n from .items import (\n     HandoffCallItem,\n     HandoffOutputItem,\n@@ -226,6 +226,10 @@ class SingleStepResult:\n     tool_output_guardrail_results: list[ToolOutputGuardrailResult]\n     \"\"\"Tool output guardrail results from this step.\"\"\"\n \n+    reset_conversation_items: bool = False\n+    \"\"\"When True, omit previously generated items from the next model call (e.g. after nesting\n+    handoff history) while still keeping them in run results.\"\"\"\n+\n     @property\n     def generated_items(self) -> list[RunItem]:\n         \"\"\"Items generated during the agent run (i.e. everything generated after\n@@ -260,6 +264,7 @@ async def execute_tools_and_side_effects(\n         hooks: RunHooks[TContext],\n         context_wrapper: RunContextWrapper[TContext],\n         run_config: RunConfig,\n+        server_conversation_mode: bool = False,\n     ) -> SingleStepResult:\n         # Make a copy of the generated items\n         pre_step_items = list(pre_step_items)\n@@ -320,6 +325,7 @@ async def execute_tools_and_side_effects(\n                 hooks=hooks,\n                 context_wrapper=context_wrapper,\n                 run_config=run_config,\n+                server_conversation_mode=server_conversation_mode,\n             )\n \n         # Next, we'll check if the tool use should result in a final output\n@@ -927,6 +933,7 @@ async def execute_handoffs(\n         hooks: RunHooks[TContext],\n         context_wrapper: RunContextWrapper[TContext],\n         run_config: RunConfig,\n+        server_conversation_mode: bool = False,\n     ) -> SingleStepResult:\n         # If there is more than one handoff, add tool responses that reject those handoffs\n         multiple_handoffs = len(run_handoffs) > 1\n@@ -998,8 +1005,29 @@ async def execute_handoffs(\n             input_filter = handoff.input_filter or (\n                 run_config.handoff_input_filter if run_config else None\n             )\n-            if input_filter:\n-                logger.debug(\"Filtering inputs for handoff\")\n+            handoff_nest_setting = handoff.nest_handoff_history\n+            should_nest_history = (\n+                handoff_nest_setting\n+                if handoff_nest_setting is not None\n+                else run_config.nest_handoff_history\n+            )\n+            reset_conversation_items = False\n+            if server_conversation_mode:\n+                if input_filter:\n+                    raise UserError(\n+                        \"Handoff input filters are not supported when using server-managed \"\n+                        \"conversations (conversation_id or previous_response_id). Remove the \"\n+                        \"input_filter or disable server-managed conversation state.\"\n+                    )\n+                if should_nest_history:\n+                    logger.warning(\n+                        \"Disabling nest_handoff_history because server-managed conversation state \"\n+                        \"is enabled via conversation_id or previous_response_id.\"\n+                    )\n+                input_filter = None\n+                should_nest_history = False\n+            handoff_input_data: HandoffInputData | None = None\n+            if input_filter or should_nest_history:\n                 handoff_input_data = HandoffInputData(\n                     input_history=tuple(original_input)\n                     if isinstance(original_input, list)\n@@ -1008,6 +1036,17 @@ async def execute_handoffs(\n                     new_items=tuple(new_step_items),\n                     run_context=context_wrapper,\n                 )\n+\n+            if input_filter and handoff_input_data is not None:\n+                filter_name = getattr(input_filter, \"__qualname__\", repr(input_filter))\n+                from_agent = getattr(agent, \"name\", agent.__class__.__name__)\n+                to_agent = getattr(new_agent, \"name\", new_agent.__class__.__name__)\n+                logger.debug(\n+                    \"Filtering handoff inputs with %s for %s -> %s\",\n+                    filter_name,\n+                    from_agent,\n+                    to_agent,\n+                )\n                 if not callable(input_filter):\n                     _error_tracing.attach_error_to_span(\n                         span_handoff,\n@@ -1037,6 +1076,19 @@ async def execute_handoffs(\n                 )\n                 pre_step_items = list(filtered.pre_handoff_items)\n                 new_step_items = list(filtered.new_items)\n+            elif should_nest_history and handoff_input_data is not None:\n+                nested = nest_handoff_history(\n+                    handoff_input_data,\n+                    history_mapper=run_config.handoff_history_mapper,\n+                )\n+                reset_conversation_items = True\n+                original_input = (\n+                    nested.input_history\n+                    if isinstance(nested.input_history, str)\n+                    else list(nested.input_history)\n+                )\n+                pre_step_items = list(nested.pre_handoff_items)\n+                new_step_items = list(nested.new_items)\n \n         return SingleStepResult(\n             original_input=original_input,\n@@ -1046,6 +1098,7 @@ async def execute_handoffs(\n             next_step=NextStepHandoff(new_agent),\n             tool_input_guardrail_results=[],\n             tool_output_guardrail_results=[],\n+            reset_conversation_items=reset_conversation_items,\n         )\n \n     @classmethod",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2F_run_impl.py",
        "sha": "4861514c74d758ac3072dc9bbd0f67c9f06f9d94",
        "status": "modified"
      },
      {
        "additions": 11,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Fextensions%2Fhandoff_filters.py",
        "changes": 12,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fhandoff_filters.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 1,
        "filename": "src/agents/extensions/handoff_filters.py",
        "patch": "@@ -1,6 +1,10 @@\n from __future__ import annotations\n \n-from ..handoffs import HandoffInputData\n+from ..handoffs import (\n+    HandoffInputData,\n+    default_handoff_history_mapper,\n+    nest_handoff_history,\n+)\n from ..items import (\n     HandoffCallItem,\n     HandoffOutputItem,\n@@ -13,6 +17,12 @@\n \n \"\"\"Contains common handoff input filters, for convenience. \"\"\"\n \n+__all__ = [\n+    \"remove_all_tools\",\n+    \"nest_handoff_history\",\n+    \"default_handoff_history_mapper\",\n+]\n+\n \n def remove_all_tools(handoff_input_data: HandoffInputData) -> HandoffInputData:\n     \"\"\"Filters out all tool items: file search, web search and function calls+output.\"\"\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Fextensions%2Fhandoff_filters.py",
        "sha": "85d68c1d8b5d65c990bb02ed9b83fbb4b47cdf5c",
        "status": "modified"
      },
      {
        "additions": 79,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Fhandoffs%2F__init__.py",
        "changes": 124,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fhandoffs%2F__init__.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 45,
        "filename": "src/agents/handoffs/__init__.py",
        "patch": "@@ -9,22 +9,29 @@\n from pydantic import TypeAdapter\n from typing_extensions import TypeAlias, TypeVar\n \n-from .exceptions import ModelBehaviorError, UserError\n-from .items import RunItem, TResponseInputItem\n-from .run_context import RunContextWrapper, TContext\n-from .strict_schema import ensure_strict_json_schema\n-from .tracing.spans import SpanError\n-from .util import _error_tracing, _json, _transforms\n-from .util._types import MaybeAwaitable\n+from ..exceptions import ModelBehaviorError, UserError\n+from ..items import RunItem, TResponseInputItem\n+from ..run_context import RunContextWrapper, TContext\n+from ..strict_schema import ensure_strict_json_schema\n+from ..tracing.spans import SpanError\n+from ..util import _error_tracing, _json, _transforms\n+from ..util._types import MaybeAwaitable\n+from .history import (\n+    default_handoff_history_mapper,\n+    get_conversation_history_wrappers,\n+    nest_handoff_history,\n+    reset_conversation_history_wrappers,\n+    set_conversation_history_wrappers,\n+)\n \n if TYPE_CHECKING:\n-    from .agent import Agent, AgentBase\n+    from ..agent import Agent, AgentBase\n \n \n # The handoff input type is the type of data passed when the agent is called via a handoff.\n THandoffInput = TypeVar(\"THandoffInput\", default=Any)\n \n-# The agent type that the handoff returns\n+# The agent type that the handoff returns.\n TAgent = TypeVar(\"TAgent\", bound=\"AgentBase[Any]\", default=\"Agent[Any]\")\n \n OnHandoffWithInput = Callable[[RunContextWrapper[Any], THandoffInput], Any]\n@@ -51,31 +58,37 @@ class HandoffInputData:\n \n     run_context: RunContextWrapper[Any] | None = None\n     \"\"\"\n-    The run context at the time the handoff was invoked.\n-    Note that, since this property was added later on, it's optional for backwards compatibility.\n+    The run context at the time the handoff was invoked. Note that, since this property was added\n+    later on, it is optional for backwards compatibility.\n     \"\"\"\n \n     def clone(self, **kwargs: Any) -> HandoffInputData:\n         \"\"\"\n         Make a copy of the handoff input data, with the given arguments changed. For example, you\n         could do:\n+\n         ```\n         new_handoff_input_data = handoff_input_data.clone(new_items=())\n         ```\n         \"\"\"\n+\n         return dataclasses_replace(self, **kwargs)\n \n \n HandoffInputFilter: TypeAlias = Callable[[HandoffInputData], MaybeAwaitable[HandoffInputData]]\n \"\"\"A function that filters the input data passed to the next agent.\"\"\"\n \n+HandoffHistoryMapper: TypeAlias = Callable[[list[TResponseInputItem]], list[TResponseInputItem]]\n+\"\"\"A function that maps the previous transcript to the nested summary payload.\"\"\"\n+\n \n @dataclass\n class Handoff(Generic[TContext, TAgent]):\n     \"\"\"A handoff is when an agent delegates a task to another agent.\n+\n     For example, in a customer support scenario you might have a \"triage agent\" that determines\n-    which agent should handle the user's request, and sub-agents that specialize in different\n-    areas like billing, account management, etc.\n+    which agent should handle the user's request, and sub-agents that specialize in different areas\n+    like billing, account management, etc.\n     \"\"\"\n \n     tool_name: str\n@@ -85,46 +98,47 @@ class Handoff(Generic[TContext, TAgent]):\n     \"\"\"The description of the tool that represents the handoff.\"\"\"\n \n     input_json_schema: dict[str, Any]\n-    \"\"\"The JSON schema for the handoff input. Can be empty if the handoff does not take an input.\n-    \"\"\"\n+    \"\"\"The JSON schema for the handoff input. Can be empty if the handoff does not take an input.\"\"\"\n \n     on_invoke_handoff: Callable[[RunContextWrapper[Any], str], Awaitable[TAgent]]\n-    \"\"\"The function that invokes the handoff. The parameters passed are:\n-    1. The handoff run context\n-    2. The arguments from the LLM, as a JSON string. Empty string if input_json_schema is empty.\n+    \"\"\"The function that invokes the handoff.\n \n-    Must return an agent.\n+    The parameters passed are: (1) the handoff run context, (2) the arguments from the LLM as a\n+    JSON string (or an empty string if ``input_json_schema`` is empty). Must return an agent.\n     \"\"\"\n \n     agent_name: str\n     \"\"\"The name of the agent that is being handed off to.\"\"\"\n \n     input_filter: HandoffInputFilter | None = None\n-    \"\"\"A function that filters the inputs that are passed to the next agent. By default, the new\n-    agent sees the entire conversation history. In some cases, you may want to filter inputs e.g.\n-    to remove older inputs, or remove tools from existing inputs.\n-\n-    The function will receive the entire conversation history so far, including the input item\n-    that triggered the handoff and a tool call output item representing the handoff tool's output.\n-\n-    You are free to modify the input history or new items as you see fit. The next agent that\n-    runs will receive `handoff_input_data.all_items`.\n-\n-    IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The\n-    items generated before will already have been streamed.\n+    \"\"\"A function that filters the inputs that are passed to the next agent.\n+\n+    By default, the new agent sees the entire conversation history. In some cases, you may want to\n+    filter inputs (for example, to remove older inputs or remove tools from existing inputs). The\n+    function receives the entire conversation history so far, including the input item that\n+    triggered the handoff and a tool call output item representing the handoff tool's output. You\n+    are free to modify the input history or new items as you see fit. The next agent that runs will\n+    receive ``handoff_input_data.all_items``. IMPORTANT: in streaming mode, we will not stream\n+    anything as a result of this function. The items generated before will already have been\n+    streamed.\n     \"\"\"\n \n+    nest_handoff_history: bool | None = None\n+    \"\"\"Override the run-level ``nest_handoff_history`` behavior for this handoff only.\"\"\"\n+\n     strict_json_schema: bool = True\n-    \"\"\"Whether the input JSON schema is in strict mode. We **strongly** recommend setting this to\n-    True, as it increases the likelihood of correct JSON input.\n-    \"\"\"\n+    \"\"\"Whether the input JSON schema is in strict mode. We strongly recommend setting this to True\n+    because it increases the likelihood of correct JSON input.\"\"\"\n \n     is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]] = (\n         True\n     )\n-    \"\"\"Whether the handoff is enabled. Either a bool or a Callable that takes the run context and\n-    agent and returns whether the handoff is enabled. You can use this to dynamically enable/disable\n-    a handoff based on your context/state.\"\"\"\n+    \"\"\"Whether the handoff is enabled.\n+\n+    Either a bool or a callable that takes the run context and agent and returns whether the\n+    handoff is enabled. You can use this to dynamically enable or disable a handoff based on your\n+    context or state.\n+    \"\"\"\n \n     def get_transfer_message(self, agent: AgentBase[Any]) -> str:\n         return json.dumps({\"assistant\": agent.name})\n@@ -148,6 +162,7 @@ def handoff(\n     tool_name_override: str | None = None,\n     tool_description_override: str | None = None,\n     input_filter: Callable[[HandoffInputData], HandoffInputData] | None = None,\n+    nest_handoff_history: bool | None = None,\n     is_enabled: bool | Callable[[RunContextWrapper[Any], Agent[Any]], MaybeAwaitable[bool]] = True,\n ) -> Handoff[TContext, Agent[TContext]]: ...\n \n@@ -161,6 +176,7 @@ def handoff(\n     tool_description_override: str | None = None,\n     tool_name_override: str | None = None,\n     input_filter: Callable[[HandoffInputData], HandoffInputData] | None = None,\n+    nest_handoff_history: bool | None = None,\n     is_enabled: bool | Callable[[RunContextWrapper[Any], Agent[Any]], MaybeAwaitable[bool]] = True,\n ) -> Handoff[TContext, Agent[TContext]]: ...\n \n@@ -173,6 +189,7 @@ def handoff(\n     tool_description_override: str | None = None,\n     tool_name_override: str | None = None,\n     input_filter: Callable[[HandoffInputData], HandoffInputData] | None = None,\n+    nest_handoff_history: bool | None = None,\n     is_enabled: bool | Callable[[RunContextWrapper[Any], Agent[Any]], MaybeAwaitable[bool]] = True,\n ) -> Handoff[TContext, Agent[TContext]]: ...\n \n@@ -184,6 +201,7 @@ def handoff(\n     on_handoff: OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None = None,\n     input_type: type[THandoffInput] | None = None,\n     input_filter: Callable[[HandoffInputData], HandoffInputData] | None = None,\n+    nest_handoff_history: bool | None = None,\n     is_enabled: bool\n     | Callable[[RunContextWrapper[Any], Agent[TContext]], MaybeAwaitable[bool]] = True,\n ) -> Handoff[TContext, Agent[TContext]]:\n@@ -195,13 +213,16 @@ def handoff(\n         tool_description_override: Optional override for the description of the tool that\n             represents the handoff.\n         on_handoff: A function that runs when the handoff is invoked.\n-        input_type: the type of the input to the handoff. If provided, the input will be validated\n+        input_type: The type of the input to the handoff. If provided, the input will be validated\n             against this type. Only relevant if you pass a function that takes an input.\n-        input_filter: a function that filters the inputs that are passed to the next agent.\n+        input_filter: A function that filters the inputs that are passed to the next agent.\n+        nest_handoff_history: Optional override for the RunConfig-level ``nest_handoff_history``\n+            flag. If ``None`` we fall back to the run's configuration.\n         is_enabled: Whether the handoff is enabled. Can be a bool or a callable that takes the run\n             context and agent and returns whether the handoff is enabled. Disabled handoffs are\n             hidden from the LLM at runtime.\n     \"\"\"\n+\n     assert (on_handoff and input_type) or not (on_handoff and input_type), (\n         \"You must provide either both on_handoff and input_type, or neither\"\n     )\n@@ -257,28 +278,41 @@ async def _invoke_handoff(\n     tool_name = tool_name_override or Handoff.default_tool_name(agent)\n     tool_description = tool_description_override or Handoff.default_tool_description(agent)\n \n-    # Always ensure the input JSON schema is in strict mode\n-    # If there is a need, we can make this configurable in the future\n+    # Always ensure the input JSON schema is in strict mode. If needed, we can make this\n+    # configurable in the future.\n     input_json_schema = ensure_strict_json_schema(input_json_schema)\n \n     async def _is_enabled(ctx: RunContextWrapper[Any], agent_base: AgentBase[Any]) -> bool:\n-        from .agent import Agent\n+        from ..agent import Agent\n \n         assert callable(is_enabled), \"is_enabled must be callable here\"\n         assert isinstance(agent_base, Agent), \"Can't handoff to a non-Agent\"\n         result = is_enabled(ctx, agent_base)\n-\n         if inspect.isawaitable(result):\n             return await result\n-\n-        return result\n+        return bool(result)\n \n     return Handoff(\n         tool_name=tool_name,\n         tool_description=tool_description,\n         input_json_schema=input_json_schema,\n         on_invoke_handoff=_invoke_handoff,\n         input_filter=input_filter,\n+        nest_handoff_history=nest_handoff_history,\n         agent_name=agent.name,\n         is_enabled=_is_enabled if callable(is_enabled) else is_enabled,\n     )\n+\n+\n+__all__ = [\n+    \"Handoff\",\n+    \"HandoffHistoryMapper\",\n+    \"HandoffInputData\",\n+    \"HandoffInputFilter\",\n+    \"default_handoff_history_mapper\",\n+    \"get_conversation_history_wrappers\",\n+    \"handoff\",\n+    \"nest_handoff_history\",\n+    \"reset_conversation_history_wrappers\",\n+    \"set_conversation_history_wrappers\",\n+]",
        "previous_filename": "src/agents/handoffs.py",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Fhandoffs%2F__init__.py",
        "sha": "655ecbdab811f7618489024f488474e7a27a8484",
        "status": "renamed"
      },
      {
        "additions": 236,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Fhandoffs%2Fhistory.py",
        "changes": 236,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fhandoffs%2Fhistory.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 0,
        "filename": "src/agents/handoffs/history.py",
        "patch": "@@ -0,0 +1,236 @@\n+from __future__ import annotations\n+\n+import json\n+from copy import deepcopy\n+from typing import TYPE_CHECKING, Any, cast\n+\n+from ..items import (\n+    ItemHelpers,\n+    RunItem,\n+    TResponseInputItem,\n+)\n+\n+if TYPE_CHECKING:\n+    from . import HandoffHistoryMapper, HandoffInputData\n+\n+__all__ = [\n+    \"default_handoff_history_mapper\",\n+    \"get_conversation_history_wrappers\",\n+    \"nest_handoff_history\",\n+    \"reset_conversation_history_wrappers\",\n+    \"set_conversation_history_wrappers\",\n+]\n+\n+_DEFAULT_CONVERSATION_HISTORY_START = \"<CONVERSATION HISTORY>\"\n+_DEFAULT_CONVERSATION_HISTORY_END = \"</CONVERSATION HISTORY>\"\n+_conversation_history_start = _DEFAULT_CONVERSATION_HISTORY_START\n+_conversation_history_end = _DEFAULT_CONVERSATION_HISTORY_END\n+\n+\n+def set_conversation_history_wrappers(\n+    *,\n+    start: str | None = None,\n+    end: str | None = None,\n+) -> None:\n+    \"\"\"Override the markers that wrap the generated conversation summary.\n+\n+    Pass ``None`` to leave either side unchanged.\n+    \"\"\"\n+\n+    global _conversation_history_start, _conversation_history_end\n+    if start is not None:\n+        _conversation_history_start = start\n+    if end is not None:\n+        _conversation_history_end = end\n+\n+\n+def reset_conversation_history_wrappers() -> None:\n+    \"\"\"Restore the default ``<CONVERSATION HISTORY>`` markers.\"\"\"\n+\n+    global _conversation_history_start, _conversation_history_end\n+    _conversation_history_start = _DEFAULT_CONVERSATION_HISTORY_START\n+    _conversation_history_end = _DEFAULT_CONVERSATION_HISTORY_END\n+\n+\n+def get_conversation_history_wrappers() -> tuple[str, str]:\n+    \"\"\"Return the current start/end markers used for the nested conversation summary.\"\"\"\n+\n+    return (_conversation_history_start, _conversation_history_end)\n+\n+\n+def nest_handoff_history(\n+    handoff_input_data: HandoffInputData,\n+    *,\n+    history_mapper: HandoffHistoryMapper | None = None,\n+) -> HandoffInputData:\n+    \"\"\"Summarize the previous transcript for the next agent.\"\"\"\n+\n+    normalized_history = _normalize_input_history(handoff_input_data.input_history)\n+    flattened_history = _flatten_nested_history_messages(normalized_history)\n+    pre_items_as_inputs = [\n+        _run_item_to_plain_input(item) for item in handoff_input_data.pre_handoff_items\n+    ]\n+    new_items_as_inputs = [_run_item_to_plain_input(item) for item in handoff_input_data.new_items]\n+    transcript = flattened_history + pre_items_as_inputs + new_items_as_inputs\n+\n+    mapper = history_mapper or default_handoff_history_mapper\n+    history_items = mapper(transcript)\n+    filtered_pre_items = tuple(\n+        item\n+        for item in handoff_input_data.pre_handoff_items\n+        if _get_run_item_role(item) != \"assistant\"\n+    )\n+\n+    return handoff_input_data.clone(\n+        input_history=tuple(deepcopy(item) for item in history_items),\n+        pre_handoff_items=filtered_pre_items,\n+    )\n+\n+\n+def default_handoff_history_mapper(\n+    transcript: list[TResponseInputItem],\n+) -> list[TResponseInputItem]:\n+    \"\"\"Return a single assistant message summarizing the transcript.\"\"\"\n+\n+    summary_message = _build_summary_message(transcript)\n+    return [summary_message]\n+\n+\n+def _normalize_input_history(\n+    input_history: str | tuple[TResponseInputItem, ...],\n+) -> list[TResponseInputItem]:\n+    if isinstance(input_history, str):\n+        return ItemHelpers.input_to_new_input_list(input_history)\n+    return [deepcopy(item) for item in input_history]\n+\n+\n+def _run_item_to_plain_input(run_item: RunItem) -> TResponseInputItem:\n+    return deepcopy(run_item.to_input_item())\n+\n+\n+def _build_summary_message(transcript: list[TResponseInputItem]) -> TResponseInputItem:\n+    transcript_copy = [deepcopy(item) for item in transcript]\n+    if transcript_copy:\n+        summary_lines = [\n+            f\"{idx + 1}. {_format_transcript_item(item)}\"\n+            for idx, item in enumerate(transcript_copy)\n+        ]\n+    else:\n+        summary_lines = [\"(no previous turns recorded)\"]\n+\n+    start_marker, end_marker = get_conversation_history_wrappers()\n+    content_lines = [\n+        \"For context, here is the conversation so far between the user and the previous agent:\",\n+        start_marker,\n+        *summary_lines,\n+        end_marker,\n+    ]\n+    content = \"\\n\".join(content_lines)\n+    assistant_message: dict[str, Any] = {\n+        \"role\": \"assistant\",\n+        \"content\": content,\n+    }\n+    return cast(TResponseInputItem, assistant_message)\n+\n+\n+def _format_transcript_item(item: TResponseInputItem) -> str:\n+    role = item.get(\"role\")\n+    if isinstance(role, str):\n+        prefix = role\n+        name = item.get(\"name\")\n+        if isinstance(name, str) and name:\n+            prefix = f\"{prefix} ({name})\"\n+        content_str = _stringify_content(item.get(\"content\"))\n+        return f\"{prefix}: {content_str}\" if content_str else prefix\n+\n+    item_type = item.get(\"type\", \"item\")\n+    rest = {k: v for k, v in item.items() if k != \"type\"}\n+    try:\n+        serialized = json.dumps(rest, ensure_ascii=False, default=str)\n+    except TypeError:\n+        serialized = str(rest)\n+    return f\"{item_type}: {serialized}\" if serialized else str(item_type)\n+\n+\n+def _stringify_content(content: Any) -> str:\n+    if content is None:\n+        return \"\"\n+    if isinstance(content, str):\n+        return content\n+    try:\n+        return json.dumps(content, ensure_ascii=False, default=str)\n+    except TypeError:\n+        return str(content)\n+\n+\n+def _flatten_nested_history_messages(\n+    items: list[TResponseInputItem],\n+) -> list[TResponseInputItem]:\n+    flattened: list[TResponseInputItem] = []\n+    for item in items:\n+        nested_transcript = _extract_nested_history_transcript(item)\n+        if nested_transcript is not None:\n+            flattened.extend(nested_transcript)\n+            continue\n+        flattened.append(deepcopy(item))\n+    return flattened\n+\n+\n+def _extract_nested_history_transcript(\n+    item: TResponseInputItem,\n+) -> list[TResponseInputItem] | None:\n+    content = item.get(\"content\")\n+    if not isinstance(content, str):\n+        return None\n+    start_marker, end_marker = get_conversation_history_wrappers()\n+    start_idx = content.find(start_marker)\n+    end_idx = content.find(end_marker)\n+    if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:\n+        return None\n+    start_idx += len(start_marker)\n+    body = content[start_idx:end_idx]\n+    lines = [line.strip() for line in body.splitlines() if line.strip()]\n+    parsed: list[TResponseInputItem] = []\n+    for line in lines:\n+        parsed_item = _parse_summary_line(line)\n+        if parsed_item is not None:\n+            parsed.append(parsed_item)\n+    return parsed\n+\n+\n+def _parse_summary_line(line: str) -> TResponseInputItem | None:\n+    stripped = line.strip()\n+    if not stripped:\n+        return None\n+    dot_index = stripped.find(\".\")\n+    if dot_index != -1 and stripped[:dot_index].isdigit():\n+        stripped = stripped[dot_index + 1 :].lstrip()\n+    role_part, sep, remainder = stripped.partition(\":\")\n+    if not sep:\n+        return None\n+    role_text = role_part.strip()\n+    if not role_text:\n+        return None\n+    role, name = _split_role_and_name(role_text)\n+    reconstructed: dict[str, Any] = {\"role\": role}\n+    if name:\n+        reconstructed[\"name\"] = name\n+    content = remainder.strip()\n+    if content:\n+        reconstructed[\"content\"] = content\n+    return cast(TResponseInputItem, reconstructed)\n+\n+\n+def _split_role_and_name(role_text: str) -> tuple[str, str | None]:\n+    if role_text.endswith(\")\") and \"(\" in role_text:\n+        open_idx = role_text.rfind(\"(\")\n+        possible_name = role_text[open_idx + 1 : -1].strip()\n+        role_candidate = role_text[:open_idx].strip()\n+        if possible_name:\n+            return (role_candidate or \"developer\", possible_name)\n+    return (role_text or \"developer\", None)\n+\n+\n+def _get_run_item_role(run_item: RunItem) -> str | None:\n+    role_candidate = run_item.to_input_item().get(\"role\")\n+    return role_candidate if isinstance(role_candidate, str) else None",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Fhandoffs%2Fhistory.py",
        "sha": "dc59547fbf58b0f90b9b28fbe9f8682541e41740",
        "status": "added"
      },
      {
        "additions": 51,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Frun.py",
        "changes": 66,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frun.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 15,
        "filename": "src/agents/run.py",
        "patch": "@@ -46,7 +46,7 @@\n     OutputGuardrail,\n     OutputGuardrailResult,\n )\n-from .handoffs import Handoff, HandoffInputFilter, handoff\n+from .handoffs import Handoff, HandoffHistoryMapper, HandoffInputFilter, handoff\n from .items import (\n     HandoffCallItem,\n     ItemHelpers,\n@@ -198,6 +198,19 @@ class RunConfig:\n     agent. See the documentation in `Handoff.input_filter` for more details.\n     \"\"\"\n \n+    nest_handoff_history: bool = True\n+    \"\"\"Wrap prior run history in a single assistant message before handing off when no custom\n+    input filter is set. Set to False to preserve the raw transcript behavior from previous\n+    releases.\n+    \"\"\"\n+\n+    handoff_history_mapper: HandoffHistoryMapper | None = None\n+    \"\"\"Optional function that receives the normalized transcript (history + handoff items) and\n+    returns the input history that should be passed to the next agent. When left as `None`, the\n+    runner collapses the transcript into a single assistant message. This function only runs when\n+    `nest_handoff_history` is True.\n+    \"\"\"\n+\n     input_guardrails: list[InputGuardrail[Any]] | None = None\n     \"\"\"A list of input guardrails to run on the initial run input.\"\"\"\n \n@@ -544,6 +557,7 @@ async def run(\n             current_turn = 0\n             original_input: str | list[TResponseInputItem] = _copy_str_or_list(prepared_input)\n             generated_items: list[RunItem] = []\n+            all_generated_items: list[RunItem] = []\n             model_responses: list[ModelResponse] = []\n \n             context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(\n@@ -639,7 +653,12 @@ async def run(\n \n                     model_responses.append(turn_result.model_response)\n                     original_input = turn_result.original_input\n-                    generated_items = turn_result.generated_items\n+                    all_generated_items.extend(turn_result.new_step_items)\n+                    generated_items = (\n+                        []\n+                        if turn_result.reset_conversation_items\n+                        else turn_result.generated_items\n+                    )\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -657,7 +676,7 @@ async def run(\n                         )\n                         result = RunResult(\n                             input=original_input,\n-                            new_items=generated_items,\n+                            new_items=all_generated_items,\n                             raw_responses=model_responses,\n                             final_output=turn_result.next_step.output,\n                             _last_agent=current_agent,\n@@ -694,14 +713,14 @@ async def run(\n                             f\"Unknown next step type: {type(turn_result.next_step)}\"\n                         )\n             except AgentsException as exc:\n-                exc.run_data = RunErrorDetails(\n-                    input=original_input,\n-                    new_items=generated_items,\n-                    raw_responses=model_responses,\n-                    last_agent=current_agent,\n-                    context_wrapper=context_wrapper,\n-                    input_guardrail_results=input_guardrail_results,\n-                    output_guardrail_results=[],\n+                    exc.run_data = RunErrorDetails(\n+                        input=original_input,\n+                        new_items=all_generated_items,\n+                        raw_responses=model_responses,\n+                        last_agent=current_agent,\n+                        context_wrapper=context_wrapper,\n+                        input_guardrail_results=input_guardrail_results,\n+                        output_guardrail_results=[],\n                 )\n                 raise\n             finally:\n@@ -1003,6 +1022,9 @@ async def _start_streaming(\n \n             await AgentRunner._save_result_to_session(session, starting_input, [])\n \n+            generated_items: list[RunItem] = []\n+            all_generated_items: list[RunItem] = []\n+\n             while True:\n                 # Check for soft cancel before starting new turn\n                 if streamed_result._cancel_mode == \"after_turn\":\n@@ -1072,14 +1094,21 @@ async def _start_streaming(\n                         tool_use_tracker,\n                         all_tools,\n                         server_conversation_tracker,\n+                        generated_items,\n                     )\n                     should_run_agent_start_hooks = False\n \n                     streamed_result.raw_responses = streamed_result.raw_responses + [\n                         turn_result.model_response\n                     ]\n                     streamed_result.input = turn_result.original_input\n-                    streamed_result.new_items = turn_result.generated_items\n+                    all_generated_items.extend(turn_result.new_step_items)\n+                    generated_items = (\n+                        []\n+                        if turn_result.reset_conversation_items\n+                        else turn_result.generated_items\n+                    )\n+                    streamed_result.new_items = list(all_generated_items)\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -1216,6 +1245,7 @@ async def _run_single_turn_streamed(\n         should_run_agent_start_hooks: bool,\n         tool_use_tracker: AgentToolUseTracker,\n         all_tools: list[Tool],\n+        generated_items: list[RunItem],\n         server_conversation_tracker: _ServerConversationTracker | None = None,\n     ) -> SingleStepResult:\n         emitted_tool_call_ids: set[str] = set()\n@@ -1250,11 +1280,11 @@ async def _run_single_turn_streamed(\n \n         if server_conversation_tracker is not None:\n             input = server_conversation_tracker.prepare_input(\n-                streamed_result.input, streamed_result.new_items\n+                streamed_result.input, generated_items\n             )\n         else:\n             input = ItemHelpers.input_to_new_input_list(streamed_result.input)\n-            input.extend([item.to_input_item() for item in streamed_result.new_items])\n+            input.extend([item.to_input_item() for item in generated_items])\n \n         # THIS IS THE RESOLVED CONFLICT BLOCK\n         filtered = await cls._maybe_filter_model_input(\n@@ -1373,7 +1403,7 @@ async def _run_single_turn_streamed(\n         single_step_result = await cls._get_single_step_result_from_response(\n             agent=agent,\n             original_input=streamed_result.input,\n-            pre_step_items=streamed_result.new_items,\n+            pre_step_items=generated_items,\n             new_response=final_response,\n             output_schema=output_schema,\n             all_tools=all_tools,\n@@ -1383,6 +1413,7 @@ async def _run_single_turn_streamed(\n             run_config=run_config,\n             tool_use_tracker=tool_use_tracker,\n             event_queue=streamed_result._event_queue,\n+            server_conversation_mode=server_conversation_tracker is not None,\n         )\n \n         import dataclasses as _dc\n@@ -1494,6 +1525,7 @@ async def _run_single_turn(\n             context_wrapper=context_wrapper,\n             run_config=run_config,\n             tool_use_tracker=tool_use_tracker,\n+            server_conversation_mode=server_conversation_tracker is not None,\n         )\n \n     @classmethod\n@@ -1512,6 +1544,7 @@ async def _get_single_step_result_from_response(\n         run_config: RunConfig,\n         tool_use_tracker: AgentToolUseTracker,\n         event_queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel] | None = None,\n+        server_conversation_mode: bool = False,\n     ) -> SingleStepResult:\n         processed_response = RunImpl.process_model_response(\n             agent=agent,\n@@ -1541,6 +1574,7 @@ async def _get_single_step_result_from_response(\n             hooks=hooks,\n             context_wrapper=context_wrapper,\n             run_config=run_config,\n+            server_conversation_mode=server_conversation_mode,\n         )\n \n     @classmethod\n@@ -1557,6 +1591,7 @@ async def _get_single_step_result_from_streamed_response(\n         context_wrapper: RunContextWrapper[TContext],\n         run_config: RunConfig,\n         tool_use_tracker: AgentToolUseTracker,\n+        server_conversation_mode: bool = False,\n     ) -> SingleStepResult:\n         original_input = streamed_result.input\n         pre_step_items = streamed_result.new_items\n@@ -1583,6 +1618,7 @@ async def _get_single_step_result_from_streamed_response(\n             hooks=hooks,\n             context_wrapper=context_wrapper,\n             run_config=run_config,\n+            server_conversation_mode=server_conversation_mode,\n         )\n         new_step_items = [\n             item",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/src%2Fagents%2Frun.py",
        "sha": "c8908c17c6dc4e833fedba6aa4d45f45780f1bbd",
        "status": "modified"
      },
      {
        "additions": 104,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_agent_runner.py",
        "changes": 109,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_agent_runner.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 5,
        "filename": "tests/test_agent_runner.py",
        "patch": "@@ -43,6 +43,14 @@\n from .utils.simple_session import SimpleListSession\n \n \n+def _as_message(item: Any) -> dict[str, Any]:\n+    assert isinstance(item, dict)\n+    role = item.get(\"role\")\n+    assert isinstance(role, str)\n+    assert role in {\"assistant\", \"user\", \"system\", \"developer\"}\n+    return cast(dict[str, Any], item)\n+\n+\n @pytest.mark.asyncio\n async def test_simple_first_run():\n     model = FakeModel()\n@@ -165,8 +173,8 @@ async def test_handoffs():\n     assert result.final_output == \"done\"\n     assert len(result.raw_responses) == 3, \"should have three model responses\"\n     assert len(result.to_input_list()) == 7, (\n-        \"should have 7 inputs: orig input, tool call, tool result, message, handoff, handoff\"\n-        \"result, and done message\"\n+        \"should have 7 inputs: summary message, tool call, tool result, message, handoff, \"\n+        \"handoff result, and done message\"\n     )\n     assert result.last_agent == agent_1, \"should have handed off to agent_1\"\n \n@@ -218,9 +226,9 @@ async def test_structured_output():\n \n     assert result.final_output == Foo(bar=\"baz\")\n     assert len(result.raw_responses) == 4, \"should have four model responses\"\n-    assert len(result.to_input_list()) == 11, (\n-        \"should have input: 2 orig inputs, function call, function call result, message, handoff, \"\n-        \"handoff output, preamble message, tool call, tool call result, final output\"\n+    assert len(result.to_input_list()) == 10, (\n+        \"should have input: conversation summary, function call, function call result, message, \"\n+        \"handoff, handoff output, preamble message, tool call, tool call result, final output\"\n     )\n \n     assert result.last_agent == agent_1, \"should have handed off to agent_1\"\n@@ -270,6 +278,97 @@ async def test_handoff_filters():\n     )\n \n \n+@pytest.mark.asyncio\n+async def test_default_handoff_history_nested_and_filters_respected():\n+    model = FakeModel()\n+    agent_1 = Agent(\n+        name=\"delegate\",\n+        model=model,\n+    )\n+    agent_2 = Agent(\n+        name=\"triage\",\n+        model=model,\n+        handoffs=[agent_1],\n+    )\n+\n+    model.add_multiple_turn_outputs(\n+        [\n+            [get_text_message(\"triage summary\"), get_handoff_tool_call(agent_1)],\n+            [get_text_message(\"resolution\")],\n+        ]\n+    )\n+\n+    result = await Runner.run(agent_2, input=\"user_message\")\n+\n+    assert isinstance(result.input, list)\n+    assert len(result.input) == 1\n+    summary = _as_message(result.input[0])\n+    assert summary[\"role\"] == \"assistant\"\n+    summary_content = summary[\"content\"]\n+    assert isinstance(summary_content, str)\n+    assert \"<CONVERSATION HISTORY>\" in summary_content\n+    assert \"triage summary\" in summary_content\n+    assert \"user_message\" in summary_content\n+\n+    passthrough_model = FakeModel()\n+    delegate = Agent(name=\"delegate\", model=passthrough_model)\n+\n+    def passthrough_filter(data: HandoffInputData) -> HandoffInputData:\n+        return data\n+\n+    triage_with_filter = Agent(\n+        name=\"triage\",\n+        model=passthrough_model,\n+        handoffs=[handoff(delegate, input_filter=passthrough_filter)],\n+    )\n+\n+    passthrough_model.add_multiple_turn_outputs(\n+        [\n+            [get_text_message(\"triage summary\"), get_handoff_tool_call(delegate)],\n+            [get_text_message(\"resolution\")],\n+        ]\n+    )\n+\n+    filtered_result = await Runner.run(triage_with_filter, input=\"user_message\")\n+\n+    assert isinstance(filtered_result.input, str)\n+    assert filtered_result.input == \"user_message\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_default_handoff_history_accumulates_across_multiple_handoffs():\n+    triage_model = FakeModel()\n+    delegate_model = FakeModel()\n+    closer_model = FakeModel()\n+\n+    closer = Agent(name=\"closer\", model=closer_model)\n+    delegate = Agent(name=\"delegate\", model=delegate_model, handoffs=[closer])\n+    triage = Agent(name=\"triage\", model=triage_model, handoffs=[delegate])\n+\n+    triage_model.add_multiple_turn_outputs(\n+        [[get_text_message(\"triage summary\"), get_handoff_tool_call(delegate)]]\n+    )\n+    delegate_model.add_multiple_turn_outputs(\n+        [[get_text_message(\"delegate update\"), get_handoff_tool_call(closer)]]\n+    )\n+    closer_model.add_multiple_turn_outputs([[get_text_message(\"resolution\")]])\n+\n+    result = await Runner.run(triage, input=\"user_question\")\n+\n+    assert result.final_output == \"resolution\"\n+    assert closer_model.first_turn_args is not None\n+    closer_input = closer_model.first_turn_args[\"input\"]\n+    assert isinstance(closer_input, list)\n+    summary = _as_message(closer_input[0])\n+    assert summary[\"role\"] == \"assistant\"\n+    summary_content = summary[\"content\"]\n+    assert isinstance(summary_content, str)\n+    assert summary_content.count(\"<CONVERSATION HISTORY>\") == 1\n+    assert \"triage summary\" in summary_content\n+    assert \"delegate update\" in summary_content\n+    assert \"user_question\" in summary_content\n+\n+\n @pytest.mark.asyncio\n async def test_async_input_filter_supported():\n     # DO NOT rename this without updating pyproject.toml",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_agent_runner.py",
        "sha": "d05496e506866fae393a4c17179f8b81441461f8",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_agent_runner_streamed.py",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_agent_runner_streamed.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 8,
        "filename": "tests/test_agent_runner_streamed.py",
        "patch": "@@ -176,8 +176,8 @@ async def test_handoffs():\n     assert result.final_output == \"done\"\n     assert len(result.raw_responses) == 3, \"should have three model responses\"\n     assert len(result.to_input_list()) == 7, (\n-        \"should have 7 inputs: orig input, tool call, tool result, message, handoff, handoff\"\n-        \"result, and done message\"\n+        \"should have 7 inputs: summary message, tool call, tool result, message, handoff, \"\n+        \"handoff result, and done message\"\n     )\n     assert result.last_agent == agent_1, \"should have handed off to agent_1\"\n \n@@ -231,9 +231,9 @@ async def test_structured_output():\n \n     assert result.final_output == Foo(bar=\"baz\")\n     assert len(result.raw_responses) == 4, \"should have four model responses\"\n-    assert len(result.to_input_list()) == 11, (\n-        \"should have input: 2 orig inputs, function call, function call result, message, handoff, \"\n-        \"handoff output, preamble message, tool call, tool call result, final output\"\n+    assert len(result.to_input_list()) == 10, (\n+        \"should have input: conversation summary, function call, function call result, message, \"\n+        \"handoff, handoff output, preamble message, tool call, tool call result, final output\"\n     )\n \n     assert result.last_agent == agent_1, \"should have handed off to agent_1\"\n@@ -717,9 +717,9 @@ async def test_streaming_events():\n \n     assert result.final_output == Foo(bar=\"baz\")\n     assert len(result.raw_responses) == 4, \"should have four model responses\"\n-    assert len(result.to_input_list()) == 10, (\n-        \"should have input: 2 orig inputs, function call, function call result, message, handoff, \"\n-        \"handoff output, tool call, tool call result, final output\"\n+    assert len(result.to_input_list()) == 9, (\n+        \"should have input: conversation summary, function call, function call result, message, \"\n+        \"handoff, handoff output, tool call, tool call result, final output\"\n     )\n \n     assert result.last_agent == agent_1, \"should have handed off to agent_1\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_agent_runner_streamed.py",
        "sha": "222afda78c16dbfd97a2492b0862ca8e88caddb7",
        "status": "modified"
      },
      {
        "additions": 179,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_extension_filters.py",
        "changes": 181,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_extension_filters.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 2,
        "filename": "tests/test_extension_filters.py",
        "patch": "@@ -1,8 +1,18 @@\n+from copy import deepcopy\n+from typing import Any, cast\n+\n from openai.types.responses import ResponseOutputMessage, ResponseOutputText\n from openai.types.responses.response_reasoning_item import ResponseReasoningItem\n \n-from agents import Agent, HandoffInputData, RunContextWrapper\n-from agents.extensions.handoff_filters import remove_all_tools\n+from agents import (\n+    Agent,\n+    HandoffInputData,\n+    RunContextWrapper,\n+    get_conversation_history_wrappers,\n+    reset_conversation_history_wrappers,\n+    set_conversation_history_wrappers,\n+)\n+from agents.extensions.handoff_filters import nest_handoff_history, remove_all_tools\n from agents.items import (\n     HandoffOutputItem,\n     MessageOutputItem,\n@@ -25,6 +35,13 @@ def _get_message_input_item(content: str) -> TResponseInputItem:\n     }\n \n \n+def _get_user_input_item(content: str) -> TResponseInputItem:\n+    return {\n+        \"role\": \"user\",\n+        \"content\": content,\n+    }\n+\n+\n def _get_reasoning_input_item() -> TResponseInputItem:\n     return {\"id\": \"rid\", \"summary\": [], \"type\": \"reasoning\"}\n \n@@ -91,6 +108,14 @@ def _get_reasoning_output_run_item() -> ReasoningItem:\n     )\n \n \n+def _as_message(item: TResponseInputItem) -> dict[str, Any]:\n+    assert isinstance(item, dict)\n+    role = item.get(\"role\")\n+    assert isinstance(role, str)\n+    assert role in {\"assistant\", \"user\", \"system\", \"developer\"}\n+    return cast(dict[str, Any], item)\n+\n+\n def test_empty_data():\n     handoff_input_data = HandoffInputData(\n         input_history=(),\n@@ -221,3 +246,155 @@ def test_removes_handoffs_from_history():\n     assert len(filtered_data.input_history) == 1\n     assert len(filtered_data.pre_handoff_items) == 1\n     assert len(filtered_data.new_items) == 1\n+\n+\n+def test_nest_handoff_history_wraps_transcript() -> None:\n+    data = HandoffInputData(\n+        input_history=(_get_user_input_item(\"Hello\"),),\n+        pre_handoff_items=(_get_message_output_run_item(\"Assist reply\"),),\n+        new_items=(\n+            _get_message_output_run_item(\"Handoff request\"),\n+            _get_handoff_output_run_item(\"transfer\"),\n+        ),\n+        run_context=RunContextWrapper(context=()),\n+    )\n+\n+    nested = nest_handoff_history(data)\n+\n+    assert isinstance(nested.input_history, tuple)\n+    assert len(nested.input_history) == 1\n+    summary = _as_message(nested.input_history[0])\n+    assert summary[\"role\"] == \"assistant\"\n+    summary_content = summary[\"content\"]\n+    assert isinstance(summary_content, str)\n+    start_marker, end_marker = get_conversation_history_wrappers()\n+    assert start_marker in summary_content\n+    assert end_marker in summary_content\n+    assert \"Assist reply\" in summary_content\n+    assert \"Hello\" in summary_content\n+    assert len(nested.pre_handoff_items) == 0\n+    assert nested.new_items == data.new_items\n+\n+\n+def test_nest_handoff_history_handles_missing_user() -> None:\n+    data = HandoffInputData(\n+        input_history=(),\n+        pre_handoff_items=(_get_reasoning_output_run_item(),),\n+        new_items=(),\n+        run_context=RunContextWrapper(context=()),\n+    )\n+\n+    nested = nest_handoff_history(data)\n+\n+    assert isinstance(nested.input_history, tuple)\n+    assert len(nested.input_history) == 1\n+    summary = _as_message(nested.input_history[0])\n+    assert summary[\"role\"] == \"assistant\"\n+    summary_content = summary[\"content\"]\n+    assert isinstance(summary_content, str)\n+    assert \"reasoning\" in summary_content.lower()\n+\n+\n+def test_nest_handoff_history_appends_existing_history() -> None:\n+    first = HandoffInputData(\n+        input_history=(_get_user_input_item(\"Hello\"),),\n+        pre_handoff_items=(_get_message_output_run_item(\"First reply\"),),\n+        new_items=(),\n+        run_context=RunContextWrapper(context=()),\n+    )\n+\n+    first_nested = nest_handoff_history(first)\n+    assert isinstance(first_nested.input_history, tuple)\n+    summary_message = first_nested.input_history[0]\n+\n+    follow_up_history: tuple[TResponseInputItem, ...] = (\n+        summary_message,\n+        _get_user_input_item(\"Another question\"),\n+    )\n+\n+    second = HandoffInputData(\n+        input_history=follow_up_history,\n+        pre_handoff_items=(_get_message_output_run_item(\"Second reply\"),),\n+        new_items=(_get_handoff_output_run_item(\"transfer\"),),\n+        run_context=RunContextWrapper(context=()),\n+    )\n+\n+    second_nested = nest_handoff_history(second)\n+\n+    assert isinstance(second_nested.input_history, tuple)\n+    summary = _as_message(second_nested.input_history[0])\n+    assert summary[\"role\"] == \"assistant\"\n+    content = summary[\"content\"]\n+    assert isinstance(content, str)\n+    start_marker, end_marker = get_conversation_history_wrappers()\n+    assert content.count(start_marker) == 1\n+    assert content.count(end_marker) == 1\n+    assert \"First reply\" in content\n+    assert \"Second reply\" in content\n+    assert \"Another question\" in content\n+\n+\n+def test_nest_handoff_history_honors_custom_wrappers() -> None:\n+    data = HandoffInputData(\n+        input_history=(_get_user_input_item(\"Hello\"),),\n+        pre_handoff_items=(_get_message_output_run_item(\"First reply\"),),\n+        new_items=(_get_message_output_run_item(\"Second reply\"),),\n+        run_context=RunContextWrapper(context=()),\n+    )\n+\n+    set_conversation_history_wrappers(start=\"<<START>>\", end=\"<<END>>\")\n+    try:\n+        nested = nest_handoff_history(data)\n+        assert isinstance(nested.input_history, tuple)\n+        assert len(nested.input_history) == 1\n+        summary = _as_message(nested.input_history[0])\n+        summary_content = summary[\"content\"]\n+        assert isinstance(summary_content, str)\n+        lines = summary_content.splitlines()\n+        assert lines[0] == (\n+            \"For context, here is the conversation so far between the user and the previous agent:\"\n+        )\n+        assert lines[1].startswith(\"<<START>>\")\n+        assert summary_content.endswith(\"<<END>>\")\n+\n+        # Ensure the custom markers are parsed correctly when nesting again.\n+        second_nested = nest_handoff_history(nested)\n+        assert isinstance(second_nested.input_history, tuple)\n+        second_summary = _as_message(second_nested.input_history[0])\n+        content = second_summary[\"content\"]\n+        assert isinstance(content, str)\n+        assert content.count(\"<<START>>\") == 1\n+        assert content.count(\"<<END>>\") == 1\n+    finally:\n+        reset_conversation_history_wrappers()\n+\n+\n+def test_nest_handoff_history_supports_custom_mapper() -> None:\n+    data = HandoffInputData(\n+        input_history=(_get_user_input_item(\"Hello\"),),\n+        pre_handoff_items=(_get_message_output_run_item(\"Assist reply\"),),\n+        new_items=(),\n+        run_context=RunContextWrapper(context=()),\n+    )\n+\n+    def map_history(items: list[TResponseInputItem]) -> list[TResponseInputItem]:\n+        reversed_items = list(reversed(items))\n+        return [deepcopy(item) for item in reversed_items]\n+\n+    nested = nest_handoff_history(data, history_mapper=map_history)\n+\n+    assert isinstance(nested.input_history, tuple)\n+    assert len(nested.input_history) == 2\n+    first = _as_message(nested.input_history[0])\n+    second = _as_message(nested.input_history[1])\n+    assert first[\"role\"] == \"assistant\"\n+    first_content = first.get(\"content\")\n+    assert isinstance(first_content, list)\n+    assert any(\n+        isinstance(chunk, dict)\n+        and chunk.get(\"type\") == \"output_text\"\n+        and chunk.get(\"text\") == \"Assist reply\"\n+        for chunk in first_content\n+    )\n+    assert second[\"role\"] == \"user\"\n+    assert second[\"content\"] == \"Hello\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_extension_filters.py",
        "sha": "86161bbb748ee01a34855563435f337cb552863c",
        "status": "modified"
      },
      {
        "additions": 177,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_run_step_processing.py",
        "changes": 178,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_run_step_processing.py?ref=c27f31d8eb9fb53daacd09f85027f0c9ba454744",
        "deletions": 1,
        "filename": "tests/test_run_step_processing.py",
        "patch": "@@ -1,9 +1,12 @@\n from __future__ import annotations\n \n+from typing import Any, cast\n+\n import pytest\n from openai.types.responses import (\n     ResponseComputerToolCall,\n     ResponseFileSearchToolCall,\n+    ResponseFunctionToolCall,\n     ResponseFunctionWebSearch,\n )\n from openai.types.responses.response_computer_tool_call import ActionClick\n@@ -16,21 +19,28 @@\n     Computer,\n     ComputerTool,\n     Handoff,\n+    HandoffInputData,\n     ModelBehaviorError,\n     ModelResponse,\n     ReasoningItem,\n+    RunConfig,\n     RunContextWrapper,\n+    RunHooks,\n+    RunItem,\n     ToolCallItem,\n     Usage,\n+    UserError,\n+    handoff,\n )\n-from agents._run_impl import RunImpl\n+from agents._run_impl import RunImpl, ToolRunHandoff\n from agents.run import AgentRunner\n \n from .test_responses import (\n     get_final_output_message,\n     get_function_tool,\n     get_function_tool_call,\n     get_handoff_tool_call,\n+    get_text_input_item,\n     get_text_message,\n )\n \n@@ -202,6 +212,172 @@ async def test_handoffs_parsed_correctly():\n     assert handoff_agent == agent_1\n \n \n+@pytest.mark.asyncio\n+async def test_handoff_can_disable_run_level_history_nesting(monkeypatch: pytest.MonkeyPatch):\n+    source_agent = Agent(name=\"source\")\n+    target_agent = Agent(name=\"target\")\n+    override_handoff = handoff(target_agent, nest_handoff_history=False)\n+    tool_call = cast(ResponseFunctionToolCall, get_handoff_tool_call(target_agent))\n+    run_handoffs = [ToolRunHandoff(handoff=override_handoff, tool_call=tool_call)]\n+    run_config = RunConfig(nest_handoff_history=True)\n+    context_wrapper = RunContextWrapper(context=None)\n+    hooks = RunHooks()\n+    original_input = [get_text_input_item(\"hello\")]\n+    pre_step_items: list[RunItem] = []\n+    new_step_items: list[RunItem] = []\n+    new_response = ModelResponse(output=[tool_call], usage=Usage(), response_id=None)\n+\n+    calls: list[HandoffInputData] = []\n+\n+    def fake_nest(\n+        handoff_input_data: HandoffInputData,\n+        *,\n+        history_mapper: Any,\n+    ) -> HandoffInputData:\n+        calls.append(handoff_input_data)\n+        return handoff_input_data\n+\n+    monkeypatch.setattr(\"agents._run_impl.nest_handoff_history\", fake_nest)\n+\n+    result = await RunImpl.execute_handoffs(\n+        agent=source_agent,\n+        original_input=list(original_input),\n+        pre_step_items=pre_step_items,\n+        new_step_items=new_step_items,\n+        new_response=new_response,\n+        run_handoffs=run_handoffs,\n+        hooks=hooks,\n+        context_wrapper=context_wrapper,\n+        run_config=run_config,\n+    )\n+\n+    assert calls == []\n+    assert result.original_input == original_input\n+\n+\n+@pytest.mark.asyncio\n+async def test_handoff_can_enable_history_nesting(monkeypatch: pytest.MonkeyPatch):\n+    source_agent = Agent(name=\"source\")\n+    target_agent = Agent(name=\"target\")\n+    override_handoff = handoff(target_agent, nest_handoff_history=True)\n+    tool_call = cast(ResponseFunctionToolCall, get_handoff_tool_call(target_agent))\n+    run_handoffs = [ToolRunHandoff(handoff=override_handoff, tool_call=tool_call)]\n+    run_config = RunConfig(nest_handoff_history=False)\n+    context_wrapper = RunContextWrapper(context=None)\n+    hooks = RunHooks()\n+    original_input = [get_text_input_item(\"hello\")]\n+    pre_step_items: list[RunItem] = []\n+    new_step_items: list[RunItem] = []\n+    new_response = ModelResponse(output=[tool_call], usage=Usage(), response_id=None)\n+\n+    def fake_nest(\n+        handoff_input_data: HandoffInputData,\n+        *,\n+        history_mapper: Any,\n+    ) -> HandoffInputData:\n+        return handoff_input_data.clone(\n+            input_history=(\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": \"nested\",\n+                },\n+            )\n+        )\n+\n+    monkeypatch.setattr(\"agents._run_impl.nest_handoff_history\", fake_nest)\n+\n+    result = await RunImpl.execute_handoffs(\n+        agent=source_agent,\n+        original_input=list(original_input),\n+        pre_step_items=pre_step_items,\n+        new_step_items=new_step_items,\n+        new_response=new_response,\n+        run_handoffs=run_handoffs,\n+        hooks=hooks,\n+        context_wrapper=context_wrapper,\n+        run_config=run_config,\n+    )\n+\n+    assert result.original_input == [\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": \"nested\",\n+        }\n+    ]\n+\n+\n+@pytest.mark.asyncio\n+async def test_server_conversation_disables_history_nesting(monkeypatch: pytest.MonkeyPatch):\n+    source_agent = Agent(name=\"source\")\n+    target_agent = Agent(name=\"target\")\n+    override_handoff = handoff(target_agent)\n+    tool_call = cast(ResponseFunctionToolCall, get_handoff_tool_call(target_agent))\n+    run_handoffs = [ToolRunHandoff(handoff=override_handoff, tool_call=tool_call)]\n+    run_config = RunConfig()\n+    context_wrapper = RunContextWrapper(context=None)\n+    hooks = RunHooks()\n+    original_input = [get_text_input_item(\"hello\")]\n+    pre_step_items: list[RunItem] = []\n+    new_step_items: list[RunItem] = []\n+    new_response = ModelResponse(output=[tool_call], usage=Usage(), response_id=None)\n+\n+    calls: list[HandoffInputData] = []\n+\n+    def fake_nest(\n+        handoff_input_data: HandoffInputData,\n+        *,\n+        history_mapper: Any,\n+    ) -> HandoffInputData:\n+        calls.append(handoff_input_data)\n+        return handoff_input_data\n+\n+    monkeypatch.setattr(\"agents._run_impl.nest_handoff_history\", fake_nest)\n+\n+    result = await RunImpl.execute_handoffs(\n+        agent=source_agent,\n+        original_input=list(original_input),\n+        pre_step_items=pre_step_items,\n+        new_step_items=new_step_items,\n+        new_response=new_response,\n+        run_handoffs=run_handoffs,\n+        hooks=hooks,\n+        context_wrapper=context_wrapper,\n+        run_config=run_config,\n+        server_conversation_mode=True,\n+    )\n+\n+    assert calls == []\n+    assert result.original_input == original_input\n+    assert result.reset_conversation_items is False\n+\n+\n+@pytest.mark.asyncio\n+async def test_server_conversation_rejects_input_filter():\n+    source_agent = Agent(name=\"source\")\n+    target_agent = Agent(name=\"target\")\n+    override_handoff = handoff(target_agent, input_filter=lambda d: d)\n+    tool_call = cast(ResponseFunctionToolCall, get_handoff_tool_call(target_agent))\n+    run_handoffs = [ToolRunHandoff(handoff=override_handoff, tool_call=tool_call)]\n+    run_config = RunConfig()\n+    context_wrapper = RunContextWrapper(context=None)\n+    hooks = RunHooks()\n+    new_response = ModelResponse(output=[tool_call], usage=Usage(), response_id=None)\n+\n+    with pytest.raises(UserError):\n+        await RunImpl.execute_handoffs(\n+            agent=source_agent,\n+            original_input=[get_text_input_item(\"hello\")],\n+            pre_step_items=[],\n+            new_step_items=[],\n+            new_response=new_response,\n+            run_handoffs=run_handoffs,\n+            hooks=hooks,\n+            context_wrapper=context_wrapper,\n+            run_config=run_config,\n+            server_conversation_mode=True,\n+        )\n+\n+\n @pytest.mark.asyncio\n async def test_missing_handoff_fails():\n     agent_1 = Agent(name=\"test_1\")",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c27f31d8eb9fb53daacd09f85027f0c9ba454744/tests%2Ftest_run_step_processing.py",
        "sha": "88656b0c6f4472221a9475b4d3787165e74e8a05",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:53:23.745093Z"
}
