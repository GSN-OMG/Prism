{
  "finished_at": "2026-01-20T04:57:36.541134Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "cf3f43428919924b",
    "tag": "rest_pr_files_pr2320_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2320/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "168877",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:57:36 GMT",
      "etag": "\"d4fa96cb63cfaebf4d354aa0d3b6972f70789b39f2b22f98b5457bf22eaffe2d\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Fri, 16 Jan 2026 22:27:58 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DCCB:37BFB7:1612841:1F1E5C5:696F0B3F",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4939",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "61",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 163,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/examples%2Ftools%2Fcodex.py",
        "changes": 163,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Ftools%2Fcodex.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "examples/tools/codex.py",
        "patch": "@@ -0,0 +1,163 @@\n+import asyncio\n+from datetime import datetime\n+\n+from agents import Agent, Runner, gen_trace_id, trace\n+\n+# This tool is still in experimental phase and the details could be changed until being GAed.\n+from agents.extensions.experimental.codex import (\n+    CodexToolStreamEvent,\n+    CommandExecutionItem,\n+    ErrorItem,\n+    FileChangeItem,\n+    ItemCompletedEvent,\n+    ItemStartedEvent,\n+    ItemUpdatedEvent,\n+    McpToolCallItem,\n+    ReasoningItem,\n+    ThreadErrorEvent,\n+    ThreadOptions,\n+    ThreadStartedEvent,\n+    TodoListItem,\n+    TurnCompletedEvent,\n+    TurnFailedEvent,\n+    TurnOptions,\n+    TurnStartedEvent,\n+    WebSearchItem,\n+    codex_tool,\n+)\n+\n+# This example runs the Codex CLI via the Codex tool wrapper.\n+# You can configure the CLI path with CODEX_PATH or CodexOptions(codex_path_override=\"...\").\n+# codex_tool accepts options as keyword arguments or a plain dict.\n+# For example: codex_tool(sandbox_mode=\"read-only\") or codex_tool({\"sandbox_mode\": \"read-only\"}).\n+# The prompt below asks Codex to use the $openai-knowledge skill (Docs MCP) for API lookups.\n+\n+\n+async def on_codex_stream(payload: CodexToolStreamEvent) -> None:\n+    event = payload.event\n+\n+    if isinstance(event, ThreadStartedEvent):\n+        log(f\"codex thread started: {event.thread_id}\")\n+        return\n+    if isinstance(event, TurnStartedEvent):\n+        log(\"codex turn started\")\n+        return\n+    if isinstance(event, TurnCompletedEvent):\n+        usage = event.usage\n+        log(f\"codex turn completed, usage: {usage}\")\n+        return\n+    if isinstance(event, TurnFailedEvent):\n+        error = event.error.message\n+        log(f\"codex turn failed: {error}\")\n+        return\n+    if isinstance(event, ThreadErrorEvent):\n+        log(f\"codex stream error: {event.message}\")\n+        return\n+\n+    if not isinstance(event, (ItemStartedEvent, ItemUpdatedEvent, ItemCompletedEvent)):\n+        return\n+\n+    item = event.item\n+\n+    if isinstance(item, ReasoningItem):\n+        text = item.text\n+        log(f\"codex reasoning ({event.type}): {text}\")\n+        return\n+    if isinstance(item, CommandExecutionItem):\n+        command = item.command\n+        output = item.aggregated_output\n+        output_preview = output[-200:] if isinstance(output, str) else \"\"\n+        status = item.status\n+        log(f\"codex command {event.type}: {command} | status={status} | output={output_preview}\")\n+        return\n+    if isinstance(item, McpToolCallItem):\n+        server = item.server\n+        tool = item.tool\n+        status = item.status\n+        log(f\"codex mcp {event.type}: {server}.{tool} | status={status}\")\n+        return\n+    if isinstance(item, FileChangeItem):\n+        changes = item.changes\n+        status = item.status\n+        log(f\"codex file change {event.type}: {status} | {changes}\")\n+        return\n+    if isinstance(item, WebSearchItem):\n+        log(f\"codex web search {event.type}: {item.query}\")\n+        return\n+    if isinstance(item, TodoListItem):\n+        items = item.items\n+        log(f\"codex todo list {event.type}: {len(items)} items\")\n+        return\n+    if isinstance(item, ErrorItem):\n+        log(f\"codex error {event.type}: {item.message}\")\n+\n+\n+def _timestamp() -> str:\n+    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n+\n+\n+def log(message: str) -> None:\n+    timestamp = _timestamp()\n+    lines = str(message).splitlines() or [\"\"]\n+    for line in lines:\n+        print(f\"{timestamp} {line}\")\n+\n+\n+async def main() -> None:\n+    agent = Agent(\n+        name=\"Codex Agent\",\n+        instructions=(\n+            \"Use the codex tool to inspect the workspace and answer the question. \"\n+            \"When skill names, which usually starts with `$`, are mentioned, \"\n+            \"you must rely on the codex tool to use the skill and answer the question.\\n\\n\"\n+            \"When you send the final answer, you must include the following info at the end:\\n\\n\"\n+            \"Run `codex resume <thread_id>` to continue the codex session.\"\n+        ),\n+        tools=[\n+            # Run local Codex CLI as a sub process\n+            codex_tool(\n+                sandbox_mode=\"workspace-write\",\n+                default_thread_options=ThreadOptions(\n+                    # You can pass a Codex instance to customize CLI details\n+                    # codex=Codex(executable_path=\"/path/to/codex\", base_url=\"...\"),\n+                    model=\"gpt-5.2-codex\",\n+                    model_reasoning_effort=\"low\",\n+                    network_access_enabled=True,\n+                    web_search_enabled=False,\n+                    approval_policy=\"never\",  # We'll update this example once the HITL is implemented\n+                ),\n+                default_turn_options=TurnOptions(\n+                    # Abort Codex CLI if no events arrive within this many seconds.\n+                    idle_timeout_seconds=60,\n+                ),\n+                on_stream=on_codex_stream,\n+            )\n+        ],\n+    )\n+    trace_id = gen_trace_id()\n+    log(f\"View trace: https://platform.openai.com/traces/trace?trace_id={trace_id}\")\n+\n+    with trace(\"Codex tool example\", trace_id=trace_id):\n+        # Use a skill that requires network access and MCP server settings\n+        log(\"Using $openai-knowledge skill to fetch the latest realtime model name...\")\n+        result = await Runner.run(\n+            agent,\n+            \"You must use `$openai-knowledge` skill to fetch the latest realtime model name.\",\n+        )\n+        log(result.final_output)\n+        # The latest realtime model name, according to the $openai-knowledge skill, is gpt-realtime.\n+\n+        # Use a skill that runs local command and analyzes the output\n+        log(\n+            \"Using $test-coverage-improver skill to analyze the test coverage of the project and improve it...\"\n+        )\n+        result = await Runner.run(\n+            agent,\n+            \"You must use `$test-coverage-improver` skill to analyze the test coverage of the project and improve it.\",\n+        )\n+        log(result.final_output)\n+        # (Aa few suggestions for improving the test coverage will be displayed.)\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/examples%2Ftools%2Fcodex.py",
        "sha": "32ec9b85178e20db138453ed3cd0680519ce1947",
        "status": "added"
      },
      {
        "additions": 6,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2F__init__.py",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2F__init__.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/__init__.py",
        "patch": "@@ -0,0 +1,6 @@\n+# This package contains experimental extensions to the agents package.\n+# The interface and implementation details could be changed until being GAed.\n+\n+__all__ = [\n+    \"codex\",\n+]",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2F__init__.py",
        "sha": "b32ae7a985651148bf4575d9dda340c1ac6dd449",
        "status": "added"
      },
      {
        "additions": 92,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2F__init__.py",
        "changes": 92,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2F__init__.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/__init__.py",
        "patch": "@@ -0,0 +1,92 @@\n+from .codex import Codex\n+from .codex_options import CodexOptions\n+from .codex_tool import (\n+    CodexToolOptions,\n+    CodexToolResult,\n+    CodexToolStreamEvent,\n+    OutputSchemaDescriptor,\n+    codex_tool,\n+)\n+from .events import (\n+    ItemCompletedEvent,\n+    ItemStartedEvent,\n+    ItemUpdatedEvent,\n+    ThreadError,\n+    ThreadErrorEvent,\n+    ThreadEvent,\n+    ThreadStartedEvent,\n+    TurnCompletedEvent,\n+    TurnFailedEvent,\n+    TurnStartedEvent,\n+    Usage,\n+)\n+from .items import (\n+    AgentMessageItem,\n+    CommandExecutionItem,\n+    ErrorItem,\n+    FileChangeItem,\n+    FileUpdateChange,\n+    McpToolCallError,\n+    McpToolCallItem,\n+    McpToolCallResult,\n+    ReasoningItem,\n+    ThreadItem,\n+    TodoItem,\n+    TodoListItem,\n+    WebSearchItem,\n+)\n+from .thread import Input, RunResult, RunStreamedResult, Thread, Turn, UserInput\n+from .thread_options import (\n+    ApprovalMode,\n+    ModelReasoningEffort,\n+    SandboxMode,\n+    ThreadOptions,\n+    WebSearchMode,\n+)\n+from .turn_options import TurnOptions\n+\n+__all__ = [\n+    \"Codex\",\n+    \"CodexOptions\",\n+    \"Thread\",\n+    \"Turn\",\n+    \"RunResult\",\n+    \"RunStreamedResult\",\n+    \"Input\",\n+    \"UserInput\",\n+    \"ThreadOptions\",\n+    \"TurnOptions\",\n+    \"ApprovalMode\",\n+    \"SandboxMode\",\n+    \"ModelReasoningEffort\",\n+    \"WebSearchMode\",\n+    \"ThreadEvent\",\n+    \"ThreadStartedEvent\",\n+    \"TurnStartedEvent\",\n+    \"TurnCompletedEvent\",\n+    \"TurnFailedEvent\",\n+    \"ItemStartedEvent\",\n+    \"ItemUpdatedEvent\",\n+    \"ItemCompletedEvent\",\n+    \"ThreadError\",\n+    \"ThreadErrorEvent\",\n+    \"Usage\",\n+    \"ThreadItem\",\n+    \"AgentMessageItem\",\n+    \"ReasoningItem\",\n+    \"CommandExecutionItem\",\n+    \"FileChangeItem\",\n+    \"FileUpdateChange\",\n+    \"McpToolCallItem\",\n+    \"McpToolCallResult\",\n+    \"McpToolCallError\",\n+    \"WebSearchItem\",\n+    \"TodoItem\",\n+    \"TodoListItem\",\n+    \"ErrorItem\",\n+    \"codex_tool\",\n+    \"CodexToolOptions\",\n+    \"CodexToolResult\",\n+    \"CodexToolStreamEvent\",\n+    \"OutputSchemaDescriptor\",\n+]",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2F__init__.py",
        "sha": "538b766a187e23209504f157f50b964bbba99822",
        "status": "added"
      },
      {
        "additions": 89,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex.py",
        "changes": 89,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/codex.py",
        "patch": "@@ -0,0 +1,89 @@\n+from __future__ import annotations\n+\n+from collections.abc import Mapping\n+from typing import Any, overload\n+\n+from agents.exceptions import UserError\n+\n+from .codex_options import CodexOptions, coerce_codex_options\n+from .exec import CodexExec\n+from .thread import Thread\n+from .thread_options import ThreadOptions, coerce_thread_options\n+\n+\n+class _UnsetType:\n+    pass\n+\n+\n+_UNSET = _UnsetType()\n+\n+\n+class Codex:\n+    @overload\n+    def __init__(self, options: CodexOptions | Mapping[str, Any] | None = None) -> None: ...\n+\n+    @overload\n+    def __init__(\n+        self,\n+        *,\n+        codex_path_override: str | None = None,\n+        base_url: str | None = None,\n+        api_key: str | None = None,\n+        env: Mapping[str, str] | None = None,\n+    ) -> None: ...\n+\n+    def __init__(\n+        self,\n+        options: CodexOptions | Mapping[str, Any] | None = None,\n+        *,\n+        codex_path_override: str | None | _UnsetType = _UNSET,\n+        base_url: str | None | _UnsetType = _UNSET,\n+        api_key: str | None | _UnsetType = _UNSET,\n+        env: Mapping[str, str] | None | _UnsetType = _UNSET,\n+    ) -> None:\n+        kw_values = {\n+            \"codex_path_override\": codex_path_override,\n+            \"base_url\": base_url,\n+            \"api_key\": api_key,\n+            \"env\": env,\n+        }\n+        has_kwargs = any(value is not _UNSET for value in kw_values.values())\n+        if options is not None and has_kwargs:\n+            raise UserError(\n+                \"Codex options must be provided as a CodexOptions/mapping or keyword arguments, \"\n+                \"not both.\"\n+            )\n+        if has_kwargs:\n+            options = {key: value for key, value in kw_values.items() if value is not _UNSET}\n+        resolved_options = coerce_codex_options(options) or CodexOptions()\n+        self._exec = CodexExec(\n+            executable_path=resolved_options.codex_path_override,\n+            env=_normalize_env(resolved_options),\n+        )\n+        self._options = resolved_options\n+\n+    def start_thread(self, options: ThreadOptions | Mapping[str, Any] | None = None) -> Thread:\n+        resolved_options = coerce_thread_options(options) or ThreadOptions()\n+        return Thread(\n+            exec_client=self._exec,\n+            options=self._options,\n+            thread_options=resolved_options,\n+        )\n+\n+    def resume_thread(\n+        self, thread_id: str, options: ThreadOptions | Mapping[str, Any] | None = None\n+    ) -> Thread:\n+        resolved_options = coerce_thread_options(options) or ThreadOptions()\n+        return Thread(\n+            exec_client=self._exec,\n+            options=self._options,\n+            thread_options=resolved_options,\n+            thread_id=thread_id,\n+        )\n+\n+\n+def _normalize_env(options: CodexOptions) -> dict[str, str] | None:\n+    if options.env is None:\n+        return None\n+    # Normalize mapping values to strings for subprocess environment.\n+    return {str(key): str(value) for key, value in options.env.items()}",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex.py",
        "sha": "f3419d23698f0b630d37c4f63075c83d84e93e72",
        "status": "added"
      },
      {
        "additions": 35,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex_options.py",
        "changes": 35,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex_options.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/codex_options.py",
        "patch": "@@ -0,0 +1,35 @@\n+from __future__ import annotations\n+\n+from collections.abc import Mapping\n+from dataclasses import dataclass, fields\n+from typing import Any\n+\n+from agents.exceptions import UserError\n+\n+\n+@dataclass(frozen=True)\n+class CodexOptions:\n+    # Optional absolute path to the codex CLI binary.\n+    codex_path_override: str | None = None\n+    # Override OpenAI base URL for the Codex CLI process.\n+    base_url: str | None = None\n+    # API key passed to the Codex CLI (CODEX_API_KEY).\n+    api_key: str | None = None\n+    # Environment variables for the Codex CLI process (do not inherit os.environ).\n+    env: Mapping[str, str] | None = None\n+\n+\n+def coerce_codex_options(\n+    options: CodexOptions | Mapping[str, Any] | None,\n+) -> CodexOptions | None:\n+    if options is None or isinstance(options, CodexOptions):\n+        return options\n+    if not isinstance(options, Mapping):\n+        raise UserError(\"CodexOptions must be a CodexOptions or a mapping.\")\n+\n+    allowed = {field.name for field in fields(CodexOptions)}\n+    unknown = set(options.keys()) - allowed\n+    if unknown:\n+        raise UserError(f\"Unknown CodexOptions field(s): {sorted(unknown)}\")\n+\n+    return CodexOptions(**dict(options))",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex_options.py",
        "sha": "f958d261da0937aa50f65500c0bf2c91ceef61be",
        "status": "added"
      },
      {
        "additions": 1142,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex_tool.py",
        "changes": 1142,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex_tool.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/codex_tool.py",
        "patch": "@@ -0,0 +1,1142 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import dataclasses\n+import inspect\n+import json\n+import os\n+from collections.abc import AsyncGenerator, Awaitable, Mapping\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional, Union\n+\n+from openai.types.responses.response_usage import InputTokensDetails, OutputTokensDetails\n+from pydantic import BaseModel, ConfigDict, Field, ValidationError, model_validator\n+from typing_extensions import Literal, NotRequired, TypeAlias, TypedDict, TypeGuard\n+\n+from agents import _debug\n+from agents.exceptions import ModelBehaviorError, UserError\n+from agents.logger import logger\n+from agents.models import _openai_shared\n+from agents.run_context import RunContextWrapper\n+from agents.strict_schema import ensure_strict_json_schema\n+from agents.tool import FunctionTool, ToolErrorFunction, default_tool_error_function\n+from agents.tool_context import ToolContext\n+from agents.tracing import SpanError, custom_span\n+from agents.usage import Usage as AgentsUsage\n+from agents.util import _error_tracing\n+from agents.util._types import MaybeAwaitable\n+\n+from .codex import Codex\n+from .codex_options import CodexOptions, coerce_codex_options\n+from .events import (\n+    ItemCompletedEvent,\n+    ItemStartedEvent,\n+    ItemUpdatedEvent,\n+    ThreadErrorEvent,\n+    ThreadEvent,\n+    TurnCompletedEvent,\n+    TurnFailedEvent,\n+    Usage,\n+    coerce_thread_event,\n+)\n+from .items import (\n+    CommandExecutionItem,\n+    McpToolCallItem,\n+    ReasoningItem,\n+    ThreadItem,\n+    is_agent_message_item,\n+)\n+from .payloads import _DictLike\n+from .thread import Input, Thread, UserInput\n+from .thread_options import SandboxMode, ThreadOptions, coerce_thread_options\n+from .turn_options import TurnOptions, coerce_turn_options\n+\n+JSON_PRIMITIVE_TYPES = {\"string\", \"number\", \"integer\", \"boolean\"}\n+SPAN_TRIM_KEYS = (\n+    \"arguments\",\n+    \"command\",\n+    \"output\",\n+    \"result\",\n+    \"error\",\n+    \"text\",\n+    \"changes\",\n+    \"items\",\n+)\n+\n+\n+class CodexToolInputItem(BaseModel):\n+    type: Literal[\"text\", \"local_image\"]\n+    text: str | None = None\n+    path: str | None = None\n+\n+    model_config = ConfigDict(extra=\"forbid\")\n+\n+    @model_validator(mode=\"after\")\n+    def validate_item(self) -> CodexToolInputItem:\n+        text_value = (self.text or \"\").strip()\n+        path_value = (self.path or \"\").strip()\n+\n+        if self.type == \"text\":\n+            if not text_value:\n+                raise ValueError('Text inputs must include a non-empty \"text\" field.')\n+            if path_value:\n+                raise ValueError('\"path\" is not allowed when type is \"text\".')\n+            self.text = text_value\n+            self.path = None\n+            return self\n+\n+        if not path_value:\n+            raise ValueError('Local image inputs must include a non-empty \"path\" field.')\n+        if text_value:\n+            raise ValueError('\"text\" is not allowed when type is \"local_image\".')\n+        self.path = path_value\n+        self.text = None\n+        return self\n+\n+\n+class CodexToolParameters(BaseModel):\n+    inputs: list[CodexToolInputItem] = Field(\n+        ...,\n+        min_length=1,\n+        description=(\n+            \"Structured inputs appended to the Codex task. Provide at least one input item.\"\n+        ),\n+    )\n+\n+    model_config = ConfigDict(extra=\"forbid\")\n+\n+\n+class OutputSchemaPrimitive(TypedDict, total=False):\n+    type: Literal[\"string\", \"number\", \"integer\", \"boolean\"]\n+    description: NotRequired[str]\n+    enum: NotRequired[list[str]]\n+\n+\n+class OutputSchemaArray(TypedDict, total=False):\n+    type: Literal[\"array\"]\n+    description: NotRequired[str]\n+    items: OutputSchemaPrimitive\n+\n+\n+OutputSchemaField: TypeAlias = Union[OutputSchemaPrimitive, OutputSchemaArray]\n+\n+\n+class OutputSchemaPropertyDescriptor(TypedDict, total=False):\n+    name: str\n+    description: NotRequired[str]\n+    schema: OutputSchemaField\n+\n+\n+class OutputSchemaDescriptor(TypedDict, total=False):\n+    title: NotRequired[str]\n+    description: NotRequired[str]\n+    properties: list[OutputSchemaPropertyDescriptor]\n+    required: NotRequired[list[str]]\n+\n+\n+@dataclass(frozen=True)\n+class CodexToolResult:\n+    thread_id: str | None\n+    response: str\n+    usage: Usage | None\n+\n+    def as_dict(self) -> dict[str, Any]:\n+        return {\n+            \"thread_id\": self.thread_id,\n+            \"response\": self.response,\n+            \"usage\": self.usage.as_dict() if isinstance(self.usage, Usage) else self.usage,\n+        }\n+\n+    def __str__(self) -> str:\n+        return json.dumps(self.as_dict())\n+\n+\n+@dataclass(frozen=True)\n+class CodexToolStreamEvent(_DictLike):\n+    event: ThreadEvent\n+    thread: Thread\n+    tool_call: Any\n+\n+\n+@dataclass\n+class CodexToolOptions:\n+    name: str | None = None\n+    description: str | None = None\n+    parameters: type[BaseModel] | None = None\n+    output_schema: OutputSchemaDescriptor | Mapping[str, Any] | None = None\n+    codex: Codex | None = None\n+    codex_options: CodexOptions | Mapping[str, Any] | None = None\n+    default_thread_options: ThreadOptions | Mapping[str, Any] | None = None\n+    thread_id: str | None = None\n+    sandbox_mode: SandboxMode | None = None\n+    working_directory: str | None = None\n+    skip_git_repo_check: bool | None = None\n+    default_turn_options: TurnOptions | Mapping[str, Any] | None = None\n+    span_data_max_chars: int | None = 8192\n+    persist_session: bool = False\n+    on_stream: Callable[[CodexToolStreamEvent], MaybeAwaitable[None]] | None = None\n+    is_enabled: bool | Callable[[RunContextWrapper[Any], Any], MaybeAwaitable[bool]] = True\n+    failure_error_function: ToolErrorFunction | None = default_tool_error_function\n+\n+\n+CodexToolCallArguments: TypeAlias = dict[str, Optional[list[UserInput]]]\n+\n+\n+class _UnsetType:\n+    pass\n+\n+\n+_UNSET = _UnsetType()\n+\n+\n+def codex_tool(\n+    options: CodexToolOptions | Mapping[str, Any] | None = None,\n+    *,\n+    name: str | None = None,\n+    description: str | None = None,\n+    parameters: type[BaseModel] | None = None,\n+    output_schema: OutputSchemaDescriptor | Mapping[str, Any] | None = None,\n+    codex: Codex | None = None,\n+    codex_options: CodexOptions | Mapping[str, Any] | None = None,\n+    default_thread_options: ThreadOptions | Mapping[str, Any] | None = None,\n+    thread_id: str | None = None,\n+    sandbox_mode: SandboxMode | None = None,\n+    working_directory: str | None = None,\n+    skip_git_repo_check: bool | None = None,\n+    default_turn_options: TurnOptions | Mapping[str, Any] | None = None,\n+    span_data_max_chars: int | None | _UnsetType = _UNSET,\n+    persist_session: bool | None = None,\n+    on_stream: Callable[[CodexToolStreamEvent], MaybeAwaitable[None]] | None = None,\n+    is_enabled: bool | Callable[[RunContextWrapper[Any], Any], MaybeAwaitable[bool]] | None = None,\n+    failure_error_function: ToolErrorFunction | None | _UnsetType = _UNSET,\n+) -> FunctionTool:\n+    resolved_options = _coerce_tool_options(options)\n+    if name is not None:\n+        resolved_options.name = name\n+    if description is not None:\n+        resolved_options.description = description\n+    if parameters is not None:\n+        resolved_options.parameters = parameters\n+    if output_schema is not None:\n+        resolved_options.output_schema = output_schema\n+    if codex is not None:\n+        resolved_options.codex = codex\n+    if codex_options is not None:\n+        resolved_options.codex_options = codex_options\n+    if default_thread_options is not None:\n+        resolved_options.default_thread_options = default_thread_options\n+    if thread_id is not None:\n+        resolved_options.thread_id = thread_id\n+    if sandbox_mode is not None:\n+        resolved_options.sandbox_mode = sandbox_mode\n+    if working_directory is not None:\n+        resolved_options.working_directory = working_directory\n+    if skip_git_repo_check is not None:\n+        resolved_options.skip_git_repo_check = skip_git_repo_check\n+    if default_turn_options is not None:\n+        resolved_options.default_turn_options = default_turn_options\n+    if not isinstance(span_data_max_chars, _UnsetType):\n+        resolved_options.span_data_max_chars = span_data_max_chars\n+    if persist_session is not None:\n+        resolved_options.persist_session = persist_session\n+    if on_stream is not None:\n+        resolved_options.on_stream = on_stream\n+    if is_enabled is not None:\n+        resolved_options.is_enabled = is_enabled\n+    if not isinstance(failure_error_function, _UnsetType):\n+        resolved_options.failure_error_function = failure_error_function\n+\n+    resolved_options.codex_options = coerce_codex_options(resolved_options.codex_options)\n+    resolved_options.default_thread_options = coerce_thread_options(\n+        resolved_options.default_thread_options\n+    )\n+    resolved_options.default_turn_options = coerce_turn_options(\n+        resolved_options.default_turn_options\n+    )\n+    name = resolved_options.name or \"codex\"\n+    description = resolved_options.description or (\n+        \"Executes an agentic Codex task against the current workspace.\"\n+    )\n+    parameters_model = resolved_options.parameters or CodexToolParameters\n+\n+    params_schema = ensure_strict_json_schema(parameters_model.model_json_schema())\n+    resolved_codex_options = _resolve_codex_options(resolved_options.codex_options)\n+    resolve_codex = _create_codex_resolver(resolved_options.codex, resolved_codex_options)\n+\n+    validated_output_schema = _resolve_output_schema(resolved_options.output_schema)\n+    resolved_thread_options = _resolve_thread_options(\n+        resolved_options.default_thread_options,\n+        resolved_options.sandbox_mode,\n+        resolved_options.working_directory,\n+        resolved_options.skip_git_repo_check,\n+    )\n+\n+    persisted_thread: Thread | None = None\n+\n+    async def _on_invoke_tool(ctx: ToolContext[Any], input_json: str) -> Any:\n+        nonlocal persisted_thread\n+        try:\n+            parsed = _parse_tool_input(parameters_model, input_json)\n+            args = _normalize_parameters(parsed)\n+\n+            codex = await resolve_codex()\n+            if resolved_options.persist_session:\n+                # Reuse a single Codex thread across tool calls.\n+                thread = _get_or_create_persisted_thread(\n+                    codex,\n+                    resolved_options.thread_id,\n+                    resolved_thread_options,\n+                    persisted_thread,\n+                )\n+                if persisted_thread is None:\n+                    persisted_thread = thread\n+            else:\n+                thread = _get_thread(codex, resolved_options.thread_id, resolved_thread_options)\n+\n+            turn_options = _build_turn_options(\n+                resolved_options.default_turn_options, validated_output_schema\n+            )\n+            codex_input = _build_codex_input(args)\n+\n+            # Always stream and aggregate locally to enable on_stream callbacks.\n+            stream_result = await thread.run_streamed(codex_input, turn_options)\n+            response, usage = await _consume_events(\n+                stream_result.events,\n+                args,\n+                ctx,\n+                thread,\n+                resolved_options.on_stream,\n+                resolved_options.span_data_max_chars,\n+            )\n+\n+            if usage is not None:\n+                ctx.usage.add(_to_agent_usage(usage))\n+\n+            return CodexToolResult(thread_id=thread.id, response=response, usage=usage)\n+        except Exception as exc:  # noqa: BLE001\n+            if resolved_options.failure_error_function is None:\n+                raise\n+\n+            result = resolved_options.failure_error_function(ctx, exc)\n+            if inspect.isawaitable(result):\n+                result = await result\n+\n+            _error_tracing.attach_error_to_current_span(\n+                SpanError(\n+                    message=\"Error running Codex tool (non-fatal)\",\n+                    data={\"tool_name\": name, \"error\": str(exc)},\n+                )\n+            )\n+            if _debug.DONT_LOG_TOOL_DATA:\n+                logger.debug(\"Codex tool failed\")\n+            else:\n+                logger.error(\"Codex tool failed: %s\", exc, exc_info=exc)\n+            return result\n+\n+    return FunctionTool(\n+        name=name,\n+        description=description,\n+        params_json_schema=params_schema,\n+        on_invoke_tool=_on_invoke_tool,\n+        strict_json_schema=True,\n+        is_enabled=resolved_options.is_enabled,\n+    )\n+\n+\n+def _coerce_tool_options(\n+    options: CodexToolOptions | Mapping[str, Any] | None,\n+) -> CodexToolOptions:\n+    if options is None:\n+        return CodexToolOptions()\n+    if isinstance(options, CodexToolOptions):\n+        resolved = options\n+    else:\n+        if not isinstance(options, Mapping):\n+            raise UserError(\"Codex tool options must be a CodexToolOptions or a mapping.\")\n+\n+        allowed = {field.name for field in dataclasses.fields(CodexToolOptions)}\n+        unknown = set(options.keys()) - allowed\n+        if unknown:\n+            raise UserError(f\"Unknown Codex tool option(s): {sorted(unknown)}\")\n+\n+        resolved = CodexToolOptions(**dict(options))\n+    # Normalize nested option dictionaries to their dataclass equivalents.\n+    resolved.codex_options = coerce_codex_options(resolved.codex_options)\n+    resolved.default_thread_options = coerce_thread_options(resolved.default_thread_options)\n+    resolved.default_turn_options = coerce_turn_options(resolved.default_turn_options)\n+    return resolved\n+\n+\n+def _parse_tool_input(parameters_model: type[BaseModel], input_json: str) -> BaseModel:\n+    try:\n+        json_data = json.loads(input_json) if input_json else {}\n+    except Exception as exc:  # noqa: BLE001\n+        if _debug.DONT_LOG_TOOL_DATA:\n+            logger.debug(\"Invalid JSON input for codex tool\")\n+        else:\n+            logger.debug(\"Invalid JSON input for codex tool: %s\", input_json)\n+        raise ModelBehaviorError(f\"Invalid JSON input for codex tool: {input_json}\") from exc\n+\n+    try:\n+        return parameters_model.model_validate(json_data)\n+    except ValidationError as exc:\n+        raise ModelBehaviorError(f\"Invalid JSON input for codex tool: {exc}\") from exc\n+\n+\n+def _normalize_parameters(params: BaseModel) -> CodexToolCallArguments:\n+    inputs_value = getattr(params, \"inputs\", None)\n+    if inputs_value is None:\n+        raise UserError(\"Codex tool parameters must include an inputs field.\")\n+\n+    inputs = [{\"type\": item.type, \"text\": item.text, \"path\": item.path} for item in inputs_value]\n+\n+    normalized_inputs: list[UserInput] = []\n+    for item in inputs:\n+        if item[\"type\"] == \"text\":\n+            normalized_inputs.append({\"type\": \"text\", \"text\": item[\"text\"] or \"\"})\n+        else:\n+            normalized_inputs.append({\"type\": \"local_image\", \"path\": item[\"path\"] or \"\"})\n+\n+    return {\"inputs\": normalized_inputs if normalized_inputs else None}\n+\n+\n+def _build_codex_input(args: CodexToolCallArguments) -> Input:\n+    if args.get(\"inputs\"):\n+        return args[\"inputs\"]  # type: ignore[return-value]\n+    return \"\"\n+\n+\n+def _resolve_codex_options(\n+    options: CodexOptions | Mapping[str, Any] | None,\n+) -> CodexOptions | None:\n+    options = coerce_codex_options(options)\n+    if options and options.api_key:\n+        return options\n+\n+    api_key = _resolve_default_codex_api_key(options)\n+    if not api_key:\n+        return options\n+\n+    if options is None:\n+        return CodexOptions(api_key=api_key)\n+\n+    return CodexOptions(\n+        codex_path_override=options.codex_path_override,\n+        base_url=options.base_url,\n+        api_key=api_key,\n+        env=options.env,\n+    )\n+\n+\n+def _resolve_default_codex_api_key(options: CodexOptions | None) -> str | None:\n+    if options and options.api_key:\n+        return options.api_key\n+\n+    env_override = options.env if options else None\n+    if env_override:\n+        env_codex = env_override.get(\"CODEX_API_KEY\")\n+        if env_codex:\n+            return env_codex\n+        env_openai = env_override.get(\"OPENAI_API_KEY\")\n+        if env_openai:\n+            return env_openai\n+\n+    env_codex = os.environ.get(\"CODEX_API_KEY\")\n+    if env_codex:\n+        return env_codex\n+\n+    env_openai = os.environ.get(\"OPENAI_API_KEY\")\n+    if env_openai:\n+        return env_openai\n+\n+    return _openai_shared.get_default_openai_key()\n+\n+\n+def _create_codex_resolver(\n+    provided: Codex | None, options: CodexOptions | None\n+) -> Callable[[], Awaitable[Codex]]:\n+    if provided is not None:\n+\n+        async def _return_provided() -> Codex:\n+            return provided\n+\n+        return _return_provided\n+\n+    codex_instance: Codex | None = None\n+\n+    async def _get_or_create() -> Codex:\n+        nonlocal codex_instance\n+        if codex_instance is None:\n+            codex_instance = Codex(options)\n+        return codex_instance\n+\n+    return _get_or_create\n+\n+\n+def _resolve_thread_options(\n+    defaults: ThreadOptions | Mapping[str, Any] | None,\n+    sandbox_mode: SandboxMode | None,\n+    working_directory: str | None,\n+    skip_git_repo_check: bool | None,\n+) -> ThreadOptions | None:\n+    defaults = coerce_thread_options(defaults)\n+    if not defaults and not sandbox_mode and not working_directory and skip_git_repo_check is None:\n+        return None\n+\n+    return ThreadOptions(\n+        **{\n+            **(defaults.__dict__ if defaults else {}),\n+            **({\"sandbox_mode\": sandbox_mode} if sandbox_mode else {}),\n+            **({\"working_directory\": working_directory} if working_directory else {}),\n+            **(\n+                {\"skip_git_repo_check\": skip_git_repo_check}\n+                if skip_git_repo_check is not None\n+                else {}\n+            ),\n+        }\n+    )\n+\n+\n+def _build_turn_options(\n+    defaults: TurnOptions | Mapping[str, Any] | None,\n+    output_schema: dict[str, Any] | None,\n+) -> TurnOptions:\n+    defaults = coerce_turn_options(defaults)\n+    if defaults is None and output_schema is None:\n+        return TurnOptions()\n+\n+    if defaults is None:\n+        return TurnOptions(output_schema=output_schema, signal=None, idle_timeout_seconds=None)\n+\n+    merged_output_schema = output_schema if output_schema is not None else defaults.output_schema\n+    return TurnOptions(\n+        output_schema=merged_output_schema,\n+        signal=defaults.signal,\n+        idle_timeout_seconds=defaults.idle_timeout_seconds,\n+    )\n+\n+\n+def _resolve_output_schema(\n+    option: OutputSchemaDescriptor | Mapping[str, Any] | None,\n+) -> dict[str, Any] | None:\n+    if option is None:\n+        return None\n+\n+    if isinstance(option, Mapping) and _looks_like_descriptor(option):\n+        # Descriptor input is converted to a strict JSON schema for Codex.\n+        descriptor = _validate_descriptor(option)\n+        return _build_codex_output_schema(descriptor)\n+\n+    if isinstance(option, Mapping):\n+        schema = dict(option)\n+        if \"type\" in schema and schema.get(\"type\") != \"object\":\n+            raise UserError('Codex output schema must be a JSON object schema with type \"object\".')\n+        return ensure_strict_json_schema(schema)\n+\n+    raise UserError(\"Codex output schema must be a JSON schema or descriptor.\")\n+\n+\n+def _looks_like_descriptor(option: Mapping[str, Any]) -> bool:\n+    properties = option.get(\"properties\")\n+    if not isinstance(properties, list):\n+        return False\n+    return all(isinstance(item, Mapping) and \"name\" in item for item in properties)\n+\n+\n+def _validate_descriptor(option: Mapping[str, Any]) -> OutputSchemaDescriptor:\n+    properties = option.get(\"properties\")\n+    if not isinstance(properties, list) or not properties:\n+        raise UserError(\"Codex output schema descriptor must include properties.\")\n+\n+    seen: set[str] = set()\n+    for prop in properties:\n+        name = prop.get(\"name\") if isinstance(prop, Mapping) else None\n+        if not isinstance(name, str) or not name.strip():\n+            raise UserError(\"Codex output schema properties must include non-empty names.\")\n+        if name in seen:\n+            raise UserError(f'Duplicate property name \"{name}\" in output_schema.')\n+        seen.add(name)\n+\n+        schema = prop.get(\"schema\")\n+        if not _is_valid_field(schema):\n+            raise UserError(f'Invalid schema for output property \"{name}\".')\n+\n+    required = option.get(\"required\")\n+    if required is not None:\n+        if not isinstance(required, list) or not all(isinstance(item, str) for item in required):\n+            raise UserError(\"output_schema.required must be a list of strings.\")\n+        for name in required:\n+            if name not in seen:\n+                raise UserError(f'Required property \"{name}\" must also be defined in \"properties\".')\n+\n+    return option  # type: ignore[return-value]\n+\n+\n+def _is_valid_field(field: Any) -> bool:\n+    if not isinstance(field, Mapping):\n+        return False\n+    field_type = field.get(\"type\")\n+    if field_type in JSON_PRIMITIVE_TYPES:\n+        enum = field.get(\"enum\")\n+        if enum is not None and (\n+            not isinstance(enum, list) or not all(isinstance(item, str) for item in enum)\n+        ):\n+            return False\n+        return True\n+    if field_type == \"array\":\n+        items = field.get(\"items\")\n+        return _is_valid_field(items)\n+    return False\n+\n+\n+def _build_codex_output_schema(descriptor: OutputSchemaDescriptor) -> dict[str, Any]:\n+    # Compose the strict object schema required by Codex structured outputs.\n+    properties: dict[str, Any] = {}\n+    for prop in descriptor[\"properties\"]:\n+        prop_schema = _build_codex_output_schema_field(prop[\"schema\"])\n+        if prop.get(\"description\"):\n+            prop_schema[\"description\"] = prop[\"description\"]\n+        properties[prop[\"name\"]] = prop_schema\n+\n+    required = list(descriptor.get(\"required\", []))\n+\n+    schema: dict[str, Any] = {\n+        \"type\": \"object\",\n+        \"additionalProperties\": False,\n+        \"properties\": properties,\n+        \"required\": required,\n+    }\n+\n+    if \"title\" in descriptor and descriptor[\"title\"]:\n+        schema[\"title\"] = descriptor[\"title\"]\n+    if \"description\" in descriptor and descriptor[\"description\"]:\n+        schema[\"description\"] = descriptor[\"description\"]\n+\n+    return schema\n+\n+\n+def _build_codex_output_schema_field(field: OutputSchemaField) -> dict[str, Any]:\n+    if field[\"type\"] == \"array\":\n+        schema: dict[str, Any] = {\n+            \"type\": \"array\",\n+            \"items\": _build_codex_output_schema_field(field[\"items\"]),\n+        }\n+        if \"description\" in field and field[\"description\"]:\n+            schema[\"description\"] = field[\"description\"]\n+        return schema\n+    result: dict[str, Any] = {\"type\": field[\"type\"]}\n+    if \"description\" in field and field[\"description\"]:\n+        result[\"description\"] = field[\"description\"]\n+    if \"enum\" in field:\n+        result[\"enum\"] = field[\"enum\"]\n+    return result\n+\n+\n+def _get_thread(codex: Codex, thread_id: str | None, defaults: ThreadOptions | None) -> Thread:\n+    if thread_id:\n+        return codex.resume_thread(thread_id, defaults)\n+    return codex.start_thread(defaults)\n+\n+\n+def _get_or_create_persisted_thread(\n+    codex: Codex,\n+    thread_id: str | None,\n+    thread_options: ThreadOptions | None,\n+    existing_thread: Thread | None,\n+) -> Thread:\n+    if existing_thread is not None:\n+        if thread_id:\n+            existing_id = existing_thread.id\n+            if existing_id and existing_id != thread_id:\n+                raise UserError(\n+                    \"Codex tool is configured with persist_session=true \"\n+                    + \"and already has an active thread.\"\n+                )\n+        return existing_thread\n+\n+    return _get_thread(codex, thread_id, thread_options)\n+\n+\n+def _to_agent_usage(usage: Usage) -> AgentsUsage:\n+    return AgentsUsage(\n+        requests=1,\n+        input_tokens=usage.input_tokens,\n+        output_tokens=usage.output_tokens,\n+        total_tokens=usage.input_tokens + usage.output_tokens,\n+        input_tokens_details=InputTokensDetails(cached_tokens=usage.cached_input_tokens),\n+        output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n+    )\n+\n+\n+async def _consume_events(\n+    events: AsyncGenerator[ThreadEvent | Mapping[str, Any], None],\n+    args: CodexToolCallArguments,\n+    ctx: ToolContext[Any],\n+    thread: Thread,\n+    on_stream: Callable[[CodexToolStreamEvent], MaybeAwaitable[None]] | None,\n+    span_data_max_chars: int | None,\n+) -> tuple[str, Usage | None]:\n+    # Track spans keyed by item id for command/mcp/reasoning events.\n+    active_spans: dict[str, Any] = {}\n+    final_response = \"\"\n+    usage: Usage | None = None\n+\n+    event_queue: asyncio.Queue[CodexToolStreamEvent | None] | None = None\n+    dispatch_task: asyncio.Task[None] | None = None\n+\n+    if on_stream is not None:\n+        # Buffer events so user callbacks cannot block the Codex stream loop.\n+        event_queue = asyncio.Queue()\n+\n+        async def _run_handler(payload: CodexToolStreamEvent) -> None:\n+            # Dispatch user callbacks asynchronously to avoid blocking the stream.\n+            try:\n+                maybe_result = on_stream(payload)\n+                if inspect.isawaitable(maybe_result):\n+                    await maybe_result\n+            except Exception:\n+                logger.exception(\"Error while handling Codex on_stream event.\")\n+\n+        async def _dispatch() -> None:\n+            assert event_queue is not None\n+            while True:\n+                payload = await event_queue.get()\n+                is_sentinel = payload is None\n+                try:\n+                    if payload is not None:\n+                        await _run_handler(payload)\n+                finally:\n+                    event_queue.task_done()\n+                if is_sentinel:\n+                    break\n+\n+        dispatch_task = asyncio.create_task(_dispatch())\n+\n+    try:\n+        async for raw_event in events:\n+            event = coerce_thread_event(raw_event)\n+            if event_queue is not None:\n+                await event_queue.put(\n+                    CodexToolStreamEvent(\n+                        event=event,\n+                        thread=thread,\n+                        tool_call=ctx.tool_call,\n+                    )\n+                )\n+\n+            if isinstance(event, ItemStartedEvent):\n+                _handle_item_started(event.item, active_spans, span_data_max_chars)\n+            elif isinstance(event, ItemUpdatedEvent):\n+                _handle_item_updated(event.item, active_spans, span_data_max_chars)\n+            elif isinstance(event, ItemCompletedEvent):\n+                _handle_item_completed(event.item, active_spans, span_data_max_chars)\n+                if is_agent_message_item(event.item):\n+                    final_response = event.item.text\n+            elif isinstance(event, TurnCompletedEvent):\n+                usage = event.usage\n+            elif isinstance(event, TurnFailedEvent):\n+                error = event.error.message\n+                raise UserError(f\"Codex turn failed{(': ' + error) if error else ''}\")\n+            elif isinstance(event, ThreadErrorEvent):\n+                raise UserError(f\"Codex stream error: {event.message}\")\n+    finally:\n+        if event_queue is not None:\n+            await event_queue.put(None)\n+            await event_queue.join()\n+        if dispatch_task is not None:\n+            await dispatch_task\n+\n+        # Ensure any open spans are closed even on failure.\n+        for span in active_spans.values():\n+            span.finish()\n+        active_spans.clear()\n+\n+    if not final_response:\n+        final_response = _build_default_response(args)\n+\n+    return final_response, usage\n+\n+\n+def _handle_item_started(\n+    item: ThreadItem, spans: dict[str, Any], span_data_max_chars: int | None\n+) -> None:\n+    item_id = getattr(item, \"id\", None)\n+    if not item_id:\n+        return\n+\n+    if _is_command_execution_item(item):\n+        output = item.aggregated_output\n+        updates = {\n+            \"command\": item.command,\n+            \"status\": item.status,\n+            \"exit_code\": item.exit_code,\n+        }\n+        if output not in (None, \"\"):\n+            updates[\"output\"] = _truncate_span_value(output, span_data_max_chars)\n+        data = _merge_span_data(\n+            {},\n+            updates,\n+            span_data_max_chars,\n+        )\n+        span = custom_span(\n+            name=\"Codex command execution\",\n+            data=data,\n+        )\n+        span.start()\n+        spans[item_id] = span\n+        return\n+\n+    if _is_mcp_tool_call_item(item):\n+        data = _merge_span_data(\n+            {},\n+            {\n+                \"server\": item.server,\n+                \"tool\": item.tool,\n+                \"status\": item.status,\n+                \"arguments\": _truncate_span_value(\n+                    _maybe_as_dict(item.arguments), span_data_max_chars\n+                ),\n+            },\n+            span_data_max_chars,\n+        )\n+        span = custom_span(\n+            name=\"Codex MCP tool call\",\n+            data=data,\n+        )\n+        span.start()\n+        spans[item_id] = span\n+        return\n+\n+    if _is_reasoning_item(item):\n+        data = _merge_span_data(\n+            {},\n+            {\"text\": _truncate_span_value(item.text, span_data_max_chars)},\n+            span_data_max_chars,\n+        )\n+        span = custom_span(\n+            name=\"Codex reasoning\",\n+            data=data,\n+        )\n+        span.start()\n+        spans[item_id] = span\n+\n+\n+def _handle_item_updated(\n+    item: ThreadItem, spans: dict[str, Any], span_data_max_chars: int | None\n+) -> None:\n+    item_id = getattr(item, \"id\", None)\n+    if not item_id:\n+        return\n+    span = spans.get(item_id)\n+    if span is None:\n+        return\n+\n+    if _is_command_execution_item(item):\n+        _update_command_span(span, item, span_data_max_chars)\n+    elif _is_mcp_tool_call_item(item):\n+        _update_mcp_tool_span(span, item, span_data_max_chars)\n+    elif _is_reasoning_item(item):\n+        _update_reasoning_span(span, item, span_data_max_chars)\n+\n+\n+def _handle_item_completed(\n+    item: ThreadItem, spans: dict[str, Any], span_data_max_chars: int | None\n+) -> None:\n+    item_id = getattr(item, \"id\", None)\n+    if not item_id:\n+        return\n+    span = spans.get(item_id)\n+    if span is None:\n+        return\n+\n+    if _is_command_execution_item(item):\n+        _update_command_span(span, item, span_data_max_chars)\n+        if item.status == \"failed\":\n+            error_data: dict[str, Any] = {\n+                \"exit_code\": item.exit_code,\n+            }\n+            output = item.aggregated_output\n+            if output not in (None, \"\"):\n+                error_data[\"output\"] = _truncate_span_value(output, span_data_max_chars)\n+            span.set_error(\n+                SpanError(\n+                    message=\"Codex command execution failed.\",\n+                    data=error_data,\n+                )\n+            )\n+    elif _is_mcp_tool_call_item(item):\n+        _update_mcp_tool_span(span, item, span_data_max_chars)\n+        error = item.error\n+        if item.status == \"failed\" and error is not None and error.message:\n+            span.set_error(SpanError(message=error.message, data={}))\n+    elif _is_reasoning_item(item):\n+        _update_reasoning_span(span, item, span_data_max_chars)\n+\n+    span.finish()\n+    spans.pop(item_id, None)\n+\n+\n+def _truncate_span_string(value: str, max_chars: int | None) -> str:\n+    if max_chars is None:\n+        return value\n+    if max_chars <= 0:\n+        return \"\"\n+    if len(value) <= max_chars:\n+        return value\n+\n+    suffix = f\"... [truncated, {len(value)} chars]\"\n+    max_prefix = max_chars - len(suffix)\n+    if max_prefix <= 0:\n+        return value[:max_chars]\n+    return value[:max_prefix] + suffix\n+\n+\n+def _json_char_size(value: Any) -> int:\n+    try:\n+        return len(json.dumps(value, ensure_ascii=True, separators=(\",\", \":\"), default=str))\n+    except Exception:\n+        return len(str(value))\n+\n+\n+def _drop_empty_string_fields(data: dict[str, Any]) -> dict[str, Any]:\n+    return {key: value for key, value in data.items() if value != \"\"}\n+\n+\n+def _stringify_span_value(value: Any) -> str:\n+    if value is None:\n+        return \"\"\n+    if isinstance(value, str):\n+        return value\n+    try:\n+        return json.dumps(value, ensure_ascii=True, separators=(\",\", \":\"), default=str)\n+    except Exception:\n+        return str(value)\n+\n+\n+def _maybe_as_dict(value: Any) -> Any:\n+    if isinstance(value, _DictLike):\n+        return value.as_dict()\n+    if isinstance(value, list):\n+        return [_maybe_as_dict(item) for item in value]\n+    if isinstance(value, dict):\n+        return {key: _maybe_as_dict(item) for key, item in value.items()}\n+    return value\n+\n+\n+def _truncate_span_value(value: Any, max_chars: int | None) -> Any:\n+    if max_chars is None:\n+        return value\n+    if value is None or isinstance(value, (bool, int, float)):\n+        return value\n+    if isinstance(value, str):\n+        return _truncate_span_string(value, max_chars)\n+\n+    try:\n+        encoded = json.dumps(value, ensure_ascii=True, separators=(\",\", \":\"), default=str)\n+    except Exception:\n+        encoded = str(value)\n+\n+    if len(encoded) <= max_chars:\n+        return value\n+\n+    return {\n+        \"preview\": _truncate_span_string(encoded, max_chars),\n+        \"truncated\": True,\n+        \"original_length\": len(encoded),\n+    }\n+\n+\n+def _enforce_span_data_budget(data: dict[str, Any], max_chars: int | None) -> dict[str, Any]:\n+    # Trim span payloads to fit the overall JSON size budget while preserving keys.\n+    if max_chars is None:\n+        return _drop_empty_string_fields(data)\n+    if max_chars <= 0:\n+        return {}\n+\n+    trimmed = _drop_empty_string_fields(dict(data))\n+    if _json_char_size(trimmed) <= max_chars:\n+        return trimmed\n+\n+    trim_keys = SPAN_TRIM_KEYS\n+    kept_keys = [key for key in trim_keys if key in trimmed]\n+    if not kept_keys:\n+        return trimmed\n+\n+    base = dict(trimmed)\n+    for key in kept_keys:\n+        base[key] = \"\"\n+    base_size = _json_char_size(base)\n+\n+    while base_size > max_chars and kept_keys:\n+        # Drop lowest-priority keys only if the empty base cannot fit.\n+        drop_key = kept_keys.pop()\n+        base.pop(drop_key, None)\n+        trimmed.pop(drop_key, None)\n+        base_size = _json_char_size(base)\n+\n+    if base_size > max_chars:\n+        return _drop_empty_string_fields(base)\n+\n+    values = {\n+        key: _stringify_span_value(trimmed[key])\n+        for key in kept_keys\n+        if trimmed.get(key) not in (\"\", None)\n+    }\n+    for key, value in list(values.items()):\n+        if value == \"\":\n+            values.pop(key, None)\n+            trimmed[key] = \"\"\n+    kept_keys = [key for key in kept_keys if key in values or key in trimmed]\n+\n+    if not kept_keys:\n+        return _drop_empty_string_fields(base)\n+\n+    base_size = _json_char_size(base)\n+    available = max_chars - base_size\n+    if available <= 0:\n+        return _drop_empty_string_fields(base)\n+\n+    ordered_keys = [key for key in trim_keys if key in values]\n+    min_budget = 1\n+    budgets = {key: 0 for key in values}\n+    if available >= len(values):\n+        for key in values:\n+            budgets[key] = min_budget\n+        remaining = available - len(values)\n+    else:\n+        for key in ordered_keys[:available]:\n+            budgets[key] = min_budget\n+        remaining = 0\n+\n+    if \"arguments\" in values and remaining > 0:\n+        # Keep arguments intact when they already fit within the budget.\n+        needed = len(values[\"arguments\"]) - budgets[\"arguments\"]\n+        if needed > 0:\n+            grant = min(needed, remaining)\n+            budgets[\"arguments\"] += grant\n+            remaining -= grant\n+\n+    if remaining > 0:\n+        weights = {key: max(len(values[key]) - budgets[key], 0) for key in values}\n+        weight_total = sum(weights.values())\n+        if weight_total > 0:\n+            for key, weight in weights.items():\n+                if weight == 0:\n+                    continue\n+                budgets[key] += int(remaining * (weight / weight_total))\n+        for key in list(budgets.keys()):\n+            budgets[key] = min(budgets[key], len(values[key]))\n+        allocated = sum(budgets.values())\n+        leftover = available - allocated\n+        if leftover > 0:\n+            ordered = sorted(values.keys(), key=lambda k: weights.get(k, 0), reverse=True)\n+            idx = 0\n+            while leftover > 0:\n+                expandable = [key for key in ordered if budgets[key] < len(values[key])]\n+                if not expandable:\n+                    break\n+                key = expandable[idx % len(expandable)]\n+                budgets[key] += 1\n+                leftover -= 1\n+                idx += 1\n+\n+    for key in kept_keys:\n+        if key in values:\n+            trimmed[key] = _truncate_span_string(values[key], budgets.get(key, 0))\n+        else:\n+            trimmed[key] = \"\"\n+\n+    size = _json_char_size(trimmed)\n+    while size > max_chars and kept_keys:\n+        key = max(kept_keys, key=lambda k: len(str(trimmed.get(k, \"\"))))\n+        current = str(trimmed.get(key, \"\"))\n+        if len(current) > 0:\n+            trimmed[key] = _truncate_span_string(values.get(key, \"\"), len(current) - 1)\n+        else:\n+            kept_keys.remove(key)\n+        size = _json_char_size(trimmed)\n+\n+    if _json_char_size(trimmed) <= max_chars:\n+        return _drop_empty_string_fields(trimmed)\n+    return _drop_empty_string_fields(base)\n+\n+\n+def _merge_span_data(\n+    current: dict[str, Any],\n+    updates: dict[str, Any],\n+    max_chars: int | None,\n+) -> dict[str, Any]:\n+    merged = {**current, **updates}\n+    return _enforce_span_data_budget(merged, max_chars)\n+\n+\n+def _apply_span_updates(\n+    span: Any,\n+    updates: dict[str, Any],\n+    max_chars: int | None,\n+) -> None:\n+    # Update span data in place to keep references stable for tracing processors.\n+    current = span.span_data.data\n+    trimmed = _merge_span_data(current, updates, max_chars)\n+    current.clear()\n+    current.update(trimmed)\n+\n+\n+def _update_command_span(\n+    span: Any, item: CommandExecutionItem, span_data_max_chars: int | None\n+) -> None:\n+    updates: dict[str, Any] = {\n+        \"command\": item.command,\n+        \"status\": item.status,\n+        \"exit_code\": item.exit_code,\n+    }\n+    output = item.aggregated_output\n+    if output not in (None, \"\"):\n+        updates[\"output\"] = _truncate_span_value(output, span_data_max_chars)\n+    _apply_span_updates(\n+        span,\n+        updates,\n+        span_data_max_chars,\n+    )\n+\n+\n+def _update_mcp_tool_span(\n+    span: Any, item: McpToolCallItem, span_data_max_chars: int | None\n+) -> None:\n+    _apply_span_updates(\n+        span,\n+        {\n+            \"server\": item.server,\n+            \"tool\": item.tool,\n+            \"status\": item.status,\n+            \"arguments\": _truncate_span_value(_maybe_as_dict(item.arguments), span_data_max_chars),\n+            \"result\": _truncate_span_value(_maybe_as_dict(item.result), span_data_max_chars),\n+            \"error\": _truncate_span_value(_maybe_as_dict(item.error), span_data_max_chars),\n+        },\n+        span_data_max_chars,\n+    )\n+\n+\n+def _update_reasoning_span(span: Any, item: ReasoningItem, span_data_max_chars: int | None) -> None:\n+    _apply_span_updates(\n+        span,\n+        {\"text\": _truncate_span_value(item.text, span_data_max_chars)},\n+        span_data_max_chars,\n+    )\n+\n+\n+def _build_default_response(args: CodexToolCallArguments) -> str:\n+    input_summary = \"with inputs.\" if args.get(\"inputs\") else \"with no inputs.\"\n+    return f\"Codex task completed {input_summary}\"\n+\n+\n+def _is_command_execution_item(item: ThreadItem) -> TypeGuard[CommandExecutionItem]:\n+    return isinstance(item, CommandExecutionItem)\n+\n+\n+def _is_mcp_tool_call_item(item: ThreadItem) -> TypeGuard[McpToolCallItem]:\n+    return isinstance(item, McpToolCallItem)\n+\n+\n+def _is_reasoning_item(item: ThreadItem) -> TypeGuard[ReasoningItem]:\n+    return isinstance(item, ReasoningItem)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fcodex_tool.py",
        "sha": "925dc7d06b7c8c9b8c7fd7af4ebb69677c645159",
        "status": "added"
      },
      {
        "additions": 162,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fevents.py",
        "changes": 162,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fevents.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/events.py",
        "patch": "@@ -0,0 +1,162 @@\n+from __future__ import annotations\n+\n+from collections.abc import Mapping\n+from dataclasses import dataclass, field\n+from typing import Any, Union, cast\n+\n+from typing_extensions import Literal, TypeAlias\n+\n+from .items import ThreadItem, coerce_thread_item\n+from .payloads import _DictLike\n+\n+# Event payloads emitted by the Codex CLI JSONL stream.\n+\n+\n+@dataclass(frozen=True)\n+class ThreadStartedEvent(_DictLike):\n+    thread_id: str\n+    type: Literal[\"thread.started\"] = field(default=\"thread.started\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class TurnStartedEvent(_DictLike):\n+    type: Literal[\"turn.started\"] = field(default=\"turn.started\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class Usage(_DictLike):\n+    input_tokens: int\n+    cached_input_tokens: int\n+    output_tokens: int\n+\n+\n+@dataclass(frozen=True)\n+class TurnCompletedEvent(_DictLike):\n+    usage: Usage | None = None\n+    type: Literal[\"turn.completed\"] = field(default=\"turn.completed\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ThreadError(_DictLike):\n+    message: str\n+\n+\n+@dataclass(frozen=True)\n+class TurnFailedEvent(_DictLike):\n+    error: ThreadError\n+    type: Literal[\"turn.failed\"] = field(default=\"turn.failed\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ItemStartedEvent(_DictLike):\n+    item: ThreadItem\n+    type: Literal[\"item.started\"] = field(default=\"item.started\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ItemUpdatedEvent(_DictLike):\n+    item: ThreadItem\n+    type: Literal[\"item.updated\"] = field(default=\"item.updated\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ItemCompletedEvent(_DictLike):\n+    item: ThreadItem\n+    type: Literal[\"item.completed\"] = field(default=\"item.completed\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ThreadErrorEvent(_DictLike):\n+    message: str\n+    type: Literal[\"error\"] = field(default=\"error\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class _UnknownThreadEvent(_DictLike):\n+    type: str\n+    payload: Mapping[str, Any] = field(default_factory=dict)\n+\n+\n+ThreadEvent: TypeAlias = Union[\n+    ThreadStartedEvent,\n+    TurnStartedEvent,\n+    TurnCompletedEvent,\n+    TurnFailedEvent,\n+    ItemStartedEvent,\n+    ItemUpdatedEvent,\n+    ItemCompletedEvent,\n+    ThreadErrorEvent,\n+    _UnknownThreadEvent,\n+]\n+\n+\n+def _coerce_thread_error(raw: ThreadError | Mapping[str, Any]) -> ThreadError:\n+    if isinstance(raw, ThreadError):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"ThreadError must be a mapping.\")\n+    return ThreadError(message=cast(str, raw.get(\"message\", \"\")))\n+\n+\n+def coerce_usage(raw: Usage | Mapping[str, Any]) -> Usage:\n+    if isinstance(raw, Usage):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"Usage must be a mapping.\")\n+    return Usage(\n+        input_tokens=cast(int, raw[\"input_tokens\"]),\n+        cached_input_tokens=cast(int, raw[\"cached_input_tokens\"]),\n+        output_tokens=cast(int, raw[\"output_tokens\"]),\n+    )\n+\n+\n+def coerce_thread_event(raw: ThreadEvent | Mapping[str, Any]) -> ThreadEvent:\n+    if isinstance(raw, _DictLike):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"Thread event payload must be a mapping.\")\n+\n+    event_type = raw.get(\"type\")\n+    if event_type == \"thread.started\":\n+        return ThreadStartedEvent(thread_id=cast(str, raw[\"thread_id\"]))\n+    if event_type == \"turn.started\":\n+        return TurnStartedEvent()\n+    if event_type == \"turn.completed\":\n+        usage_raw = raw.get(\"usage\")\n+        usage = coerce_usage(cast(Mapping[str, Any], usage_raw)) if usage_raw is not None else None\n+        return TurnCompletedEvent(usage=usage)\n+    if event_type == \"turn.failed\":\n+        error_raw = raw.get(\"error\", {})\n+        error = _coerce_thread_error(cast(Mapping[str, Any], error_raw))\n+        return TurnFailedEvent(error=error)\n+    if event_type == \"item.started\":\n+        item_raw = raw.get(\"item\")\n+        item = (\n+            coerce_thread_item(cast(Union[ThreadItem, Mapping[str, Any]], item_raw))\n+            if item_raw is not None\n+            else coerce_thread_item({\"type\": \"unknown\"})\n+        )\n+        return ItemStartedEvent(item=item)\n+    if event_type == \"item.updated\":\n+        item_raw = raw.get(\"item\")\n+        item = (\n+            coerce_thread_item(cast(Union[ThreadItem, Mapping[str, Any]], item_raw))\n+            if item_raw is not None\n+            else coerce_thread_item({\"type\": \"unknown\"})\n+        )\n+        return ItemUpdatedEvent(item=item)\n+    if event_type == \"item.completed\":\n+        item_raw = raw.get(\"item\")\n+        item = (\n+            coerce_thread_item(cast(Union[ThreadItem, Mapping[str, Any]], item_raw))\n+            if item_raw is not None\n+            else coerce_thread_item({\"type\": \"unknown\"})\n+        )\n+        return ItemCompletedEvent(item=item)\n+    if event_type == \"error\":\n+        return ThreadErrorEvent(message=cast(str, raw.get(\"message\", \"\")))\n+\n+    return _UnknownThreadEvent(\n+        type=cast(str, event_type) if event_type is not None else \"unknown\",\n+        payload=dict(raw),\n+    )",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fevents.py",
        "sha": "9514a81a3c35dc160fb568d47038c659212ca085",
        "status": "added"
      },
      {
        "additions": 263,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fexec.py",
        "changes": 263,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fexec.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/exec.py",
        "patch": "@@ -0,0 +1,263 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import contextlib\n+import os\n+import platform\n+import shutil\n+import sys\n+from collections.abc import AsyncGenerator\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+from .thread_options import ApprovalMode, ModelReasoningEffort, SandboxMode, WebSearchMode\n+\n+_INTERNAL_ORIGINATOR_ENV = \"CODEX_INTERNAL_ORIGINATOR_OVERRIDE\"\n+_TYPESCRIPT_SDK_ORIGINATOR = \"codex_sdk_ts\"\n+\n+\n+@dataclass(frozen=True)\n+class CodexExecArgs:\n+    input: str\n+    base_url: str | None = None\n+    api_key: str | None = None\n+    thread_id: str | None = None\n+    images: list[str] | None = None\n+    model: str | None = None\n+    sandbox_mode: SandboxMode | None = None\n+    working_directory: str | None = None\n+    additional_directories: list[str] | None = None\n+    skip_git_repo_check: bool | None = None\n+    output_schema_file: str | None = None\n+    model_reasoning_effort: ModelReasoningEffort | None = None\n+    signal: asyncio.Event | None = None\n+    idle_timeout_seconds: float | None = None\n+    network_access_enabled: bool | None = None\n+    web_search_mode: WebSearchMode | None = None\n+    web_search_enabled: bool | None = None\n+    approval_policy: ApprovalMode | None = None\n+\n+\n+class CodexExec:\n+    def __init__(\n+        self,\n+        *,\n+        executable_path: str | None = None,\n+        env: dict[str, str] | None = None,\n+    ) -> None:\n+        self._executable_path = executable_path or find_codex_path()\n+        self._env_override = env\n+\n+    async def run(self, args: CodexExecArgs) -> AsyncGenerator[str, None]:\n+        # Build the CLI args for `codex exec --experimental-json`.\n+        command_args: list[str] = [\"exec\", \"--experimental-json\"]\n+\n+        if args.model:\n+            command_args.extend([\"--model\", args.model])\n+\n+        if args.sandbox_mode:\n+            command_args.extend([\"--sandbox\", args.sandbox_mode])\n+\n+        if args.working_directory:\n+            command_args.extend([\"--cd\", args.working_directory])\n+\n+        if args.additional_directories:\n+            for directory in args.additional_directories:\n+                command_args.extend([\"--add-dir\", directory])\n+\n+        if args.skip_git_repo_check:\n+            command_args.append(\"--skip-git-repo-check\")\n+\n+        if args.output_schema_file:\n+            command_args.extend([\"--output-schema\", args.output_schema_file])\n+\n+        if args.model_reasoning_effort:\n+            command_args.extend(\n+                [\"--config\", f'model_reasoning_effort=\"{args.model_reasoning_effort}\"']\n+            )\n+\n+        if args.network_access_enabled is not None:\n+            command_args.extend(\n+                [\n+                    \"--config\",\n+                    f\"sandbox_workspace_write.network_access={str(args.network_access_enabled).lower()}\",\n+                ]\n+            )\n+\n+        if args.web_search_mode:\n+            command_args.extend([\"--config\", f'web_search=\"{args.web_search_mode}\"'])\n+        elif args.web_search_enabled is True:\n+            command_args.extend([\"--config\", 'web_search=\"live\"'])\n+        elif args.web_search_enabled is False:\n+            command_args.extend([\"--config\", 'web_search=\"disabled\"'])\n+\n+        if args.approval_policy:\n+            command_args.extend([\"--config\", f'approval_policy=\"{args.approval_policy}\"'])\n+\n+        if args.thread_id:\n+            command_args.extend([\"resume\", args.thread_id])\n+\n+        if args.images:\n+            for image in args.images:\n+                command_args.extend([\"--image\", image])\n+\n+        # Codex CLI expects a prompt argument; \"-\" tells it to read from stdin.\n+        command_args.append(\"-\")\n+\n+        env = self._build_env(args)\n+\n+        process = await asyncio.create_subprocess_exec(\n+            self._executable_path,\n+            *command_args,\n+            stdin=asyncio.subprocess.PIPE,\n+            stdout=asyncio.subprocess.PIPE,\n+            stderr=asyncio.subprocess.PIPE,\n+            env=env,\n+        )\n+\n+        stderr_chunks: list[bytes] = []\n+\n+        async def _drain_stderr() -> None:\n+            # Preserve stderr for error reporting without blocking stdout reads.\n+            if process.stderr is None:\n+                return\n+            while True:\n+                chunk = await process.stderr.read(1024)\n+                if not chunk:\n+                    break\n+                stderr_chunks.append(chunk)\n+\n+        stderr_task = asyncio.create_task(_drain_stderr())\n+\n+        if process.stdin is None:\n+            process.kill()\n+            raise RuntimeError(\"Codex subprocess has no stdin\")\n+\n+        process.stdin.write(args.input.encode(\"utf-8\"))\n+        await process.stdin.drain()\n+        process.stdin.close()\n+\n+        if process.stdout is None:\n+            process.kill()\n+            raise RuntimeError(\"Codex subprocess has no stdout\")\n+        stdout = process.stdout\n+\n+        cancel_task: asyncio.Task[None] | None = None\n+        if args.signal is not None:\n+            # Mirror AbortSignal semantics by terminating the subprocess.\n+            cancel_task = asyncio.create_task(_watch_signal(args.signal, process))\n+\n+        async def _read_stdout_line() -> bytes:\n+            if args.idle_timeout_seconds is None:\n+                return await stdout.readline()\n+\n+            read_task: asyncio.Task[bytes] = asyncio.create_task(stdout.readline())\n+            done, _ = await asyncio.wait(\n+                {read_task}, timeout=args.idle_timeout_seconds, return_when=asyncio.FIRST_COMPLETED\n+            )\n+            if read_task in done:\n+                return read_task.result()\n+\n+            if args.signal is not None:\n+                args.signal.set()\n+            if process.returncode is None:\n+                process.terminate()\n+\n+            read_task.cancel()\n+            with contextlib.suppress(asyncio.CancelledError, asyncio.TimeoutError):\n+                await asyncio.wait_for(read_task, timeout=1)\n+\n+            raise RuntimeError(f\"Codex stream idle for {args.idle_timeout_seconds} seconds.\")\n+\n+        try:\n+            while True:\n+                line = await _read_stdout_line()\n+                if not line:\n+                    break\n+                yield line.decode(\"utf-8\").rstrip(\"\\n\")\n+\n+            await process.wait()\n+            if cancel_task is not None:\n+                cancel_task.cancel()\n+                with contextlib.suppress(asyncio.CancelledError):\n+                    await cancel_task\n+\n+            if process.returncode not in (0, None):\n+                await stderr_task\n+                stderr_text = b\"\".join(stderr_chunks).decode(\"utf-8\")\n+                raise RuntimeError(\n+                    f\"Codex exec exited with code {process.returncode}: {stderr_text}\"\n+                )\n+        finally:\n+            if cancel_task is not None and not cancel_task.done():\n+                cancel_task.cancel()\n+            await stderr_task\n+            if process.returncode is None:\n+                process.kill()\n+\n+    def _build_env(self, args: CodexExecArgs) -> dict[str, str]:\n+        # Respect env overrides when provided; otherwise copy from os.environ.\n+        env: dict[str, str] = {}\n+        if self._env_override is not None:\n+            env.update(self._env_override)\n+        else:\n+            env.update({key: value for key, value in os.environ.items() if value is not None})\n+\n+        # Preserve originator metadata used by the CLI.\n+        if _INTERNAL_ORIGINATOR_ENV not in env:\n+            env[_INTERNAL_ORIGINATOR_ENV] = _TYPESCRIPT_SDK_ORIGINATOR\n+\n+        if args.base_url:\n+            env[\"OPENAI_BASE_URL\"] = args.base_url\n+        if args.api_key:\n+            env[\"CODEX_API_KEY\"] = args.api_key\n+\n+        return env\n+\n+\n+async def _watch_signal(signal: asyncio.Event, process: asyncio.subprocess.Process) -> None:\n+    await signal.wait()\n+    if process.returncode is None:\n+        process.terminate()\n+\n+\n+def _platform_target_triple() -> str:\n+    # Map the running platform to the vendor layout used in Codex releases.\n+    system = sys.platform\n+    arch = platform.machine().lower()\n+\n+    if system.startswith(\"linux\"):\n+        if arch in {\"x86_64\", \"amd64\"}:\n+            return \"x86_64-unknown-linux-musl\"\n+        if arch in {\"aarch64\", \"arm64\"}:\n+            return \"aarch64-unknown-linux-musl\"\n+    if system == \"darwin\":\n+        if arch in {\"x86_64\", \"amd64\"}:\n+            return \"x86_64-apple-darwin\"\n+        if arch in {\"arm64\", \"aarch64\"}:\n+            return \"aarch64-apple-darwin\"\n+    if system in {\"win32\", \"cygwin\"}:\n+        if arch in {\"x86_64\", \"amd64\"}:\n+            return \"x86_64-pc-windows-msvc\"\n+        if arch in {\"arm64\", \"aarch64\"}:\n+            return \"aarch64-pc-windows-msvc\"\n+\n+    raise RuntimeError(f\"Unsupported platform: {system} ({arch})\")\n+\n+\n+def find_codex_path() -> str:\n+    # Resolution order: CODEX_PATH env, PATH lookup, bundled vendor binary.\n+    path_override = os.environ.get(\"CODEX_PATH\")\n+    if path_override:\n+        return path_override\n+\n+    which_path = shutil.which(\"codex\")\n+    if which_path:\n+        return which_path\n+\n+    target_triple = _platform_target_triple()\n+    vendor_root = Path(__file__).resolve().parent.parent.parent / \"vendor\"\n+    arch_root = vendor_root / target_triple\n+    binary_name = \"codex.exe\" if sys.platform.startswith(\"win\") else \"codex\"\n+    binary_path = arch_root / \"codex\" / binary_name\n+    return str(binary_path)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fexec.py",
        "sha": "1d82d71247e1f443b1843f2907339c0f41552d93",
        "status": "added"
      },
      {
        "additions": 245,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fitems.py",
        "changes": 245,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fitems.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/items.py",
        "patch": "@@ -0,0 +1,245 @@\n+from __future__ import annotations\n+\n+from collections.abc import Mapping\n+from dataclasses import dataclass, field\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+\n+from typing_extensions import Literal, TypeAlias, TypeGuard\n+\n+from .payloads import _DictLike\n+\n+# Item payloads are emitted inside item.* events from the Codex CLI JSONL stream.\n+\n+if TYPE_CHECKING:\n+    from mcp.types import ContentBlock as McpContentBlock\n+else:\n+    McpContentBlock = Any  # type: ignore[assignment]\n+\n+CommandExecutionStatus = Literal[\"in_progress\", \"completed\", \"failed\"]\n+PatchChangeKind = Literal[\"add\", \"delete\", \"update\"]\n+PatchApplyStatus = Literal[\"completed\", \"failed\"]\n+McpToolCallStatus = Literal[\"in_progress\", \"completed\", \"failed\"]\n+\n+\n+@dataclass(frozen=True)\n+class CommandExecutionItem(_DictLike):\n+    id: str\n+    command: str\n+    status: CommandExecutionStatus\n+    aggregated_output: str = \"\"\n+    exit_code: int | None = None\n+    type: Literal[\"command_execution\"] = field(default=\"command_execution\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class FileUpdateChange(_DictLike):\n+    path: str\n+    kind: PatchChangeKind\n+\n+\n+@dataclass(frozen=True)\n+class FileChangeItem(_DictLike):\n+    id: str\n+    changes: list[FileUpdateChange]\n+    status: PatchApplyStatus\n+    type: Literal[\"file_change\"] = field(default=\"file_change\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class McpToolCallResult(_DictLike):\n+    content: list[McpContentBlock]\n+    structured_content: Any\n+\n+\n+@dataclass(frozen=True)\n+class McpToolCallError(_DictLike):\n+    message: str\n+\n+\n+@dataclass(frozen=True)\n+class McpToolCallItem(_DictLike):\n+    id: str\n+    server: str\n+    tool: str\n+    arguments: Any\n+    status: McpToolCallStatus\n+    result: McpToolCallResult | None = None\n+    error: McpToolCallError | None = None\n+    type: Literal[\"mcp_tool_call\"] = field(default=\"mcp_tool_call\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class AgentMessageItem(_DictLike):\n+    id: str\n+    text: str\n+    type: Literal[\"agent_message\"] = field(default=\"agent_message\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ReasoningItem(_DictLike):\n+    id: str\n+    text: str\n+    type: Literal[\"reasoning\"] = field(default=\"reasoning\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class WebSearchItem(_DictLike):\n+    id: str\n+    query: str\n+    type: Literal[\"web_search\"] = field(default=\"web_search\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class ErrorItem(_DictLike):\n+    id: str\n+    message: str\n+    type: Literal[\"error\"] = field(default=\"error\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class TodoItem(_DictLike):\n+    text: str\n+    completed: bool\n+\n+\n+@dataclass(frozen=True)\n+class TodoListItem(_DictLike):\n+    id: str\n+    items: list[TodoItem]\n+    type: Literal[\"todo_list\"] = field(default=\"todo_list\", init=False)\n+\n+\n+@dataclass(frozen=True)\n+class _UnknownThreadItem(_DictLike):\n+    type: str\n+    payload: Mapping[str, Any] = field(default_factory=dict)\n+    id: str | None = None\n+\n+\n+ThreadItem: TypeAlias = Union[\n+    AgentMessageItem,\n+    ReasoningItem,\n+    CommandExecutionItem,\n+    FileChangeItem,\n+    McpToolCallItem,\n+    WebSearchItem,\n+    TodoListItem,\n+    ErrorItem,\n+    _UnknownThreadItem,\n+]\n+\n+\n+def is_agent_message_item(item: ThreadItem) -> TypeGuard[AgentMessageItem]:\n+    return isinstance(item, AgentMessageItem)\n+\n+\n+def _coerce_file_update_change(\n+    raw: FileUpdateChange | Mapping[str, Any],\n+) -> FileUpdateChange:\n+    if isinstance(raw, FileUpdateChange):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"FileUpdateChange must be a mapping.\")\n+    return FileUpdateChange(\n+        path=cast(str, raw[\"path\"]),\n+        kind=cast(PatchChangeKind, raw[\"kind\"]),\n+    )\n+\n+\n+def _coerce_mcp_tool_call_result(\n+    raw: McpToolCallResult | Mapping[str, Any],\n+) -> McpToolCallResult:\n+    if isinstance(raw, McpToolCallResult):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"McpToolCallResult must be a mapping.\")\n+    content = cast(list[McpContentBlock], raw.get(\"content\", []))\n+    return McpToolCallResult(\n+        content=content,\n+        structured_content=raw.get(\"structured_content\"),\n+    )\n+\n+\n+def _coerce_mcp_tool_call_error(\n+    raw: McpToolCallError | Mapping[str, Any],\n+) -> McpToolCallError:\n+    if isinstance(raw, McpToolCallError):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"McpToolCallError must be a mapping.\")\n+    return McpToolCallError(message=cast(str, raw.get(\"message\", \"\")))\n+\n+\n+def coerce_thread_item(raw: ThreadItem | Mapping[str, Any]) -> ThreadItem:\n+    if isinstance(raw, _DictLike):\n+        return raw\n+    if not isinstance(raw, Mapping):\n+        raise TypeError(\"Thread item payload must be a mapping.\")\n+\n+    item_type = raw.get(\"type\")\n+    if item_type == \"command_execution\":\n+        return CommandExecutionItem(\n+            id=cast(str, raw[\"id\"]),\n+            command=cast(str, raw[\"command\"]),\n+            aggregated_output=cast(str, raw.get(\"aggregated_output\", \"\")),\n+            status=cast(CommandExecutionStatus, raw[\"status\"]),\n+            exit_code=cast(Optional[int], raw.get(\"exit_code\")),\n+        )\n+    if item_type == \"file_change\":\n+        changes = [_coerce_file_update_change(change) for change in raw.get(\"changes\", [])]\n+        return FileChangeItem(\n+            id=cast(str, raw[\"id\"]),\n+            changes=changes,\n+            status=cast(PatchApplyStatus, raw[\"status\"]),\n+        )\n+    if item_type == \"mcp_tool_call\":\n+        result_raw = raw.get(\"result\")\n+        error_raw = raw.get(\"error\")\n+        result = None\n+        error = None\n+        if result_raw is not None:\n+            result = _coerce_mcp_tool_call_result(cast(Mapping[str, Any], result_raw))\n+        if error_raw is not None:\n+            error = _coerce_mcp_tool_call_error(cast(Mapping[str, Any], error_raw))\n+        return McpToolCallItem(\n+            id=cast(str, raw[\"id\"]),\n+            server=cast(str, raw[\"server\"]),\n+            tool=cast(str, raw[\"tool\"]),\n+            arguments=raw.get(\"arguments\"),\n+            status=cast(McpToolCallStatus, raw[\"status\"]),\n+            result=result,\n+            error=error,\n+        )\n+    if item_type == \"agent_message\":\n+        return AgentMessageItem(\n+            id=cast(str, raw[\"id\"]),\n+            text=cast(str, raw.get(\"text\", \"\")),\n+        )\n+    if item_type == \"reasoning\":\n+        return ReasoningItem(\n+            id=cast(str, raw[\"id\"]),\n+            text=cast(str, raw.get(\"text\", \"\")),\n+        )\n+    if item_type == \"web_search\":\n+        return WebSearchItem(\n+            id=cast(str, raw[\"id\"]),\n+            query=cast(str, raw.get(\"query\", \"\")),\n+        )\n+    if item_type == \"todo_list\":\n+        items_raw = raw.get(\"items\", [])\n+        items = [\n+            TodoItem(text=cast(str, item.get(\"text\", \"\")), completed=bool(item.get(\"completed\")))\n+            for item in cast(list[Mapping[str, Any]], items_raw)\n+        ]\n+        return TodoListItem(id=cast(str, raw[\"id\"]), items=items)\n+    if item_type == \"error\":\n+        return ErrorItem(\n+            id=cast(str, raw.get(\"id\", \"\")),\n+            message=cast(str, raw.get(\"message\", \"\")),\n+        )\n+\n+    return _UnknownThreadItem(\n+        type=cast(str, item_type) if item_type is not None else \"unknown\",\n+        payload=dict(raw),\n+        id=cast(Optional[str], raw.get(\"id\")),\n+    )",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fitems.py",
        "sha": "63d80f0dca6843d940864f56a5d264bab9e8de48",
        "status": "added"
      },
      {
        "additions": 50,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Foutput_schema_file.py",
        "changes": 50,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Foutput_schema_file.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/output_schema_file.py",
        "patch": "@@ -0,0 +1,50 @@\n+from __future__ import annotations\n+\n+import json\n+import os\n+import shutil\n+import tempfile\n+from dataclasses import dataclass\n+from typing import Any, Callable\n+\n+from agents.exceptions import UserError\n+\n+\n+@dataclass\n+class OutputSchemaFile:\n+    # Holds the on-disk schema path and cleanup callback.\n+    schema_path: str | None\n+    cleanup: Callable[[], None]\n+\n+\n+def _is_plain_json_object(schema: Any) -> bool:\n+    return isinstance(schema, dict)\n+\n+\n+def create_output_schema_file(schema: dict[str, Any] | None) -> OutputSchemaFile:\n+    \"\"\"Materialize a JSON schema into a temp file for the Codex CLI.\"\"\"\n+    if schema is None:\n+        # No schema means there is no temp file to manage.\n+        return OutputSchemaFile(schema_path=None, cleanup=lambda: None)\n+\n+    if not _is_plain_json_object(schema):\n+        raise UserError(\"output_schema must be a plain JSON object\")\n+\n+    # The Codex CLI expects a schema file path, so write to a temp directory.\n+    schema_dir = tempfile.mkdtemp(prefix=\"codex-output-schema-\")\n+    schema_path = os.path.join(schema_dir, \"schema.json\")\n+\n+    def cleanup() -> None:\n+        # Best-effort cleanup since this runs in finally blocks.\n+        try:\n+            shutil.rmtree(schema_dir, ignore_errors=True)\n+        except Exception:\n+            pass\n+\n+    try:\n+        with open(schema_path, \"w\", encoding=\"utf-8\") as handle:\n+            json.dump(schema, handle)\n+        return OutputSchemaFile(schema_path=schema_path, cleanup=cleanup)\n+    except Exception:\n+        cleanup()\n+        raise",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Foutput_schema_file.py",
        "sha": "a794bd9caad9e5ac5662936526d50f665e3812b3",
        "status": "added"
      },
      {
        "additions": 31,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fpayloads.py",
        "changes": 31,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fpayloads.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/payloads.py",
        "patch": "@@ -0,0 +1,31 @@\n+from __future__ import annotations\n+\n+import dataclasses\n+from collections.abc import Iterable\n+from typing import Any, cast\n+\n+\n+class _DictLike:\n+    def __getitem__(self, key: str) -> Any:\n+        if key in self._field_names():\n+            return getattr(self, key)\n+        raise KeyError(key)\n+\n+    def get(self, key: str, default: Any = None) -> Any:\n+        if key in self._field_names():\n+            return getattr(self, key)\n+        return default\n+\n+    def __contains__(self, key: object) -> bool:\n+        if not isinstance(key, str):\n+            return False\n+        return key in self._field_names()\n+\n+    def keys(self) -> Iterable[str]:\n+        return iter(self._field_names())\n+\n+    def as_dict(self) -> dict[str, Any]:\n+        return dataclasses.asdict(cast(Any, self))\n+\n+    def _field_names(self) -> list[str]:\n+        return [field.name for field in dataclasses.fields(cast(Any, self))]",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fpayloads.py",
        "sha": "91d54ea93fd8c6fc8052afd09c60f327c5cbafe3",
        "status": "added"
      },
      {
        "additions": 214,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fthread.py",
        "changes": 214,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fthread.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/thread.py",
        "patch": "@@ -0,0 +1,214 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import contextlib\n+from collections.abc import AsyncGenerator\n+from dataclasses import dataclass\n+from typing import Any, Union, cast\n+\n+from typing_extensions import Literal, TypeAlias, TypedDict\n+\n+from .codex_options import CodexOptions\n+from .events import (\n+    ItemCompletedEvent,\n+    ThreadError,\n+    ThreadErrorEvent,\n+    ThreadEvent,\n+    ThreadStartedEvent,\n+    TurnCompletedEvent,\n+    TurnFailedEvent,\n+    Usage,\n+    coerce_thread_event,\n+)\n+from .exec import CodexExec, CodexExecArgs\n+from .items import ThreadItem, is_agent_message_item\n+from .output_schema_file import create_output_schema_file\n+from .thread_options import ThreadOptions\n+from .turn_options import TurnOptions\n+\n+\n+@contextlib.asynccontextmanager\n+async def _aclosing(\n+    generator: AsyncGenerator[str, None],\n+) -> AsyncGenerator[AsyncGenerator[str, None], None]:\n+    try:\n+        yield generator\n+    finally:\n+        await generator.aclose()\n+\n+\n+class TextInput(TypedDict):\n+    type: Literal[\"text\"]\n+    text: str\n+\n+\n+class LocalImageInput(TypedDict):\n+    type: Literal[\"local_image\"]\n+    path: str\n+\n+\n+UserInput: TypeAlias = Union[TextInput, LocalImageInput]\n+Input: TypeAlias = Union[str, list[UserInput]]\n+\n+\n+@dataclass(frozen=True)\n+class Turn:\n+    items: list[ThreadItem]\n+    final_response: str\n+    usage: Usage | None\n+\n+\n+RunResult = Turn\n+\n+\n+@dataclass(frozen=True)\n+class StreamedTurn:\n+    events: AsyncGenerator[ThreadEvent, None]\n+\n+\n+RunStreamedResult = StreamedTurn\n+\n+\n+class Thread:\n+    def __init__(\n+        self,\n+        *,\n+        exec_client: CodexExec,\n+        options: CodexOptions,\n+        thread_options: ThreadOptions,\n+        thread_id: str | None = None,\n+    ) -> None:\n+        self._exec = exec_client\n+        self._options = options\n+        self._id = thread_id\n+        self._thread_options = thread_options\n+\n+    @property\n+    def id(self) -> str | None:\n+        return self._id\n+\n+    async def run_streamed(\n+        self, input: Input, turn_options: TurnOptions | None = None\n+    ) -> StreamedTurn:\n+        options = turn_options or TurnOptions()\n+        return StreamedTurn(events=self._run_streamed_internal(input, options))\n+\n+    async def _run_streamed_internal(\n+        self, input: Input, turn_options: TurnOptions\n+    ) -> AsyncGenerator[ThreadEvent, None]:\n+        # The Codex CLI expects an output schema file path for structured output.\n+        output_schema_file = create_output_schema_file(turn_options.output_schema)\n+        options = self._thread_options\n+        prompt, images = _normalize_input(input)\n+        idle_timeout = turn_options.idle_timeout_seconds\n+        signal = turn_options.signal\n+        if idle_timeout is not None and signal is None:\n+            signal = asyncio.Event()\n+        generator = self._exec.run(\n+            CodexExecArgs(\n+                input=prompt,\n+                base_url=self._options.base_url,\n+                api_key=self._options.api_key,\n+                thread_id=self._id,\n+                images=images,\n+                model=options.model,\n+                sandbox_mode=options.sandbox_mode,\n+                working_directory=options.working_directory,\n+                skip_git_repo_check=options.skip_git_repo_check,\n+                output_schema_file=output_schema_file.schema_path,\n+                model_reasoning_effort=options.model_reasoning_effort,\n+                signal=signal,\n+                idle_timeout_seconds=idle_timeout,\n+                network_access_enabled=options.network_access_enabled,\n+                web_search_mode=options.web_search_mode,\n+                web_search_enabled=options.web_search_enabled,\n+                approval_policy=options.approval_policy,\n+                additional_directories=list(options.additional_directories)\n+                if options.additional_directories\n+                else None,\n+            )\n+        )\n+\n+        try:\n+            async with _aclosing(generator) as stream:\n+                while True:\n+                    try:\n+                        if idle_timeout is None or isinstance(self._exec, CodexExec):\n+                            item = await stream.__anext__()\n+                        else:\n+                            item = await asyncio.wait_for(\n+                                stream.__anext__(),\n+                                timeout=idle_timeout,\n+                            )\n+                    except StopAsyncIteration:\n+                        break\n+                    except asyncio.TimeoutError as exc:\n+                        if signal is not None:\n+                            signal.set()\n+                        raise RuntimeError(\n+                            f\"Codex stream idle for {idle_timeout} seconds.\"\n+                        ) from exc\n+                    try:\n+                        parsed = _parse_event(item)\n+                    except Exception as exc:  # noqa: BLE001\n+                        raise RuntimeError(f\"Failed to parse event: {item}\") from exc\n+                    if isinstance(parsed, ThreadStartedEvent):\n+                        # Capture the thread id so callers can resume later.\n+                        self._id = parsed.thread_id\n+                    yield parsed\n+        finally:\n+            output_schema_file.cleanup()\n+\n+    async def run(self, input: Input, turn_options: TurnOptions | None = None) -> Turn:\n+        # Aggregate events into a single Turn result (matching the TS SDK behavior).\n+        options = turn_options or TurnOptions()\n+        generator = self._run_streamed_internal(input, options)\n+        items: list[ThreadItem] = []\n+        final_response = \"\"\n+        usage: Usage | None = None\n+        turn_failure: ThreadError | None = None\n+\n+        async for event in generator:\n+            if isinstance(event, ItemCompletedEvent):\n+                item = event.item\n+                if is_agent_message_item(item):\n+                    final_response = item.text\n+                items.append(item)\n+            elif isinstance(event, TurnCompletedEvent):\n+                usage = event.usage\n+            elif isinstance(event, TurnFailedEvent):\n+                turn_failure = event.error\n+                break\n+            elif isinstance(event, ThreadErrorEvent):\n+                raise RuntimeError(f\"Codex stream error: {event.message}\")\n+\n+        if turn_failure:\n+            raise RuntimeError(turn_failure.message)\n+\n+        return Turn(items=items, final_response=final_response, usage=usage)\n+\n+\n+def _normalize_input(input: Input) -> tuple[str, list[str]]:\n+    # Merge text items into a single prompt and collect image paths.\n+    if isinstance(input, str):\n+        return input, []\n+\n+    prompt_parts: list[str] = []\n+    images: list[str] = []\n+    for item in input:\n+        if item[\"type\"] == \"text\":\n+            text = item.get(\"text\", \"\")\n+            prompt_parts.append(text)\n+        elif item[\"type\"] == \"local_image\":\n+            path = item.get(\"path\", \"\")\n+            if path:\n+                images.append(path)\n+\n+    return \"\\n\\n\".join(prompt_parts), images\n+\n+\n+def _parse_event(raw: str) -> ThreadEvent:\n+    import json\n+\n+    parsed = json.loads(raw)\n+    return coerce_thread_event(cast(dict[str, Any], parsed))",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fthread.py",
        "sha": "522f6e9551559ca301aa02c95ff1894c0d34334b",
        "status": "added"
      },
      {
        "additions": 54,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fthread_options.py",
        "changes": 54,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fthread_options.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/thread_options.py",
        "patch": "@@ -0,0 +1,54 @@\n+from __future__ import annotations\n+\n+from collections.abc import Mapping, Sequence\n+from dataclasses import dataclass, fields\n+from typing import Any\n+\n+from typing_extensions import Literal\n+\n+from agents.exceptions import UserError\n+\n+ApprovalMode = Literal[\"never\", \"on-request\", \"on-failure\", \"untrusted\"]\n+SandboxMode = Literal[\"read-only\", \"workspace-write\", \"danger-full-access\"]\n+ModelReasoningEffort = Literal[\"minimal\", \"low\", \"medium\", \"high\", \"xhigh\"]\n+WebSearchMode = Literal[\"disabled\", \"cached\", \"live\"]\n+\n+\n+@dataclass(frozen=True)\n+class ThreadOptions:\n+    # Model identifier passed to the Codex CLI (--model).\n+    model: str | None = None\n+    # Sandbox permissions for filesystem/network access.\n+    sandbox_mode: SandboxMode | None = None\n+    # Working directory for the Codex CLI process.\n+    working_directory: str | None = None\n+    # Allow running outside a Git repository.\n+    skip_git_repo_check: bool | None = None\n+    # Configure model reasoning effort.\n+    model_reasoning_effort: ModelReasoningEffort | None = None\n+    # Toggle network access in sandboxed workspace writes.\n+    network_access_enabled: bool | None = None\n+    # Configure web search mode via codex config.\n+    web_search_mode: WebSearchMode | None = None\n+    # Legacy toggle for web search behavior.\n+    web_search_enabled: bool | None = None\n+    # Approval policy for tool invocations within Codex.\n+    approval_policy: ApprovalMode | None = None\n+    # Additional filesystem roots available to Codex.\n+    additional_directories: Sequence[str] | None = None\n+\n+\n+def coerce_thread_options(\n+    options: ThreadOptions | Mapping[str, Any] | None,\n+) -> ThreadOptions | None:\n+    if options is None or isinstance(options, ThreadOptions):\n+        return options\n+    if not isinstance(options, Mapping):\n+        raise UserError(\"ThreadOptions must be a ThreadOptions or a mapping.\")\n+\n+    allowed = {field.name for field in fields(ThreadOptions)}\n+    unknown = set(options.keys()) - allowed\n+    if unknown:\n+        raise UserError(f\"Unknown ThreadOptions field(s): {sorted(unknown)}\")\n+\n+    return ThreadOptions(**dict(options))",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fthread_options.py",
        "sha": "75e7882cea0706a23d7b7c662929b83cdb0917a9",
        "status": "added"
      },
      {
        "additions": 36,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fturn_options.py",
        "changes": 36,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fturn_options.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "src/agents/extensions/experimental/codex/turn_options.py",
        "patch": "@@ -0,0 +1,36 @@\n+from __future__ import annotations\n+\n+import asyncio\n+from collections.abc import Mapping\n+from dataclasses import dataclass, fields\n+from typing import Any\n+\n+from agents.exceptions import UserError\n+\n+AbortSignal = asyncio.Event\n+\n+\n+@dataclass(frozen=True)\n+class TurnOptions:\n+    # JSON schema used by Codex for structured output.\n+    output_schema: dict[str, Any] | None = None\n+    # Cancellation signal for the Codex CLI subprocess.\n+    signal: AbortSignal | None = None\n+    # Abort the Codex CLI if no events arrive within this many seconds.\n+    idle_timeout_seconds: float | None = None\n+\n+\n+def coerce_turn_options(\n+    options: TurnOptions | Mapping[str, Any] | None,\n+) -> TurnOptions | None:\n+    if options is None or isinstance(options, TurnOptions):\n+        return options\n+    if not isinstance(options, Mapping):\n+        raise UserError(\"TurnOptions must be a TurnOptions or a mapping.\")\n+\n+    allowed = {field.name for field in fields(TurnOptions)}\n+    unknown = set(options.keys()) - allowed\n+    if unknown:\n+        raise UserError(f\"Unknown TurnOptions field(s): {sorted(unknown)}\")\n+\n+    return TurnOptions(**dict(options))",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/src%2Fagents%2Fextensions%2Fexperimental%2Fcodex%2Fturn_options.py",
        "sha": "7a35f918825bb294603ad32563e98542faefd503",
        "status": "added"
      },
      {
        "additions": 644,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/tests%2Fextensions%2Fexperiemental%2Fcodex%2Ftest_codex_exec_thread.py",
        "changes": 644,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Fextensions%2Fexperiemental%2Fcodex%2Ftest_codex_exec_thread.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "tests/extensions/experiemental/codex/test_codex_exec_thread.py",
        "patch": "@@ -0,0 +1,644 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import importlib\n+import inspect\n+import json\n+import os\n+from dataclasses import fields\n+from pathlib import Path\n+from typing import Any, cast\n+\n+import pytest\n+\n+from agents.exceptions import UserError\n+from agents.extensions.experimental.codex import Usage\n+from agents.extensions.experimental.codex.codex import Codex, _normalize_env\n+from agents.extensions.experimental.codex.codex_options import CodexOptions, coerce_codex_options\n+from agents.extensions.experimental.codex.exec import CodexExec\n+from agents.extensions.experimental.codex.output_schema_file import (\n+    OutputSchemaFile,\n+    create_output_schema_file,\n+)\n+from agents.extensions.experimental.codex.thread import Thread, _normalize_input\n+from agents.extensions.experimental.codex.thread_options import ThreadOptions, coerce_thread_options\n+from agents.extensions.experimental.codex.turn_options import TurnOptions\n+\n+exec_module = importlib.import_module(\"agents.extensions.experimental.codex.exec\")\n+thread_module = importlib.import_module(\"agents.extensions.experimental.codex.thread\")\n+output_schema_module = importlib.import_module(\n+    \"agents.extensions.experimental.codex.output_schema_file\"\n+)\n+\n+\n+class FakeStdin:\n+    def __init__(self) -> None:\n+        self.buffer = b\"\"\n+        self.closed = False\n+\n+    def write(self, data: bytes) -> None:\n+        self.buffer += data\n+\n+    async def drain(self) -> None:\n+        return None\n+\n+    def close(self) -> None:\n+        self.closed = True\n+\n+\n+class FakeStdout:\n+    def __init__(self, lines: list[str]) -> None:\n+        self._lines = [line.encode(\"utf-8\") for line in lines]\n+\n+    async def readline(self) -> bytes:\n+        if not self._lines:\n+            return b\"\"\n+        return self._lines.pop(0)\n+\n+\n+class FakeStderr:\n+    def __init__(self, chunks: list[bytes]) -> None:\n+        self._chunks = list(chunks)\n+\n+    async def read(self, _size: int) -> bytes:\n+        if not self._chunks:\n+            return b\"\"\n+        return self._chunks.pop(0)\n+\n+\n+class FakeProcess:\n+    def __init__(\n+        self,\n+        stdout_lines: list[str],\n+        stderr_chunks: list[bytes] | None = None,\n+        *,\n+        returncode: int | None = 0,\n+        stdin_present: bool = True,\n+        stdout_present: bool = True,\n+        stderr_present: bool = True,\n+    ) -> None:\n+        self.stdin = FakeStdin() if stdin_present else None\n+        self.stdout = FakeStdout(stdout_lines) if stdout_present else None\n+        self.stderr = FakeStderr(stderr_chunks or []) if stderr_present else None\n+        self.returncode = returncode\n+        self.killed = False\n+        self.terminated = False\n+\n+    async def wait(self) -> None:\n+        if self.returncode is None:\n+            self.returncode = 0\n+\n+    def kill(self) -> None:\n+        self.killed = True\n+\n+    def terminate(self) -> None:\n+        self.terminated = True\n+\n+\n+class FakeExec:\n+    def __init__(self, events: list[Any], delay: float = 0.0) -> None:\n+        self.events = events\n+        self.delay = delay\n+        self.last_args: Any = None\n+\n+    async def run(self, args: Any):\n+        self.last_args = args\n+        for event in self.events:\n+            if self.delay:\n+                await asyncio.sleep(self.delay)\n+            payload = event if isinstance(event, str) else json.dumps(event)\n+            yield payload\n+\n+\n+def test_output_schema_file_none_schema() -> None:\n+    result = create_output_schema_file(None)\n+    assert result.schema_path is None\n+    result.cleanup()\n+\n+\n+def test_output_schema_file_rejects_non_object() -> None:\n+    with pytest.raises(UserError, match=\"output_schema must be a plain JSON object\"):\n+        create_output_schema_file(cast(Any, [\"not\", \"an\", \"object\"]))\n+\n+\n+def test_output_schema_file_creates_and_cleans() -> None:\n+    schema = {\"type\": \"object\", \"properties\": {\"foo\": {\"type\": \"string\"}}}\n+    result = create_output_schema_file(schema)\n+    assert result.schema_path is not None\n+    with open(result.schema_path, encoding=\"utf-8\") as handle:\n+        assert json.load(handle) == schema\n+    result.cleanup()\n+    assert not os.path.exists(result.schema_path)\n+\n+\n+def test_output_schema_file_cleanup_swallows_rmtree_errors(\n+    monkeypatch: pytest.MonkeyPatch,\n+) -> None:\n+    schema = {\"type\": \"object\"}\n+    called = False\n+\n+    def bad_rmtree(_path: str, ignore_errors: bool = True) -> None:\n+        nonlocal called\n+        called = True\n+        raise OSError(\"boom\")\n+\n+    monkeypatch.setattr(output_schema_module.shutil, \"rmtree\", bad_rmtree)\n+\n+    result = create_output_schema_file(schema)\n+    result.cleanup()\n+\n+    assert called is True\n+\n+\n+def test_output_schema_file_cleanup_on_write_error(\n+    monkeypatch: pytest.MonkeyPatch,\n+) -> None:\n+    schema = {\"type\": \"object\"}\n+    cleanup_called = False\n+\n+    def bad_dump(*_args: Any, **_kwargs: Any) -> None:\n+        raise RuntimeError(\"boom\")\n+\n+    def fake_rmtree(_path: str, ignore_errors: bool = True) -> None:\n+        nonlocal cleanup_called\n+        cleanup_called = True\n+\n+    monkeypatch.setattr(output_schema_module.json, \"dump\", bad_dump)\n+    monkeypatch.setattr(output_schema_module.shutil, \"rmtree\", fake_rmtree)\n+\n+    with pytest.raises(RuntimeError, match=\"boom\"):\n+        create_output_schema_file(schema)\n+\n+    assert cleanup_called is True\n+\n+\n+def test_normalize_input_merges_text_and_images() -> None:\n+    prompt, images = _normalize_input(\n+        [\n+            {\"type\": \"text\", \"text\": \"first\"},\n+            {\"type\": \"local_image\", \"path\": \"/tmp/a.png\"},\n+            {\"type\": \"text\", \"text\": \"second\"},\n+            {\"type\": \"local_image\", \"path\": \"\"},\n+        ]\n+    )\n+    assert prompt == \"first\\n\\nsecond\"\n+    assert images == [\"/tmp/a.png\"]\n+\n+\n+def test_normalize_env_stringifies_values() -> None:\n+    env = _normalize_env(CodexOptions(env=cast(dict[str, str], {\"FOO\": 1, 2: \"bar\"})))\n+    assert env == {\"FOO\": \"1\", \"2\": \"bar\"}\n+\n+\n+def test_coerce_codex_options_rejects_unknown_fields() -> None:\n+    with pytest.raises(UserError, match=\"Unknown CodexOptions field\"):\n+        coerce_codex_options({\"unknown\": \"value\"})\n+\n+\n+def test_coerce_thread_options_rejects_unknown_fields() -> None:\n+    with pytest.raises(UserError, match=\"Unknown ThreadOptions field\"):\n+        coerce_thread_options({\"unknown\": \"value\"})\n+\n+\n+def test_codex_start_and_resume_thread() -> None:\n+    codex = Codex(CodexOptions(codex_path_override=\"/bin/codex\"))\n+    thread = codex.start_thread({\"model\": \"gpt\"})\n+    assert thread.id is None\n+    resumed = codex.resume_thread(\"thread-1\", {\"model\": \"gpt\"})\n+    assert resumed.id == \"thread-1\"\n+\n+\n+def test_codex_init_accepts_mapping_options() -> None:\n+    codex = Codex({\"codex_path_override\": \"/bin/codex\"})\n+    assert codex._exec._executable_path == \"/bin/codex\"\n+\n+\n+def test_codex_init_accepts_kwargs() -> None:\n+    codex = Codex(codex_path_override=\"/bin/codex\", base_url=\"https://example.com\")\n+    assert codex._exec._executable_path == \"/bin/codex\"\n+    assert codex._options.base_url == \"https://example.com\"\n+\n+\n+def test_codex_init_rejects_options_and_kwargs() -> None:\n+    with pytest.raises(UserError, match=\"Codex options must be provided\"):\n+        Codex(  # type: ignore[call-overload]\n+            cast(Any, CodexOptions()), codex_path_override=\"/bin/codex\"\n+        )\n+\n+\n+def test_codex_init_kw_matches_codex_options() -> None:\n+    signature = inspect.signature(Codex.__init__)\n+    kw_only = [\n+        param.name\n+        for param in signature.parameters.values()\n+        if param.kind == inspect.Parameter.KEYWORD_ONLY\n+    ]\n+    option_fields = [field.name for field in fields(CodexOptions)]\n+    assert kw_only == option_fields\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_exec_run_builds_command_args_and_env(monkeypatch: pytest.MonkeyPatch) -> None:\n+    captured: dict[str, Any] = {}\n+    process = FakeProcess(stdout_lines=[\"line-1\\n\", \"line-2\\n\"])\n+\n+    async def fake_create_subprocess_exec(*args: Any, **kwargs: Any) -> FakeProcess:\n+        captured[\"args\"] = args\n+        captured[\"kwargs\"] = kwargs\n+        return process\n+\n+    monkeypatch.setattr(exec_module.asyncio, \"create_subprocess_exec\", fake_create_subprocess_exec)\n+\n+    exec_client = exec_module.CodexExec(executable_path=\"/bin/codex\", env={\"FOO\": \"bar\"})\n+    args = exec_module.CodexExecArgs(\n+        input=\"hello\",\n+        base_url=\"https://example.com\",\n+        api_key=\"api-key\",\n+        thread_id=\"thread-123\",\n+        images=[\"/tmp/img.png\"],\n+        model=\"gpt-4.1-mini\",\n+        sandbox_mode=\"read-only\",\n+        working_directory=\"/work\",\n+        additional_directories=[\"/extra-a\", \"/extra-b\"],\n+        skip_git_repo_check=True,\n+        output_schema_file=\"/tmp/schema.json\",\n+        model_reasoning_effort=\"high\",\n+        network_access_enabled=True,\n+        web_search_mode=\"live\",\n+        approval_policy=\"on-request\",\n+    )\n+\n+    output = [line async for line in exec_client.run(args)]\n+\n+    assert output == [\"line-1\", \"line-2\"]\n+    assert process.stdin is not None\n+    assert process.stdin.buffer == b\"hello\"\n+    assert process.stdin.closed is True\n+\n+    assert captured[\"args\"][0] == \"/bin/codex\"\n+    assert list(captured[\"args\"][1:]) == [\n+        \"exec\",\n+        \"--experimental-json\",\n+        \"--model\",\n+        \"gpt-4.1-mini\",\n+        \"--sandbox\",\n+        \"read-only\",\n+        \"--cd\",\n+        \"/work\",\n+        \"--add-dir\",\n+        \"/extra-a\",\n+        \"--add-dir\",\n+        \"/extra-b\",\n+        \"--skip-git-repo-check\",\n+        \"--output-schema\",\n+        \"/tmp/schema.json\",\n+        \"--config\",\n+        'model_reasoning_effort=\"high\"',\n+        \"--config\",\n+        \"sandbox_workspace_write.network_access=true\",\n+        \"--config\",\n+        'web_search=\"live\"',\n+        \"--config\",\n+        'approval_policy=\"on-request\"',\n+        \"resume\",\n+        \"thread-123\",\n+        \"--image\",\n+        \"/tmp/img.png\",\n+        \"-\",\n+    ]\n+\n+    env = captured[\"kwargs\"][\"env\"]\n+    assert env[\"FOO\"] == \"bar\"\n+    assert env[exec_module._INTERNAL_ORIGINATOR_ENV] == exec_module._TYPESCRIPT_SDK_ORIGINATOR\n+    assert env[\"OPENAI_BASE_URL\"] == \"https://example.com\"\n+    assert env[\"CODEX_API_KEY\"] == \"api-key\"\n+\n+\n+@pytest.mark.asyncio\n+@pytest.mark.parametrize(\n+    (\"enabled\", \"expected_config\"),\n+    [\n+        (True, 'web_search=\"live\"'),\n+        (False, 'web_search=\"disabled\"'),\n+    ],\n+)\n+async def test_codex_exec_run_web_search_enabled_flags(\n+    monkeypatch: pytest.MonkeyPatch, enabled: bool, expected_config: str\n+) -> None:\n+    captured: dict[str, Any] = {}\n+    process = FakeProcess(stdout_lines=[])\n+\n+    async def fake_create_subprocess_exec(*args: Any, **kwargs: Any) -> FakeProcess:\n+        captured[\"args\"] = args\n+        return process\n+\n+    monkeypatch.setattr(exec_module.asyncio, \"create_subprocess_exec\", fake_create_subprocess_exec)\n+\n+    exec_client = exec_module.CodexExec(executable_path=\"/bin/codex\")\n+    args = exec_module.CodexExecArgs(input=\"hello\", web_search_enabled=enabled)\n+\n+    _ = [line async for line in exec_client.run(args)]\n+    command_args = list(captured[\"args\"][1:])\n+    assert \"--config\" in command_args\n+    assert expected_config in command_args\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_exec_run_raises_on_non_zero_exit(\n+    monkeypatch: pytest.MonkeyPatch,\n+) -> None:\n+    process = FakeProcess(stdout_lines=[], stderr_chunks=[b\"bad\"], returncode=2)\n+\n+    async def fake_create_subprocess_exec(*args: Any, **kwargs: Any) -> FakeProcess:\n+        return process\n+\n+    monkeypatch.setattr(exec_module.asyncio, \"create_subprocess_exec\", fake_create_subprocess_exec)\n+\n+    exec_client = exec_module.CodexExec(executable_path=\"/bin/codex\")\n+    args = exec_module.CodexExecArgs(input=\"hello\")\n+\n+    with pytest.raises(RuntimeError, match=\"exited with code 2\"):\n+        async for _ in exec_client.run(args):\n+            pass\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_exec_run_raises_without_stdin(monkeypatch: pytest.MonkeyPatch) -> None:\n+    process = FakeProcess(stdout_lines=[], stdin_present=False)\n+\n+    async def fake_create_subprocess_exec(*args: Any, **kwargs: Any) -> FakeProcess:\n+        return process\n+\n+    monkeypatch.setattr(exec_module.asyncio, \"create_subprocess_exec\", fake_create_subprocess_exec)\n+\n+    exec_client = exec_module.CodexExec(executable_path=\"/bin/codex\")\n+    args = exec_module.CodexExecArgs(input=\"hello\")\n+\n+    with pytest.raises(RuntimeError, match=\"no stdin\"):\n+        async for _ in exec_client.run(args):\n+            pass\n+    assert process.killed is True\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_exec_run_raises_without_stdout(monkeypatch: pytest.MonkeyPatch) -> None:\n+    process = FakeProcess(stdout_lines=[], stdout_present=False)\n+\n+    async def fake_create_subprocess_exec(*args: Any, **kwargs: Any) -> FakeProcess:\n+        return process\n+\n+    monkeypatch.setattr(exec_module.asyncio, \"create_subprocess_exec\", fake_create_subprocess_exec)\n+\n+    exec_client = exec_module.CodexExec(executable_path=\"/bin/codex\")\n+    args = exec_module.CodexExecArgs(input=\"hello\")\n+\n+    with pytest.raises(RuntimeError, match=\"no stdout\"):\n+        async for _ in exec_client.run(args):\n+            pass\n+    assert process.killed is True\n+\n+\n+@pytest.mark.asyncio\n+async def test_watch_signal_terminates_process() -> None:\n+    signal = asyncio.Event()\n+    process = FakeProcess(stdout_lines=[], returncode=None)\n+\n+    task = asyncio.create_task(exec_module._watch_signal(signal, process))\n+    signal.set()\n+    await task\n+\n+    assert process.terminated is True\n+\n+\n+@pytest.mark.parametrize(\n+    (\"system\", \"arch\", \"expected\"),\n+    [\n+        (\"linux\", \"x86_64\", \"x86_64-unknown-linux-musl\"),\n+        (\"linux\", \"aarch64\", \"aarch64-unknown-linux-musl\"),\n+        (\"darwin\", \"x86_64\", \"x86_64-apple-darwin\"),\n+        (\"darwin\", \"arm64\", \"aarch64-apple-darwin\"),\n+        (\"win32\", \"x86_64\", \"x86_64-pc-windows-msvc\"),\n+        (\"win32\", \"arm64\", \"aarch64-pc-windows-msvc\"),\n+    ],\n+)\n+def test_platform_target_triple_mapping(\n+    monkeypatch: pytest.MonkeyPatch, system: str, arch: str, expected: str\n+) -> None:\n+    monkeypatch.setattr(exec_module.sys, \"platform\", system)\n+    monkeypatch.setattr(exec_module.platform, \"machine\", lambda: arch)\n+    assert exec_module._platform_target_triple() == expected\n+\n+\n+def test_platform_target_triple_unsupported(monkeypatch: pytest.MonkeyPatch) -> None:\n+    monkeypatch.setattr(exec_module.sys, \"platform\", \"solaris\")\n+    monkeypatch.setattr(exec_module.platform, \"machine\", lambda: \"sparc\")\n+    with pytest.raises(RuntimeError, match=\"Unsupported platform\"):\n+        exec_module._platform_target_triple()\n+\n+\n+def test_find_codex_path_env_override(monkeypatch: pytest.MonkeyPatch) -> None:\n+    monkeypatch.setenv(\"CODEX_PATH\", \"/custom/codex\")\n+    assert exec_module.find_codex_path() == \"/custom/codex\"\n+\n+\n+def test_find_codex_path_uses_shutil_which(monkeypatch: pytest.MonkeyPatch) -> None:\n+    monkeypatch.delenv(\"CODEX_PATH\", raising=False)\n+    monkeypatch.setattr(exec_module.shutil, \"which\", lambda _name: \"/usr/local/bin/codex\")\n+    assert exec_module.find_codex_path() == \"/usr/local/bin/codex\"\n+\n+\n+def test_find_codex_path_fallback(monkeypatch: pytest.MonkeyPatch) -> None:\n+    monkeypatch.delenv(\"CODEX_PATH\", raising=False)\n+    monkeypatch.setattr(exec_module.shutil, \"which\", lambda _name: None)\n+    monkeypatch.setattr(exec_module, \"_platform_target_triple\", lambda: \"dummy-triple\")\n+    monkeypatch.setattr(exec_module.sys, \"platform\", \"linux\")\n+    result = exec_module.find_codex_path()\n+    expected_root = (\n+        Path(cast(str, exec_module.__file__)).resolve().parent.parent.parent\n+        / \"vendor\"\n+        / \"dummy-triple\"\n+        / \"codex\"\n+        / \"codex\"\n+    )\n+    assert result == str(expected_root)\n+\n+\n+@pytest.mark.asyncio\n+async def test_thread_run_streamed_passes_options_and_updates_id(\n+    monkeypatch: pytest.MonkeyPatch,\n+) -> None:\n+    events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-42\"},\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+    fake_exec = FakeExec(events)\n+    options = CodexOptions(base_url=\"https://example.com\", api_key=\"api-key\")\n+    thread_options = ThreadOptions(\n+        model=\"gpt-4.1-mini\",\n+        sandbox_mode=\"read-only\",\n+        working_directory=\"/work\",\n+        skip_git_repo_check=True,\n+        model_reasoning_effort=\"low\",\n+        network_access_enabled=False,\n+        web_search_mode=\"cached\",\n+        approval_policy=\"on-request\",\n+        additional_directories=[\"/extra\"],\n+    )\n+    thread = Thread(\n+        exec_client=cast(CodexExec, fake_exec),\n+        options=options,\n+        thread_options=thread_options,\n+    )\n+    cleanup_called = False\n+\n+    def fake_create_output_schema_file(schema: dict[str, Any] | None) -> OutputSchemaFile:\n+        nonlocal cleanup_called\n+\n+        def cleanup() -> None:\n+            nonlocal cleanup_called\n+            cleanup_called = True\n+\n+        return OutputSchemaFile(schema_path=\"/tmp/schema.json\", cleanup=cleanup)\n+\n+    monkeypatch.setattr(thread_module, \"create_output_schema_file\", fake_create_output_schema_file)\n+\n+    streamed = await thread.run_streamed(\n+        [\n+            {\"type\": \"text\", \"text\": \"hello\"},\n+            {\"type\": \"local_image\", \"path\": \"/tmp/a.png\"},\n+        ],\n+        TurnOptions(output_schema={\"type\": \"object\"}),\n+    )\n+    collected = [event async for event in streamed.events]\n+\n+    assert collected[0].type == \"thread.started\"\n+    assert thread.id == \"thread-42\"\n+    assert cleanup_called is True\n+\n+    assert fake_exec.last_args is not None\n+    assert fake_exec.last_args.output_schema_file == \"/tmp/schema.json\"\n+    assert fake_exec.last_args.model == \"gpt-4.1-mini\"\n+    assert fake_exec.last_args.sandbox_mode == \"read-only\"\n+    assert fake_exec.last_args.working_directory == \"/work\"\n+    assert fake_exec.last_args.skip_git_repo_check is True\n+    assert fake_exec.last_args.model_reasoning_effort == \"low\"\n+    assert fake_exec.last_args.network_access_enabled is False\n+    assert fake_exec.last_args.web_search_mode == \"cached\"\n+    assert fake_exec.last_args.approval_policy == \"on-request\"\n+    assert fake_exec.last_args.additional_directories == [\"/extra\"]\n+    assert fake_exec.last_args.images == [\"/tmp/a.png\"]\n+\n+\n+@pytest.mark.asyncio\n+async def test_thread_run_aggregates_items_and_usage() -> None:\n+    events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"done\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 2, \"cached_input_tokens\": 1, \"output_tokens\": 3},\n+        },\n+    ]\n+    thread = Thread(\n+        exec_client=cast(CodexExec, FakeExec(events)),\n+        options=CodexOptions(),\n+        thread_options=ThreadOptions(),\n+    )\n+    result = await thread.run(\"hello\")\n+\n+    assert result.final_response == \"done\"\n+    assert result.usage == Usage(\n+        input_tokens=2,\n+        cached_input_tokens=1,\n+        output_tokens=3,\n+    )\n+    assert len(result.items) == 1\n+\n+\n+@pytest.mark.asyncio\n+async def test_thread_run_raises_on_failure() -> None:\n+    events = [\n+        {\"type\": \"turn.failed\", \"error\": {\"message\": \"boom\"}},\n+    ]\n+    thread = Thread(\n+        exec_client=cast(CodexExec, FakeExec(events)),\n+        options=CodexOptions(),\n+        thread_options=ThreadOptions(),\n+    )\n+    with pytest.raises(RuntimeError, match=\"boom\"):\n+        await thread.run(\"hello\")\n+\n+\n+@pytest.mark.asyncio\n+async def test_thread_run_raises_on_stream_error() -> None:\n+    events = [\n+        {\"type\": \"error\", \"message\": \"boom\"},\n+    ]\n+    thread = Thread(\n+        exec_client=cast(CodexExec, FakeExec(events)),\n+        options=CodexOptions(),\n+        thread_options=ThreadOptions(),\n+    )\n+    with pytest.raises(RuntimeError, match=\"Codex stream error: boom\"):\n+        await thread.run(\"hello\")\n+\n+\n+@pytest.mark.asyncio\n+async def test_thread_run_streamed_raises_on_parse_error(\n+    monkeypatch: pytest.MonkeyPatch,\n+) -> None:\n+    events = [\"not-json\"]\n+    fake_exec = FakeExec(events)\n+    thread = Thread(\n+        exec_client=cast(CodexExec, fake_exec),\n+        options=CodexOptions(),\n+        thread_options=ThreadOptions(),\n+    )\n+\n+    def fake_create_output_schema_file(schema: dict[str, Any] | None) -> OutputSchemaFile:\n+        return OutputSchemaFile(schema_path=None, cleanup=lambda: None)\n+\n+    monkeypatch.setattr(thread_module, \"create_output_schema_file\", fake_create_output_schema_file)\n+\n+    streamed = await thread.run_streamed(\"hello\")\n+    with pytest.raises(RuntimeError, match=\"Failed to parse event\"):\n+        async for _ in streamed.events:\n+            pass\n+\n+\n+@pytest.mark.asyncio\n+async def test_thread_run_streamed_idle_timeout_sets_signal(\n+    monkeypatch: pytest.MonkeyPatch,\n+) -> None:\n+    events = [\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        }\n+    ]\n+    fake_exec = FakeExec(events, delay=0.2)\n+    thread = Thread(\n+        exec_client=cast(CodexExec, fake_exec),\n+        options=CodexOptions(),\n+        thread_options=ThreadOptions(),\n+    )\n+    signal = asyncio.Event()\n+\n+    def fake_create_output_schema_file(schema: dict[str, Any] | None) -> OutputSchemaFile:\n+        return OutputSchemaFile(schema_path=None, cleanup=lambda: None)\n+\n+    monkeypatch.setattr(thread_module, \"create_output_schema_file\", fake_create_output_schema_file)\n+\n+    with pytest.raises(RuntimeError, match=\"Codex stream idle for\"):\n+        async for _ in thread._run_streamed_internal(\n+            \"hello\", TurnOptions(signal=signal, idle_timeout_seconds=0.01)\n+        ):\n+            pass\n+\n+    assert signal.is_set() is True",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/tests%2Fextensions%2Fexperiemental%2Fcodex%2Ftest_codex_exec_thread.py",
        "sha": "aafc1908ba253814c8d98b383fa789c30aec21a5",
        "status": "added"
      },
      {
        "additions": 1119,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/117af2bf982842b122d1f109be18f764b6c81b37/tests%2Fextensions%2Fexperiemental%2Fcodex%2Ftest_codex_tool.py",
        "changes": 1119,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Fextensions%2Fexperiemental%2Fcodex%2Ftest_codex_tool.py?ref=117af2bf982842b122d1f109be18f764b6c81b37",
        "deletions": 0,
        "filename": "tests/extensions/experiemental/codex/test_codex_tool.py",
        "patch": "@@ -0,0 +1,1119 @@\n+from __future__ import annotations\n+\n+import importlib\n+import inspect\n+import json\n+from dataclasses import fields\n+from types import SimpleNamespace\n+from typing import Any, cast\n+\n+import pytest\n+from pydantic import BaseModel, ConfigDict\n+\n+from agents.exceptions import ModelBehaviorError, UserError\n+from agents.extensions.experimental.codex import (\n+    Codex,\n+    CodexToolOptions,\n+    CodexToolResult,\n+    CodexToolStreamEvent,\n+    Usage,\n+    codex_tool,\n+)\n+from agents.extensions.experimental.codex.codex_tool import CodexToolInputItem\n+from agents.run_context import RunContextWrapper\n+from agents.tool_context import ToolContext\n+from agents.tracing import function_span, trace\n+from tests.testing_processor import SPAN_PROCESSOR_TESTING\n+\n+codex_tool_module = importlib.import_module(\"agents.extensions.experimental.codex.codex_tool\")\n+\n+\n+class CodexMockState:\n+    def __init__(self) -> None:\n+        self.events: list[dict[str, Any]] = []\n+        self.thread_id = \"thread-1\"\n+        self.last_turn_options: Any = None\n+        self.start_calls = 0\n+        self.resume_calls = 0\n+        self.options: Any = None\n+\n+\n+class FakeThread:\n+    def __init__(self, state: CodexMockState) -> None:\n+        self._state = state\n+        self.id: str | None = None\n+\n+    async def run_streamed(self, _input: Any, turn_options: Any = None) -> Any:\n+        self._state.last_turn_options = turn_options\n+        self.id = self._state.thread_id\n+\n+        async def event_stream() -> Any:\n+            for event in self._state.events:\n+                yield event\n+\n+        return SimpleNamespace(events=event_stream())\n+\n+\n+class FakeCodex:\n+    def __init__(self, state: CodexMockState, options: Any = None) -> None:\n+        self._state = state\n+        self._state.options = options\n+\n+    def start_thread(self, _options: Any = None) -> FakeThread:\n+        self._state.start_calls += 1\n+        return FakeThread(self._state)\n+\n+    def resume_thread(self, _thread_id: str, _options: Any = None) -> FakeThread:\n+        self._state.resume_calls += 1\n+        return FakeThread(self._state)\n+\n+\n+def test_codex_tool_kw_matches_codex_tool_options() -> None:\n+    signature = inspect.signature(codex_tool)\n+    kw_only = [\n+        param.name\n+        for param in signature.parameters.values()\n+        if param.kind == inspect.Parameter.KEYWORD_ONLY\n+    ]\n+    option_fields = [field.name for field in fields(CodexToolOptions)]\n+    assert kw_only == option_fields\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_streams_events_and_updates_usage() -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\"type\": \"turn.started\"},\n+        {\n+            \"type\": \"item.started\",\n+            \"item\": {\"id\": \"reason-1\", \"type\": \"reasoning\", \"text\": \"Initial reasoning\"},\n+        },\n+        {\n+            \"type\": \"item.updated\",\n+            \"item\": {\"id\": \"reason-1\", \"type\": \"reasoning\", \"text\": \"Refined reasoning\"},\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"reason-1\", \"type\": \"reasoning\", \"text\": \"Final reasoning\"},\n+        },\n+        {\n+            \"type\": \"item.started\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"pytest\",\n+                \"aggregated_output\": \"\",\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.updated\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"pytest\",\n+                \"aggregated_output\": \"Running tests\",\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"pytest\",\n+                \"aggregated_output\": \"All good\",\n+                \"exit_code\": 0,\n+                \"status\": \"completed\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.started\",\n+            \"item\": {\n+                \"id\": \"mcp-1\",\n+                \"type\": \"mcp_tool_call\",\n+                \"server\": \"gitmcp\",\n+                \"tool\": \"search_codex_code\",\n+                \"arguments\": {\"query\": \"foo\"},\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.updated\",\n+            \"item\": {\n+                \"id\": \"mcp-1\",\n+                \"type\": \"mcp_tool_call\",\n+                \"server\": \"gitmcp\",\n+                \"tool\": \"search_codex_code\",\n+                \"arguments\": {\"query\": \"foo\"},\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\n+                \"id\": \"mcp-1\",\n+                \"type\": \"mcp_tool_call\",\n+                \"server\": \"gitmcp\",\n+                \"tool\": \"search_codex_code\",\n+                \"arguments\": {\"query\": \"foo\"},\n+                \"status\": \"completed\",\n+                \"result\": {\"content\": [], \"structured_content\": None},\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex finished.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 10, \"cached_input_tokens\": 1, \"output_tokens\": 5},\n+        },\n+    ]\n+\n+    tool = codex_tool(CodexToolOptions(codex=cast(Codex, FakeCodex(state))))\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Diagnose failure\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    with trace(\"codex-test\"):\n+        with function_span(tool.name):\n+            result = await tool.on_invoke_tool(context, input_json)\n+\n+    assert isinstance(result, CodexToolResult)\n+    assert result.thread_id == \"thread-1\"\n+    assert result.response == \"Codex finished.\"\n+    assert result.usage == Usage(\n+        input_tokens=10,\n+        cached_input_tokens=1,\n+        output_tokens=5,\n+    )\n+\n+    assert context.usage.total_tokens == 15\n+    assert context.usage.requests == 1\n+\n+    spans = SPAN_PROCESSOR_TESTING.get_ordered_spans()\n+    function_span_obj = next(\n+        span\n+        for span in spans\n+        if span.span_data.type == \"function\" and span.span_data.name == tool.name\n+    )\n+\n+    custom_spans = [span for span in spans if span.span_data.type == \"custom\"]\n+    assert len(custom_spans) == 3\n+\n+    for span in custom_spans:\n+        assert span.parent_id == function_span_obj.span_id\n+\n+    reasoning_span = next(span for span in custom_spans if span.span_data.name == \"Codex reasoning\")\n+    assert reasoning_span.span_data.data[\"text\"] == \"Final reasoning\"\n+\n+    command_span = next(\n+        span for span in custom_spans if span.span_data.name == \"Codex command execution\"\n+    )\n+    assert command_span.span_data.data[\"command\"] == \"pytest\"\n+    assert command_span.span_data.data[\"status\"] == \"completed\"\n+    assert command_span.span_data.data[\"output\"] == \"All good\"\n+    assert command_span.span_data.data[\"exit_code\"] == 0\n+\n+    mcp_span = next(span for span in custom_spans if span.span_data.name == \"Codex MCP tool call\")\n+    assert mcp_span.span_data.data[\"server\"] == \"gitmcp\"\n+    assert mcp_span.span_data.data[\"tool\"] == \"search_codex_code\"\n+    assert mcp_span.span_data.data[\"status\"] == \"completed\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_keeps_command_output_when_completed_missing_output() -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.started\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"ls\",\n+                \"aggregated_output\": \"\",\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.updated\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"ls\",\n+                \"aggregated_output\": \"first output\",\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"ls\",\n+                \"exit_code\": 0,\n+                \"status\": \"completed\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex finished.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    tool = codex_tool(CodexToolOptions(codex=cast(Codex, FakeCodex(state))))\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"List files\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    with trace(\"codex-test\"):\n+        with function_span(tool.name):\n+            await tool.on_invoke_tool(context, input_json)\n+\n+    spans = SPAN_PROCESSOR_TESTING.get_ordered_spans()\n+    command_span = next(span for span in spans if span.span_data.name == \"Codex command execution\")\n+\n+    assert command_span.span_data.data[\"output\"] == \"first output\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_defaults_to_openai_api_key(monkeypatch: pytest.MonkeyPatch) -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    monkeypatch.setenv(\"OPENAI_API_KEY\", \"openai-key\")\n+    monkeypatch.delenv(\"CODEX_API_KEY\", raising=False)\n+\n+    class CaptureCodex(FakeCodex):\n+        def __init__(self, options: Any = None) -> None:\n+            super().__init__(state, options)\n+\n+    monkeypatch.setattr(codex_tool_module, \"Codex\", CaptureCodex)\n+\n+    tool = codex_tool()\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Check default api key\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    await tool.on_invoke_tool(context, input_json)\n+\n+    assert state.options is not None\n+    assert getattr(state.options, \"api_key\", None) == \"openai-key\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_accepts_codex_options_dict(monkeypatch: pytest.MonkeyPatch) -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    class CaptureCodex(FakeCodex):\n+        def __init__(self, options: Any = None) -> None:\n+            super().__init__(state, options)\n+\n+    monkeypatch.setattr(codex_tool_module, \"Codex\", CaptureCodex)\n+\n+    tool = codex_tool({\"codex_options\": {\"api_key\": \"from-options\"}})\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Check dict options\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    await tool.on_invoke_tool(context, input_json)\n+\n+    assert state.options is not None\n+    assert getattr(state.options, \"api_key\", None) == \"from-options\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_accepts_output_schema_descriptor() -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    descriptor = {\n+        \"title\": \"Summary\",\n+        \"properties\": [\n+            {\n+                \"name\": \"summary\",\n+                \"description\": \"Short summary\",\n+                \"schema\": {\"type\": \"string\", \"description\": \"Summary field\"},\n+            }\n+        ],\n+    }\n+\n+    tool = codex_tool(\n+        CodexToolOptions(codex=cast(Codex, FakeCodex(state)), output_schema=descriptor)\n+    )\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Check schema\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    await tool.on_invoke_tool(context, input_json)\n+\n+    output_schema = state.last_turn_options.output_schema\n+    assert output_schema[\"type\"] == \"object\"\n+    assert output_schema[\"additionalProperties\"] is False\n+    assert output_schema[\"properties\"][\"summary\"][\"type\"] == \"string\"\n+    assert output_schema[\"properties\"][\"summary\"][\"description\"] == \"Short summary\"\n+    assert output_schema[\"required\"] == []\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_accepts_dict_options() -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    options_dict: dict[str, Any] = {\n+        \"codex\": cast(Codex, FakeCodex(state)),\n+        \"sandbox_mode\": \"read-only\",\n+    }\n+\n+    tool = codex_tool(options_dict)\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Check dict options\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    result = await tool.on_invoke_tool(context, input_json)\n+\n+    assert isinstance(result, CodexToolResult)\n+    assert result.response == \"Codex done.\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_accepts_keyword_options(monkeypatch: pytest.MonkeyPatch) -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    class CaptureCodex(FakeCodex):\n+        def __init__(self, options: Any = None) -> None:\n+            super().__init__(state, options)\n+\n+    monkeypatch.setattr(codex_tool_module, \"Codex\", CaptureCodex)\n+\n+    tool = codex_tool(name=\"codex-keyword\", codex_options={\"api_key\": \"from-kwargs\"})\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Check keyword options\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    await tool.on_invoke_tool(context, input_json)\n+\n+    assert tool.name == \"codex-keyword\"\n+    assert state.options is not None\n+    assert getattr(state.options, \"api_key\", None) == \"from-kwargs\"\n+\n+\n+def test_codex_tool_truncates_span_values() -> None:\n+    value = {\"payload\": \"x\" * 200}\n+    truncated = codex_tool_module._truncate_span_value(value, 40)\n+\n+    assert isinstance(truncated, dict)\n+    assert truncated[\"truncated\"] is True\n+    assert truncated[\"original_length\"] > 40\n+    preview = truncated[\"preview\"]\n+    assert isinstance(preview, str)\n+    assert len(preview) <= 40\n+\n+\n+def test_codex_tool_enforces_span_data_budget() -> None:\n+    data = {\n+        \"command\": \"run\",\n+        \"output\": \"x\" * 5000,\n+        \"arguments\": {\"payload\": \"y\" * 5000},\n+    }\n+    trimmed = codex_tool_module._enforce_span_data_budget(data, 512)\n+\n+    assert \"command\" in trimmed\n+    assert trimmed[\"command\"]\n+    assert \"output\" in trimmed\n+    assert \"arguments\" in trimmed\n+    assert codex_tool_module._json_char_size(trimmed) <= 512\n+\n+\n+def test_codex_tool_keeps_output_preview_with_budget() -> None:\n+    data = {\"output\": \"x\" * 1000}\n+    trimmed = codex_tool_module._enforce_span_data_budget(data, 120)\n+\n+    assert \"output\" in trimmed\n+    assert isinstance(trimmed[\"output\"], str)\n+    assert trimmed[\"output\"]\n+    assert codex_tool_module._json_char_size(trimmed) <= 120\n+\n+\n+def test_codex_tool_prioritizes_arguments_over_large_results() -> None:\n+    data = {\"arguments\": {\"foo\": \"bar\"}, \"result\": \"x\" * 2000}\n+    trimmed = codex_tool_module._enforce_span_data_budget(data, 200)\n+\n+    assert trimmed[\"arguments\"] == codex_tool_module._stringify_span_value({\"foo\": \"bar\"})\n+    assert \"result\" in trimmed\n+    assert codex_tool_module._json_char_size(trimmed) <= 200\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_passes_idle_timeout_seconds() -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    tool = codex_tool(\n+        CodexToolOptions(\n+            codex=cast(Codex, FakeCodex(state)),\n+            default_turn_options={\"idle_timeout_seconds\": 3.5},\n+        )\n+    )\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"Check timeout option\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    await tool.on_invoke_tool(context, input_json)\n+\n+    assert state.last_turn_options is not None\n+    assert state.last_turn_options.idle_timeout_seconds == 3.5\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_persists_session() -> None:\n+    state = CodexMockState()\n+    state.events = [\n+        {\"type\": \"thread.started\", \"thread_id\": \"thread-1\"},\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"Codex done.\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    tool = codex_tool(\n+        CodexToolOptions(\n+            codex=cast(Codex, FakeCodex(state)),\n+            persist_session=True,\n+        )\n+    )\n+    input_json = '{\"inputs\": [{\"type\": \"text\", \"text\": \"First call\", \"path\": \"\"}]}'\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    await tool.on_invoke_tool(context, input_json)\n+    await tool.on_invoke_tool(context, input_json)\n+\n+    assert state.start_calls == 1\n+    assert state.resume_calls == 0\n+\n+\n+@pytest.mark.parametrize(\n+    (\"payload\", \"message\"),\n+    [\n+        ({\"type\": \"text\", \"text\": \"\", \"path\": \"\"}, 'non-empty \"text\"'),\n+        ({\"type\": \"text\", \"text\": \"hello\", \"path\": \"x\"}, '\"path\" is not allowed'),\n+        ({\"type\": \"local_image\", \"path\": \"\"}, 'non-empty \"path\"'),\n+        ({\"type\": \"local_image\", \"path\": \"img.png\", \"text\": \"hi\"}, '\"text\" is not allowed'),\n+    ],\n+)\n+def test_codex_tool_input_item_validation_errors(payload: dict[str, Any], message: str) -> None:\n+    with pytest.raises(ValueError, match=message):\n+        codex_tool_module.CodexToolInputItem(**payload)\n+\n+\n+def test_codex_tool_result_stringifies() -> None:\n+    result = CodexToolResult(thread_id=\"thread-1\", response=\"ok\", usage=None)\n+    assert json.loads(str(result)) == result.as_dict()\n+\n+\n+def test_codex_tool_parse_input_rejects_invalid_json() -> None:\n+    with pytest.raises(ModelBehaviorError, match=\"Invalid JSON input for codex tool\"):\n+        codex_tool_module._parse_tool_input(codex_tool_module.CodexToolParameters, \"{bad\")\n+\n+\n+def test_codex_tool_normalize_parameters_requires_inputs() -> None:\n+    class Dummy(BaseModel):\n+        model_config = ConfigDict(extra=\"forbid\")\n+\n+    with pytest.raises(UserError, match=\"must include an inputs field\"):\n+        codex_tool_module._normalize_parameters(Dummy())\n+\n+\n+def test_codex_tool_coerce_options_rejects_unknown_fields() -> None:\n+    with pytest.raises(UserError, match=\"Unknown Codex tool option\"):\n+        codex_tool_module._coerce_tool_options({\"unknown\": \"value\"})\n+\n+\n+def test_codex_tool_resolve_output_schema_validation_errors() -> None:\n+    with pytest.raises(UserError, match=\"must include properties\"):\n+        codex_tool_module._resolve_output_schema({\"properties\": []})\n+    with pytest.raises(UserError, match=\"Invalid schema for output property\"):\n+        codex_tool_module._resolve_output_schema(\n+            {\"properties\": [{\"name\": \"bad\", \"schema\": {\"type\": \"bogus\"}}]}\n+        )\n+    with pytest.raises(UserError, match=\"Required property\"):\n+        codex_tool_module._resolve_output_schema(\n+            {\n+                \"properties\": [{\"name\": \"name\", \"schema\": {\"type\": \"string\"}}],\n+                \"required\": [\"missing\"],\n+            }\n+        )\n+    with pytest.raises(UserError, match='type \"object\"'):\n+        codex_tool_module._resolve_output_schema({\"type\": \"string\"})\n+\n+\n+def test_codex_tool_resolve_output_schema_descriptor() -> None:\n+    descriptor = {\n+        \"title\": \"Report\",\n+        \"description\": \"Structured output\",\n+        \"properties\": [\n+            {\n+                \"name\": \"tags\",\n+                \"description\": \"Tag list\",\n+                \"schema\": {\n+                    \"type\": \"array\",\n+                    \"description\": \"Tags array\",\n+                    \"items\": {\"type\": \"string\", \"description\": \"Tag value\"},\n+                },\n+            },\n+            {\n+                \"name\": \"summary\",\n+                \"description\": \"Summary text\",\n+                \"schema\": {\"type\": \"string\"},\n+            },\n+        ],\n+        \"required\": [\"tags\"],\n+    }\n+    schema = codex_tool_module._resolve_output_schema(descriptor)\n+    assert schema[\"title\"] == \"Report\"\n+    assert schema[\"description\"] == \"Structured output\"\n+    assert schema[\"properties\"][\"tags\"][\"type\"] == \"array\"\n+    assert schema[\"properties\"][\"tags\"][\"description\"] == \"Tag list\"\n+    assert schema[\"properties\"][\"tags\"][\"items\"][\"description\"] == \"Tag value\"\n+    assert schema[\"properties\"][\"tags\"][\"items\"][\"type\"] == \"string\"\n+    assert schema[\"required\"] == [\"tags\"]\n+\n+\n+def test_codex_tool_resolve_codex_options_reads_env_override() -> None:\n+    options = codex_tool_module.CodexOptions(\n+        codex_path_override=\"/bin/codex\",\n+        env={\"CODEX_API_KEY\": \"env-key\"},\n+    )\n+    resolved = codex_tool_module._resolve_codex_options(options)\n+    assert resolved is not None\n+    assert resolved.api_key == \"env-key\"\n+    assert resolved.codex_path_override == \"/bin/codex\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_create_codex_resolver_caches_instance() -> None:\n+    options = codex_tool_module.CodexOptions(codex_path_override=\"/bin/codex\")\n+    resolver = codex_tool_module._create_codex_resolver(None, options)\n+    first = await resolver()\n+    second = await resolver()\n+    assert first is second\n+\n+\n+def test_codex_tool_resolve_thread_options_merges_values() -> None:\n+    resolved = codex_tool_module._resolve_thread_options(\n+        {\"model\": \"gpt-4.1-mini\"},\n+        sandbox_mode=\"read-only\",\n+        working_directory=\"/work\",\n+        skip_git_repo_check=True,\n+    )\n+    assert resolved is not None\n+    assert resolved.model == \"gpt-4.1-mini\"\n+    assert resolved.sandbox_mode == \"read-only\"\n+    assert resolved.working_directory == \"/work\"\n+    assert resolved.skip_git_repo_check is True\n+\n+\n+def test_codex_tool_resolve_thread_options_empty_is_none() -> None:\n+    assert codex_tool_module._resolve_thread_options(None, None, None, None) is None\n+\n+\n+def test_codex_tool_build_turn_options_merges_output_schema() -> None:\n+    output_schema = {\"type\": \"object\", \"properties\": {}, \"additionalProperties\": False}\n+    turn = codex_tool_module._build_turn_options(None, output_schema)\n+    assert turn.output_schema == output_schema\n+\n+    turn_defaults = codex_tool_module.TurnOptions(\n+        output_schema={\"type\": \"object\", \"properties\": {\"x\": {\"type\": \"string\"}}},\n+        idle_timeout_seconds=1.0,\n+    )\n+    turn = codex_tool_module._build_turn_options(turn_defaults, None)\n+    assert turn.output_schema == turn_defaults.output_schema\n+    assert turn.idle_timeout_seconds == 1.0\n+\n+\n+def test_codex_tool_persisted_thread_mismatch_raises() -> None:\n+    class DummyThread:\n+        def __init__(self, thread_id: str) -> None:\n+            self.id = thread_id\n+\n+    with pytest.raises(UserError, match=\"already has an active thread\"):\n+        codex_tool_module._get_or_create_persisted_thread(\n+            codex=object(),\n+            thread_id=\"thread-2\",\n+            thread_options=None,\n+            existing_thread=DummyThread(\"thread-1\"),\n+        )\n+\n+\n+def test_codex_tool_default_response_text() -> None:\n+    assert (\n+        codex_tool_module._build_default_response({\"inputs\": None})\n+        == \"Codex task completed with no inputs.\"\n+    )\n+\n+\n+def test_codex_tool_input_item_accepts_local_image() -> None:\n+    item = codex_tool_module.CodexToolInputItem(type=\"local_image\", path=\" /tmp/img.png \")\n+    assert item.path == \"/tmp/img.png\"\n+    assert item.text is None\n+\n+\n+def test_codex_tool_normalize_parameters_handles_local_image() -> None:\n+    params = codex_tool_module.CodexToolParameters(\n+        inputs=[\n+            codex_tool_module.CodexToolInputItem(type=\"text\", text=\"hello\"),\n+            codex_tool_module.CodexToolInputItem(type=\"local_image\", path=\"/tmp/img.png\"),\n+        ]\n+    )\n+    normalized = codex_tool_module._normalize_parameters(params)\n+    assert normalized[\"inputs\"] == [\n+        {\"type\": \"text\", \"text\": \"hello\"},\n+        {\"type\": \"local_image\", \"path\": \"/tmp/img.png\"},\n+    ]\n+\n+\n+def test_codex_tool_build_codex_input_empty() -> None:\n+    assert codex_tool_module._build_codex_input({\"inputs\": None}) == \"\"\n+\n+\n+def test_codex_tool_truncate_span_string_limits() -> None:\n+    assert codex_tool_module._truncate_span_string(\"hello\", 0) == \"\"\n+    long_value = \"x\" * 100\n+    assert codex_tool_module._truncate_span_string(long_value, 3) == \"xxx\"\n+\n+\n+def test_codex_tool_truncate_span_value_handles_circular_reference() -> None:\n+    value: list[Any] = []\n+    value.append(value)\n+    truncated = codex_tool_module._truncate_span_value(value, 1)\n+    assert isinstance(truncated, dict)\n+    assert truncated[\"truncated\"] is True\n+\n+\n+def test_codex_tool_enforce_span_data_budget_zero_max() -> None:\n+    assert codex_tool_module._enforce_span_data_budget({\"output\": \"x\"}, 0) == {}\n+\n+\n+def test_codex_tool_enforce_span_data_budget_trims_values_when_budget_tight() -> None:\n+    data = {\"command\": \"run\", \"output\": \"x\" * 50, \"arguments\": \"y\" * 50}\n+    base = {\"command\": \"run\", \"output\": \"\", \"arguments\": \"\"}\n+    max_chars = codex_tool_module._json_char_size(base) + 1\n+    trimmed = codex_tool_module._enforce_span_data_budget(data, max_chars)\n+    assert codex_tool_module._json_char_size(trimmed) <= max_chars\n+    assert \"command\" in trimmed\n+    assert \"output\" in trimmed\n+    assert \"arguments\" in trimmed\n+\n+\n+def test_codex_tool_enforce_span_data_budget_drops_until_base_fits() -> None:\n+    data = {\"command\": \"run\", \"output\": \"x\" * 50}\n+    base = {\"command\": \"\", \"output\": \"\"}\n+    max_chars = codex_tool_module._json_char_size(base) - 1\n+    trimmed = codex_tool_module._enforce_span_data_budget(data, max_chars)\n+    assert not (\"command\" in trimmed and \"output\" in trimmed)\n+\n+\n+def test_codex_tool_handle_item_started_ignores_missing_id() -> None:\n+    spans: dict[str, Any] = {}\n+    codex_tool_module._handle_item_started({\"type\": \"reasoning\", \"text\": \"hi\"}, spans, None)\n+    assert spans == {}\n+\n+\n+def test_codex_tool_handle_item_updated_ignores_missing_span() -> None:\n+    codex_tool_module._handle_item_updated(\n+        {\"id\": \"missing\", \"type\": \"reasoning\", \"text\": \"hi\"}, {}, None\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_on_invoke_tool_handles_failure_error_function_sync() -> None:\n+    def failure_error_function(_ctx: RunContextWrapper[Any], _exc: Exception) -> str:\n+        return \"handled\"\n+\n+    tool = codex_tool(CodexToolOptions(failure_error_function=failure_error_function))\n+    input_json = \"{bad\"\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    result = await tool.on_invoke_tool(context, input_json)\n+    assert result == \"handled\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_on_invoke_tool_handles_failure_error_function_async() -> None:\n+    async def failure_error_function(_ctx: RunContextWrapper[Any], _exc: Exception) -> str:\n+        return \"handled-async\"\n+\n+    tool = codex_tool(CodexToolOptions(failure_error_function=failure_error_function))\n+    input_json = \"{bad\"\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    result = await tool.on_invoke_tool(context, input_json)\n+    assert result == \"handled-async\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_on_invoke_tool_raises_without_failure_handler() -> None:\n+    tool = codex_tool(CodexToolOptions(failure_error_function=None))\n+    input_json = \"{bad\"\n+    context = ToolContext(\n+        context=None,\n+        tool_name=tool.name,\n+        tool_call_id=\"call-1\",\n+        tool_arguments=input_json,\n+    )\n+\n+    with pytest.raises(ModelBehaviorError):\n+        await tool.on_invoke_tool(context, input_json)\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_consume_events_with_on_stream_error() -> None:\n+    events = [\n+        {\n+            \"type\": \"item.started\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"ls\",\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\n+                \"id\": \"cmd-1\",\n+                \"type\": \"command_execution\",\n+                \"command\": \"ls\",\n+                \"status\": \"completed\",\n+                \"exit_code\": 0,\n+            },\n+        },\n+        {\n+            \"type\": \"item.started\",\n+            \"item\": {\n+                \"id\": \"mcp-1\",\n+                \"type\": \"mcp_tool_call\",\n+                \"server\": \"server\",\n+                \"tool\": \"tool\",\n+                \"arguments\": {\"q\": \"x\"},\n+                \"status\": \"in_progress\",\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\n+                \"id\": \"mcp-1\",\n+                \"type\": \"mcp_tool_call\",\n+                \"server\": \"server\",\n+                \"tool\": \"tool\",\n+                \"arguments\": {\"q\": \"x\"},\n+                \"status\": \"failed\",\n+                \"error\": {\"message\": \"boom\"},\n+            },\n+        },\n+        {\n+            \"type\": \"item.completed\",\n+            \"item\": {\"id\": \"agent-1\", \"type\": \"agent_message\", \"text\": \"done\"},\n+        },\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        },\n+    ]\n+\n+    async def event_stream():\n+        for event in events:\n+            yield event\n+\n+    callbacks: list[str] = []\n+\n+    def on_stream(payload: CodexToolStreamEvent) -> None:\n+        callbacks.append(payload.event.type)\n+        if payload.event.type == \"item.started\":\n+            raise RuntimeError(\"boom\")\n+\n+    context = ToolContext(\n+        context=None,\n+        tool_name=\"codex\",\n+        tool_call_id=\"call-1\",\n+        tool_arguments=\"{}\",\n+    )\n+\n+    with trace(\"codex-test\"):\n+        response, usage = await codex_tool_module._consume_events(\n+            event_stream(),\n+            {\"inputs\": [{\"type\": \"text\", \"text\": \"hello\"}]},\n+            context,\n+            SimpleNamespace(id=\"thread-1\"),\n+            on_stream,\n+            64,\n+        )\n+\n+    assert response == \"done\"\n+    assert usage == Usage(input_tokens=1, cached_input_tokens=0, output_tokens=1)\n+    assert \"item.started\" in callbacks\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_consume_events_default_response() -> None:\n+    events = [\n+        {\n+            \"type\": \"turn.completed\",\n+            \"usage\": {\"input_tokens\": 1, \"cached_input_tokens\": 0, \"output_tokens\": 1},\n+        }\n+    ]\n+\n+    async def event_stream():\n+        for event in events:\n+            yield event\n+\n+    context = ToolContext(\n+        context=None,\n+        tool_name=\"codex\",\n+        tool_call_id=\"call-1\",\n+        tool_arguments=\"{}\",\n+    )\n+\n+    response, usage = await codex_tool_module._consume_events(\n+        event_stream(),\n+        {\"inputs\": [{\"type\": \"text\", \"text\": \"hello\"}]},\n+        context,\n+        SimpleNamespace(id=\"thread-1\"),\n+        None,\n+        None,\n+    )\n+\n+    assert response == \"Codex task completed with inputs.\"\n+    assert usage == Usage(input_tokens=1, cached_input_tokens=0, output_tokens=1)\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_consume_events_turn_failed() -> None:\n+    events = [{\"type\": \"turn.failed\", \"error\": {\"message\": \"boom\"}}]\n+\n+    async def event_stream():\n+        for event in events:\n+            yield event\n+\n+    context = ToolContext(\n+        context=None,\n+        tool_name=\"codex\",\n+        tool_call_id=\"call-1\",\n+        tool_arguments=\"{}\",\n+    )\n+\n+    with pytest.raises(UserError, match=\"Codex turn failed: boom\"):\n+        await codex_tool_module._consume_events(\n+            event_stream(),\n+            {\"inputs\": [{\"type\": \"text\", \"text\": \"hello\"}]},\n+            context,\n+            SimpleNamespace(id=\"thread-1\"),\n+            None,\n+            None,\n+        )\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_consume_events_error_event() -> None:\n+    events = [{\"type\": \"error\", \"message\": \"boom\"}]\n+\n+    async def event_stream():\n+        for event in events:\n+            yield event\n+\n+    context = ToolContext(\n+        context=None,\n+        tool_name=\"codex\",\n+        tool_call_id=\"call-1\",\n+        tool_arguments=\"{}\",\n+    )\n+\n+    with pytest.raises(UserError, match=\"Codex stream error\"):\n+        await codex_tool_module._consume_events(\n+            event_stream(),\n+            {\"inputs\": [{\"type\": \"text\", \"text\": \"hello\"}]},\n+            context,\n+            SimpleNamespace(id=\"thread-1\"),\n+            None,\n+            None,\n+        )\n+\n+\n+@pytest.mark.asyncio\n+async def test_codex_tool_create_codex_resolver_with_provided() -> None:\n+    state = CodexMockState()\n+    provided = cast(Codex, FakeCodex(state))\n+    resolver = codex_tool_module._create_codex_resolver(provided, None)\n+    resolved = await resolver()\n+    assert resolved is provided\n+\n+\n+def test_codex_tool_build_turn_options_overrides_schema() -> None:\n+    output_schema = {\"type\": \"object\", \"properties\": {}, \"additionalProperties\": False}\n+    turn_defaults = codex_tool_module.TurnOptions(\n+        output_schema={\"type\": \"object\", \"properties\": {\"x\": {\"type\": \"string\"}}},\n+        idle_timeout_seconds=1.0,\n+    )\n+    turn = codex_tool_module._build_turn_options(turn_defaults, output_schema)\n+    assert turn.output_schema == output_schema\n+\n+\n+def test_codex_tool_resolve_codex_options_reads_env(monkeypatch: pytest.MonkeyPatch) -> None:\n+    monkeypatch.setenv(\"CODEX_API_KEY\", \"env-key\")\n+    monkeypatch.delenv(\"OPENAI_API_KEY\", raising=False)\n+\n+    resolved = codex_tool_module._resolve_codex_options(None)\n+    assert resolved is not None\n+    assert resolved.api_key == \"env-key\"\n+\n+\n+def test_codex_tool_accepts_all_keyword_overrides() -> None:\n+    state = CodexMockState()\n+\n+    class CustomParams(BaseModel):\n+        inputs: list[CodexToolInputItem]\n+\n+        model_config = ConfigDict(extra=\"forbid\")\n+\n+    tool = codex_tool(\n+        CodexToolOptions(codex=cast(Codex, FakeCodex(state))),\n+        name=\"codex-overrides\",\n+        description=\"desc\",\n+        parameters=CustomParams,\n+        output_schema={\"type\": \"object\", \"properties\": {}, \"additionalProperties\": False},\n+        codex=cast(Codex, FakeCodex(state)),\n+        codex_options={\"api_key\": \"from-kwargs\"},\n+        default_thread_options={\"model\": \"gpt\"},\n+        thread_id=\"thread-1\",\n+        sandbox_mode=\"read-only\",\n+        working_directory=\"/work\",\n+        skip_git_repo_check=True,\n+        default_turn_options={\"idle_timeout_seconds\": 1.0},\n+        span_data_max_chars=10,\n+        persist_session=True,\n+        on_stream=lambda _payload: None,\n+        is_enabled=False,\n+        failure_error_function=lambda _ctx, _exc: \"handled\",\n+    )\n+\n+    assert tool.name == \"codex-overrides\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/117af2bf982842b122d1f109be18f764b6c81b37/tests%2Fextensions%2Fexperiemental%2Fcodex%2Ftest_codex_tool.py",
        "sha": "a54e13854f82a3578b07dfd8f5df594252a9c7c6",
        "status": "added"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:57:35.705691Z"
}
