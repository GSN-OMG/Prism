{
  "finished_at": "2026-01-20T04:58:03.303488Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "c82396bc2c2b13de",
    "tag": "rest_pr_files_pr2328_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2328/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "19158",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:58:03 GMT",
      "etag": "\"92d830db02e8db10c5db3264343310d7d861e76d888abc2e896a9274a76d0466\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Mon, 19 Jan 2026 03:22:49 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DD01:3782ED:162EB07:1F3AC06:696F0B5A",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4932",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "68",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 31,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c63270458baf7a5caea180c999dfda795501c15f/src%2Fagents%2Fmodels%2Fchatcmpl_converter.py",
        "changes": 32,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmodels%2Fchatcmpl_converter.py?ref=c63270458baf7a5caea180c999dfda795501c15f",
        "deletions": 1,
        "filename": "src/agents/models/chatcmpl_converter.py",
        "patch": "@@ -428,15 +428,20 @@ def items_to_messages(\n         result: list[ChatCompletionMessageParam] = []\n         current_assistant_msg: ChatCompletionAssistantMessageParam | None = None\n         pending_thinking_blocks: list[dict[str, str]] | None = None\n+        pending_reasoning_content: str | None = None  # For DeepSeek reasoning_content\n \n         def flush_assistant_message() -> None:\n-            nonlocal current_assistant_msg\n+            nonlocal current_assistant_msg, pending_reasoning_content\n             if current_assistant_msg is not None:\n                 # The API doesn't support empty arrays for tool_calls\n                 if not current_assistant_msg.get(\"tool_calls\"):\n                     del current_assistant_msg[\"tool_calls\"]\n+                    # prevents stale reasoning_content from contaminating later turns\n+                    pending_reasoning_content = None\n                 result.append(current_assistant_msg)\n                 current_assistant_msg = None\n+            else:\n+                pending_reasoning_content = None\n \n         def ensure_assistant_message() -> ChatCompletionAssistantMessageParam:\n             nonlocal current_assistant_msg, pending_thinking_blocks\n@@ -579,6 +584,11 @@ def ensure_assistant_message() -> ChatCompletionAssistantMessageParam:\n             elif func_call := cls.maybe_function_tool_call(item):\n                 asst = ensure_assistant_message()\n \n+                # If we have pending reasoning content for DeepSeek, add it to the assistant message\n+                if pending_reasoning_content:\n+                    asst[\"reasoning_content\"] = pending_reasoning_content  # type: ignore[typeddict-unknown-key]\n+                    pending_reasoning_content = None  # Clear after using\n+\n                 # If we have pending thinking blocks, use them as the content\n                 # This is required for Anthropic API tool calls with interleaved thinking\n                 if pending_thinking_blocks:\n@@ -687,6 +697,26 @@ def ensure_assistant_message() -> ChatCompletionAssistantMessageParam:\n                     # This preserves the original behavior\n                     pending_thinking_blocks = reconstructed_thinking_blocks\n \n+                # DeepSeek requires reasoning_content field in assistant messages with tool calls\n+                # Items may not all originate from DeepSeek, so need to check for model match.\n+                # For backward compatibility, if provider_data is missing, ignore the check.\n+                elif (\n+                    model\n+                    and \"deepseek\" in model.lower()\n+                    and (\n+                        (item_model and \"deepseek\" in item_model.lower())\n+                        or item_provider_data == {}\n+                    )\n+                ):\n+                    summary_items = reasoning_item.get(\"summary\", [])\n+                    if summary_items:\n+                        reasoning_texts = []\n+                        for summary_item in summary_items:\n+                            if isinstance(summary_item, dict) and summary_item.get(\"text\"):\n+                                reasoning_texts.append(summary_item[\"text\"])\n+                        if reasoning_texts:\n+                            pending_reasoning_content = \"\\n\".join(reasoning_texts)\n+\n             # 8) compaction items => reject for chat completions\n             elif isinstance(item, dict) and item.get(\"type\") == \"compaction\":\n                 raise UserError(",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c63270458baf7a5caea180c999dfda795501c15f/src%2Fagents%2Fmodels%2Fchatcmpl_converter.py",
        "sha": "7444507ab3ec51ef53b48f89d787d0f95fa8e551",
        "status": "modified"
      },
      {
        "additions": 361,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/c63270458baf7a5caea180c999dfda795501c15f/tests%2Fmodels%2Ftest_deepseek_reasoning_content.py",
        "changes": 361,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Fmodels%2Ftest_deepseek_reasoning_content.py?ref=c63270458baf7a5caea180c999dfda795501c15f",
        "deletions": 0,
        "filename": "tests/models/test_deepseek_reasoning_content.py",
        "patch": "@@ -0,0 +1,361 @@\n+from typing import Any\n+\n+import litellm\n+import pytest\n+from litellm.types.utils import (\n+    ChatCompletionMessageToolCall,\n+    Choices,\n+    Function,\n+    Message,\n+    ModelResponse,\n+    Usage,\n+)\n+\n+from agents.extensions.models.litellm_model import LitellmModel\n+from agents.model_settings import ModelSettings\n+from agents.models.chatcmpl_converter import Converter\n+from agents.models.interface import ModelTracing\n+\n+\n+@pytest.mark.allow_call_model_methods\n+@pytest.mark.asyncio\n+async def test_deepseek_reasoning_content_preserved_in_tool_calls(monkeypatch):\n+    \"\"\"\n+    Ensure DeepSeek reasoning_content is preserved when converting items to messages.\n+\n+    DeepSeek requires reasoning_content field in assistant messages with tool_calls.\n+    This test verifies that reasoning content from reasoning items is correctly\n+    extracted and added to assistant messages during conversion.\n+    \"\"\"\n+    # Capture the messages sent to the model\n+    captured_calls: list[dict[str, Any]] = []\n+\n+    async def fake_acompletion(model, messages=None, **kwargs):\n+        captured_calls.append({\"model\": model, \"messages\": messages, **kwargs})\n+\n+        # First call: model returns reasoning_content + tool_call\n+        if len(captured_calls) == 1:\n+            tool_call = ChatCompletionMessageToolCall(\n+                id=\"call_123\",\n+                type=\"function\",\n+                function=Function(name=\"get_weather\", arguments='{\"city\": \"Tokyo\"}'),\n+            )\n+            msg = Message(\n+                role=\"assistant\",\n+                content=None,\n+                tool_calls=[tool_call],\n+            )\n+            # DeepSeek adds reasoning_content to the message\n+            msg.reasoning_content = \"Let me think about getting the weather for Tokyo...\"\n+\n+            choice = Choices(index=0, message=msg)\n+            return ModelResponse(choices=[choice], usage=Usage(100, 50, 150))\n+\n+        # Second call: model returns final response\n+        msg = Message(role=\"assistant\", content=\"The weather in Tokyo is sunny.\")\n+        choice = Choices(index=0, message=msg)\n+        return ModelResponse(choices=[choice], usage=Usage(100, 50, 150))\n+\n+    monkeypatch.setattr(litellm, \"acompletion\", fake_acompletion)\n+\n+    model = LitellmModel(model=\"deepseek/deepseek-reasoner\")\n+\n+    # First call: get the tool call response\n+    first_response = await model.get_response(\n+        system_instructions=\"You are a helpful assistant.\",\n+        input=\"What's the weather in Tokyo?\",\n+        model_settings=ModelSettings(),\n+        tools=[],  # We'll simulate the tool response manually\n+        output_schema=None,\n+        handoffs=[],\n+        tracing=ModelTracing.DISABLED,\n+    )\n+\n+    assert len(first_response.output) >= 1\n+\n+    input_items: list[Any] = []\n+    input_items.append({\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"})\n+\n+    for item in first_response.output:\n+        if hasattr(item, \"model_dump\"):\n+            input_items.append(item.model_dump())\n+        else:\n+            input_items.append(item)\n+\n+    input_items.append(\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call_123\",\n+            \"output\": \"The weather in Tokyo is sunny.\",\n+        }\n+    )\n+\n+    messages = Converter.items_to_messages(\n+        input_items,\n+        model=\"deepseek/deepseek-reasoner\",\n+    )\n+\n+    assistant_messages_with_tool_calls = [\n+        m\n+        for m in messages\n+        if isinstance(m, dict) and m.get(\"role\") == \"assistant\" and m.get(\"tool_calls\")\n+    ]\n+\n+    assert len(assistant_messages_with_tool_calls) > 0\n+    assistant_msg = assistant_messages_with_tool_calls[0]\n+    assert \"reasoning_content\" in assistant_msg\n+\n+\n+@pytest.mark.allow_call_model_methods\n+@pytest.mark.asyncio\n+async def test_deepseek_reasoning_content_in_multi_turn_conversation(monkeypatch):\n+    \"\"\"\n+    Verify reasoning_content is included in assistant messages during multi-turn conversations.\n+\n+    When DeepSeek returns reasoning_content with tool_calls, subsequent API calls must\n+    include the reasoning_content field in the assistant message to avoid 400 errors.\n+    \"\"\"\n+    captured_calls: list[dict[str, Any]] = []\n+\n+    async def fake_acompletion(model, messages=None, **kwargs):\n+        captured_calls.append({\"model\": model, \"messages\": messages, **kwargs})\n+\n+        # First call: model returns reasoning_content + tool_call\n+        if len(captured_calls) == 1:\n+            tool_call = ChatCompletionMessageToolCall(\n+                id=\"call_weather_123\",\n+                type=\"function\",\n+                function=Function(name=\"get_weather\", arguments='{\"city\": \"Tokyo\"}'),\n+            )\n+            msg = Message(\n+                role=\"assistant\",\n+                content=None,\n+                tool_calls=[tool_call],\n+            )\n+            # DeepSeek adds reasoning_content\n+            msg.reasoning_content = \"I need to get the weather for Tokyo first.\"\n+            choice = Choices(index=0, message=msg)\n+            return ModelResponse(choices=[choice], usage=Usage(100, 50, 150))\n+\n+        # Second call: check if reasoning_content was in the request\n+        # In real DeepSeek API, this would fail with 400 if reasoning_content is missing\n+        msg = Message(\n+            role=\"assistant\", content=\"Based on my findings, the weather in Tokyo is sunny.\"\n+        )\n+        choice = Choices(index=0, message=msg)\n+        return ModelResponse(choices=[choice], usage=Usage(100, 50, 150))\n+\n+    monkeypatch.setattr(litellm, \"acompletion\", fake_acompletion)\n+\n+    model = LitellmModel(model=\"deepseek/deepseek-reasoner\")\n+\n+    # First call\n+    first_response = await model.get_response(\n+        system_instructions=\"You are a helpful assistant.\",\n+        input=\"What's the weather in Tokyo?\",\n+        model_settings=ModelSettings(),\n+        tools=[],\n+        output_schema=None,\n+        handoffs=[],\n+        tracing=ModelTracing.DISABLED,\n+    )\n+\n+    input_items: list[Any] = []\n+    input_items.append({\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"})\n+\n+    for item in first_response.output:\n+        if hasattr(item, \"model_dump\"):\n+            input_items.append(item.model_dump())\n+        else:\n+            input_items.append(item)\n+\n+    input_items.append(\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call_weather_123\",\n+            \"output\": \"The weather in Tokyo is sunny and 22°C.\",\n+        }\n+    )\n+\n+    await model.get_response(\n+        system_instructions=\"You are a helpful assistant.\",\n+        input=input_items,\n+        model_settings=ModelSettings(),\n+        tools=[],\n+        output_schema=None,\n+        handoffs=[],\n+        tracing=ModelTracing.DISABLED,\n+    )\n+\n+    assert len(captured_calls) == 2\n+\n+    second_call_messages = captured_calls[1][\"messages\"]\n+\n+    assistant_with_tools = None\n+    for msg in second_call_messages:\n+        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n+            assistant_with_tools = msg\n+            break\n+\n+    assert assistant_with_tools is not None\n+    assert \"reasoning_content\" in assistant_with_tools\n+\n+\n+def test_deepseek_reasoning_content_with_openai_chatcompletions_path():\n+    \"\"\"\n+    Verify reasoning_content works when using OpenAIChatCompletionsModel.\n+\n+    This ensures the fix works for both LiteLLM and OpenAI ChatCompletions code paths.\n+    \"\"\"\n+    from agents.models.chatcmpl_converter import Converter\n+\n+    input_items: list[Any] = [\n+        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"},\n+        {\n+            \"id\": \"__fake_id__\",\n+            \"summary\": [{\"text\": \"I need to check the weather in Paris.\", \"type\": \"summary_text\"}],\n+            \"type\": \"reasoning\",\n+            \"content\": None,\n+            \"encrypted_content\": None,\n+            \"status\": None,\n+            \"provider_data\": {\"model\": \"deepseek-reasoner\", \"response_id\": \"chatcmpl-test\"},\n+        },\n+        {\n+            \"arguments\": '{\"city\": \"Paris\"}',\n+            \"call_id\": \"call_weather_456\",\n+            \"name\": \"get_weather\",\n+            \"type\": \"function_call\",\n+            \"id\": \"__fake_id__\",\n+            \"status\": None,\n+            \"provider_data\": {\"model\": \"deepseek-reasoner\"},\n+        },\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call_weather_456\",\n+            \"output\": \"The weather in Paris is cloudy and 15°C.\",\n+        },\n+    ]\n+\n+    messages = Converter.items_to_messages(\n+        input_items,\n+        model=\"deepseek-reasoner\",\n+    )\n+\n+    assistant_with_tools = None\n+    for msg in messages:\n+        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n+            assistant_with_tools = msg\n+            break\n+\n+    assert assistant_with_tools is not None\n+    assert \"reasoning_content\" in assistant_with_tools\n+    # Use type: ignore since reasoning_content is a dynamic field not in OpenAI's TypedDict\n+    assert assistant_with_tools[\"reasoning_content\"] == \"I need to check the weather in Paris.\"  # type: ignore[typeddict-item]\n+\n+\n+def test_reasoning_content_from_other_provider_not_attached_to_deepseek():\n+    \"\"\"\n+    Verify reasoning_content from non-DeepSeek providers is NOT attached to DeepSeek messages.\n+\n+    When switching models mid-conversation (e.g., from Claude to DeepSeek), reasoning items\n+    that originated from Claude should not have their summaries attached as reasoning_content\n+    to DeepSeek assistant messages, as this would leak unrelated reasoning and may trigger\n+    DeepSeek 400 errors.\n+    \"\"\"\n+    from agents.models.chatcmpl_converter import Converter\n+\n+    input_items: list[Any] = [\n+        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"},\n+        {\n+            \"id\": \"__fake_id__\",\n+            \"summary\": [{\"text\": \"Claude's reasoning about the weather.\", \"type\": \"summary_text\"}],\n+            \"type\": \"reasoning\",\n+            \"content\": None,\n+            \"encrypted_content\": None,\n+            \"status\": None,\n+            # this one came from Claude, not DeepSeek\n+            \"provider_data\": {\"model\": \"claude-sonnet-4-20250514\", \"response_id\": \"chatcmpl-test\"},\n+        },\n+        {\n+            \"arguments\": '{\"city\": \"Paris\"}',\n+            \"call_id\": \"call_weather_789\",\n+            \"name\": \"get_weather\",\n+            \"type\": \"function_call\",\n+            \"id\": \"__fake_id__\",\n+            \"status\": None,\n+            \"provider_data\": {\"model\": \"claude-sonnet-4-20250514\"},\n+        },\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call_weather_789\",\n+            \"output\": \"The weather in Paris is cloudy.\",\n+        },\n+    ]\n+\n+    messages = Converter.items_to_messages(\n+        input_items,\n+        model=\"deepseek-reasoner\",\n+    )\n+\n+    assistant_with_tools = None\n+    for msg in messages:\n+        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n+            assistant_with_tools = msg\n+            break\n+\n+    assert assistant_with_tools is not None\n+    # reasoning_content should NOT be present since the reasoning came from Claude, not DeepSeek\n+    assert \"reasoning_content\" not in assistant_with_tools\n+\n+\n+def test_reasoning_content_without_provider_data_attached_for_backward_compat():\n+    \"\"\"\n+    Verify reasoning_content from items without provider_data is attached for backward compat.\n+\n+    For older items that don't have provider_data (before provider tracking was added),\n+    we should still attach reasoning_content to maintain backward compatibility.\n+    \"\"\"\n+    from agents.models.chatcmpl_converter import Converter\n+\n+    # Reasoning item without provider_data (older format)\n+    input_items: list[Any] = [\n+        {\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"},\n+        {\n+            \"id\": \"__fake_id__\",\n+            \"summary\": [{\"text\": \"Reasoning without provider info.\", \"type\": \"summary_text\"}],\n+            \"type\": \"reasoning\",\n+            \"content\": None,\n+            \"encrypted_content\": None,\n+            \"status\": None,\n+            # No provider_data\n+        },\n+        {\n+            \"arguments\": '{\"city\": \"Tokyo\"}',\n+            \"call_id\": \"call_weather_101\",\n+            \"name\": \"get_weather\",\n+            \"type\": \"function_call\",\n+            \"id\": \"__fake_id__\",\n+            \"status\": None,\n+        },\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call_weather_101\",\n+            \"output\": \"The weather in Tokyo is sunny.\",\n+        },\n+    ]\n+\n+    messages = Converter.items_to_messages(\n+        input_items,\n+        model=\"deepseek-reasoner\",\n+    )\n+\n+    assistant_with_tools = None\n+    for msg in messages:\n+        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n+            assistant_with_tools = msg\n+            break\n+\n+    assert assistant_with_tools is not None\n+    # reasoning_content SHOULD be present for backward compatibility\n+    assert \"reasoning_content\" in assistant_with_tools\n+    assert assistant_with_tools[\"reasoning_content\"] == \"Reasoning without provider info.\"  # type: ignore[typeddict-item]",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/c63270458baf7a5caea180c999dfda795501c15f/tests%2Fmodels%2Ftest_deepseek_reasoning_content.py",
        "sha": "edef8b5bfa088dbfec8ee36f113051516b9cba67",
        "status": "added"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:58:02.785301Z"
}
