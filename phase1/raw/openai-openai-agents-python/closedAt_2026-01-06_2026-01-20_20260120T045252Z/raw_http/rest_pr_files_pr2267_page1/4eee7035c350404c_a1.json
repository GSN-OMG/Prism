{
  "finished_at": "2026-01-20T04:54:50.750483Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "4eee7035c350404c",
    "tag": "rest_pr_files_pr2267_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2267/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "46065",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:54:50 GMT",
      "etag": "\"c803a6d15413bfce0e18f25058309f3226e9f9fc4460bc9b117fd3b17919f3e1\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Wed, 07 Jan 2026 09:18:50 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DB44:F2819:169ED41:1FA8E92:696F0A9A",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4979",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "21",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 722,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_example_workflows.py",
        "changes": 722,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_example_workflows.py?ref=1bd1b1d31d24ef8fb169dd47c9fed837cb617223",
        "deletions": 0,
        "filename": "tests/test_example_workflows.py",
        "patch": "@@ -0,0 +1,722 @@\n+from __future__ import annotations\n+\n+import json\n+from dataclasses import dataclass\n+from typing import Any, Literal, cast\n+\n+import pytest\n+from openai.types.responses import ResponseTextDeltaEvent\n+from pydantic import BaseModel\n+\n+from agents import (\n+    Agent,\n+    AgentBase,\n+    AgentToolStreamEvent,\n+    AgentUpdatedStreamEvent,\n+    GuardrailFunctionOutput,\n+    InputGuardrailTripwireTriggered,\n+    ItemHelpers,\n+    ModelSettings,\n+    OutputGuardrailTripwireTriggered,\n+    RawResponsesStreamEvent,\n+    RunContextWrapper,\n+    Runner,\n+    input_guardrail,\n+    output_guardrail,\n+)\n+from agents.agent import ToolsToFinalOutputResult\n+from agents.items import TResponseInputItem\n+from agents.tool import FunctionTool, FunctionToolResult, function_tool\n+\n+from .fake_model import FakeModel\n+from .test_responses import (\n+    get_final_output_message,\n+    get_function_tool_call,\n+    get_handoff_tool_call,\n+    get_text_input_item,\n+    get_text_message,\n+)\n+\n+\n+@dataclass\n+class EvaluationFeedback:\n+    feedback: str\n+    score: Literal[\"pass\", \"needs_improvement\"]\n+\n+\n+@dataclass\n+class OutlineCheckerOutput:\n+    good_quality: bool\n+    is_scifi: bool\n+\n+\n+@pytest.mark.asyncio\n+async def test_llm_as_judge_loop_handles_dataclass_feedback() -> None:\n+    \"\"\"Mimics the llm_as_a_judge example: loop until the evaluator passes the outline.\"\"\"\n+    outline_model = FakeModel()\n+    outline_model.add_multiple_turn_outputs(\n+        [\n+            [get_text_message(\"Outline v1\")],\n+            [get_text_message(\"Outline v2\")],\n+        ]\n+    )\n+\n+    judge_model = FakeModel()\n+    judge_model.add_multiple_turn_outputs(\n+        [\n+            [\n+                get_final_output_message(\n+                    json.dumps(\n+                        {\n+                            \"response\": {\n+                                \"feedback\": \"Add more suspense\",\n+                                \"score\": \"needs_improvement\",\n+                            }\n+                        }\n+                    )\n+                )\n+            ],\n+            [\n+                get_final_output_message(\n+                    json.dumps({\"response\": {\"feedback\": \"Looks good\", \"score\": \"pass\"}})\n+                )\n+            ],\n+        ]\n+    )\n+\n+    outline_agent = Agent(name=\"outline\", model=outline_model)\n+    judge_agent = Agent(name=\"judge\", model=judge_model, output_type=EvaluationFeedback)\n+\n+    conversation: list[TResponseInputItem] = [get_text_input_item(\"Tell me a space story\")]\n+    latest_outline: str | None = None\n+\n+    for expected_outline, expected_score in [\n+        (\"Outline v1\", \"needs_improvement\"),\n+        (\"Outline v2\", \"pass\"),\n+    ]:\n+        outline_result = await Runner.run(outline_agent, conversation)\n+        latest_outline = ItemHelpers.text_message_outputs(outline_result.new_items)\n+        assert latest_outline == expected_outline\n+\n+        conversation = outline_result.to_input_list()\n+\n+        judge_result = await Runner.run(judge_agent, conversation)\n+        feedback = judge_result.final_output\n+        assert isinstance(feedback, EvaluationFeedback)\n+        assert feedback.score == expected_score\n+\n+        if feedback.score == \"pass\":\n+            break\n+\n+        conversation.append({\"content\": f\"Feedback: {feedback.feedback}\", \"role\": \"user\"})\n+\n+    assert latest_outline == \"Outline v2\"\n+    assert len(conversation) == 4\n+    assert judge_model.last_turn_args[\"input\"] == conversation\n+\n+\n+@pytest.mark.asyncio\n+async def test_parallel_translation_flow_reuses_runner_outputs() -> None:\n+    \"\"\"Covers the parallelization example by feeding multiple translations into a picker agent.\"\"\"\n+    translation_model = FakeModel()\n+    translation_model.add_multiple_turn_outputs(\n+        [\n+            [get_text_message(\"Uno\")],\n+            [get_text_message(\"Dos\")],\n+            [get_text_message(\"Tres\")],\n+        ]\n+    )\n+    spanish_agent = Agent(name=\"spanish_agent\", model=translation_model)\n+\n+    picker_model = FakeModel()\n+    picker_model.set_next_output([get_text_message(\"Pick: Dos\")])\n+    picker_agent = Agent(name=\"picker\", model=picker_model)\n+\n+    translations: list[str] = []\n+    for _ in range(3):\n+        result = await Runner.run(spanish_agent, input=\"Hello\")\n+        translations.append(ItemHelpers.text_message_outputs(result.new_items))\n+\n+    combined = \"\\n\\n\".join(translations)\n+    picker_result = await Runner.run(\n+        picker_agent,\n+        input=f\"Input: Hello\\n\\nTranslations:\\n{combined}\",\n+    )\n+\n+    assert translations == [\"Uno\", \"Dos\", \"Tres\"]\n+    assert picker_result.final_output == \"Pick: Dos\"\n+    assert picker_model.last_turn_args[\"input\"] == [\n+        {\"content\": f\"Input: Hello\\n\\nTranslations:\\n{combined}\", \"role\": \"user\"}\n+    ]\n+\n+\n+@pytest.mark.asyncio\n+async def test_deterministic_story_flow_stops_when_checker_blocks() -> None:\n+    \"\"\"Mimics deterministic flow: stop early when quality gate fails.\"\"\"\n+    outline_model = FakeModel()\n+    outline_model.set_next_output([get_text_message(\"Outline v1\")])\n+    checker_model = FakeModel()\n+    checker_model.set_next_output(\n+        [\n+            get_final_output_message(\n+                json.dumps({\"response\": {\"good_quality\": False, \"is_scifi\": True}})\n+            )\n+        ]\n+    )\n+    story_model = FakeModel()\n+    story_model.set_next_output(RuntimeError(\"story should not run\"))\n+\n+    outline_agent = Agent(name=\"outline\", model=outline_model)\n+    checker_agent = Agent(\n+        name=\"checker\",\n+        model=checker_model,\n+        output_type=OutlineCheckerOutput,\n+    )\n+    story_agent = Agent(name=\"story\", model=story_model)\n+\n+    inputs: list[TResponseInputItem] = [get_text_input_item(\"Sci-fi please\")]\n+    outline_result = await Runner.run(outline_agent, inputs)\n+    inputs = outline_result.to_input_list()\n+\n+    checker_result = await Runner.run(checker_agent, inputs)\n+    decision = checker_result.final_output\n+\n+    assert isinstance(decision, OutlineCheckerOutput)\n+    assert decision.good_quality is False\n+    assert decision.is_scifi is True\n+    if decision.good_quality and decision.is_scifi:\n+        await Runner.run(story_agent, outline_result.final_output)\n+    assert story_model.first_turn_args is None, \"story agent should never be invoked when gated\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_deterministic_story_flow_runs_story_on_pass() -> None:\n+    \"\"\"Mimics deterministic flow: run full path when checker approves.\"\"\"\n+    outline_model = FakeModel()\n+    outline_model.set_next_output([get_text_message(\"Outline ready\")])\n+    checker_model = FakeModel()\n+    checker_model.set_next_output(\n+        [\n+            get_final_output_message(\n+                json.dumps({\"response\": {\"good_quality\": True, \"is_scifi\": True}})\n+            )\n+        ]\n+    )\n+    story_model = FakeModel()\n+    story_model.set_next_output([get_text_message(\"Final story\")])\n+\n+    outline_agent = Agent(name=\"outline\", model=outline_model)\n+    checker_agent = Agent(\n+        name=\"checker\",\n+        model=checker_model,\n+        output_type=OutlineCheckerOutput,\n+    )\n+    story_agent = Agent(name=\"story\", model=story_model)\n+\n+    inputs: list[TResponseInputItem] = [get_text_input_item(\"Sci-fi please\")]\n+    outline_result = await Runner.run(outline_agent, inputs)\n+    inputs = outline_result.to_input_list()\n+\n+    checker_result = await Runner.run(checker_agent, inputs)\n+    decision = checker_result.final_output\n+    assert isinstance(decision, OutlineCheckerOutput)\n+    assert decision.good_quality is True\n+    assert decision.is_scifi is True\n+\n+    story_result = await Runner.run(story_agent, outline_result.final_output)\n+    assert story_result.final_output == \"Final story\"\n+    assert story_model.last_turn_args[\"input\"] == [{\"content\": \"Outline ready\", \"role\": \"user\"}]\n+\n+\n+@pytest.mark.asyncio\n+async def test_routing_stream_emits_text_and_updates_inputs() -> None:\n+    \"\"\"Mimics routing example stream: text deltas flow through and input history updates.\"\"\"\n+    model = FakeModel()\n+    model.set_next_output([get_text_message(\"Bonjour\")])\n+    triage_agent = Agent(name=\"triage_agent\", model=model)\n+\n+    streamed = Runner.run_streamed(triage_agent, input=\"Salut\")\n+\n+    deltas: list[str] = []\n+    async for event in streamed.stream_events():\n+        if isinstance(event, RawResponsesStreamEvent) and isinstance(\n+            event.data, ResponseTextDeltaEvent\n+        ):\n+            deltas.append(event.data.delta)\n+\n+    assert \"\".join(deltas) == \"Bonjour\"\n+    assert streamed.final_output == \"Bonjour\"\n+    assert len(streamed.new_items) == 1\n+    input_list = streamed.to_input_list()\n+    assert len(input_list) == 2\n+    assert input_list[0] == {\"content\": \"Salut\", \"role\": \"user\"}\n+    assistant_item = input_list[1]\n+    assert isinstance(assistant_item, dict)\n+    assert assistant_item.get(\"role\") == \"assistant\"\n+    assert assistant_item.get(\"type\") == \"message\"\n+    content: Any = assistant_item.get(\"content\")\n+    assert isinstance(content, list)\n+    first_content = content[0]\n+    assert isinstance(first_content, dict)\n+    assert first_content.get(\"text\") == \"Bonjour\"\n+\n+\n+class MathHomeworkOutput(BaseModel):\n+    reasoning: str\n+    is_math_homework: bool\n+\n+\n+@pytest.mark.asyncio\n+async def test_input_guardrail_agent_trips_and_returns_info() -> None:\n+    \"\"\"Mimics math guardrail example: guardrail agent runs and trips before main agent completes.\"\"\"\n+    guardrail_model = FakeModel()\n+    guardrail_model.set_next_output(\n+        [\n+            get_final_output_message(\n+                json.dumps({\"reasoning\": \"math detected\", \"is_math_homework\": True})\n+            )\n+        ]\n+    )\n+    guardrail_agent = Agent(name=\"guardrail\", model=guardrail_model, output_type=MathHomeworkOutput)\n+\n+    @input_guardrail\n+    async def math_guardrail(\n+        context: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n+    ) -> GuardrailFunctionOutput:\n+        result = await Runner.run(guardrail_agent, input, context=context.context)\n+        output = result.final_output_as(MathHomeworkOutput)\n+        return GuardrailFunctionOutput(\n+            output_info=output, tripwire_triggered=output.is_math_homework\n+        )\n+\n+    main_model = FakeModel()\n+    main_model.set_next_output([get_text_message(\"Should not run\")])\n+    main_agent = Agent(name=\"main\", model=main_model, input_guardrails=[math_guardrail])\n+\n+    with pytest.raises(InputGuardrailTripwireTriggered) as excinfo:\n+        await Runner.run(main_agent, \"Solve 2x+5=11\")\n+\n+    guardrail_result = excinfo.value.guardrail_result\n+    assert isinstance(guardrail_result.output.output_info, MathHomeworkOutput)\n+    assert guardrail_result.output.output_info.is_math_homework is True\n+    assert guardrail_result.output.output_info.reasoning == \"math detected\"\n+\n+\n+class MessageOutput(BaseModel):\n+    reasoning: str\n+    response: str\n+    user_name: str | None\n+\n+\n+@pytest.mark.asyncio\n+async def test_output_guardrail_blocks_sensitive_data() -> None:\n+    \"\"\"Mimics sensitive data guardrail example: trips when phone number is present.\"\"\"\n+\n+    @output_guardrail\n+    async def sensitive_data_check(\n+        context: RunContextWrapper, agent: Agent, output: MessageOutput\n+    ) -> GuardrailFunctionOutput:\n+        contains_phone = \"650\" in output.response or \"650\" in output.reasoning\n+        return GuardrailFunctionOutput(\n+            output_info={\"contains_phone\": contains_phone},\n+            tripwire_triggered=contains_phone,\n+        )\n+\n+    model = FakeModel()\n+    model.set_next_output(\n+        [\n+            get_final_output_message(\n+                json.dumps(\n+                    {\n+                        \"reasoning\": \"User shared phone 650-123-4567\",\n+                        \"response\": \"Thanks!\",\n+                        \"user_name\": None,\n+                    }\n+                )\n+            )\n+        ]\n+    )\n+    agent = Agent(\n+        name=\"Assistant\",\n+        model=model,\n+        output_type=MessageOutput,\n+        output_guardrails=[sensitive_data_check],\n+    )\n+\n+    with pytest.raises(OutputGuardrailTripwireTriggered) as excinfo:\n+        await Runner.run(agent, \"My phone number is 650-123-4567.\")\n+\n+    guardrail_output = excinfo.value.guardrail_result.output.output_info\n+    assert isinstance(guardrail_output, dict)\n+    assert guardrail_output[\"contains_phone\"] is True\n+\n+\n+@pytest.mark.asyncio\n+async def test_streaming_guardrail_style_cancel_after_threshold() -> None:\n+    \"\"\"Mimics streaming guardrail example: stop streaming once threshold is reached.\"\"\"\n+    model = FakeModel()\n+    model.set_next_output(\n+        [\n+            get_text_message(\"Chunk1 \"),\n+            get_text_message(\"Chunk2 \"),\n+            get_text_message(\"Chunk3\"),\n+        ]\n+    )\n+    agent = Agent(name=\"talkative\", model=model)\n+\n+    streamed = Runner.run_streamed(agent, input=\"Start\")\n+\n+    deltas: list[str] = []\n+    async for event in streamed.stream_events():\n+        if isinstance(event, RawResponsesStreamEvent) and isinstance(\n+            event.data, ResponseTextDeltaEvent\n+        ):\n+            deltas.append(event.data.delta)\n+            if len(\"\".join(deltas)) >= len(\"Chunk1 Chunk2 \"):\n+                streamed.cancel(mode=\"immediate\")\n+\n+    collected = \"\".join(deltas)\n+    assert \"Chunk1\" in collected\n+    assert \"Chunk3\" not in collected\n+    assert streamed.final_output is None\n+    assert streamed.is_complete is True\n+\n+\n+@pytest.mark.asyncio\n+async def test_streaming_cancel_after_turn_allows_turn_completion() -> None:\n+    \"\"\"Ensure cancel(after_turn) lets the current turn finish and final_output is populated.\"\"\"\n+    model = FakeModel()\n+    model.set_next_output([get_text_message(\"Hello\"), get_text_message(\"World\")])\n+    agent = Agent(name=\"talkative\", model=model)\n+\n+    streamed = Runner.run_streamed(agent, input=\"Hi\")\n+\n+    deltas: list[str] = []\n+    async for event in streamed.stream_events():\n+        if isinstance(event, RawResponsesStreamEvent) and isinstance(\n+            event.data, ResponseTextDeltaEvent\n+        ):\n+            deltas.append(event.data.delta)\n+            streamed.cancel(mode=\"after_turn\")\n+\n+    assert \"\".join(deltas).startswith(\"Hello\")\n+    assert streamed.final_output == \"World\"\n+    assert streamed.is_complete is True\n+    assert len(streamed.new_items) == 2\n+\n+\n+@pytest.mark.asyncio\n+async def test_streaming_handoff_emits_agent_updated_event() -> None:\n+    \"\"\"Mimics routing handoff stream: emits AgentUpdatedStreamEvent and switches agent.\"\"\"\n+    delegate_model = FakeModel()\n+    delegate_model.set_next_output([get_text_message(\"delegate reply\")])\n+    delegate_agent = Agent(name=\"delegate\", model=delegate_model)\n+\n+    triage_model = FakeModel()\n+    triage_model.set_next_output(\n+        [\n+            get_text_message(\"triage summary\"),\n+            get_handoff_tool_call(delegate_agent),\n+        ]\n+    )\n+    triage_agent = Agent(name=\"triage\", model=triage_model, handoffs=[delegate_agent])\n+\n+    streamed = Runner.run_streamed(triage_agent, input=\"Help me\")\n+\n+    agent_updates: list[AgentUpdatedStreamEvent] = []\n+    async for event in streamed.stream_events():\n+        if isinstance(event, AgentUpdatedStreamEvent):\n+            agent_updates.append(event)\n+\n+    assert streamed.final_output == \"delegate reply\"\n+    assert streamed.last_agent == delegate_agent\n+    assert len(agent_updates) >= 1\n+    assert any(update.new_agent == delegate_agent for update in agent_updates)\n+\n+\n+@pytest.mark.asyncio\n+async def test_agent_as_tool_streaming_example_collects_events() -> None:\n+    \"\"\"Mimics agents_as_tools_streaming example: on_stream receives nested streaming events.\"\"\"\n+    billing_agent = Agent(name=\"billing\")\n+\n+    received: list[AgentToolStreamEvent] = []\n+\n+    async def on_stream(event: AgentToolStreamEvent) -> None:\n+        received.append(event)\n+\n+    billing_tool = cast(\n+        FunctionTool,\n+        billing_agent.as_tool(\n+            tool_name=\"billing_agent\",\n+            tool_description=\"Answer billing questions\",\n+            on_stream=on_stream,\n+        ),\n+    )\n+\n+    async def fake_invoke(ctx, input: str) -> str:\n+        event_payload: AgentToolStreamEvent = {\n+            \"event\": RawResponsesStreamEvent(data=cast(Any, {\"type\": \"output_text_delta\"})),\n+            \"agent\": billing_agent,\n+            \"tool_call\": ctx.tool_call,\n+        }\n+        await on_stream(event_payload)\n+        return \"Billing: $100\"\n+\n+    billing_tool.on_invoke_tool = fake_invoke\n+\n+    main_model = FakeModel()\n+    main_model.add_multiple_turn_outputs(\n+        [\n+            [get_function_tool_call(\"billing_agent\", json.dumps({\"input\": \"Need bill\"}))],\n+            [get_text_message(\"Final answer\")],\n+        ]\n+    )\n+\n+    main_agent = Agent(\n+        name=\"support\",\n+        model=main_model,\n+        tools=[billing_tool],\n+        model_settings=ModelSettings(tool_choice=\"required\"),\n+    )\n+\n+    result = await Runner.run(main_agent, \"How much is my bill?\")\n+\n+    assert result.final_output == \"Final answer\"\n+    assert received, \"on_stream should capture nested streaming events\"\n+    assert all(event[\"agent\"] == billing_agent for event in received)\n+    assert all(\n+        event[\"tool_call\"] and event[\"tool_call\"].name == \"billing_agent\" for event in received\n+    )\n+\n+\n+@pytest.mark.asyncio\n+async def test_forcing_tool_use_behaviors_align_with_example() -> None:\n+    \"\"\"Mimics forcing_tool_use example: default vs first_tool vs custom behaviors.\"\"\"\n+\n+    @function_tool\n+    def get_weather(city: str) -> str:\n+        return f\"{city}: Sunny\"\n+\n+    # default: run_llm_again -> model responds after tool call\n+    default_model = FakeModel()\n+    default_model.add_multiple_turn_outputs(\n+        [\n+            [\n+                get_text_message(\"Tool call coming\"),\n+                get_function_tool_call(\"get_weather\", json.dumps({\"city\": \"Tokyo\"})),\n+            ],\n+            [get_text_message(\"Done after tool\")],\n+        ]\n+    )\n+\n+    default_agent = Agent(\n+        name=\"default\",\n+        model=default_model,\n+        tools=[get_weather],\n+        tool_use_behavior=\"run_llm_again\",\n+        model_settings=ModelSettings(tool_choice=None),\n+    )\n+\n+    default_result = await Runner.run(default_agent, \"Weather?\")\n+    assert default_result.final_output == \"Done after tool\"\n+    assert len(default_result.raw_responses) == 2\n+\n+    # first_tool: stop_on_first_tool -> final output from first tool result\n+    first_model = FakeModel()\n+    first_model.set_next_output(\n+        [\n+            get_text_message(\"Tool call coming\"),\n+            get_function_tool_call(\"get_weather\", json.dumps({\"city\": \"Paris\"})),\n+        ]\n+    )\n+\n+    first_agent = Agent(\n+        name=\"first\",\n+        model=first_model,\n+        tools=[get_weather],\n+        tool_use_behavior=\"stop_on_first_tool\",\n+        model_settings=ModelSettings(tool_choice=\"required\"),\n+    )\n+\n+    first_result = await Runner.run(first_agent, \"Weather?\")\n+    assert first_result.final_output == \"Paris: Sunny\"\n+    assert len(first_result.raw_responses) == 1\n+\n+    # custom: uses custom tool_use_behavior to format output, still with required tool choice\n+    async def custom_tool_use_behavior(\n+        context: RunContextWrapper[Any], results: list[FunctionToolResult]\n+    ) -> ToolsToFinalOutputResult:\n+        return ToolsToFinalOutputResult(\n+            is_final_output=True, final_output=f\"Custom:{results[0].output}\"\n+        )\n+\n+    custom_model = FakeModel()\n+    custom_model.set_next_output(\n+        [\n+            get_text_message(\"Tool call coming\"),\n+            get_function_tool_call(\"get_weather\", json.dumps({\"city\": \"Berlin\"})),\n+        ]\n+    )\n+\n+    custom_agent = Agent(\n+        name=\"custom\",\n+        model=custom_model,\n+        tools=[get_weather],\n+        tool_use_behavior=custom_tool_use_behavior,\n+        model_settings=ModelSettings(tool_choice=\"required\"),\n+    )\n+\n+    custom_result = await Runner.run(custom_agent, \"Weather?\")\n+    assert custom_result.final_output == \"Custom:Berlin: Sunny\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_routing_multi_turn_continues_with_handoff_agent() -> None:\n+    \"\"\"Mimics routing example multi-turn: first handoff, then continue with delegated agent.\"\"\"\n+    delegate_model = FakeModel()\n+    delegate_model.set_next_output([get_text_message(\"Bonjour\")])\n+    delegate_agent = Agent(name=\"delegate\", model=delegate_model)\n+\n+    triage_model = FakeModel()\n+    triage_model.add_multiple_turn_outputs(\n+        [\n+            [get_handoff_tool_call(delegate_agent)],\n+            [get_text_message(\"handoff completed\")],\n+        ]\n+    )\n+    triage_agent = Agent(name=\"triage\", model=triage_model, handoffs=[delegate_agent])\n+\n+    first_result = await Runner.run(triage_agent, \"Help me in French\")\n+    assert first_result.final_output == \"Bonjour\"\n+    assert first_result.last_agent == delegate_agent\n+\n+    # Next user turn continues with delegate.\n+    delegate_model.set_next_output([get_text_message(\"Encore?\")])\n+    follow_up_input = first_result.to_input_list()\n+    follow_up_input.append({\"role\": \"user\", \"content\": \"Encore!\"})\n+\n+    second_result = await Runner.run(delegate_agent, follow_up_input)\n+    assert second_result.final_output == \"Encore?\"\n+    assert delegate_model.last_turn_args[\"input\"] == follow_up_input\n+\n+\n+@pytest.mark.asyncio\n+async def test_agents_as_tools_conditional_enabling_matches_preference() -> None:\n+    \"\"\"Mimics agents_as_tools_conditional example: only enabled tools are invoked per preference.\"\"\"\n+\n+    class AppContext(BaseModel):\n+        language_preference: str\n+\n+    def french_spanish_enabled(ctx: RunContextWrapper[AppContext], _agent: AgentBase) -> bool:\n+        return ctx.context.language_preference in [\"french_spanish\", \"european\"]\n+\n+    def european_enabled(ctx: RunContextWrapper[AppContext], _agent: AgentBase) -> bool:\n+        return ctx.context.language_preference == \"european\"\n+\n+    scenarios = [\n+        (\"spanish_only\", {\"respond_spanish\"}),\n+        (\"french_spanish\", {\"respond_spanish\", \"respond_french\"}),\n+        (\"european\", {\"respond_spanish\", \"respond_french\", \"respond_italian\"}),\n+    ]\n+\n+    for preference, expected_tools in scenarios:\n+        spanish_model = FakeModel()\n+        spanish_model.set_next_output([get_text_message(\"ES hola\")])\n+        spanish_agent = Agent(name=\"spanish\", model=spanish_model)\n+\n+        french_model = FakeModel()\n+        french_model.set_next_output([get_text_message(\"FR bonjour\")])\n+        french_agent = Agent(name=\"french\", model=french_model)\n+\n+        italian_model = FakeModel()\n+        italian_model.set_next_output([get_text_message(\"IT ciao\")])\n+        italian_agent = Agent(name=\"italian\", model=italian_model)\n+\n+        orchestrator_model = FakeModel()\n+        # Build tool calls only for expected tools to avoid missing-tool errors.\n+        tool_calls = [\n+            get_function_tool_call(tool_name, json.dumps({\"input\": \"Hi\"}))\n+            for tool_name in sorted(expected_tools)\n+        ]\n+        orchestrator_model.add_multiple_turn_outputs([tool_calls, [get_text_message(\"Done\")]])\n+\n+        context = AppContext(language_preference=preference)\n+\n+        orchestrator = Agent(\n+            name=\"orchestrator\",\n+            model=orchestrator_model,\n+            tools=[\n+                spanish_agent.as_tool(\n+                    tool_name=\"respond_spanish\",\n+                    tool_description=\"Spanish\",\n+                    is_enabled=True,\n+                ),\n+                french_agent.as_tool(\n+                    tool_name=\"respond_french\",\n+                    tool_description=\"French\",\n+                    is_enabled=french_spanish_enabled,\n+                ),\n+                italian_agent.as_tool(\n+                    tool_name=\"respond_italian\",\n+                    tool_description=\"Italian\",\n+                    is_enabled=european_enabled,\n+                ),\n+            ],\n+            model_settings=ModelSettings(tool_choice=\"required\"),\n+        )\n+\n+        result = await Runner.run(orchestrator, \"Hello\", context=context)\n+\n+        assert result.final_output == \"Done\"\n+        assert (\n+            spanish_model.first_turn_args is not None\n+            if \"respond_spanish\" in expected_tools\n+            else spanish_model.first_turn_args is None\n+        )\n+        assert (\n+            french_model.first_turn_args is not None\n+            if \"respond_french\" in expected_tools\n+            else french_model.first_turn_args is None\n+        )\n+        assert (\n+            italian_model.first_turn_args is not None\n+            if \"respond_italian\" in expected_tools\n+            else italian_model.first_turn_args is None\n+        )\n+\n+\n+@pytest.mark.asyncio\n+async def test_agents_as_tools_orchestrator_runs_multiple_translations() -> None:\n+    \"\"\"Orchestrator calls multiple translation agent tools then summarizes.\"\"\"\n+    spanish_model = FakeModel()\n+    spanish_model.set_next_output([get_text_message(\"ES hola\")])\n+    spanish_agent = Agent(name=\"spanish\", model=spanish_model)\n+\n+    french_model = FakeModel()\n+    french_model.set_next_output([get_text_message(\"FR bonjour\")])\n+    french_agent = Agent(name=\"french\", model=french_model)\n+\n+    orchestrator_model = FakeModel()\n+    orchestrator_model.add_multiple_turn_outputs(\n+        [\n+            [get_function_tool_call(\"translate_to_spanish\", json.dumps({\"input\": \"Hi\"}))],\n+            [get_function_tool_call(\"translate_to_french\", json.dumps({\"input\": \"Hi\"}))],\n+            [get_text_message(\"Summary complete\")],\n+        ]\n+    )\n+\n+    orchestrator = Agent(\n+        name=\"orchestrator\",\n+        model=orchestrator_model,\n+        tools=[\n+            spanish_agent.as_tool(\"translate_to_spanish\", \"Spanish\"),\n+            french_agent.as_tool(\"translate_to_french\", \"French\"),\n+        ],\n+    )\n+\n+    result = await Runner.run(orchestrator, \"Hi\")\n+\n+    assert result.final_output == \"Summary complete\"\n+    assert spanish_model.last_turn_args[\"input\"] == [{\"content\": \"Hi\", \"role\": \"user\"}]\n+    assert french_model.last_turn_args[\"input\"] == [{\"content\": \"Hi\", \"role\": \"user\"}]\n+    assert len(result.raw_responses) == 3",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_example_workflows.py",
        "sha": "a3603acc21c38c23c946551c2fb866b164f385d5",
        "status": "added"
      },
      {
        "additions": 69,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_items_helpers.py",
        "changes": 69,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_items_helpers.py?ref=1bd1b1d31d24ef8fb169dd47c9fed837cb617223",
        "deletions": 0,
        "filename": "tests/test_items_helpers.py",
        "patch": "@@ -3,6 +3,7 @@\n import gc\n import json\n import weakref\n+from typing import cast\n \n from openai.types.responses.response_computer_tool_call import (\n     ActionScreenshot,\n@@ -40,6 +41,7 @@\n     TResponseInputItem,\n     Usage,\n )\n+from agents.items import ToolCallOutputItem\n \n \n def make_message(\n@@ -209,6 +211,73 @@ def test_handoff_output_item_retains_agents_until_gc() -> None:\n     assert item.target_agent is None\n \n \n+def test_handoff_output_item_converts_api_payload() -> None:\n+    raw_item = cast(\n+        TResponseInputItem,\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call-123\",\n+            \"output\": \"ok\",\n+        },\n+    )\n+    owner_agent = Agent(name=\"owner\")\n+    source_agent = Agent(name=\"source\")\n+    target_agent = Agent(name=\"target\")\n+    item = HandoffOutputItem(\n+        agent=owner_agent,\n+        raw_item=raw_item,\n+        source_agent=source_agent,\n+        target_agent=target_agent,\n+    )\n+\n+    converted = item.to_input_item()\n+    # HandoffOutputItem should be passthrough for API-shaped payloads, not mutate fields.\n+    assert converted[\"type\"] == \"function_call_output\"\n+    assert converted[\"call_id\"] == \"call-123\"\n+    assert converted[\"output\"] == \"ok\"\n+\n+\n+def test_handoff_output_item_stringifies_object_output() -> None:\n+    raw_item = cast(\n+        TResponseInputItem,\n+        {\n+            \"type\": \"function_call_output\",\n+            \"call_id\": \"call-obj\",\n+            \"output\": {\"assistant\": \"Weather Assistant\"},\n+        },\n+    )\n+    owner_agent = Agent(name=\"owner\")\n+    source_agent = Agent(name=\"source\")\n+    target_agent = Agent(name=\"target\")\n+    item = HandoffOutputItem(\n+        agent=owner_agent,\n+        raw_item=raw_item,\n+        source_agent=source_agent,\n+        target_agent=target_agent,\n+    )\n+\n+    converted = item.to_input_item()\n+    assert converted[\"type\"] == \"function_call_output\"\n+    assert converted[\"call_id\"] == \"call-obj\"\n+    assert isinstance(converted[\"output\"], dict)\n+    assert converted[\"output\"] == {\"assistant\": \"Weather Assistant\"}\n+\n+\n+def test_tool_call_output_item_preserves_function_output_structure() -> None:\n+    agent = Agent(name=\"tester\")\n+    raw_item = {\n+        \"type\": \"function_call_output\",\n+        \"call_id\": \"call-keep\",\n+        \"output\": [{\"type\": \"output_text\", \"text\": \"value\"}],\n+    }\n+    item = ToolCallOutputItem(agent=agent, raw_item=raw_item, output=\"value\")\n+\n+    payload = item.to_input_item()\n+    assert isinstance(payload, dict)\n+    assert payload[\"type\"] == \"function_call_output\"\n+    assert payload[\"output\"] == raw_item[\"output\"]\n+\n+\n def test_tool_call_output_item_constructs_function_call_output_dict():\n     # Build a simple ResponseFunctionToolCall.\n     call = ResponseFunctionToolCall(",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_items_helpers.py",
        "sha": "0464719fcb35076a672429c5f62848ddb131d3de",
        "status": "modified"
      },
      {
        "additions": 134,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_process_model_response.py",
        "changes": 134,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_process_model_response.py?ref=1bd1b1d31d24ef8fb169dd47c9fed837cb617223",
        "deletions": 0,
        "filename": "tests/test_process_model_response.py",
        "patch": "@@ -0,0 +1,134 @@\n+from __future__ import annotations\n+\n+import pytest\n+from openai.types.responses import ResponseCustomToolCall, ResponseFunctionToolCall\n+\n+from agents import Agent, ApplyPatchTool\n+from agents._run_impl import RunImpl\n+from agents.editor import ApplyPatchOperation, ApplyPatchResult\n+from agents.exceptions import ModelBehaviorError\n+from agents.items import ModelResponse\n+from agents.usage import Usage\n+\n+\n+class RecordingEditor:\n+    def __init__(self) -> None:\n+        self.operations: list[dict[str, str]] = []\n+\n+    def create_file(self, operation: ApplyPatchOperation) -> ApplyPatchResult | str:\n+        self.operations.append({\"op\": \"create\", \"path\": operation.path})\n+        return f\"created {operation.path}\"\n+\n+    def update_file(self, operation: ApplyPatchOperation) -> ApplyPatchResult | str:\n+        self.operations.append({\"op\": \"update\", \"path\": operation.path})\n+        return f\"patched {operation.path}\"\n+\n+    def delete_file(self, operation: ApplyPatchOperation) -> ApplyPatchResult | str:\n+        self.operations.append({\"op\": \"delete\", \"path\": operation.path})\n+        return f\"deleted {operation.path}\"\n+\n+\n+def _response(output: list[object]) -> ModelResponse:\n+    response = ModelResponse(output=[], usage=Usage(), response_id=\"resp\")\n+    response.output = output  # type: ignore[assignment]\n+    return response\n+\n+\n+def _shell_call(call_id: str = \"shell-1\") -> dict[str, object]:\n+    return {\n+        \"type\": \"shell_call\",\n+        \"call_id\": call_id,\n+        \"status\": \"in_progress\",\n+        \"action\": {\"commands\": [\"echo hi\"]},\n+    }\n+\n+\n+def _apply_patch_dict(call_id: str = \"apply-1\") -> dict[str, object]:\n+    return {\n+        \"type\": \"apply_patch_call\",\n+        \"call_id\": call_id,\n+        \"operation\": {\"type\": \"update_file\", \"path\": \"tasks.md\", \"diff\": \"+a\\n-b\\n\"},\n+    }\n+\n+\n+def test_process_model_response_shell_call_without_tool_raises() -> None:\n+    agent = Agent(name=\"no-shell\")\n+    shell_call = _shell_call()\n+\n+    with pytest.raises(ModelBehaviorError, match=\"shell tool\"):\n+        RunImpl.process_model_response(\n+            agent=agent,\n+            all_tools=[],\n+            response=_response([shell_call]),\n+            output_schema=None,\n+            handoffs=[],\n+        )\n+\n+\n+def test_process_model_response_apply_patch_call_without_tool_raises() -> None:\n+    agent = Agent(name=\"no-apply\")\n+    apply_patch_call = _apply_patch_dict()\n+\n+    with pytest.raises(ModelBehaviorError, match=\"apply_patch tool\"):\n+        RunImpl.process_model_response(\n+            agent=agent,\n+            all_tools=[],\n+            response=_response([apply_patch_call]),\n+            output_schema=None,\n+            handoffs=[],\n+        )\n+\n+\n+def test_process_model_response_converts_custom_apply_patch_call() -> None:\n+    editor = RecordingEditor()\n+    apply_patch_tool = ApplyPatchTool(editor=editor)\n+    agent = Agent(name=\"apply-agent\")\n+    custom_call = ResponseCustomToolCall(\n+        name=\"apply_patch\",\n+        call_id=\"custom-apply-1\",\n+        input='{\"type\": \"update_file\", \"path\": \"file.txt\", \"diff\": \"+new\"}',\n+        type=\"custom_tool_call\",\n+    )\n+\n+    processed = RunImpl.process_model_response(\n+        agent=agent,\n+        all_tools=[apply_patch_tool],\n+        response=_response([custom_call]),\n+        output_schema=None,\n+        handoffs=[],\n+    )\n+\n+    assert processed.apply_patch_calls, \"Custom apply_patch call should be converted\"\n+    converted_call = processed.apply_patch_calls[0].tool_call\n+    assert isinstance(converted_call, dict)\n+    assert converted_call.get(\"type\") == \"apply_patch_call\"\n+    assert converted_call.get(\"call_id\") == \"custom-apply-1\"\n+    assert converted_call.get(\"operation\", {}).get(\"path\") == \"file.txt\"\n+\n+\n+def test_process_model_response_converts_apply_patch_function_call() -> None:\n+    editor = RecordingEditor()\n+    apply_patch_tool = ApplyPatchTool(editor=editor)\n+    agent = Agent(name=\"apply-agent\")\n+    func_call = ResponseFunctionToolCall(\n+        id=\"fc-1\",\n+        type=\"function_call\",\n+        name=\"apply_patch\",\n+        call_id=\"func-apply-1\",\n+        arguments='{\"type\": \"update_file\", \"path\": \"data.txt\", \"diff\": \"+x\"}',\n+        status=\"completed\",\n+    )\n+\n+    processed = RunImpl.process_model_response(\n+        agent=agent,\n+        all_tools=[apply_patch_tool],\n+        response=_response([func_call]),\n+        output_schema=None,\n+        handoffs=[],\n+    )\n+\n+    assert processed.apply_patch_calls, \"Function apply_patch call should be converted\"\n+    converted_call = processed.apply_patch_calls[0].tool_call\n+    assert isinstance(converted_call, dict)\n+    assert converted_call.get(\"call_id\") == \"func-apply-1\"\n+    assert converted_call.get(\"operation\", {}).get(\"path\") == \"data.txt\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_process_model_response.py",
        "sha": "a0dc6a351e25d643871fdabeb36988e94c58041e",
        "status": "added"
      },
      {
        "additions": 40,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_usage.py",
        "changes": 40,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_usage.py?ref=1bd1b1d31d24ef8fb169dd47c9fed837cb617223",
        "deletions": 0,
        "filename": "tests/test_usage.py",
        "patch": "@@ -1,7 +1,47 @@\n+from __future__ import annotations\n+\n+import pytest\n from openai.types.completion_usage import CompletionTokensDetails, PromptTokensDetails\n from openai.types.responses.response_usage import InputTokensDetails, OutputTokensDetails\n \n+from agents import Agent, Runner\n from agents.usage import RequestUsage, Usage\n+from tests.fake_model import FakeModel\n+from tests.test_responses import get_text_message\n+\n+\n+@pytest.mark.asyncio\n+async def test_runner_run_carries_request_usage_entries() -> None:\n+    \"\"\"Ensure usage produced by the model propagates to RunResult context.\"\"\"\n+    usage = Usage(\n+        requests=1,\n+        input_tokens=10,\n+        output_tokens=5,\n+        total_tokens=15,\n+        request_usage_entries=[\n+            RequestUsage(\n+                input_tokens=10,\n+                output_tokens=5,\n+                total_tokens=15,\n+                input_tokens_details=InputTokensDetails(cached_tokens=0),\n+                output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n+            )\n+        ],\n+    )\n+    model = FakeModel(initial_output=[get_text_message(\"done\")])\n+    model.set_hardcoded_usage(usage)\n+    agent = Agent(name=\"usage-agent\", model=model)\n+\n+    result = await Runner.run(agent, input=\"hi\")\n+\n+    propagated = result.context_wrapper.usage\n+    assert propagated.requests == 1\n+    assert propagated.total_tokens == 15\n+    assert len(propagated.request_usage_entries) == 1\n+    entry = propagated.request_usage_entries[0]\n+    assert entry.input_tokens == 10\n+    assert entry.output_tokens == 5\n+    assert entry.total_tokens == 15\n \n \n def test_usage_add_aggregates_all_fields():",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Ftest_usage.py",
        "sha": "2a8fcaa6d0062f9955d03b6959360fc8f6ca81a4",
        "status": "modified"
      },
      {
        "additions": 51,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Futils%2Fsimple_session.py",
        "changes": 53,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Futils%2Fsimple_session.py?ref=1bd1b1d31d24ef8fb169dd47c9fed837cb617223",
        "deletions": 2,
        "filename": "tests/utils/simple_session.py",
        "patch": "@@ -1,15 +1,25 @@\n from __future__ import annotations\n \n+from typing import cast\n+\n from agents.items import TResponseInputItem\n from agents.memory.session import Session\n \n \n class SimpleListSession(Session):\n     \"\"\"A minimal in-memory session implementation for tests.\"\"\"\n \n-    def __init__(self, session_id: str = \"test\") -> None:\n+    def __init__(\n+        self,\n+        session_id: str = \"test\",\n+        history: list[TResponseInputItem] | None = None,\n+    ) -> None:\n         self.session_id = session_id\n-        self._items: list[TResponseInputItem] = []\n+        self._items: list[TResponseInputItem] = list(history) if history else []\n+        # Some session implementations strip IDs on write; tests can opt-in via attribute.\n+        self._ignore_ids_for_matching = False\n+        # Mirror saved_items used by some tests for inspection.\n+        self.saved_items: list[TResponseInputItem] = self._items\n \n     async def get_items(self, limit: int | None = None) -> list[TResponseInputItem]:\n         if limit is None:\n@@ -28,3 +38,42 @@ async def pop_item(self) -> TResponseInputItem | None:\n \n     async def clear_session(self) -> None:\n         self._items.clear()\n+\n+\n+class CountingSession(SimpleListSession):\n+    \"\"\"Session that tracks how many times pop_item is invoked (for rewind tests).\"\"\"\n+\n+    def __init__(\n+        self,\n+        session_id: str = \"test\",\n+        history: list[TResponseInputItem] | None = None,\n+    ) -> None:\n+        super().__init__(session_id=session_id, history=history)\n+        self.pop_calls = 0\n+\n+    async def pop_item(self) -> TResponseInputItem | None:\n+        self.pop_calls += 1\n+        return await super().pop_item()\n+\n+\n+class IdStrippingSession(CountingSession):\n+    \"\"\"Session that strips IDs on add to mimic hosted stores that reassign IDs.\"\"\"\n+\n+    def __init__(\n+        self,\n+        session_id: str = \"test\",\n+        history: list[TResponseInputItem] | None = None,\n+    ) -> None:\n+        super().__init__(session_id=session_id, history=history)\n+        self._ignore_ids_for_matching = True\n+\n+    async def add_items(self, items: list[TResponseInputItem]) -> None:\n+        sanitized: list[TResponseInputItem] = []\n+        for item in items:\n+            if isinstance(item, dict):\n+                clean = dict(item)\n+                clean.pop(\"id\", None)\n+                sanitized.append(cast(TResponseInputItem, clean))\n+            else:\n+                sanitized.append(item)\n+        await super().add_items(sanitized)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Futils%2Fsimple_session.py",
        "sha": "7dee6d8a6934fa19dc9bfe00f226a466bdd11c9f",
        "status": "modified"
      },
      {
        "additions": 54,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Futils%2Ftest_simple_session.py",
        "changes": 54,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Futils%2Ftest_simple_session.py?ref=1bd1b1d31d24ef8fb169dd47c9fed837cb617223",
        "deletions": 0,
        "filename": "tests/utils/test_simple_session.py",
        "patch": "@@ -0,0 +1,54 @@\n+from __future__ import annotations\n+\n+from typing import cast\n+\n+import pytest\n+\n+from agents.items import TResponseInputItem\n+from tests.utils.simple_session import CountingSession, IdStrippingSession, SimpleListSession\n+\n+\n+@pytest.mark.asyncio\n+async def test_simple_list_session_preserves_history_and_saved_items() -> None:\n+    history: list[TResponseInputItem] = [\n+        cast(TResponseInputItem, {\"id\": \"msg1\", \"content\": \"hi\", \"role\": \"user\"}),\n+        cast(TResponseInputItem, {\"id\": \"msg2\", \"content\": \"hello\", \"role\": \"assistant\"}),\n+    ]\n+    session = SimpleListSession(history=history)\n+\n+    items = await session.get_items()\n+    # get_items should return a copy, not the original list.\n+    assert items == history\n+    assert items is not history\n+    # saved_items should mirror the stored list.\n+    assert session.saved_items == history\n+\n+\n+@pytest.mark.asyncio\n+async def test_counting_session_tracks_pop_calls() -> None:\n+    session = CountingSession(\n+        history=[cast(TResponseInputItem, {\"id\": \"x\", \"content\": \"hi\", \"role\": \"user\"})]\n+    )\n+\n+    assert session.pop_calls == 0\n+    await session.pop_item()\n+    assert session.pop_calls == 1\n+    await session.pop_item()\n+    assert session.pop_calls == 2\n+\n+\n+@pytest.mark.asyncio\n+async def test_id_stripping_session_removes_ids_on_add() -> None:\n+    session = IdStrippingSession()\n+    items: list[TResponseInputItem] = [\n+        cast(TResponseInputItem, {\"id\": \"keep-removed\", \"content\": \"hello\", \"role\": \"user\"}),\n+        cast(TResponseInputItem, {\"content\": \"no-id\", \"role\": \"assistant\"}),\n+    ]\n+\n+    await session.add_items(items)\n+    stored = await session.get_items()\n+\n+    assert all(\"id\" not in item for item in stored if isinstance(item, dict))\n+    # pop_calls should increment when rewinding.\n+    await session.pop_item()\n+    assert session.pop_calls == 1",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/1bd1b1d31d24ef8fb169dd47c9fed837cb617223/tests%2Futils%2Ftest_simple_session.py",
        "sha": "b3629bdbbce1b24ca3fcbfd2be6787566a714d07",
        "status": "added"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:54:50.337830Z"
}
