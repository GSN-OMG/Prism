{
  "finished_at": "2026-01-20T04:58:22.750499Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "4114e21fcdc2d7fc",
    "tag": "rest_pr_files_pr2334_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2334/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "33970",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:58:22 GMT",
      "etag": "\"309927a726c8e3d1dbaee782eb133858b684f35f81822e0b3057836481128afa\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Tue, 20 Jan 2026 00:35:02 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DD2D:221B33:1544A34:1E4FF22:696F0B6E",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4927",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "73",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 23,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/examples%2Fmemory%2Fcompaction_session_example.py",
        "changes": 25,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmemory%2Fcompaction_session_example.py?ref=d65019177f5905355ce2d5f3ad4516a7fdc66fdf",
        "deletions": 2,
        "filename": "examples/memory/compaction_session_example.py",
        "patch": "@@ -45,9 +45,9 @@ async def main():\n         result = await Runner.run(agent, prompt, session=session)\n         print(f\"Assistant: {result.final_output}\\n\")\n \n-    # Show final session state\n+    # Show session state after automatic compaction (if triggered)\n     items = await session.get_items()\n-    print(\"=== Final Session State ===\")\n+    print(\"=== Session State (Auto Compaction) ===\")\n     print(f\"Total items: {len(items)}\")\n     for item in items:\n         # Some inputs are stored as easy messages (only `role` and `content`).\n@@ -59,6 +59,27 @@ async def main():\n             print(f\"  - message ({role})\")\n         else:\n             print(f\"  - {item_type}\")\n+    print()\n+\n+    # Manual compaction after inspecting the auto-compacted state.\n+    print(\"=== Manual Compaction ===\")\n+    await session.run_compaction({\"force\": True})\n+    print(\"Done\")\n+    print()\n+\n+    # Show final session state after manual compaction\n+    items = await session.get_items()\n+    print(\"=== Session State (Manual Compaction) ===\")\n+    print(f\"Total items: {len(items)}\")\n+    for item in items:\n+        item_type = item.get(\"type\") or (\"message\" if \"role\" in item else \"unknown\")\n+        if item_type == \"compaction\":\n+            print(\"  - compaction (encrypted content)\")\n+        elif item_type == \"message\":\n+            role = item.get(\"role\", \"unknown\")\n+            print(f\"  - message ({role})\")\n+        else:\n+            print(f\"  - {item_type}\")\n \n \n if __name__ == \"__main__\":",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/examples%2Fmemory%2Fcompaction_session_example.py",
        "sha": "73c539f30ad97f738341ba1e2a99a2e4e4ec17f4",
        "status": "modified"
      },
      {
        "additions": 85,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/examples%2Fmemory%2Fcompaction_session_stateless_example.py",
        "changes": 85,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmemory%2Fcompaction_session_stateless_example.py?ref=d65019177f5905355ce2d5f3ad4516a7fdc66fdf",
        "deletions": 0,
        "filename": "examples/memory/compaction_session_stateless_example.py",
        "patch": "@@ -0,0 +1,85 @@\n+\"\"\"\n+Example demonstrating stateless compaction with store=False.\n+\n+In auto mode, OpenAIResponsesCompactionSession uses input-based compaction when\n+responses are not stored on the server.\n+\"\"\"\n+\n+import asyncio\n+\n+from agents import Agent, ModelSettings, OpenAIResponsesCompactionSession, Runner, SQLiteSession\n+\n+\n+async def main():\n+    # Create an underlying session for storage\n+    underlying = SQLiteSession(\":memory:\")\n+\n+    # Wrap with compaction session in auto mode. When store=False, this will\n+    # compact using the locally stored input items.\n+    session = OpenAIResponsesCompactionSession(\n+        session_id=\"demo-session\",\n+        underlying_session=underlying,\n+        model=\"gpt-4.1\",\n+        compaction_mode=\"auto\",\n+        should_trigger_compaction=lambda ctx: len(ctx[\"compaction_candidate_items\"]) >= 3,\n+    )\n+\n+    agent = Agent(\n+        name=\"Assistant\",\n+        instructions=\"Reply concisely. Keep answers to 1-2 sentences.\",\n+        model_settings=ModelSettings(store=False),\n+    )\n+\n+    print(\"=== Stateless Compaction Session Example ===\\n\")\n+\n+    prompts = [\n+        \"What is the tallest mountain in the world?\",\n+        \"How tall is it in feet?\",\n+        \"When was it first climbed?\",\n+        \"Who was on that expedition?\",\n+    ]\n+\n+    for i, prompt in enumerate(prompts, 1):\n+        print(f\"Turn {i}:\")\n+        print(f\"User: {prompt}\")\n+        result = await Runner.run(agent, prompt, session=session)\n+        print(f\"Assistant: {result.final_output}\\n\")\n+\n+    # Show session state after automatic compaction (if triggered)\n+    items = await session.get_items()\n+    print(\"=== Session State (Auto Compaction) ===\")\n+    print(f\"Total items: {len(items)}\")\n+    for item in items:\n+        item_type = item.get(\"type\") or (\"message\" if \"role\" in item else \"unknown\")\n+        if item_type == \"compaction\":\n+            print(\"  - compaction (encrypted content)\")\n+        elif item_type == \"message\":\n+            role = item.get(\"role\", \"unknown\")\n+            print(f\"  - message ({role})\")\n+        else:\n+            print(f\"  - {item_type}\")\n+    print()\n+\n+    # Manual compaction in stateless mode.\n+    print(\"=== Manual Compaction ===\")\n+    await session.run_compaction({\"force\": True})\n+    print(\"Done\")\n+    print()\n+\n+    # Show final session state\n+    items = await session.get_items()\n+    print(\"=== Final Session State ===\")\n+    print(f\"Total items: {len(items)}\")\n+    for item in items:\n+        item_type = item.get(\"type\") or (\"message\" if \"role\" in item else \"unknown\")\n+        if item_type == \"compaction\":\n+            print(\"  - compaction (encrypted content)\")\n+        elif item_type == \"message\":\n+            role = item.get(\"role\", \"unknown\")\n+            print(f\"  - message ({role})\")\n+        else:\n+            print(f\"  - {item_type}\")\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/examples%2Fmemory%2Fcompaction_session_stateless_example.py",
        "sha": "87c685aca7a7201c3c2701b9885df98e7799d3fd",
        "status": "added"
      },
      {
        "additions": 85,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/src%2Fagents%2Fmemory%2Fopenai_responses_compaction_session.py",
        "changes": 96,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmemory%2Fopenai_responses_compaction_session.py?ref=d65019177f5905355ce2d5f3ad4516a7fdc66fdf",
        "deletions": 11,
        "filename": "src/agents/memory/openai_responses_compaction_session.py",
        "patch": "@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Callable\n+from typing import TYPE_CHECKING, Any, Callable, Literal\n \n from openai import AsyncOpenAI\n \n@@ -21,6 +21,8 @@\n \n DEFAULT_COMPACTION_THRESHOLD = 10\n \n+OpenAIResponsesCompactionMode = Literal[\"previous_response_id\", \"input\", \"auto\"]\n+\n \n def select_compaction_candidate_items(\n     items: list[TResponseInputItem],\n@@ -85,6 +87,7 @@ def __init__(\n         *,\n         client: AsyncOpenAI | None = None,\n         model: str = \"gpt-4.1\",\n+        compaction_mode: OpenAIResponsesCompactionMode = \"auto\",\n         should_trigger_compaction: Callable[[dict[str, Any]], bool] | None = None,\n     ):\n         \"\"\"Initialize the compaction session.\n@@ -97,6 +100,9 @@ def __init__(\n                 get_default_openai_client() or new AsyncOpenAI().\n             model: Model to use for responses.compact. Defaults to \"gpt-4.1\". Must be an\n                 OpenAI model name (gpt-*, o*, or ft:gpt-*).\n+            compaction_mode: Controls how the compaction request provides conversation\n+                history. \"auto\" (default) uses input when the last response was not\n+                stored or no response_id is available.\n             should_trigger_compaction: Custom decision hook. Defaults to triggering when\n                 10+ compaction candidates exist.\n         \"\"\"\n@@ -113,6 +119,7 @@ def __init__(\n         self.underlying_session = underlying_session\n         self._client = client\n         self.model = model\n+        self.compaction_mode = compaction_mode\n         self.should_trigger_compaction = (\n             should_trigger_compaction or default_should_trigger_compaction\n         )\n@@ -122,21 +129,54 @@ def __init__(\n         self._session_items: list[TResponseInputItem] | None = None\n         self._response_id: str | None = None\n         self._deferred_response_id: str | None = None\n+        self._last_unstored_response_id: str | None = None\n \n     @property\n     def client(self) -> AsyncOpenAI:\n         if self._client is None:\n             self._client = get_default_openai_client() or AsyncOpenAI()\n         return self._client\n \n+    def _resolve_compaction_mode_for_response(\n+        self,\n+        *,\n+        response_id: str | None,\n+        store: bool | None,\n+        requested_mode: OpenAIResponsesCompactionMode | None,\n+    ) -> _ResolvedCompactionMode:\n+        mode = requested_mode or self.compaction_mode\n+        if (\n+            mode == \"auto\"\n+            and store is None\n+            and response_id is not None\n+            and response_id == self._last_unstored_response_id\n+        ):\n+            return \"input\"\n+        return _resolve_compaction_mode(mode, response_id=response_id, store=store)\n+\n     async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -> None:\n         \"\"\"Run compaction using responses.compact API.\"\"\"\n         if args and args.get(\"response_id\"):\n             self._response_id = args[\"response_id\"]\n+        requested_mode = args.get(\"compaction_mode\") if args else None\n+        if args and \"store\" in args:\n+            store = args[\"store\"]\n+            if store is False and self._response_id:\n+                self._last_unstored_response_id = self._response_id\n+            elif store is True and self._response_id == self._last_unstored_response_id:\n+                self._last_unstored_response_id = None\n+        else:\n+            store = None\n+        resolved_mode = self._resolve_compaction_mode_for_response(\n+            response_id=self._response_id,\n+            store=store,\n+            requested_mode=requested_mode,\n+        )\n \n-        if not self._response_id:\n+        if resolved_mode == \"previous_response_id\" and not self._response_id:\n             raise ValueError(\n-                \"OpenAIResponsesCompactionSession.run_compaction requires a response_id\"\n+                \"OpenAIResponsesCompactionSession.run_compaction requires a response_id \"\n+                \"when using previous_response_id compaction.\"\n             )\n \n         compaction_candidate_items, session_items = await self._ensure_compaction_candidates()\n@@ -145,23 +185,32 @@ async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None\n         should_compact = force or self.should_trigger_compaction(\n             {\n                 \"response_id\": self._response_id,\n+                \"compaction_mode\": resolved_mode,\n                 \"compaction_candidate_items\": compaction_candidate_items,\n                 \"session_items\": session_items,\n             }\n         )\n \n         if not should_compact:\n-            logger.debug(f\"skip: decision hook declined compaction for {self._response_id}\")\n+            logger.debug(\n+                f\"skip: decision hook declined compaction for {self._response_id} \"\n+                f\"(mode={resolved_mode})\"\n+            )\n             return\n \n         self._deferred_response_id = None\n-        logger.debug(f\"compact: start for {self._response_id} using {self.model}\")\n-\n-        compacted = await self.client.responses.compact(\n-            previous_response_id=self._response_id,\n-            model=self.model,\n+        logger.debug(\n+            f\"compact: start for {self._response_id} using {self.model} (mode={resolved_mode})\"\n         )\n \n+        compact_kwargs: dict[str, Any] = {\"model\": self.model}\n+        if resolved_mode == \"previous_response_id\":\n+            compact_kwargs[\"previous_response_id\"] = self._response_id\n+        else:\n+            compact_kwargs[\"input\"] = session_items\n+\n+        compacted = await self.client.responses.compact(**compact_kwargs)\n+\n         await self.underlying_session.clear_session()\n         output_items: list[TResponseInputItem] = []\n         if compacted.output:\n@@ -183,19 +232,26 @@ async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None\n \n         logger.debug(\n             f\"compact: done for {self._response_id} \"\n-            f\"(output={len(output_items)}, candidates={len(self._compaction_candidate_items)})\"\n+            f\"(mode={resolved_mode}, output={len(output_items)}, \"\n+            f\"candidates={len(self._compaction_candidate_items)})\"\n         )\n \n     async def get_items(self, limit: int | None = None) -> list[TResponseInputItem]:\n         return await self.underlying_session.get_items(limit)\n \n-    async def _defer_compaction(self, response_id: str) -> None:\n+    async def _defer_compaction(self, response_id: str, store: bool | None = None) -> None:\n         if self._deferred_response_id is not None:\n             return\n         compaction_candidate_items, session_items = await self._ensure_compaction_candidates()\n+        resolved_mode = self._resolve_compaction_mode_for_response(\n+            response_id=response_id,\n+            store=store,\n+            requested_mode=None,\n+        )\n         should_compact = self.should_trigger_compaction(\n             {\n                 \"response_id\": response_id,\n+                \"compaction_mode\": resolved_mode,\n                 \"compaction_candidate_items\": compaction_candidate_items,\n                 \"session_items\": session_items,\n             }\n@@ -247,3 +303,21 @@ async def _ensure_compaction_candidates(\n             f\"candidates: initialized (history={len(history)}, candidates={len(candidates)})\"\n         )\n         return (candidates[:], history[:])\n+\n+\n+_ResolvedCompactionMode = Literal[\"previous_response_id\", \"input\"]\n+\n+\n+def _resolve_compaction_mode(\n+    requested_mode: OpenAIResponsesCompactionMode,\n+    *,\n+    response_id: str | None,\n+    store: bool | None,\n+) -> _ResolvedCompactionMode:\n+    if requested_mode != \"auto\":\n+        return requested_mode\n+    if store is False:\n+        return \"input\"\n+    if not response_id:\n+        return \"input\"\n+    return \"previous_response_id\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/src%2Fagents%2Fmemory%2Fopenai_responses_compaction_session.py",
        "sha": "e2148f48689a650174deb210f95ce4866a6c5cf9",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/src%2Fagents%2Fmemory%2Fsession.py",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmemory%2Fsession.py?ref=d65019177f5905355ce2d5f3ad4516a7fdc66fdf",
        "deletions": 1,
        "filename": "src/agents/memory/session.py",
        "patch": "@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from typing import TYPE_CHECKING, Protocol, runtime_checkable\n+from typing import TYPE_CHECKING, Literal, Protocol, runtime_checkable\n \n from typing_extensions import TypedDict, TypeGuard\n \n@@ -107,6 +107,20 @@ class OpenAIResponsesCompactionArgs(TypedDict, total=False):\n     response_id: str\n     \"\"\"The ID of the last response to use for compaction.\"\"\"\n \n+    compaction_mode: Literal[\"previous_response_id\", \"input\", \"auto\"]\n+    \"\"\"How to provide history for compaction.\n+\n+    - \"auto\": Use input when the last response was not stored or no response ID is available.\n+    - \"previous_response_id\": Use server-managed response history.\n+    - \"input\": Send locally stored session items as input.\n+    \"\"\"\n+\n+    store: bool\n+    \"\"\"Whether the last model response was stored on the server.\n+\n+    When set to False, compaction should avoid \"previous_response_id\" unless explicitly requested.\n+    \"\"\"\n+\n     force: bool\n     \"\"\"Whether to force compaction even if the threshold is not met.\"\"\"\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/src%2Fagents%2Fmemory%2Fsession.py",
        "sha": "1f5b33663f570943247747bd290c657ab4650cf4",
        "status": "modified"
      },
      {
        "additions": 27,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/src%2Fagents%2Frun.py",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frun.py?ref=d65019177f5905355ce2d5f3ad4516a7fdc66fdf",
        "deletions": 3,
        "filename": "src/agents/run.py",
        "patch": "@@ -61,7 +61,12 @@\n )\n from .lifecycle import AgentHooksBase, RunHooks, RunHooksBase\n from .logger import logger\n-from .memory import Session, SessionInputCallback, is_openai_responses_compaction_aware_session\n+from .memory import (\n+    OpenAIResponsesCompactionArgs,\n+    Session,\n+    SessionInputCallback,\n+    is_openai_responses_compaction_aware_session,\n+)\n from .model_settings import ModelSettings\n from .models.interface import Model, ModelProvider\n from .models.multi_provider import MultiProvider\n@@ -717,6 +722,9 @@ async def run(\n                         else turn_result.new_step_items\n                     )\n                     session_items.extend(session_items_for_turn)\n+                    store_setting = current_agent.model_settings.resolve(\n+                        run_config.model_settings\n+                    ).store\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -757,6 +765,7 @@ async def run(\n                                     if turn_result.session_step_items is not None\n                                     else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n+                                    store=store_setting,\n                                 )\n \n                             return result\n@@ -774,6 +783,7 @@ async def run(\n                                         if turn_result.session_step_items is not None\n                                         else turn_result.new_step_items,\n                                         turn_result.model_response.response_id,\n+                                        store=store_setting,\n                                     )\n                             current_agent = cast(Agent[TContext], turn_result.next_step.new_agent)\n                             current_span.finish(reset_current=True)\n@@ -791,6 +801,7 @@ async def run(\n                                     if turn_result.session_step_items is not None\n                                     else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n+                                    store=store_setting,\n                                 )\n                         else:\n                             raise AgentsException(\n@@ -1255,6 +1266,9 @@ async def _start_streaming(\n                         else turn_result.new_step_items\n                     )\n                     streamed_result.new_items.extend(session_items_for_turn)\n+                    store_setting = current_agent.model_settings.resolve(\n+                        run_config.model_settings\n+                    ).store\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -1276,6 +1290,7 @@ async def _start_streaming(\n                                     if turn_result.session_step_items is not None\n                                     else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n+                                    store=store_setting,\n                                 )\n \n                         current_agent = turn_result.next_step.new_agent\n@@ -1327,6 +1342,7 @@ async def _start_streaming(\n                                     if turn_result.session_step_items is not None\n                                     else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n+                                    store=store_setting,\n                                 )\n \n                         streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n@@ -1345,6 +1361,7 @@ async def _start_streaming(\n                                     if turn_result.session_step_items is not None\n                                     else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n+                                    store=store_setting,\n                                 )\n \n                         # Check for soft cancel after turn completion\n@@ -2075,6 +2092,7 @@ async def _save_result_to_session(\n         original_input: str | list[TResponseInputItem],\n         new_items: list[RunItem],\n         response_id: str | None = None,\n+        store: bool | None = None,\n     ) -> None:\n         \"\"\"\n         Save the conversation turn to session.\n@@ -2102,7 +2120,7 @@ async def _save_result_to_session(\n             if has_local_tool_outputs:\n                 defer_compaction = getattr(session, \"_defer_compaction\", None)\n                 if callable(defer_compaction):\n-                    result = defer_compaction(response_id)\n+                    result = defer_compaction(response_id, store=store)\n                     if inspect.isawaitable(result):\n                         await result\n                 logger.debug(\n@@ -2121,7 +2139,13 @@ async def _save_result_to_session(\n                     response_id,\n                     deferred_response_id,\n                 )\n-            await session.run_compaction({\"response_id\": response_id, \"force\": force_compaction})\n+            compaction_args: OpenAIResponsesCompactionArgs = {\n+                \"response_id\": response_id,\n+                \"force\": force_compaction,\n+            }\n+            if store is not None:\n+                compaction_args[\"store\"] = store\n+            await session.run_compaction(compaction_args)\n \n     @staticmethod\n     async def _input_guardrail_tripwire_triggered_for_stream(",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/src%2Fagents%2Frun.py",
        "sha": "d8d2a82c8b0be205f2ec14748c207efa296fe914",
        "status": "modified"
      },
      {
        "additions": 212,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/tests%2Fmemory%2Ftest_openai_responses_compaction_session.py",
        "changes": 213,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Fmemory%2Ftest_openai_responses_compaction_session.py?ref=d65019177f5905355ce2d5f3ad4516a7fdc66fdf",
        "deletions": 1,
        "filename": "tests/memory/test_openai_responses_compaction_session.py",
        "patch": "@@ -142,11 +142,189 @@ async def test_run_compaction_requires_response_id(self) -> None:\n         session = OpenAIResponsesCompactionSession(\n             session_id=\"test\",\n             underlying_session=mock_session,\n+            compaction_mode=\"previous_response_id\",\n         )\n \n-        with pytest.raises(ValueError, match=\"requires a response_id\"):\n+        with pytest.raises(ValueError, match=\"previous_response_id compaction\"):\n             await session.run_compaction()\n \n+    @pytest.mark.asyncio\n+    async def test_run_compaction_input_mode_without_response_id(self) -> None:\n+        mock_session = self.create_mock_session()\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"user\", \"content\": \"hello\"}),\n+            cast(\n+                TResponseInputItem,\n+                {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"world\"},\n+            ),\n+        ]\n+        mock_session.get_items.return_value = items\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = [\n+            {\n+                \"type\": \"message\",\n+                \"role\": \"assistant\",\n+                \"content\": \"compacted\",\n+            }\n+        ]\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+            compaction_mode=\"input\",\n+        )\n+\n+        await session.run_compaction({\"force\": True})\n+\n+        mock_client.responses.compact.assert_called_once()\n+        call_kwargs = mock_client.responses.compact.call_args.kwargs\n+        assert call_kwargs.get(\"model\") == \"gpt-4.1\"\n+        assert \"previous_response_id\" not in call_kwargs\n+        assert call_kwargs.get(\"input\") == items\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_auto_without_response_id_uses_input(self) -> None:\n+        mock_session = self.create_mock_session()\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"user\", \"content\": \"hello\"}),\n+        ]\n+        mock_session.get_items.return_value = items\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = []\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+        )\n+\n+        await session.run_compaction({\"force\": True})\n+\n+        mock_client.responses.compact.assert_called_once()\n+        call_kwargs = mock_client.responses.compact.call_args.kwargs\n+        assert \"previous_response_id\" not in call_kwargs\n+        assert call_kwargs.get(\"input\") == items\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_auto_uses_input_when_store_false(self) -> None:\n+        mock_session = self.create_mock_session()\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"user\", \"content\": \"hello\"}),\n+            cast(\n+                TResponseInputItem,\n+                {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"world\"},\n+            ),\n+        ]\n+        mock_session.get_items.return_value = items\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = []\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+            compaction_mode=\"auto\",\n+        )\n+\n+        await session.run_compaction({\"response_id\": \"resp-auto\", \"store\": False, \"force\": True})\n+\n+        mock_client.responses.compact.assert_called_once()\n+        call_kwargs = mock_client.responses.compact.call_args.kwargs\n+        assert call_kwargs.get(\"model\") == \"gpt-4.1\"\n+        assert \"previous_response_id\" not in call_kwargs\n+        assert call_kwargs.get(\"input\") == items\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_auto_uses_default_store_when_unset(self) -> None:\n+        mock_session = self.create_mock_session()\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"user\", \"content\": \"hello\"}),\n+            cast(\n+                TResponseInputItem,\n+                {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"world\"},\n+            ),\n+        ]\n+        mock_session.get_items.return_value = items\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = []\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+            compaction_mode=\"auto\",\n+        )\n+\n+        await session.run_compaction({\"response_id\": \"resp-auto\", \"store\": False, \"force\": True})\n+        await session.run_compaction({\"response_id\": \"resp-stored\", \"force\": True})\n+\n+        assert mock_client.responses.compact.call_count == 2\n+        first_kwargs = mock_client.responses.compact.call_args_list[0].kwargs\n+        second_kwargs = mock_client.responses.compact.call_args_list[1].kwargs\n+        assert \"previous_response_id\" not in first_kwargs\n+        assert second_kwargs.get(\"previous_response_id\") == \"resp-stored\"\n+        assert \"input\" not in second_kwargs\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_auto_uses_input_when_last_response_unstored(self) -> None:\n+        mock_session = self.create_mock_session()\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"user\", \"content\": \"hello\"}),\n+            cast(\n+                TResponseInputItem,\n+                {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"world\"},\n+            ),\n+        ]\n+        mock_session.get_items.return_value = items\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = [\n+            {\n+                \"type\": \"message\",\n+                \"role\": \"assistant\",\n+                \"content\": \"compacted\",\n+            }\n+        ]\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+            compaction_mode=\"auto\",\n+        )\n+\n+        await session.run_compaction(\n+            {\"response_id\": \"resp-unstored\", \"store\": False, \"force\": True}\n+        )\n+        await session.run_compaction({\"force\": True})\n+\n+        assert mock_client.responses.compact.call_count == 2\n+        first_kwargs = mock_client.responses.compact.call_args_list[0].kwargs\n+        second_kwargs = mock_client.responses.compact.call_args_list[1].kwargs\n+        assert \"previous_response_id\" not in first_kwargs\n+        assert \"previous_response_id\" not in second_kwargs\n+        assert second_kwargs.get(\"input\") == mock_compact_response.output\n+\n     @pytest.mark.asyncio\n     async def test_run_compaction_skips_when_below_threshold(self) -> None:\n         mock_session = self.create_mock_session()\n@@ -315,6 +493,39 @@ async def test_compaction_skips_when_tool_outputs_present(self) -> None:\n \n         mock_client.responses.compact.assert_not_called()\n \n+    @pytest.mark.asyncio\n+    async def test_deferred_compaction_includes_compaction_mode_in_context(self) -> None:\n+        underlying = SimpleListSession()\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock()\n+        observed = {}\n+\n+        def should_trigger_compaction(context: dict[str, Any]) -> bool:\n+            observed[\"mode\"] = context[\"compaction_mode\"]\n+            return False\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"demo\",\n+            underlying_session=underlying,\n+            client=mock_client,\n+            compaction_mode=\"input\",\n+            should_trigger_compaction=should_trigger_compaction,\n+        )\n+\n+        tool = get_function_tool(name=\"do_thing\", return_value=\"done\")\n+        model = FakeModel(initial_output=[get_function_tool_call(\"do_thing\")])\n+        agent = Agent(\n+            name=\"assistant\",\n+            model=model,\n+            tools=[tool],\n+            tool_use_behavior=\"stop_on_first_tool\",\n+        )\n+\n+        await Runner.run(agent, \"hello\", session=session)\n+\n+        assert observed[\"mode\"] == \"input\"\n+        mock_client.responses.compact.assert_not_called()\n+\n     @pytest.mark.asyncio\n     async def test_compaction_runs_after_deferred_tool_outputs_when_due(self) -> None:\n         underlying = SimpleListSession()",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/d65019177f5905355ce2d5f3ad4516a7fdc66fdf/tests%2Fmemory%2Ftest_openai_responses_compaction_session.py",
        "sha": "7af406a60294830f3f3d7753ffb9a8923b31b6be",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:58:22.150725Z"
}
