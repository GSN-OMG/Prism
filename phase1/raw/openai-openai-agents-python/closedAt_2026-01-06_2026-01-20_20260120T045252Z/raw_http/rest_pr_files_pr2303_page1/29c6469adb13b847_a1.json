{
  "finished_at": "2026-01-20T04:56:40.075972Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "29c6469adb13b847",
    "tag": "rest_pr_files_pr2303_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2303/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "86689",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:56:39 GMT",
      "etag": "\"b2a0b49db0600ca9bbe307256f051a196c9c33533f2cdd83813663124b8349a0\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Tue, 13 Jan 2026 13:59:17 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DC38:E9ECE:1618CE1:1F23EEF:696F0B07",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4952",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "48",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 77,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/.codex%2Fskills%2Fexamples-auto-run%2FSKILL.md",
        "changes": 77,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/.codex%2Fskills%2Fexamples-auto-run%2FSKILL.md?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 0,
        "filename": ".codex/skills/examples-auto-run/SKILL.md",
        "patch": "@@ -0,0 +1,77 @@\n+---\n+name: examples-auto-run\n+description: Run python examples in auto mode with logging, rerun helpers, and background control.\n+---\n+\n+# examples-auto-run\n+\n+## What it does\n+\n+- Runs `uv run examples/run_examples.py` with:\n+  - `EXAMPLES_INTERACTIVE_MODE=auto` (auto-input/auto-approve).\n+  - Per-example logs under `.tmp/examples-start-logs/`.\n+  - Main summary log path passed via `--main-log` (also under `.tmp/examples-start-logs/`).\n+  - Generates a rerun list of failures at `.tmp/examples-rerun.txt` when `--write-rerun` is set.\n+- Provides start/stop/status/logs/tail/collect/rerun helpers via `run.sh`.\n+- Background option keeps the process running with a pidfile; `stop` cleans it up.\n+\n+## Usage\n+\n+```bash\n+# Start (auto mode; interactive included by default)\n+.codex/skills/examples-auto-run/scripts/run.sh start [extra args to run_examples.py]\n+# Examples:\n+.codex/skills/examples-auto-run/scripts/run.sh start --filter basic\n+.codex/skills/examples-auto-run/scripts/run.sh start --include-server --include-audio\n+\n+# Check status\n+.codex/skills/examples-auto-run/scripts/run.sh status\n+\n+# Stop running job\n+.codex/skills/examples-auto-run/scripts/run.sh stop\n+\n+# List logs\n+.codex/skills/examples-auto-run/scripts/run.sh logs\n+\n+# Tail latest log (or specify one)\n+.codex/skills/examples-auto-run/scripts/run.sh tail\n+.codex/skills/examples-auto-run/scripts/run.sh tail main_20260113-123000.log\n+\n+# Collect rerun list from a main log (defaults to latest main_*.log)\n+.codex/skills/examples-auto-run/scripts/run.sh collect\n+\n+# Rerun only failed entries from rerun file (auto mode)\n+.codex/skills/examples-auto-run/scripts/run.sh rerun\n+```\n+\n+## Defaults (overridable via env)\n+\n+- `EXAMPLES_INTERACTIVE_MODE=auto`\n+- `EXAMPLES_INCLUDE_INTERACTIVE=1`\n+- `EXAMPLES_INCLUDE_SERVER=0`\n+- `EXAMPLES_INCLUDE_AUDIO=0`\n+- `EXAMPLES_INCLUDE_EXTERNAL=0`\n+- Auto-approvals in auto mode: `APPLY_PATCH_AUTO_APPROVE=1`, `SHELL_AUTO_APPROVE=1`, `AUTO_APPROVE_MCP=1`\n+\n+## Log locations\n+\n+- Main logs: `.tmp/examples-start-logs/main_*.log`\n+- Per-example logs (from `run_examples.py`): `.tmp/examples-start-logs/<module_path>.log`\n+- Rerun list: `.tmp/examples-rerun.txt`\n+- Stdout logs: `.tmp/examples-start-logs/stdout_*.log`\n+\n+## Notes\n+\n+- The runner delegates to `uv run examples/run_examples.py`, which already writes per-example logs and supports `--collect`, `--rerun-file`, and `--print-auto-skip`.\n+- `start` uses `--write-rerun` so failures are captured automatically.\n+- If `.tmp/examples-rerun.txt` exists and is non-empty, invoking the skill with no args runs `rerun` by default.\n+\n+## Behavioral validation (Codex/LLM responsibility)\n+\n+The runner does not perform any automated behavioral validation. After every foreground `start` or `rerun`, **Codex must manually validate** all exit-0 entries:\n+\n+1. Read the example source (and comments) to infer intended flow, tools used, and expected key outputs.\n+2. Open the matching per-example log under `.tmp/examples-start-logs/`.\n+3. Confirm the intended actions/results occurred; flag omissions or divergences.\n+4. Do this for **all passed examples**, not just a sample.\n+5. Report immediately after the run with concise citations to the exact log lines that justify the validation.",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/.codex%2Fskills%2Fexamples-auto-run%2FSKILL.md",
        "sha": "cca492b6a8dbc122013e485d960467d28e5c7739",
        "status": "added"
      },
      {
        "additions": 215,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/.codex%2Fskills%2Fexamples-auto-run%2Fscripts%2Frun.sh",
        "changes": 215,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/.codex%2Fskills%2Fexamples-auto-run%2Fscripts%2Frun.sh?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 0,
        "filename": ".codex/skills/examples-auto-run/scripts/run.sh",
        "patch": "@@ -0,0 +1,215 @@\n+#!/usr/bin/env bash\n+set -euo pipefail\n+\n+ROOT=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")/../../../..\" && pwd)\"\n+PID_FILE=\"$ROOT/.tmp/examples-auto-run.pid\"\n+LOG_DIR=\"$ROOT/.tmp/examples-start-logs\"\n+RERUN_FILE=\"$ROOT/.tmp/examples-rerun.txt\"\n+\n+ensure_dirs() {\n+  mkdir -p \"$LOG_DIR\" \"$ROOT/.tmp\"\n+}\n+\n+is_running() {\n+  local pid=\"$1\"\n+  [[ -n \"$pid\" ]] && ps -p \"$pid\" >/dev/null 2>&1\n+}\n+\n+cmd_start() {\n+  ensure_dirs\n+  local background=0\n+  if [[ \"${1:-}\" == \"--background\" ]]; then\n+    background=1\n+    shift\n+  fi\n+\n+  local ts main_log stdout_log\n+  ts=\"$(date +%Y%m%d-%H%M%S)\"\n+  main_log=\"$LOG_DIR/main_${ts}.log\"\n+  stdout_log=\"$LOG_DIR/stdout_${ts}.log\"\n+\n+  local run_cmd=(\n+    uv run examples/run_examples.py\n+    --auto-mode\n+    --write-rerun\n+    --main-log \"$main_log\"\n+    --logs-dir \"$LOG_DIR\"\n+  )\n+\n+  if [[ \"$background\" -eq 1 ]]; then\n+    if [[ -f \"$PID_FILE\" ]]; then\n+      local pid\n+      pid=\"$(cat \"$PID_FILE\" 2>/dev/null || true)\"\n+      if is_running \"$pid\"; then\n+        echo \"examples/run_examples.py already running (pid=$pid).\"\n+        exit 1\n+      fi\n+    fi\n+    (\n+      trap '' HUP\n+      export EXAMPLES_INTERACTIVE_MODE=\"${EXAMPLES_INTERACTIVE_MODE:-auto}\"\n+      export APPLY_PATCH_AUTO_APPROVE=\"${APPLY_PATCH_AUTO_APPROVE:-1}\"\n+      export SHELL_AUTO_APPROVE=\"${SHELL_AUTO_APPROVE:-1}\"\n+      export AUTO_APPROVE_MCP=\"${AUTO_APPROVE_MCP:-1}\"\n+      export EXAMPLES_INCLUDE_INTERACTIVE=\"${EXAMPLES_INCLUDE_INTERACTIVE:-1}\"\n+      export EXAMPLES_INCLUDE_SERVER=\"${EXAMPLES_INCLUDE_SERVER:-0}\"\n+      export EXAMPLES_INCLUDE_AUDIO=\"${EXAMPLES_INCLUDE_AUDIO:-0}\"\n+      export EXAMPLES_INCLUDE_EXTERNAL=\"${EXAMPLES_INCLUDE_EXTERNAL:-0}\"\n+      cd \"$ROOT\"\n+      exec \"${run_cmd[@]}\" \"$@\" > >(tee \"$stdout_log\") 2>&1\n+    ) &\n+    local pid=$!\n+    echo \"$pid\" >\"$PID_FILE\"\n+    echo \"Started run_examples.py (pid=$pid)\"\n+    echo \"Main log: $main_log\"\n+    echo \"Stdout log: $stdout_log\"\n+    echo \"Run '.codex/skills/examples-auto-run/scripts/run.sh validate \\\"$main_log\\\"' after it finishes.\"\n+    return 0\n+  fi\n+\n+  export EXAMPLES_INTERACTIVE_MODE=\"${EXAMPLES_INTERACTIVE_MODE:-auto}\"\n+  export APPLY_PATCH_AUTO_APPROVE=\"${APPLY_PATCH_AUTO_APPROVE:-1}\"\n+  export SHELL_AUTO_APPROVE=\"${SHELL_AUTO_APPROVE:-1}\"\n+  export AUTO_APPROVE_MCP=\"${AUTO_APPROVE_MCP:-1}\"\n+  export EXAMPLES_INCLUDE_INTERACTIVE=\"${EXAMPLES_INCLUDE_INTERACTIVE:-1}\"\n+  export EXAMPLES_INCLUDE_SERVER=\"${EXAMPLES_INCLUDE_SERVER:-0}\"\n+  export EXAMPLES_INCLUDE_AUDIO=\"${EXAMPLES_INCLUDE_AUDIO:-0}\"\n+  export EXAMPLES_INCLUDE_EXTERNAL=\"${EXAMPLES_INCLUDE_EXTERNAL:-0}\"\n+  cd \"$ROOT\"\n+  set +e\n+  \"${run_cmd[@]}\" \"$@\" 2>&1 | tee \"$stdout_log\"\n+  local run_status=${PIPESTATUS[0]}\n+  set -e\n+  return \"$run_status\"\n+}\n+\n+cmd_stop() {\n+  if [[ ! -f \"$PID_FILE\" ]]; then\n+    echo \"No pid file; nothing to stop.\"\n+    return 0\n+  fi\n+  local pid\n+  pid=\"$(cat \"$PID_FILE\" 2>/dev/null || true)\"\n+  if [[ -z \"$pid\" ]]; then\n+    rm -f \"$PID_FILE\"\n+    echo \"Pid file empty; cleaned.\"\n+    return 0\n+  fi\n+  if ! is_running \"$pid\"; then\n+    rm -f \"$PID_FILE\"\n+    echo \"Process $pid not running; cleaned pid file.\"\n+    return 0\n+  fi\n+  echo \"Stopping pid $pid ...\"\n+  kill \"$pid\" 2>/dev/null || true\n+  sleep 1\n+  if is_running \"$pid\"; then\n+    echo \"Sending SIGKILL to $pid ...\"\n+    kill -9 \"$pid\" 2>/dev/null || true\n+  fi\n+  rm -f \"$PID_FILE\"\n+  echo \"Stopped.\"\n+}\n+\n+cmd_status() {\n+  if [[ -f \"$PID_FILE\" ]]; then\n+    local pid\n+    pid=\"$(cat \"$PID_FILE\" 2>/dev/null || true)\"\n+    if is_running \"$pid\"; then\n+      echo \"Running (pid=$pid)\"\n+      return 0\n+    fi\n+  fi\n+  echo \"Not running.\"\n+}\n+\n+cmd_logs() {\n+  ensure_dirs\n+  ls -1t \"$LOG_DIR\"\n+}\n+\n+cmd_tail() {\n+  ensure_dirs\n+  local file=\"${1:-}\"\n+  if [[ -z \"$file\" ]]; then\n+    file=\"$(ls -1t \"$LOG_DIR\" | head -n1)\"\n+  fi\n+  if [[ -z \"$file\" ]]; then\n+    echo \"No log files yet.\"\n+    exit 1\n+  fi\n+  tail -f \"$LOG_DIR/$file\"\n+}\n+\n+collect_rerun() {\n+  ensure_dirs\n+  local log_file=\"${1:-}\"\n+  if [[ -z \"$log_file\" ]]; then\n+    log_file=\"$(ls -1t \"$LOG_DIR\"/main_*.log 2>/dev/null | head -n1)\"\n+  fi\n+  if [[ -z \"$log_file\" ]] || [[ ! -f \"$log_file\" ]]; then\n+    echo \"No main log file found.\"\n+    exit 1\n+  fi\n+  cd \"$ROOT\"\n+  uv run examples/run_examples.py --collect \"$log_file\" --output \"$RERUN_FILE\"\n+}\n+\n+cmd_rerun() {\n+  ensure_dirs\n+  local file=\"${1:-$RERUN_FILE}\"\n+  if [[ ! -s \"$file\" ]]; then\n+    echo \"Rerun list is empty: $file\"\n+    exit 0\n+  fi\n+  local ts main_log stdout_log\n+  ts=\"$(date +%Y%m%d-%H%M%S)\"\n+  main_log=\"$LOG_DIR/main_${ts}.log\"\n+  stdout_log=\"$LOG_DIR/stdout_${ts}.log\"\n+  cd \"$ROOT\"\n+  export EXAMPLES_INTERACTIVE_MODE=\"${EXAMPLES_INTERACTIVE_MODE:-auto}\"\n+  export APPLY_PATCH_AUTO_APPROVE=\"${APPLY_PATCH_AUTO_APPROVE:-1}\"\n+  export SHELL_AUTO_APPROVE=\"${SHELL_AUTO_APPROVE:-1}\"\n+  export AUTO_APPROVE_MCP=\"${AUTO_APPROVE_MCP:-1}\"\n+  set +e\n+  uv run examples/run_examples.py --auto-mode --rerun-file \"$file\" --write-rerun --main-log \"$main_log\" --logs-dir \"$LOG_DIR\" 2>&1 | tee \"$stdout_log\"\n+  local run_status=${PIPESTATUS[0]}\n+  set -e\n+  return \"$run_status\"\n+}\n+\n+usage() {\n+  cat <<'EOF'\n+Usage: run.sh <start|stop|status|logs|tail|collect|rerun> [args...]\n+\n+Commands:\n+  start [--filter ... | other args]   Run examples in auto mode (foreground). Pass --background to run detached.\n+  stop                                Kill the running auto-run (if any).\n+  status                              Show whether it is running.\n+  logs                                List log files (.tmp/examples-start-logs).\n+  tail [logfile]                      Tail the latest (or specified) log.\n+  collect [main_log]                  Parse a main log and write failed examples to .tmp/examples-rerun.txt.\n+  rerun [rerun_file]                  Run only the examples listed in .tmp/examples-rerun.txt.\n+\n+Environment overrides:\n+  EXAMPLES_INTERACTIVE_MODE (default auto)\n+  EXAMPLES_INCLUDE_SERVER/INTERACTIVE/AUDIO/EXTERNAL (defaults: 0/1/0/0)\n+  APPLY_PATCH_AUTO_APPROVE, SHELL_AUTO_APPROVE, AUTO_APPROVE_MCP (default 1 in auto mode)\n+EOF\n+}\n+\n+default_cmd=\"start\"\n+if [[ $# -eq 0 && -s \"$RERUN_FILE\" ]]; then\n+  default_cmd=\"rerun\"\n+fi\n+\n+case \"${1:-$default_cmd}\" in\n+  start) shift || true; cmd_start \"$@\" ;;\n+  stop) shift || true; cmd_stop ;;\n+  status) shift || true; cmd_status ;;\n+  logs) shift || true; cmd_logs ;;\n+  tail) shift; cmd_tail \"${1:-}\" ;;\n+  collect) shift || true; collect_rerun \"${1:-}\" ;;\n+  rerun) shift || true; cmd_rerun \"${1:-}\" ;;\n+  *) usage; exit 1 ;;\n+esac",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/.codex%2Fskills%2Fexamples-auto-run%2Fscripts%2Frun.sh",
        "sha": "74258f8cac96701546cb38aee054c60038fc8df6",
        "status": "added"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/.gitignore",
        "changes": 1,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/.gitignore?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 0,
        "filename": ".gitignore",
        "patch": "@@ -45,6 +45,7 @@ htmlcov/\n .coverage\n .coverage.*\n .cache\n+.tmp/\n nosetests.xml\n coverage.xml\n *.cover",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/.gitignore",
        "sha": "ac32a2998d2cd0a5e06ba7acfd34c085ed689608",
        "status": "modified"
      },
      {
        "additions": 5,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fagents_as_tools.py",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Fagents_as_tools.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/agent_patterns/agents_as_tools.py",
        "patch": "@@ -1,6 +1,7 @@\n import asyncio\n \n from agents import Agent, ItemHelpers, MessageOutputItem, Runner, trace\n+from examples.auto_mode import input_with_fallback\n \n \"\"\"\n This example shows the agents-as-tools pattern. The frontline agent receives a user message and\n@@ -56,7 +57,10 @@\n \n \n async def main():\n-    msg = input(\"Hi! What would you like translated, and to which languages? \")\n+    msg = input_with_fallback(\n+        \"Hi! What would you like translated, and to which languages? \",\n+        \"Translate 'Hello, world!' to French and Spanish.\",\n+    )\n \n     # Run the entire orchestration in a single trace\n     with trace(\"Orchestrator evaluator\"):",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fagents_as_tools.py",
        "sha": "b670e2fe0647847b2a75f1609931c2f7d76d21ab",
        "status": "modified"
      },
      {
        "additions": 6,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fagents_as_tools_conditional.py",
        "changes": 8,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Fagents_as_tools_conditional.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 2,
        "filename": "examples/agent_patterns/agents_as_tools_conditional.py",
        "patch": "@@ -3,6 +3,7 @@\n from pydantic import BaseModel\n \n from agents import Agent, AgentBase, RunContextWrapper, Runner, trace\n+from examples.auto_mode import input_with_fallback\n \n \"\"\"\n This example demonstrates the agents-as-tools pattern with conditional tool enabling.\n@@ -81,7 +82,7 @@ async def main():\n     print(\"2. French and Spanish (2 tools)\")\n     print(\"3. European languages (3 tools)\")\n \n-    choice = input(\"\\nSelect option (1-3): \").strip()\n+    choice = input_with_fallback(\"\\nSelect option (1-3): \", \"2\").strip()\n     preference_map = {\"1\": \"spanish_only\", \"2\": \"french_spanish\", \"3\": \"european\"}\n     language_preference = preference_map.get(choice, \"spanish_only\")\n \n@@ -95,7 +96,10 @@ async def main():\n     print(f\"The LLM will only see and can use these {len(available_tools)} tools\\n\")\n \n     # Get user request\n-    user_request = input(\"Ask a question and see responses in available languages:\\n\")\n+    user_request = input_with_fallback(\n+        \"Ask a question and see responses in available languages:\\n\",\n+        \"How do you say good morning?\",\n+    )\n \n     # Run with LLM interaction\n     print(\"\\nProcessing request...\")",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fagents_as_tools_conditional.py",
        "sha": "87533721d38b635c34a557890f7c9ef5d23e6131",
        "status": "modified"
      },
      {
        "additions": 5,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fdeterministic.py",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Fdeterministic.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/agent_patterns/deterministic.py",
        "patch": "@@ -3,6 +3,7 @@\n from pydantic import BaseModel\n \n from agents import Agent, Runner, trace\n+from examples.auto_mode import input_with_fallback\n \n \"\"\"\n This example demonstrates a deterministic flow, where each step is performed by an agent.\n@@ -39,7 +40,10 @@ class OutlineCheckerOutput(BaseModel):\n \n \n async def main():\n-    input_prompt = input(\"What kind of story do you want? \")\n+    input_prompt = input_with_fallback(\n+        \"What kind of story do you want? \",\n+        \"Write a short sci-fi story.\",\n+    )\n \n     # Ensure the entire workflow is a single trace\n     with trace(\"Deterministic story flow\"):",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fdeterministic.py",
        "sha": "30bef35e25ddc856bafed70ad321a006300ae6c5",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Finput_guardrails.py",
        "changes": 9,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Finput_guardrails.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/agent_patterns/input_guardrails.py",
        "patch": "@@ -13,6 +13,7 @@\n     TResponseInputItem,\n     input_guardrail,\n )\n+from examples.auto_mode import input_with_fallback, is_auto_mode\n \n \"\"\"\n This example shows how to use guardrails.\n@@ -68,9 +69,13 @@ async def main():\n     )\n \n     input_data: list[TResponseInputItem] = []\n+    auto_mode = is_auto_mode()\n \n     while True:\n-        user_input = input(\"Enter a message: \")\n+        user_input = input_with_fallback(\n+            \"Enter a message: \",\n+            \"What's the capital of California?\",\n+        )\n         input_data.append(\n             {\n                 \"role\": \"user\",\n@@ -93,6 +98,8 @@ async def main():\n                     \"content\": message,\n                 }\n             )\n+        if auto_mode:\n+            break\n \n     # Sample run:\n     # Enter a message: What's the capital of California?",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Finput_guardrails.py",
        "sha": "7e4210d6af899fee65a10607a80507516ba10499",
        "status": "modified"
      },
      {
        "additions": 14,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fllm_as_a_judge.py",
        "changes": 15,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Fllm_as_a_judge.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/agent_patterns/llm_as_a_judge.py",
        "patch": "@@ -5,6 +5,7 @@\n from typing import Literal\n \n from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace\n+from examples.auto_mode import input_with_fallback, is_auto_mode\n \n \"\"\"\n This example shows the LLM as a judge pattern. The first agent generates an outline for a story.\n@@ -39,10 +40,16 @@ class EvaluationFeedback:\n \n \n async def main() -> None:\n-    msg = input(\"What kind of story would you like to hear? \")\n+    msg = input_with_fallback(\n+        \"What kind of story would you like to hear? \",\n+        \"A detective story in space.\",\n+    )\n     input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n \n     latest_outline: str | None = None\n+    auto_mode = is_auto_mode()\n+    max_rounds = 3 if auto_mode else None\n+    rounds = 0\n \n     # We'll run the entire workflow in a single trace\n     with trace(\"LLM as a judge\"):\n@@ -65,6 +72,12 @@ async def main() -> None:\n                 print(\"Story outline is good enough, exiting.\")\n                 break\n \n+            if auto_mode:\n+                rounds += 1\n+                if max_rounds is not None and rounds >= max_rounds:\n+                    print(\"Auto mode: stopping after limited rounds.\")\n+                    break\n+\n             print(\"Re-running with feedback\")\n \n             input_items.append({\"content\": f\"Feedback: {result.feedback}\", \"role\": \"user\"})",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fllm_as_a_judge.py",
        "sha": "1ee4915e18e36a02be4f4d6f83320a9645187736",
        "status": "modified"
      },
      {
        "additions": 5,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fparallelization.py",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Fparallelization.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/agent_patterns/parallelization.py",
        "patch": "@@ -1,6 +1,7 @@\n import asyncio\n \n from agents import Agent, ItemHelpers, Runner, trace\n+from examples.auto_mode import input_with_fallback\n \n \"\"\"\n This example shows the parallelization pattern. We run the agent three times in parallel, and pick\n@@ -19,7 +20,10 @@\n \n \n async def main():\n-    msg = input(\"Hi! Enter a message, and we'll translate it to Spanish.\\n\\n\")\n+    msg = input_with_fallback(\n+        \"Hi! Enter a message, and we'll translate it to Spanish.\\n\\n\",\n+        \"Good morning!\",\n+    )\n \n     # Ensure the entire workflow is a single trace\n     with trace(\"Parallel translation\"):",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Fparallelization.py",
        "sha": "60dcfbe07f78f739a3161fa49f7206abe74f3e50",
        "status": "modified"
      },
      {
        "additions": 9,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Frouting.py",
        "changes": 11,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fagent_patterns%2Frouting.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 2,
        "filename": "examples/agent_patterns/routing.py",
        "patch": "@@ -4,6 +4,7 @@\n from openai.types.responses import ResponseContentPartDoneEvent, ResponseTextDeltaEvent\n \n from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n+from examples.auto_mode import input_with_fallback, is_auto_mode\n \n \"\"\"\n This example shows the handoffs/routing pattern. The triage agent receives the first message, and\n@@ -37,9 +38,13 @@ async def main():\n     # We'll create an ID for this conversation, so we can link each trace\n     conversation_id = str(uuid.uuid4().hex[:16])\n \n-    msg = input(\"Hi! We speak French, Spanish and English. How can I help? \")\n+    msg = input_with_fallback(\n+        \"Hi! We speak French, Spanish and English. How can I help? \",\n+        \"Hello, how do I say good evening in French?\",\n+    )\n     agent = triage_agent\n     inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n+    auto_mode = is_auto_mode()\n \n     while True:\n         # Each conversation turn is a single trace. Normally, each input from the user would be an\n@@ -61,7 +66,9 @@ async def main():\n         inputs = result.to_input_list()\n         print(\"\\n\")\n \n-        user_msg = input(\"Enter a message: \")\n+        if auto_mode:\n+            break\n+        user_msg = input_with_fallback(\"Enter a message: \", \"Thanks!\")\n         inputs.append({\"content\": user_msg, \"role\": \"user\"})\n         agent = result.current_agent\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fagent_patterns%2Frouting.py",
        "sha": "4d0a49ab742f41d55d85e1ec80fb51b39542fbf2",
        "status": "modified"
      },
      {
        "additions": 37,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fauto_mode.py",
        "changes": 37,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fauto_mode.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 0,
        "filename": "examples/auto_mode.py",
        "patch": "@@ -0,0 +1,37 @@\n+\"\"\"Utilities for running examples in automated mode.\n+\n+When ``EXAMPLES_INTERACTIVE_MODE=auto`` is set, these helpers provide\n+deterministic inputs and confirmations so examples can run without manual\n+interaction. The helpers are intentionally lightweight to avoid adding\n+dependencies to example code.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import os\n+\n+\n+def is_auto_mode() -> bool:\n+    \"\"\"Return True when examples should bypass interactive prompts.\"\"\"\n+    return os.environ.get(\"EXAMPLES_INTERACTIVE_MODE\", \"\").lower() == \"auto\"\n+\n+\n+def input_with_fallback(prompt: str, fallback: str) -> str:\n+    \"\"\"Return the fallback text in auto mode, otherwise defer to input().\"\"\"\n+    if is_auto_mode():\n+        print(f\"[auto-input] {prompt.strip()} -> {fallback}\")\n+        return fallback\n+    return input(prompt)\n+\n+\n+def confirm_with_fallback(prompt: str, default: bool = True) -> bool:\n+    \"\"\"Return default in auto mode; otherwise ask the user.\"\"\"\n+    if is_auto_mode():\n+        choice = \"yes\" if default else \"no\"\n+        print(f\"[auto-confirm] {prompt.strip()} -> {choice}\")\n+        return default\n+\n+    answer = input(prompt).strip().lower()\n+    if not answer:\n+        return default\n+    return answer in {\"y\", \"yes\"}",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fauto_mode.py",
        "sha": "9a7b71fe712617cafc2675b70f68918074f74b35",
        "status": "added"
      },
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fbasic%2Fagent_lifecycle_example.py",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fbasic%2Fagent_lifecycle_example.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/basic/agent_lifecycle_example.py",
        "patch": "@@ -13,6 +13,7 @@\n     Tool,\n     function_tool,\n )\n+from examples.auto_mode import input_with_fallback\n \n \n class CustomAgentHooks(AgentHooks):\n@@ -98,7 +99,7 @@ class FinalResult(BaseModel):\n \n \n async def main() -> None:\n-    user_input = input(\"Enter a max number: \")\n+    user_input = input_with_fallback(\"Enter a max number: \", \"50\")\n     try:\n         max_number = int(user_input)\n         await Runner.run(",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fbasic%2Fagent_lifecycle_example.py",
        "sha": "d135b8f4525bb64ae3abc2991e7b4161560bcb23",
        "status": "modified"
      },
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fbasic%2Flifecycle_example.py",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fbasic%2Flifecycle_example.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/basic/lifecycle_example.py",
        "patch": "@@ -17,6 +17,7 @@\n )\n from agents.items import ModelResponse, TResponseInputItem\n from agents.tool_context import ToolContext\n+from examples.auto_mode import input_with_fallback\n \n \n class LoggingHooks(AgentHooks[Any]):\n@@ -146,7 +147,7 @@ class FinalResult(BaseModel):\n \n \n async def main() -> None:\n-    user_input = input(\"Enter a max number: \")\n+    user_input = input_with_fallback(\"Enter a max number: \", \"50\")\n     try:\n         max_number = int(user_input)\n         await Runner.run(",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fbasic%2Flifecycle_example.py",
        "sha": "5ecd3a6b758f1dcd31a51a18de91f74677b8533f",
        "status": "modified"
      },
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fbasic%2Fprevious_response_id.py",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fbasic%2Fprevious_response_id.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/basic/previous_response_id.py",
        "patch": "@@ -1,6 +1,7 @@\n import asyncio\n \n from agents import Agent, Runner\n+from examples.auto_mode import input_with_fallback\n \n \"\"\"This demonstrates usage of the `previous_response_id` parameter to continue a conversation.\n The second run passes the previous response ID to the model, which allows it to continue the\n@@ -59,7 +60,7 @@ async def main_stream():\n \n \n if __name__ == \"__main__\":\n-    is_stream = input(\"Run in stream mode? (y/n): \")\n+    is_stream = input_with_fallback(\"Run in stream mode? (y/n): \", \"n\")\n     if is_stream == \"y\":\n         asyncio.run(main_stream())\n     else:",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fbasic%2Fprevious_response_id.py",
        "sha": "21c354219d41e9b237cfb2beffced5478fea1df6",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fcustomer_service%2Fmain.py",
        "changes": 9,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fcustomer_service%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/customer_service/main.py",
        "patch": "@@ -21,6 +21,7 @@\n     trace,\n )\n from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n+from examples.auto_mode import input_with_fallback, is_auto_mode\n \n ### CONTEXT\n \n@@ -143,13 +144,17 @@ async def main():\n     current_agent: Agent[AirlineAgentContext] = triage_agent\n     input_items: list[TResponseInputItem] = []\n     context = AirlineAgentContext()\n+    auto_mode = is_auto_mode()\n \n     # Normally, each input from the user would be an API request to your app, and you can wrap the request in a trace()\n     # Here, we'll just use a random UUID for the conversation ID\n     conversation_id = uuid.uuid4().hex[:16]\n \n     while True:\n-        user_input = input(\"Enter your message: \")\n+        user_input = input_with_fallback(\n+            \"Enter your message: \",\n+            \"What are your store hours?\",\n+        )\n         with trace(\"Customer service\", group_id=conversation_id):\n             input_items.append({\"content\": user_input, \"role\": \"user\"})\n             result = await Runner.run(current_agent, input_items, context=context)\n@@ -170,6 +175,8 @@ async def main():\n                     print(f\"{agent_name}: Skipping item: {new_item.__class__.__name__}\")\n             input_items = result.to_input_list()\n             current_agent = result.last_agent\n+        if auto_mode:\n+            break\n \n \n if __name__ == \"__main__\":",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fcustomer_service%2Fmain.py",
        "sha": "65191559c38e8e3872191578b178b11177f66c62",
        "status": "modified"
      },
      {
        "additions": 6,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Ffinancial_research_agent%2Fmain.py",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Ffinancial_research_agent%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/financial_research_agent/main.py",
        "patch": "@@ -1,5 +1,7 @@\n import asyncio\n \n+from examples.auto_mode import input_with_fallback\n+\n from .manager import FinancialResearchManager\n \n \n@@ -8,7 +10,10 @@\n # financial research query, for example:\n # \"Write up an analysis of Apple Inc.'s most recent quarter.\"\n async def main() -> None:\n-    query = input(\"Enter a financial research query: \")\n+    query = input_with_fallback(\n+        \"Enter a financial research query: \",\n+        \"Write up an analysis of Apple Inc.'s most recent quarter.\",\n+    )\n     mgr = FinancialResearchManager()\n     await mgr.run(query)\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Ffinancial_research_agent%2Fmain.py",
        "sha": "23b6d71d6bb3505670c74c6dfab3e56807b71136",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fhosted_mcp%2Fapprovals.py",
        "changes": 5,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fhosted_mcp%2Fapprovals.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 2,
        "filename": "examples/hosted_mcp/approvals.py",
        "patch": "@@ -8,14 +8,15 @@\n     MCPToolApprovalRequest,\n     Runner,\n )\n+from examples.auto_mode import confirm_with_fallback\n \n \"\"\"This example demonstrates how to use the hosted MCP support in the OpenAI Responses API, with\n approval callbacks.\"\"\"\n \n \n def approval_callback(request: MCPToolApprovalRequest) -> MCPToolApprovalFunctionResult:\n-    answer = input(f\"Approve running the tool `{request.data.name}`? (y/n) \")\n-    result: MCPToolApprovalFunctionResult = {\"approve\": answer == \"y\"}\n+    approve = confirm_with_fallback(f\"Approve running the tool `{request.data.name}`? (y/n) \", True)\n+    result: MCPToolApprovalFunctionResult = {\"approve\": approve}\n     if not result[\"approve\"]:\n         result[\"reason\"] = \"User denied\"\n     return result",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fhosted_mcp%2Fapprovals.py",
        "sha": "2aa73c1ebcddc0c3d6571244f65137f9901c179a",
        "status": "modified"
      },
      {
        "additions": 5,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fgit_example%2Fmain.py",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fgit_example%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/mcp/git_example/main.py",
        "patch": "@@ -3,6 +3,7 @@\n \n from agents import Agent, Runner, trace\n from agents.mcp import MCPServer, MCPServerStdio\n+from examples.auto_mode import input_with_fallback\n \n \n async def run(mcp_server: MCPServer, directory_path: str):\n@@ -27,7 +28,10 @@ async def run(mcp_server: MCPServer, directory_path: str):\n \n async def main():\n     # Ask the user for the directory path\n-    directory_path = input(\"Please enter the path to the git repository: \")\n+    directory_path = input_with_fallback(\n+        \"Please enter the path to the git repository: \",\n+        \".\",\n+    )\n \n     async with MCPServerStdio(\n         cache_tools_list=True,  # Cache the tools list, for demonstration",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fgit_example%2Fmain.py",
        "sha": "8a62744d186f6d5cd3075c52cd2bdcefa68d5839",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fprompt_server%2FREADME.md",
        "changes": 5,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fprompt_server%2FREADME.md?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 2,
        "filename": "examples/mcp/prompt_server/README.md",
        "patch": "@@ -10,7 +10,8 @@ uv run python examples/mcp/prompt_server/main.py\n \n ## Details\n \n-The example uses the `MCPServerStreamableHttp` class from `agents.mcp`. The server runs in a sub-process at `http://localhost:8000/mcp` and provides user-controlled prompts that generate agent instructions.\n+The example uses the `MCPServerStreamableHttp` class from `agents.mcp`. The script auto-selects an open localhost port (or honors `STREAMABLE_HTTP_PORT`) and runs the server at `http://<host>:<port>/mcp`, providing user-controlled prompts that generate agent instructions.\n+If you need a specific address, set `STREAMABLE_HTTP_PORT` and `STREAMABLE_HTTP_HOST`.\n \n The server exposes prompts like `generate_code_review_instructions` that take parameters such as focus area and programming language. The agent calls these prompts to dynamically generate its system instructions based on user-provided parameters.\n \n@@ -26,4 +27,4 @@ The example demonstrates two key functions:\n    - Runs the agent against vulnerable sample code (command injection via `os.system`)\n    - The agent analyzes the code and provides security-focused feedback using available tools\n \n-This pattern allows users to dynamically configure agent behavior through MCP prompts rather than hardcoded instructions. \n\\ No newline at end of file\n+This pattern allows users to dynamically configure agent behavior through MCP prompts rather than hardcoded instructions. ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fprompt_server%2FREADME.md",
        "sha": "c1eaa632df615c17f4d878a5077e2475e82acb7c",
        "status": "modified"
      },
      {
        "additions": 25,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fprompt_server%2Fmain.py",
        "changes": 29,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fprompt_server%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 4,
        "filename": "examples/mcp/prompt_server/main.py",
        "patch": "@@ -1,14 +1,32 @@\n import asyncio\n import os\n import shutil\n+import socket\n import subprocess\n import time\n-from typing import Any\n+from typing import Any, cast\n \n from agents import Agent, Runner, gen_trace_id, trace\n from agents.mcp import MCPServer, MCPServerStreamableHttp\n from agents.model_settings import ModelSettings\n \n+STREAMABLE_HTTP_HOST = os.getenv(\"STREAMABLE_HTTP_HOST\", \"127.0.0.1\")\n+\n+\n+def _choose_port() -> int:\n+    env_port = os.getenv(\"STREAMABLE_HTTP_PORT\")\n+    if env_port:\n+        return int(env_port)\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.bind((STREAMABLE_HTTP_HOST, 0))\n+        address = cast(tuple[str, int], s.getsockname())\n+        return address[1]\n+\n+\n+STREAMABLE_HTTP_PORT = _choose_port()\n+os.environ.setdefault(\"STREAMABLE_HTTP_PORT\", str(STREAMABLE_HTTP_PORT))\n+STREAMABLE_HTTP_URL = f\"http://{STREAMABLE_HTTP_HOST}:{STREAMABLE_HTTP_PORT}/mcp\"\n+\n \n async def get_instructions_from_prompt(mcp_server: MCPServer, prompt_name: str, **kwargs) -> str:\n     \"\"\"Get agent instructions by calling MCP prompt endpoint (user-controlled)\"\"\"\n@@ -75,7 +93,7 @@ async def show_available_prompts(mcp_server: MCPServer):\n async def main():\n     async with MCPServerStreamableHttp(\n         name=\"Simple Prompt Server\",\n-        params={\"url\": \"http://localhost:8000/mcp\"},\n+        params={\"url\": STREAMABLE_HTTP_URL},\n     ) as server:\n         trace_id = gen_trace_id()\n         with trace(workflow_name=\"Simple Prompt Demo\", trace_id=trace_id):\n@@ -94,8 +112,11 @@ async def main():\n         this_dir = os.path.dirname(os.path.abspath(__file__))\n         server_file = os.path.join(this_dir, \"server.py\")\n \n-        print(\"Starting Simple Prompt Server...\")\n-        process = subprocess.Popen([\"uv\", \"run\", server_file])\n+        print(f\"Starting Simple Prompt Server at {STREAMABLE_HTTP_URL} ...\")\n+        env = os.environ.copy()\n+        env.setdefault(\"STREAMABLE_HTTP_HOST\", STREAMABLE_HTTP_HOST)\n+        env.setdefault(\"STREAMABLE_HTTP_PORT\", str(STREAMABLE_HTTP_PORT))\n+        process = subprocess.Popen([\"uv\", \"run\", server_file], env=env)\n         time.sleep(3)\n         print(\"Server started\\n\")\n     except Exception as e:",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fprompt_server%2Fmain.py",
        "sha": "3cd045e63b14521078e9d584671f3ae5fb62846d",
        "status": "modified"
      },
      {
        "additions": 6,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fprompt_server%2Fserver.py",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fprompt_server%2Fserver.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/mcp/prompt_server/server.py",
        "patch": "@@ -1,7 +1,12 @@\n+import os\n+\n from mcp.server.fastmcp import FastMCP\n \n+STREAMABLE_HTTP_HOST = os.getenv(\"STREAMABLE_HTTP_HOST\", \"127.0.0.1\")\n+STREAMABLE_HTTP_PORT = int(os.getenv(\"STREAMABLE_HTTP_PORT\", \"18080\"))\n+\n # Create server\n-mcp = FastMCP(\"Prompt Server\")\n+mcp = FastMCP(\"Prompt Server\", host=STREAMABLE_HTTP_HOST, port=STREAMABLE_HTTP_PORT)\n \n \n # Instruction-generating prompts (user-controlled)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fprompt_server%2Fserver.py",
        "sha": "7d6629acd72b820eb07b69c3c39f52e325f6d025",
        "status": "modified"
      },
      {
        "additions": 10,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fsse_example%2Fserver.py",
        "changes": 14,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fsse_example%2Fserver.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 4,
        "filename": "examples/mcp/sse_example/server.py",
        "patch": "@@ -23,10 +23,16 @@ def get_secret_word() -> str:\n @mcp.tool()\n def get_current_weather(city: str) -> str:\n     print(f\"[debug-server] get_current_weather({city})\")\n-\n-    endpoint = \"https://wttr.in\"\n-    response = requests.get(f\"{endpoint}/{city}\")\n-    return response.text\n+    # Avoid slow or flaky network calls during automated runs.\n+    try:\n+        endpoint = \"https://wttr.in\"\n+        response = requests.get(f\"{endpoint}/{city}\", timeout=2)\n+        if response.ok:\n+            return response.text\n+    except Exception:\n+        pass\n+    # Fallback keeps the tool responsive even when offline.\n+    return f\"Weather data unavailable right now; assume clear skies in {city}.\"\n \n \n if __name__ == \"__main__\":",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fsse_example%2Fserver.py",
        "sha": "2e4fe2db863ed818e0bc003350a560592a1cf4a8",
        "status": "modified"
      },
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2FREADME.md",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2FREADME.md?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/mcp/streamablehttp_custom_client_example/README.md",
        "patch": "@@ -38,7 +38,7 @@ def create_custom_http_client() -> httpx.AsyncClient:\n async with MCPServerStreamableHttp(\n     name=\"Custom Client Server\",\n     params={\n-        \"url\": \"http://localhost:8000/mcp\",\n+        \"url\": \"http://localhost:<port>/mcp\",\n         \"httpx_client_factory\": create_custom_http_client,\n     },\n ) as server:\n@@ -60,3 +60,4 @@ async with MCPServerStreamableHttp(\n - **Performance**: Optimize timeouts and connection settings for your use case\n - **Compatibility**: Work with corporate proxies and network restrictions\n \n+This example will auto-pick a free localhost port unless you set `STREAMABLE_HTTP_PORT`; use `STREAMABLE_HTTP_HOST` to change the bind address.",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2FREADME.md",
        "sha": "fc269a06447c21ba778b1d60a53772e91b90c422",
        "status": "modified"
      },
      {
        "additions": 26,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2Fmain.py",
        "changes": 31,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 5,
        "filename": "examples/mcp/streamablehttp_custom_client_example/main.py",
        "patch": "@@ -7,16 +7,34 @@\n import asyncio\n import os\n import shutil\n+import socket\n import subprocess\n import time\n-from typing import Any\n+from typing import Any, cast\n \n import httpx\n \n from agents import Agent, Runner, gen_trace_id, trace\n from agents.mcp import MCPServer, MCPServerStreamableHttp\n from agents.model_settings import ModelSettings\n \n+STREAMABLE_HTTP_HOST = os.getenv(\"STREAMABLE_HTTP_HOST\", \"127.0.0.1\")\n+\n+\n+def _choose_port() -> int:\n+    env_port = os.getenv(\"STREAMABLE_HTTP_PORT\")\n+    if env_port:\n+        return int(env_port)\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.bind((STREAMABLE_HTTP_HOST, 0))\n+        address = cast(tuple[str, int], s.getsockname())\n+        return address[1]\n+\n+\n+STREAMABLE_HTTP_PORT = _choose_port()\n+os.environ.setdefault(\"STREAMABLE_HTTP_PORT\", str(STREAMABLE_HTTP_PORT))\n+STREAMABLE_HTTP_URL = f\"http://{STREAMABLE_HTTP_HOST}:{STREAMABLE_HTTP_PORT}/mcp\"\n+\n \n def create_custom_http_client(\n     headers: dict[str, str] | None = None,\n@@ -73,7 +91,7 @@ async def main():\n     async with MCPServerStreamableHttp(\n         name=\"Streamable HTTP with Custom Client\",\n         params={\n-            \"url\": \"http://localhost:8000/mcp\",\n+            \"url\": STREAMABLE_HTTP_URL,\n             \"httpx_client_factory\": create_custom_http_client,\n         },\n     ) as server:\n@@ -91,16 +109,19 @@ async def main():\n         )\n \n     # We'll run the Streamable HTTP server in a subprocess. Usually this would be a remote server, but for this\n-    # demo, we'll run it locally at http://localhost:8000/mcp\n+    # demo, we'll run it locally at STREAMABLE_HTTP_URL\n     process: subprocess.Popen[Any] | None = None\n     try:\n         this_dir = os.path.dirname(os.path.abspath(__file__))\n         server_file = os.path.join(this_dir, \"server.py\")\n \n-        print(\"Starting Streamable HTTP server at http://localhost:8000/mcp ...\")\n+        print(f\"Starting Streamable HTTP server at {STREAMABLE_HTTP_URL} ...\")\n \n         # Run `uv run server.py` to start the Streamable HTTP server\n-        process = subprocess.Popen([\"uv\", \"run\", server_file])\n+        env = os.environ.copy()\n+        env.setdefault(\"STREAMABLE_HTTP_HOST\", STREAMABLE_HTTP_HOST)\n+        env.setdefault(\"STREAMABLE_HTTP_PORT\", str(STREAMABLE_HTTP_PORT))\n+        process = subprocess.Popen([\"uv\", \"run\", server_file], env=env)\n         # Give it 3 seconds to start\n         time.sleep(3)\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2Fmain.py",
        "sha": "20cbef1cdc78975a68ceb82e69c3758a9acc747d",
        "status": "modified"
      },
      {
        "additions": 5,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2Fserver.py",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2Fserver.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/mcp/streamablehttp_custom_client_example/server.py",
        "patch": "@@ -1,9 +1,13 @@\n+import os\n import random\n \n from mcp.server.fastmcp import FastMCP\n \n+STREAMABLE_HTTP_HOST = os.getenv(\"STREAMABLE_HTTP_HOST\", \"127.0.0.1\")\n+STREAMABLE_HTTP_PORT = int(os.getenv(\"STREAMABLE_HTTP_PORT\", \"18080\"))\n+\n # Create server\n-mcp = FastMCP(\"Echo Server\")\n+mcp = FastMCP(\"Echo Server\", host=STREAMABLE_HTTP_HOST, port=STREAMABLE_HTTP_PORT)\n \n \n @mcp.tool()",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_custom_client_example%2Fserver.py",
        "sha": "dd0d468753b3ace313789a878db5f3e13cd3a6d4",
        "status": "modified"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_example%2FREADME.md",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fstreamablehttp_example%2FREADME.md?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/mcp/streamablehttp_example/README.md",
        "patch": "@@ -10,4 +10,4 @@ uv run python examples/mcp/streamablehttp_example/main.py\n \n ## Details\n \n-The example uses the `MCPServerStreamableHttp` class from `agents.mcp`. The server runs in a sub-process at `https://localhost:8000/mcp`.\n+The example uses the `MCPServerStreamableHttp` class from `agents.mcp`. The script picks an open localhost port automatically (or honors `STREAMABLE_HTTP_PORT` if you set it) and starts the server at `http://<host>:<port>/mcp`. Set `STREAMABLE_HTTP_HOST` if you need a different bind address.",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_example%2FREADME.md",
        "sha": "83cae670b626edd8fe42d2f4890abcd85c148f86",
        "status": "modified"
      },
      {
        "additions": 26,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_example%2Fmain.py",
        "changes": 31,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fstreamablehttp_example%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 5,
        "filename": "examples/mcp/streamablehttp_example/main.py",
        "patch": "@@ -1,14 +1,32 @@\n import asyncio\n import os\n import shutil\n+import socket\n import subprocess\n import time\n-from typing import Any\n+from typing import Any, cast\n \n from agents import Agent, Runner, gen_trace_id, trace\n from agents.mcp import MCPServer, MCPServerStreamableHttp\n from agents.model_settings import ModelSettings\n \n+STREAMABLE_HTTP_HOST = os.getenv(\"STREAMABLE_HTTP_HOST\", \"127.0.0.1\")\n+\n+\n+def _choose_port() -> int:\n+    env_port = os.getenv(\"STREAMABLE_HTTP_PORT\")\n+    if env_port:\n+        return int(env_port)\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.bind((STREAMABLE_HTTP_HOST, 0))\n+        address = cast(tuple[str, int], s.getsockname())\n+        return address[1]\n+\n+\n+STREAMABLE_HTTP_PORT = _choose_port()\n+os.environ.setdefault(\"STREAMABLE_HTTP_PORT\", str(STREAMABLE_HTTP_PORT))\n+STREAMABLE_HTTP_URL = f\"http://{STREAMABLE_HTTP_HOST}:{STREAMABLE_HTTP_PORT}/mcp\"\n+\n \n async def run(mcp_server: MCPServer):\n     agent = Agent(\n@@ -41,7 +59,7 @@ async def main():\n     async with MCPServerStreamableHttp(\n         name=\"Streamable HTTP Python Server\",\n         params={\n-            \"url\": \"http://localhost:8000/mcp\",\n+            \"url\": STREAMABLE_HTTP_URL,\n         },\n     ) as server:\n         trace_id = gen_trace_id()\n@@ -58,16 +76,19 @@ async def main():\n         )\n \n     # We'll run the Streamable HTTP server in a subprocess. Usually this would be a remote server, but for this\n-    # demo, we'll run it locally at http://localhost:8000/mcp\n+    # demo, we'll run it locally at STREAMABLE_HTTP_URL\n     process: subprocess.Popen[Any] | None = None\n     try:\n         this_dir = os.path.dirname(os.path.abspath(__file__))\n         server_file = os.path.join(this_dir, \"server.py\")\n \n-        print(\"Starting Streamable HTTP server at http://localhost:8000/mcp ...\")\n+        print(f\"Starting Streamable HTTP server at {STREAMABLE_HTTP_URL} ...\")\n \n         # Run `uv run server.py` to start the Streamable HTTP server\n-        process = subprocess.Popen([\"uv\", \"run\", server_file])\n+        env = os.environ.copy()\n+        env.setdefault(\"STREAMABLE_HTTP_HOST\", STREAMABLE_HTTP_HOST)\n+        env.setdefault(\"STREAMABLE_HTTP_PORT\", str(STREAMABLE_HTTP_PORT))\n+        process = subprocess.Popen([\"uv\", \"run\", server_file], env=env)\n         # Give it 3 seconds to start\n         time.sleep(3)\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_example%2Fmain.py",
        "sha": "564a7bf98fbea937f70e1bd414e2e651daccf3ad",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_example%2Fserver.py",
        "changes": 20,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmcp%2Fstreamablehttp_example%2Fserver.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 5,
        "filename": "examples/mcp/streamablehttp_example/server.py",
        "patch": "@@ -1,10 +1,14 @@\n+import os\n import random\n \n import requests\n from mcp.server.fastmcp import FastMCP\n \n+STREAMABLE_HTTP_HOST = os.getenv(\"STREAMABLE_HTTP_HOST\", \"127.0.0.1\")\n+STREAMABLE_HTTP_PORT = int(os.getenv(\"STREAMABLE_HTTP_PORT\", \"18080\"))\n+\n # Create server\n-mcp = FastMCP(\"Echo Server\")\n+mcp = FastMCP(\"Echo Server\", host=STREAMABLE_HTTP_HOST, port=STREAMABLE_HTTP_PORT)\n \n \n @mcp.tool()\n@@ -23,10 +27,16 @@ def get_secret_word() -> str:\n @mcp.tool()\n def get_current_weather(city: str) -> str:\n     print(f\"[debug-server] get_current_weather({city})\")\n-\n-    endpoint = \"https://wttr.in\"\n-    response = requests.get(f\"{endpoint}/{city}\")\n-    return response.text\n+    # Avoid slow or flaky network calls during automated runs.\n+    try:\n+        endpoint = \"https://wttr.in\"\n+        response = requests.get(f\"{endpoint}/{city}\", timeout=2)\n+        if response.ok:\n+            return response.text\n+    except Exception:\n+        pass\n+    # Fallback keeps the tool responsive even when offline.\n+    return f\"Weather data unavailable right now; assume clear skies in {city}.\"\n \n \n if __name__ == \"__main__\":",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmcp%2Fstreamablehttp_example%2Fserver.py",
        "sha": "d73ab895b6aa264b2cbe02834bdac6fe92e523d9",
        "status": "modified"
      },
      {
        "additions": 11,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmodel_providers%2Flitellm_provider.py",
        "changes": 18,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmodel_providers%2Flitellm_provider.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 7,
        "filename": "examples/model_providers/litellm_provider.py",
        "patch": "@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import asyncio\n+import os\n \n from agents import Agent, Runner, function_tool, set_tracing_disabled\n from agents.extensions.models.litellm_model import LitellmModel\n@@ -24,6 +25,9 @@ def get_weather(city: str):\n \n \n async def main(model: str, api_key: str):\n+    if api_key == \"dummy\":\n+        print(\"Skipping run because no valid LITELLM_API_KEY was provided.\")\n+        return\n     agent = Agent(\n         name=\"Assistant\",\n         instructions=\"You only respond in haikus.\",\n@@ -36,20 +40,20 @@ async def main(model: str, api_key: str):\n \n \n if __name__ == \"__main__\":\n-    # First try to get model/api key from args\n+    # Prefer non-interactive defaults in auto mode to avoid blocking.\n     import argparse\n \n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--model\", type=str, required=False)\n     parser.add_argument(\"--api-key\", type=str, required=False)\n     args = parser.parse_args()\n \n-    model = args.model\n-    if not model:\n-        model = input(\"Enter a model name for Litellm: \")\n+    model = args.model or os.environ.get(\"LITELLM_MODEL\", \"openai/gpt-4o-mini\")\n+    api_key = args.api_key or os.environ.get(\"LITELLM_API_KEY\", \"dummy\")\n \n-    api_key = args.api_key\n-    if not api_key:\n-        api_key = input(\"Enter an API key for Litellm: \")\n+    if not args.model:\n+        print(f\"Using default model: {model}\")\n+    if not args.api_key:\n+        print(\"Using LITELLM_API_KEY from environment (or dummy placeholder).\")\n \n     asyncio.run(main(model, api_key))",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fmodel_providers%2Flitellm_provider.py",
        "sha": "ea5f09ab32787504b45455ed7654b7916a78ed1c",
        "status": "modified"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Freasoning_content%2Fmain.py",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Freasoning_content%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/reasoning_content/main.py",
        "patch": "@@ -20,7 +20,7 @@\n from agents.models.interface import ModelTracing\n from agents.models.openai_provider import OpenAIProvider\n \n-MODEL_NAME = os.getenv(\"REASONING_MODEL_NAME\") or \"gpt-5\"\n+MODEL_NAME = os.getenv(\"REASONING_MODEL_NAME\") or \"gpt-5.2\"\n \n \n async def stream_with_reasoning_content():",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Freasoning_content%2Fmain.py",
        "sha": "3db5d5cee6cdee0a1325072eefb2dfab18bb5afc",
        "status": "modified"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Freasoning_content%2Frunner_example.py",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Freasoning_content%2Frunner_example.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/reasoning_content/runner_example.py",
        "patch": "@@ -17,7 +17,7 @@\n from agents import Agent, ModelSettings, Runner, trace\n from agents.items import ReasoningItem\n \n-MODEL_NAME = os.getenv(\"EXAMPLE_MODEL_NAME\") or \"gpt-5.2\"\n+MODEL_NAME = os.getenv(\"REASONING_MODEL_NAME\") or \"gpt-5.2\"\n \n \n async def main():",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Freasoning_content%2Frunner_example.py",
        "sha": "e3c3d22506c0569afff1f30407eabdd4ce346660",
        "status": "modified"
      },
      {
        "additions": 6,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fresearch_bot%2Fmain.py",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fresearch_bot%2Fmain.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/research_bot/main.py",
        "patch": "@@ -1,10 +1,15 @@\n import asyncio\n \n+from examples.auto_mode import input_with_fallback\n+\n from .manager import ResearchManager\n \n \n async def main() -> None:\n-    query = input(\"What would you like to research? \")\n+    query = input_with_fallback(\n+        \"What would you like to research? \",\n+        \"Impact of electric vehicles on the grid.\",\n+    )\n     await ResearchManager().run(query)\n \n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Fresearch_bot%2Fmain.py",
        "sha": "b70bc8e4831e2602e4537f1f93da8c45f0b48e01",
        "status": "modified"
      },
      {
        "additions": 400,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Frun_examples.py",
        "changes": 455,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Frun_examples.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 55,
        "filename": "examples/run_examples.py",
        "patch": "@@ -1,37 +1,54 @@\n-\"\"\"Run multiple example entry points in this repository.\n-\n-This script locates Python files under ``examples/`` that contain a\n-``__main__`` guard and executes them one by one. By default it skips\n-interactive, server-like, audio-heavy, and external-service examples so\n-that automated validation does not hang waiting for input or require\n-hardware. Use flags to opt into those categories when you want to run\n-them.\n-\n-Usage examples:\n-\n-    uv run examples/run_examples.py --dry-run\n-    uv run examples/run_examples.py --filter basic\n-    uv run examples/run_examples.py --include-interactive --include-server\n-\n-By default the script keeps running even if an example fails; use\n-``--fail-fast`` to stop on the first failure.\n+\"\"\"Run multiple example entry points with optional auto mode and logging.\n+\n+Features:\n+* Discovers ``__main__``-guarded example files under ``examples/``.\n+* Skips interactive/server/audio/external examples unless explicitly included.\n+* Auto mode (``EXAMPLES_INTERACTIVE_MODE=auto``) enables deterministic inputs,\n+  auto-approvals, and turns on interactive examples by default.\n+* Writes per-example logs to ``.tmp/examples-start-logs`` and a main summary log.\n+* Generates a rerun list of failures at ``.tmp/examples-rerun.txt``.\n \"\"\"\n \n from __future__ import annotations\n \n import argparse\n+import datetime\n+import os\n import re\n import shlex\n import subprocess\n import sys\n+import threading\n from collections.abc import Iterable, Sequence\n+from concurrent.futures import ThreadPoolExecutor, as_completed\n from dataclasses import dataclass, field\n-from pathlib import Path\n+from pathlib import Path, PurePosixPath\n \n ROOT_DIR = Path(__file__).resolve().parent.parent\n EXAMPLES_DIR = ROOT_DIR / \"examples\"\n MAIN_PATTERN = re.compile(r\"__name__\\s*==\\s*['\\\"]__main__['\\\"]\")\n \n+LOG_DIR_DEFAULT = ROOT_DIR / \".tmp\" / \"examples-start-logs\"\n+RERUN_FILE_DEFAULT = ROOT_DIR / \".tmp\" / \"examples-rerun.txt\"\n+DEFAULT_MAIN_LOG = LOG_DIR_DEFAULT / f\"main_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.log\"\n+\n+# Examples that are noisy, require extra credentials, or hang in auto runs.\n+DEFAULT_AUTO_SKIP = {\n+    \"examples/agent_patterns/llm_as_a_judge.py\",\n+    \"examples/agent_patterns/routing.py\",\n+    \"examples/customer_service/main.py\",\n+    \"examples/hosted_mcp/connectors.py\",\n+    \"examples/mcp/git_example/main.py\",\n+    \"examples/model_providers/custom_example_agent.py\",\n+    \"examples/model_providers/custom_example_global.py\",\n+    \"examples/model_providers/custom_example_provider.py\",\n+    \"examples/realtime/app/server.py\",\n+    \"examples/realtime/cli/demo.py\",\n+    \"examples/realtime/twilio/server.py\",\n+    \"examples/voice/static/main.py\",\n+    \"examples/voice/streamed/main.py\",\n+}\n+\n \n @dataclass\n class ExampleScript:\n@@ -40,7 +57,7 @@ class ExampleScript:\n \n     @property\n     def relpath(self) -> str:\n-        return str(self.path.relative_to(ROOT_DIR))\n+        return normalize_relpath(str(self.path.relative_to(ROOT_DIR)))\n \n     @property\n     def module(self) -> str:\n@@ -53,6 +70,20 @@ def command(self) -> list[str]:\n         return [\"uv\", \"run\", \"python\", \"-m\", self.module]\n \n \n+@dataclass\n+class ExampleResult:\n+    script: ExampleScript\n+    status: str\n+    reason: str = \"\"\n+    log_path: Path | None = None\n+    exit_code: int | None = None\n+\n+\n+def normalize_relpath(relpath: str) -> str:\n+    normalized = relpath.replace(\"\\\\\", \"/\")\n+    return str(PurePosixPath(normalized))\n+\n+\n def parse_args() -> argparse.Namespace:\n     parser = argparse.ArgumentParser(description=\"Run example scripts sequentially.\")\n     parser.add_argument(\n@@ -86,14 +117,58 @@ def parse_args() -> argparse.Namespace:\n         help=\"Include examples that rely on extra services like Redis, Dapr, Twilio, or Playwright.\",\n     )\n     parser.add_argument(\n-        \"--fail-fast\",\n+        \"--verbose\",\n         action=\"store_true\",\n-        help=\"Stop after the first failing example.\",\n+        help=\"Show detected tags for each example entry.\",\n     )\n     parser.add_argument(\n-        \"--verbose\",\n+        \"--logs-dir\",\n+        default=str(LOG_DIR_DEFAULT),\n+        help=\"Directory for per-example logs and main log.\",\n+    )\n+    parser.add_argument(\n+        \"--main-log\",\n+        default=str(DEFAULT_MAIN_LOG),\n+        help=\"Path to write the main summary log.\",\n+    )\n+    parser.add_argument(\n+        \"--rerun-file\",\n+        help=\"Only run examples listed in this file (one relative path per line).\",\n+    )\n+    parser.add_argument(\n+        \"--write-rerun\",\n         action=\"store_true\",\n-        help=\"Show detected tags for each example entry.\",\n+        help=\"Write failures to .tmp/examples-rerun.txt after the run.\",\n+    )\n+    parser.add_argument(\n+        \"--collect\",\n+        help=\"Parse a previous main log to emit a rerun list instead of running examples.\",\n+    )\n+    parser.add_argument(\n+        \"--output\",\n+        help=\"Output path for --collect rerun list (defaults to stdout).\",\n+    )\n+    parser.add_argument(\n+        \"--print-auto-skip\",\n+        action=\"store_true\",\n+        help=\"Show the current auto-skip list and exit.\",\n+    )\n+    parser.add_argument(\n+        \"--auto-mode\",\n+        action=\"store_true\",\n+        help=\"Force EXAMPLES_INTERACTIVE_MODE=auto for this run.\",\n+    )\n+    parser.add_argument(\n+        \"--jobs\",\n+        \"-j\",\n+        type=int,\n+        default=int(os.environ.get(\"EXAMPLES_JOBS\", \"4\")),\n+        help=\"Number of examples to run in parallel (default: 4). Use 1 to force serial execution.\",\n+    )\n+    parser.add_argument(\n+        \"--no-buffer-output\",\n+        action=\"store_true\",\n+        help=\"Stream each example's stdout directly (may interleave). By default output is buffered per example to reduce interleaving.\",\n     )\n     return parser.parse_args()\n \n@@ -103,7 +178,11 @@ def detect_tags(path: Path, source: str) -> set[str]:\n     lower_source = source.lower()\n     lower_parts = [part.lower() for part in path.parts]\n \n-    if re.search(r\"\\binput\\s*\\(\", source):\n+    if (\n+        re.search(r\"\\binput\\s*\\(\", source)\n+        or \"input_with_fallback(\" in lower_source\n+        or \"confirm_with_fallback(\" in lower_source\n+    ):\n         tags.add(\"interactive\")\n     if \"prompt_toolkit\" in lower_source or \"questionary\" in lower_source:\n         tags.add(\"interactive\")\n@@ -153,70 +232,336 @@ def discover_examples(filters: Iterable[str]) -> list[ExampleScript]:\n     return sorted(examples, key=lambda item: item.relpath)\n \n \n-def should_skip(tags: set[str], allowed_overrides: set[str]) -> tuple[bool, set[str]]:\n+def should_skip(\n+    tags: set[str],\n+    allowed_overrides: set[str],\n+    auto_skip_set: set[str],\n+    relpath: str,\n+    auto_mode: bool,\n+) -> tuple[bool, set[str]]:\n     blocked = {\"interactive\", \"server\", \"audio\", \"external\"} - allowed_overrides\n     active_blockers = tags & blocked\n+    if auto_mode and relpath in auto_skip_set:\n+        active_blockers = active_blockers | {\"auto-skip\"}\n     return (len(active_blockers) > 0, active_blockers)\n \n \n def format_command(cmd: Sequence[str]) -> str:\n     return shlex.join(cmd)\n \n \n+def display_path(path: Path) -> str:\n+    try:\n+        return str(path.relative_to(ROOT_DIR))\n+    except ValueError:\n+        return str(path)\n+\n+\n+def env_flag(name: str) -> bool | None:\n+    raw = os.environ.get(name)\n+    if raw is None:\n+        return None\n+    return raw.strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n+\n+\n+def load_auto_skip() -> set[str]:\n+    env_value = os.environ.get(\"EXAMPLES_AUTO_SKIP\", \"\")\n+    if env_value.strip():\n+        parts = re.split(r\"[\\s,]+\", env_value.strip())\n+        return {normalize_relpath(p) for p in parts if p}\n+    return {normalize_relpath(p) for p in DEFAULT_AUTO_SKIP}\n+\n+\n+def write_main_log_line(handle, line: str) -> None:\n+    handle.write(line + \"\\n\")\n+    handle.flush()\n+\n+\n+def ensure_dirs(path: Path, is_file: bool | None = None) -> None:\n+    \"\"\"Create directories for a file or directory path.\n+\n+    If `is_file` is True, always create the parent directory. If False, create the\n+    directory itself. When None, treat paths with a suffix as files and others as\n+    directories, but suffix-less file names should pass is_file=True to avoid\n+    accidental directory creation.\n+    \"\"\"\n+    if is_file is None:\n+        is_file = bool(path.suffix)\n+    target = path.parent if is_file else path\n+    target.mkdir(parents=True, exist_ok=True)\n+\n+\n+def parse_rerun_from_log(log_path: Path) -> list[str]:\n+    if not log_path.exists():\n+        raise FileNotFoundError(log_path)\n+    rerun: list[str] = []\n+    with log_path.open(\"r\", encoding=\"utf-8\") as handle:\n+        for line in handle:\n+            stripped = line.strip()\n+            if not stripped or stripped.startswith(\"#\"):\n+                continue\n+            parts = stripped.split()\n+            if len(parts) < 2:\n+                continue\n+            status, relpath = parts[0].upper(), parts[1]\n+            if status in {\"FAILED\", \"ERROR\", \"UNKNOWN\"}:\n+                rerun.append(normalize_relpath(relpath))\n+    return rerun\n+\n+\n def run_examples(examples: Sequence[ExampleScript], args: argparse.Namespace) -> int:\n     overrides: set[str] = set()\n-    if args.include_interactive:\n+    if args.include_interactive or env_flag(\"EXAMPLES_INCLUDE_INTERACTIVE\"):\n         overrides.add(\"interactive\")\n-    if args.include_server:\n+    if args.include_server or env_flag(\"EXAMPLES_INCLUDE_SERVER\"):\n         overrides.add(\"server\")\n-    if args.include_audio:\n+    if args.include_audio or env_flag(\"EXAMPLES_INCLUDE_AUDIO\"):\n         overrides.add(\"audio\")\n-    if args.include_external:\n+    if args.include_external or env_flag(\"EXAMPLES_INCLUDE_EXTERNAL\"):\n         overrides.add(\"external\")\n \n+    logs_dir = Path(args.logs_dir).resolve()\n+    main_log_path = Path(args.main_log).resolve()\n+    auto_mode = args.auto_mode or os.environ.get(\"EXAMPLES_INTERACTIVE_MODE\", \"\").lower() == \"auto\"\n+    auto_skip_set = load_auto_skip()\n+\n+    if auto_mode and \"interactive\" not in overrides:\n+        overrides.add(\"interactive\")\n+\n+    ensure_dirs(logs_dir, is_file=False)\n+    ensure_dirs(main_log_path, is_file=True)\n+    rerun_entries: list[str] = []\n+\n     if not examples:\n         print(\"No example entry points found that match the filters.\")\n         return 0\n \n+    print(f\"Interactive mode: {'auto' if auto_mode else 'prompt'}\")\n     print(f\"Found {len(examples)} example entry points under examples/.\")\n \n     executed = 0\n     skipped = 0\n     failed = 0\n+    results: list[ExampleResult] = []\n+\n+    jobs = max(1, args.jobs)\n+\n+    output_lock = threading.Lock()\n+    main_log_lock = threading.Lock()\n+    buffer_output = not args.no_buffer_output and os.environ.get(\n+        \"EXAMPLES_BUFFER_OUTPUT\", \"1\"\n+    ).lower() not in {\"0\", \"false\", \"no\", \"off\"}\n+\n+    def safe_write_main(line: str) -> None:\n+        with main_log_lock:\n+            write_main_log_line(main_log, line)\n+\n+    def run_single(example: ExampleScript) -> ExampleResult:\n+        relpath = example.relpath\n+        log_filename = f\"{relpath.replace('/', '__')}.log\"\n+        log_path = logs_dir / log_filename\n+        ensure_dirs(log_path, is_file=True)\n+\n+        env = os.environ.copy()\n+        if auto_mode:\n+            env[\"EXAMPLES_INTERACTIVE_MODE\"] = \"auto\"\n+            env[\"APPLY_PATCH_AUTO_APPROVE\"] = \"1\"\n+            env.setdefault(\"SHELL_AUTO_APPROVE\", \"1\")\n+            env.setdefault(\"AUTO_APPROVE_MCP\", \"1\")\n+\n+        proc = subprocess.Popen(\n+            example.command,\n+            cwd=ROOT_DIR,\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.STDOUT,\n+            text=True,\n+            env=env,\n+        )\n+        assert proc.stdout is not None\n+        force_prompt_stream = (not auto_mode) and (\"interactive\" in example.tags)\n+        buffer_output_local = buffer_output and not force_prompt_stream\n+        buffer_lines: list[str] = []\n+\n+        with log_path.open(\"w\", encoding=\"utf-8\") as per_log:\n+            if force_prompt_stream:\n+                at_line_start = True\n+                while True:\n+                    char = proc.stdout.read(1)\n+                    if char == \"\":\n+                        break\n+                    per_log.write(char)\n+                    with output_lock:\n+                        if at_line_start:\n+                            sys.stdout.write(f\"[{relpath}] \")\n+                        sys.stdout.write(char)\n+                        sys.stdout.flush()\n+                    at_line_start = char == \"\\n\"\n+            else:\n+                for line in proc.stdout:\n+                    per_log.write(line)\n+                    if buffer_output_local:\n+                        buffer_lines.append(line)\n+                    else:\n+                        with output_lock:\n+                            sys.stdout.write(f\"[{relpath}] {line}\")\n+        proc.wait()\n+        exit_code = proc.returncode\n+\n+        if buffer_output_local and buffer_lines:\n+            with output_lock:\n+                for line in buffer_lines:\n+                    sys.stdout.write(f\"[{relpath}] {line}\")\n+\n+        if exit_code == 0:\n+            safe_write_main(f\"PASSED {relpath} exit=0 log={display_path(log_path)}\")\n+            return ExampleResult(\n+                script=example,\n+                status=\"passed\",\n+                log_path=log_path,\n+                exit_code=exit_code,\n+            )\n+\n+        info = f\"exit={exit_code}\"\n+        with output_lock:\n+            print(f\"  !! {relpath} exited with {exit_code}\")\n+        safe_write_main(f\"FAILED {relpath} exit={exit_code} log={display_path(log_path)}\")\n+        return ExampleResult(\n+            script=example,\n+            status=\"failed\",\n+            reason=info,\n+            log_path=log_path,\n+            exit_code=exit_code,\n+        )\n+\n+    with main_log_path.open(\"w\", encoding=\"utf-8\") as main_log:\n+        safe_write_main(f\"# run started {datetime.datetime.now().isoformat()}\")\n+        safe_write_main(f\"# filters: {args.filter or '-'}\")\n+        safe_write_main(f\"# include: {sorted(overrides)}\")\n+        safe_write_main(f\"# auto_mode: {auto_mode}\")\n+        safe_write_main(f\"# logs_dir: {logs_dir}\")\n+        safe_write_main(f\"# jobs: {jobs}\")\n+        safe_write_main(f\"# buffer_output: {buffer_output}\")\n+\n+        run_list: list[ExampleScript] = []\n+\n+        for example in examples:\n+            relpath = example.relpath\n+            skip, reasons = should_skip(example.tags, overrides, auto_skip_set, relpath, auto_mode)\n+            tag_label = f\" [tags: {', '.join(sorted(example.tags))}]\" if args.verbose else \"\"\n+\n+            if skip:\n+                reason_label = f\" (skipped: {', '.join(sorted(reasons))})\" if reasons else \"\"\n+                print(f\"- SKIP {relpath}{tag_label}{reason_label}\")\n+                safe_write_main(f\"SKIPPED {relpath} reasons={','.join(sorted(reasons))}\")\n+                skipped += 1\n+                results.append(\n+                    ExampleResult(script=example, status=\"skipped\", reason=\",\".join(reasons))\n+                )\n+                continue\n+\n+            print(f\"- RUN  {relpath}{tag_label}\")\n+            print(f\"  cmd: {format_command(example.command)}\")\n+\n+            if args.dry_run:\n+                safe_write_main(f\"DRYRUN {relpath}\")\n+                results.append(ExampleResult(script=example, status=\"dry-run\"))\n+                continue\n+\n+            run_list.append(example)\n+\n+        interactive_in_run_list = any(\"interactive\" in ex.tags for ex in run_list)\n+        interactive_requested = \"interactive\" in overrides\n+\n+        if run_list and (not auto_mode) and (interactive_in_run_list or interactive_requested):\n+            if jobs != 1:\n+                print(\n+                    \"Interactive examples detected; forcing serial execution to avoid shared stdin.\"\n+                )\n+                reason = \"interactive\" if interactive_in_run_list else \"interactive-requested\"\n+                safe_write_main(f\"# jobs_adjusted: 1 reason={reason}\")\n+            jobs = 1\n+\n+        run_results: dict[str, ExampleResult] = {}\n+        if run_list:\n+            with ThreadPoolExecutor(max_workers=jobs) as executor:\n+                future_map = {executor.submit(run_single, ex): ex for ex in run_list}\n+                for future in as_completed(future_map):\n+                    result = future.result()\n+                    run_results[result.script.relpath] = result\n+\n+        for ex in run_list:\n+            result = run_results[ex.relpath]\n+            results.append(result)\n+            if result.status == \"passed\":\n+                executed += 1\n+            elif result.status == \"failed\":\n+                failed += 1\n+                rerun_entries.append(ex.relpath)\n+        safe_write_main(f\"# summary executed={executed} skipped={skipped} failed={failed}\")\n+\n+    if args.write_rerun:\n+        ensure_dirs(RERUN_FILE_DEFAULT, is_file=True)\n+        if rerun_entries:\n+            contents = \"\\n\".join(rerun_entries) + \"\\n\"\n+        else:\n+            contents = \"\"\n+        RERUN_FILE_DEFAULT.write_text(contents, encoding=\"utf-8\")\n+        print(f\"Wrote rerun list to {RERUN_FILE_DEFAULT}\")\n+\n+    print(f\"Main log: {main_log_path}\")\n+    print(f\"Done. Ran {executed} example(s), skipped {skipped}, failed {failed}.\")\n \n-    for example in examples:\n-        skip, reasons = should_skip(example.tags, overrides)\n-        tag_label = f\" [tags: {', '.join(sorted(example.tags))}]\" if args.verbose else \"\"\n-\n-        if skip:\n-            reason_label = f\" (skipped: {', '.join(sorted(reasons))})\" if reasons else \"\"\n-            print(f\"- SKIP {example.relpath}{tag_label}{reason_label}\")\n-            skipped += 1\n-            continue\n-\n-        print(f\"- RUN  {example.relpath}{tag_label}\")\n-        print(f\"  cmd: {format_command(example.command)}\")\n-\n-        if args.dry_run:\n-            continue\n-\n-        result = subprocess.run(example.command, cwd=ROOT_DIR)\n-        if result.returncode != 0:\n-            print(f\"  !! {example.relpath} exited with {result.returncode}\")\n-            failed += 1\n-            if args.fail_fast:\n-                return result.returncode\n-            continue\n-\n-        executed += 1\n+    # Summary table\n+    status_w = 9\n+    name_w = 44\n+    info_w = 32\n+    print(\"\\nResults:\")\n+    print(f\"{'status'.ljust(status_w)} {'example'.ljust(name_w)} {'info'.ljust(info_w)} log\")\n+    print(f\"{'-' * status_w} {'-' * name_w} {'-' * info_w} ---\")\n+    for result in results:\n+        info = result.reason or (\"exit 0\" if result.status == \"passed\" else \"\")\n+        log_disp = (\n+            display_path(result.log_path) if result.log_path and result.log_path.exists() else \"-\"\n+        )\n+        print(\n+            f\"{result.status.ljust(status_w)} {result.script.relpath.ljust(name_w)} {info.ljust(info_w)} {log_disp}\"\n+        )\n \n-    print(f\"Done. Ran {executed} example(s), skipped {skipped}, failed {failed}.\")\n     return 0 if failed == 0 else 1\n \n \n def main() -> int:\n     args = parse_args()\n+    if args.print_auto_skip:\n+        for entry in sorted(load_auto_skip()):\n+            print(entry)\n+        return 0\n+\n+    if args.collect:\n+        paths = parse_rerun_from_log(Path(args.collect))\n+        if args.output:\n+            out = Path(args.output)\n+            ensure_dirs(out, is_file=True)\n+            out.write_text(\"\\n\".join(paths) + \"\\n\", encoding=\"utf-8\")\n+            print(f\"Wrote {len(paths)} entries to {out}\")\n+        else:\n+            for p in paths:\n+                print(p)\n+        return 0\n+\n     examples = discover_examples(args.filter)\n+    if args.rerun_file:\n+        rerun_set = {\n+            line.strip()\n+            for line in Path(args.rerun_file).read_text(encoding=\"utf-8\").splitlines()\n+            if line.strip()\n+        }\n+        examples = [ex for ex in examples if ex.relpath in rerun_set]\n+        if not examples:\n+            print(\"Rerun list is empty; nothing to do.\")\n+            return 0\n+        print(f\"Rerun mode: {len(examples)} example(s) from {args.rerun_file}\")\n+\n     return run_examples(examples, args)\n \n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Frun_examples.py",
        "sha": "a3a8174464f4e7cd595f1a2dd4fbc190d4f61f8a",
        "status": "modified"
      },
      {
        "additions": 4,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Ftools%2Fapply_patch.py",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Ftools%2Fapply_patch.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 3,
        "filename": "examples/tools/apply_patch.py",
        "patch": "@@ -7,6 +7,7 @@\n \n from agents import Agent, ApplyPatchTool, ModelSettings, Runner, apply_diff, trace\n from agents.editor import ApplyPatchOperation, ApplyPatchResult\n+from examples.auto_mode import confirm_with_fallback, is_auto_mode\n \n \n class ApprovalTracker:\n@@ -89,8 +90,8 @@ def _require_approval(self, operation: ApplyPatchOperation, display_path: str) -\n         if operation.diff:\n             preview = operation.diff if len(operation.diff) < 400 else f\"{operation.diff[:400]}\"\n             print(\"- diff preview:\\n\", preview)\n-        answer = input(\"Proceed? [y/N] \").strip().lower()\n-        if answer not in {\"y\", \"yes\"}:\n+        approved = confirm_with_fallback(\"Proceed? [y/N] \", default=is_auto_mode())\n+        if not approved:\n             raise RuntimeError(\"Apply patch operation rejected by user.\")\n         self._approvals.remember(fingerprint)\n \n@@ -162,7 +163,7 @@ async def main(auto_approve: bool, model: str) -> None:\n     )\n     parser.add_argument(\n         \"--model\",\n-        default=\"gpt-5.1\",\n+        default=\"gpt-5.2\",\n         help=\"Model ID to use for the agent.\",\n     )\n     args = parser.parse_args()",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Ftools%2Fapply_patch.py",
        "sha": "57a49755c6ff30387795e6a267c2ea251cc1e026",
        "status": "modified"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Ftools%2Fshell.py",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Ftools%2Fshell.py?ref=46df3c3d78aff3f4642fabab1857663e16aac6bf",
        "deletions": 1,
        "filename": "examples/tools/shell.py",
        "patch": "@@ -108,7 +108,7 @@ async def main(prompt: str, model: str) -> None:\n     )\n     parser.add_argument(\n         \"--model\",\n-        default=\"gpt-5.1\",\n+        default=\"gpt-5.2\",\n     )\n     args = parser.parse_args()\n     asyncio.run(main(args.prompt, args.model))",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/46df3c3d78aff3f4642fabab1857663e16aac6bf/examples%2Ftools%2Fshell.py",
        "sha": "37e815178aef1c955448be9c5419e5e0c9163903",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:56:39.296234Z"
}
