{
  "finished_at": "2026-01-20T04:53:49.139476Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "a00b07bbab56ad69",
    "tag": "rest_pr_files_pr2224_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2224/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "41197",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:53:48 GMT",
      "etag": "\"01dd6e13c55cd26024b34278d63c3bdb0196754c4a4206efc460427e57beeee8\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Mon, 19 Jan 2026 15:15:36 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DAC3:2B2B06:1668699:1F729B5:696F0A5C",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4993",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "7",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 65,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/examples%2Fmemory%2Fcompaction_session_example.py",
        "changes": 65,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/examples%2Fmemory%2Fcompaction_session_example.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "examples/memory/compaction_session_example.py",
        "patch": "@@ -0,0 +1,65 @@\n+\"\"\"\n+Example demonstrating OpenAI responses.compact session functionality.\n+\n+This example shows how to use OpenAIResponsesCompactionSession to automatically\n+compact conversation history when it grows too large, reducing token usage\n+while preserving context.\n+\"\"\"\n+\n+import asyncio\n+\n+from agents import Agent, OpenAIResponsesCompactionSession, Runner, SQLiteSession\n+\n+\n+async def main():\n+    # Create an underlying session for storage\n+    underlying = SQLiteSession(\":memory:\")\n+\n+    # Wrap with compaction session - will automatically compact when threshold hit\n+    session = OpenAIResponsesCompactionSession(\n+        session_id=\"demo-session\",\n+        underlying_session=underlying,\n+        model=\"gpt-4.1\",\n+        # Custom compaction trigger (default is 10 candidates)\n+        should_trigger_compaction=lambda ctx: len(ctx[\"compaction_candidate_items\"]) >= 4,\n+    )\n+\n+    agent = Agent(\n+        name=\"Assistant\",\n+        instructions=\"Reply concisely. Keep answers to 1-2 sentences.\",\n+    )\n+\n+    print(\"=== Compaction Session Example ===\\n\")\n+\n+    prompts = [\n+        \"What is the tallest mountain in the world?\",\n+        \"How tall is it in feet?\",\n+        \"When was it first climbed?\",\n+        \"Who was on that expedition?\",\n+        \"What country is the mountain in?\",\n+    ]\n+\n+    for i, prompt in enumerate(prompts, 1):\n+        print(f\"Turn {i}:\")\n+        print(f\"User: {prompt}\")\n+        result = await Runner.run(agent, prompt, session=session)\n+        print(f\"Assistant: {result.final_output}\\n\")\n+\n+    # Show final session state\n+    items = await session.get_items()\n+    print(\"=== Final Session State ===\")\n+    print(f\"Total items: {len(items)}\")\n+    for item in items:\n+        # Some inputs are stored as easy messages (only `role` and `content`).\n+        item_type = item.get(\"type\") or (\"message\" if \"role\" in item else \"unknown\")\n+        if item_type == \"compaction\":\n+            print(\"  - compaction (encrypted content)\")\n+        elif item_type == \"message\":\n+            role = item.get(\"role\", \"unknown\")\n+            print(f\"  - message ({role})\")\n+        else:\n+            print(f\"  - {item_type}\")\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/examples%2Fmemory%2Fcompaction_session_example.py",
        "sha": "cc1f6cf5dbe5d223aadec63ea4ac46f9a32ceedc",
        "status": "added"
      },
      {
        "additions": 10,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2F__init__.py",
        "changes": 10,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F__init__.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "src/agents/__init__.py",
        "patch": "@@ -49,6 +49,7 @@\n     set_conversation_history_wrappers,\n )\n from .items import (\n+    CompactionItem,\n     HandoffCallItem,\n     HandoffOutputItem,\n     ItemHelpers,\n@@ -63,9 +64,13 @@\n from .lifecycle import AgentHooks, RunHooks\n from .memory import (\n     OpenAIConversationsSession,\n+    OpenAIResponsesCompactionArgs,\n+    OpenAIResponsesCompactionAwareSession,\n+    OpenAIResponsesCompactionSession,\n     Session,\n     SessionABC,\n     SQLiteSession,\n+    is_openai_responses_compaction_aware_session,\n )\n from .model_settings import ModelSettings\n from .models.interface import Model, ModelProvider, ModelTracing\n@@ -291,6 +296,11 @@ def enable_verbose_stdout_logging():\n     \"SessionABC\",\n     \"SQLiteSession\",\n     \"OpenAIConversationsSession\",\n+    \"OpenAIResponsesCompactionSession\",\n+    \"OpenAIResponsesCompactionArgs\",\n+    \"OpenAIResponsesCompactionAwareSession\",\n+    \"is_openai_responses_compaction_aware_session\",\n+    \"CompactionItem\",\n     \"AgentHookContext\",\n     \"RunContextWrapper\",\n     \"TContext\",",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2F__init__.py",
        "sha": "572e730995ae49b277e31ceb7db867ff5aa968f5",
        "status": "modified"
      },
      {
        "additions": 4,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2F_run_impl.py",
        "changes": 4,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F_run_impl.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "src/agents/_run_impl.py",
        "patch": "@@ -57,6 +57,7 @@\n from .guardrail import InputGuardrail, InputGuardrailResult, OutputGuardrail, OutputGuardrailResult\n from .handoffs import Handoff, HandoffInputData, nest_handoff_history\n from .items import (\n+    CompactionItem,\n     HandoffCallItem,\n     HandoffOutputItem,\n     ItemHelpers,\n@@ -540,6 +541,9 @@ def process_model_response(\n                 logger.debug(\"Queuing shell_call %s\", call_identifier)\n                 shell_calls.append(ToolRunShellCall(tool_call=output, shell_tool=shell_tool))\n                 continue\n+            if output_type == \"compaction\":\n+                items.append(CompactionItem(raw_item=cast(dict[str, Any], output), agent=agent))\n+                continue\n             if output_type == \"apply_patch_call\":\n                 items.append(ToolCallItem(raw_item=cast(Any, output), agent=agent))\n                 if apply_patch_tool:",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2F_run_impl.py",
        "sha": "2797113283262fb17e12f7389516cdbea8158ade",
        "status": "modified"
      },
      {
        "additions": 18,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fitems.py",
        "changes": 18,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fitems.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "src/agents/items.py",
        "patch": "@@ -327,6 +327,23 @@ class MCPApprovalResponseItem(RunItemBase[McpApprovalResponse]):\n     type: Literal[\"mcp_approval_response_item\"] = \"mcp_approval_response_item\"\n \n \n+@dataclass\n+class CompactionItem:\n+    \"\"\"Represents a compaction item from responses.compact.\"\"\"\n+\n+    agent: Agent[Any]\n+    \"\"\"The agent whose run caused this item to be generated.\"\"\"\n+\n+    raw_item: dict[str, Any]\n+    \"\"\"The raw compaction item containing encrypted_content.\"\"\"\n+\n+    type: Literal[\"compaction_item\"] = \"compaction_item\"\n+\n+    def to_input_item(self) -> TResponseInputItem:\n+        \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n+        return cast(TResponseInputItem, self.raw_item)\n+\n+\n RunItem: TypeAlias = Union[\n     MessageOutputItem,\n     HandoffCallItem,\n@@ -337,6 +354,7 @@ class MCPApprovalResponseItem(RunItemBase[McpApprovalResponse]):\n     MCPListToolsItem,\n     MCPApprovalRequestItem,\n     MCPApprovalResponseItem,\n+    CompactionItem,\n ]\n \"\"\"An item generated by an agent.\"\"\"\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fitems.py",
        "sha": "4845344e3b7f62f8d499d563aab2da5df639ec58",
        "status": "modified"
      },
      {
        "additions": 12,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmemory%2F__init__.py",
        "changes": 13,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmemory%2F__init__.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 1,
        "filename": "src/agents/memory/__init__.py",
        "patch": "@@ -1,5 +1,12 @@\n from .openai_conversations_session import OpenAIConversationsSession\n-from .session import Session, SessionABC\n+from .openai_responses_compaction_session import OpenAIResponsesCompactionSession\n+from .session import (\n+    OpenAIResponsesCompactionArgs,\n+    OpenAIResponsesCompactionAwareSession,\n+    Session,\n+    SessionABC,\n+    is_openai_responses_compaction_aware_session,\n+)\n from .sqlite_session import SQLiteSession\n from .util import SessionInputCallback\n \n@@ -9,4 +16,8 @@\n     \"SessionInputCallback\",\n     \"SQLiteSession\",\n     \"OpenAIConversationsSession\",\n+    \"OpenAIResponsesCompactionSession\",\n+    \"OpenAIResponsesCompactionArgs\",\n+    \"OpenAIResponsesCompactionAwareSession\",\n+    \"is_openai_responses_compaction_aware_session\",\n ]",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmemory%2F__init__.py",
        "sha": "e1a06156e18f733e1783486b2268996e7827dab0",
        "status": "modified"
      },
      {
        "additions": 226,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmemory%2Fopenai_responses_compaction_session.py",
        "changes": 226,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmemory%2Fopenai_responses_compaction_session.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "src/agents/memory/openai_responses_compaction_session.py",
        "patch": "@@ -0,0 +1,226 @@\n+from __future__ import annotations\n+\n+import logging\n+from typing import TYPE_CHECKING, Any, Callable\n+\n+from openai import AsyncOpenAI\n+\n+from ..models._openai_shared import get_default_openai_client\n+from .openai_conversations_session import OpenAIConversationsSession\n+from .session import (\n+    OpenAIResponsesCompactionArgs,\n+    OpenAIResponsesCompactionAwareSession,\n+    SessionABC,\n+)\n+\n+if TYPE_CHECKING:\n+    from ..items import TResponseInputItem\n+    from .session import Session\n+\n+logger = logging.getLogger(\"openai-agents.openai.compaction\")\n+\n+DEFAULT_COMPACTION_THRESHOLD = 10\n+\n+\n+def select_compaction_candidate_items(\n+    items: list[TResponseInputItem],\n+) -> list[TResponseInputItem]:\n+    \"\"\"Select compaction candidate items.\n+\n+    Excludes user messages and compaction items.\n+    \"\"\"\n+\n+    def _is_user_message(item: TResponseInputItem) -> bool:\n+        if not isinstance(item, dict):\n+            return False\n+        if item.get(\"type\") == \"message\":\n+            return item.get(\"role\") == \"user\"\n+        return item.get(\"role\") == \"user\" and \"content\" in item\n+\n+    return [\n+        item\n+        for item in items\n+        if not (\n+            _is_user_message(item) or (isinstance(item, dict) and item.get(\"type\") == \"compaction\")\n+        )\n+    ]\n+\n+\n+def default_should_trigger_compaction(context: dict[str, Any]) -> bool:\n+    \"\"\"Default decision: compact when >= 10 candidate items exist.\"\"\"\n+    return len(context[\"compaction_candidate_items\"]) >= DEFAULT_COMPACTION_THRESHOLD\n+\n+\n+def is_openai_model_name(model: str) -> bool:\n+    \"\"\"Validate model name follows OpenAI conventions.\"\"\"\n+    trimmed = model.strip()\n+    if not trimmed:\n+        return False\n+\n+    # Handle fine-tuned models: ft:gpt-4.1:org:proj:suffix\n+    without_ft_prefix = trimmed[3:] if trimmed.startswith(\"ft:\") else trimmed\n+    root = without_ft_prefix.split(\":\", 1)[0]\n+\n+    # Allow gpt-* and o* models\n+    if root.startswith(\"gpt-\"):\n+        return True\n+    if root.startswith(\"o\") and root[1:2].isdigit():\n+        return True\n+\n+    return False\n+\n+\n+class OpenAIResponsesCompactionSession(SessionABC, OpenAIResponsesCompactionAwareSession):\n+    \"\"\"Session decorator that triggers responses.compact when stored history grows.\n+\n+    Works with OpenAI Responses API models only. Wraps any Session (except\n+    OpenAIConversationsSession) and automatically calls the OpenAI responses.compact\n+    API after each turn when the decision hook returns True.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        session_id: str,\n+        underlying_session: Session,\n+        *,\n+        client: AsyncOpenAI | None = None,\n+        model: str = \"gpt-4.1\",\n+        should_trigger_compaction: Callable[[dict[str, Any]], bool] | None = None,\n+    ):\n+        \"\"\"Initialize the compaction session.\n+\n+        Args:\n+            session_id: Identifier for this session.\n+            underlying_session: Session store that holds the compacted history. Cannot be\n+                OpenAIConversationsSession.\n+            client: OpenAI client for responses.compact API calls. Defaults to\n+                get_default_openai_client() or new AsyncOpenAI().\n+            model: Model to use for responses.compact. Defaults to \"gpt-4.1\". Must be an\n+                OpenAI model name (gpt-*, o*, or ft:gpt-*).\n+            should_trigger_compaction: Custom decision hook. Defaults to triggering when\n+                10+ compaction candidates exist.\n+        \"\"\"\n+        if isinstance(underlying_session, OpenAIConversationsSession):\n+            raise ValueError(\n+                \"OpenAIResponsesCompactionSession cannot wrap OpenAIConversationsSession \"\n+                \"because it manages its own history on the server.\"\n+            )\n+\n+        if not is_openai_model_name(model):\n+            raise ValueError(f\"Unsupported model for OpenAI responses compaction: {model}\")\n+\n+        self.session_id = session_id\n+        self.underlying_session = underlying_session\n+        self._client = client\n+        self.model = model\n+        self.should_trigger_compaction = (\n+            should_trigger_compaction or default_should_trigger_compaction\n+        )\n+\n+        # cache for incremental candidate tracking\n+        self._compaction_candidate_items: list[TResponseInputItem] | None = None\n+        self._session_items: list[TResponseInputItem] | None = None\n+        self._response_id: str | None = None\n+\n+    @property\n+    def client(self) -> AsyncOpenAI:\n+        if self._client is None:\n+            self._client = get_default_openai_client() or AsyncOpenAI()\n+        return self._client\n+\n+    async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -> None:\n+        \"\"\"Run compaction using responses.compact API.\"\"\"\n+        if args and args.get(\"response_id\"):\n+            self._response_id = args[\"response_id\"]\n+\n+        if not self._response_id:\n+            raise ValueError(\n+                \"OpenAIResponsesCompactionSession.run_compaction requires a response_id\"\n+            )\n+\n+        compaction_candidate_items, session_items = await self._ensure_compaction_candidates()\n+\n+        force = args.get(\"force\", False) if args else False\n+        should_compact = force or self.should_trigger_compaction(\n+            {\n+                \"response_id\": self._response_id,\n+                \"compaction_candidate_items\": compaction_candidate_items,\n+                \"session_items\": session_items,\n+            }\n+        )\n+\n+        if not should_compact:\n+            logger.debug(f\"skip: decision hook declined compaction for {self._response_id}\")\n+            return\n+\n+        logger.debug(f\"compact: start for {self._response_id} using {self.model}\")\n+\n+        compacted = await self.client.responses.compact(\n+            previous_response_id=self._response_id,\n+            model=self.model,\n+        )\n+\n+        await self.underlying_session.clear_session()\n+        output_items: list[TResponseInputItem] = []\n+        if compacted.output:\n+            for item in compacted.output:\n+                if isinstance(item, dict):\n+                    output_items.append(item)\n+                else:\n+                    # Suppress Pydantic literal warnings: responses.compact can return\n+                    # user-style input_text content inside ResponseOutputMessage.\n+                    output_items.append(\n+                        item.model_dump(exclude_unset=True, warnings=False)  # type: ignore\n+                    )\n+\n+        if output_items:\n+            await self.underlying_session.add_items(output_items)\n+\n+        self._compaction_candidate_items = select_compaction_candidate_items(output_items)\n+        self._session_items = output_items\n+\n+        logger.debug(\n+            f\"compact: done for {self._response_id} \"\n+            f\"(output={len(output_items)}, candidates={len(self._compaction_candidate_items)})\"\n+        )\n+\n+    async def get_items(self, limit: int | None = None) -> list[TResponseInputItem]:\n+        return await self.underlying_session.get_items(limit)\n+\n+    async def add_items(self, items: list[TResponseInputItem]) -> None:\n+        await self.underlying_session.add_items(items)\n+        if self._compaction_candidate_items is not None:\n+            new_candidates = select_compaction_candidate_items(items)\n+            if new_candidates:\n+                self._compaction_candidate_items.extend(new_candidates)\n+        if self._session_items is not None:\n+            self._session_items.extend(items)\n+\n+    async def pop_item(self) -> TResponseInputItem | None:\n+        popped = await self.underlying_session.pop_item()\n+        if popped:\n+            self._compaction_candidate_items = None\n+            self._session_items = None\n+        return popped\n+\n+    async def clear_session(self) -> None:\n+        await self.underlying_session.clear_session()\n+        self._compaction_candidate_items = []\n+        self._session_items = []\n+\n+    async def _ensure_compaction_candidates(\n+        self,\n+    ) -> tuple[list[TResponseInputItem], list[TResponseInputItem]]:\n+        \"\"\"Lazy-load and cache compaction candidates.\"\"\"\n+        if self._compaction_candidate_items is not None and self._session_items is not None:\n+            return (self._compaction_candidate_items[:], self._session_items[:])\n+\n+        history = await self.underlying_session.get_items()\n+        candidates = select_compaction_candidate_items(history)\n+        self._compaction_candidate_items = candidates\n+        self._session_items = history\n+\n+        logger.debug(\n+            f\"candidates: initialized (history={len(history)}, candidates={len(candidates)})\"\n+        )\n+        return (candidates[:], history[:])",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmemory%2Fopenai_responses_compaction_session.py",
        "sha": "830fa71eb20de890e9d5709f1fc79b4428217c8a",
        "status": "added"
      },
      {
        "additions": 28,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmemory%2Fsession.py",
        "changes": 28,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmemory%2Fsession.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "src/agents/memory/session.py",
        "patch": "@@ -3,6 +3,8 @@\n from abc import ABC, abstractmethod\n from typing import TYPE_CHECKING, Protocol, runtime_checkable\n \n+from typing_extensions import TypedDict, TypeGuard\n+\n if TYPE_CHECKING:\n     from ..items import TResponseInputItem\n \n@@ -97,3 +99,29 @@ async def pop_item(self) -> TResponseInputItem | None:\n     async def clear_session(self) -> None:\n         \"\"\"Clear all items for this session.\"\"\"\n         ...\n+\n+\n+class OpenAIResponsesCompactionArgs(TypedDict, total=False):\n+    \"\"\"Arguments for the run_compaction method.\"\"\"\n+\n+    response_id: str\n+    \"\"\"The ID of the last response to use for compaction.\"\"\"\n+\n+    force: bool\n+    \"\"\"Whether to force compaction even if the threshold is not met.\"\"\"\n+\n+\n+@runtime_checkable\n+class OpenAIResponsesCompactionAwareSession(Session, Protocol):\n+    \"\"\"Protocol for session implementations that support responses compaction.\"\"\"\n+\n+    async def run_compaction(self, args: OpenAIResponsesCompactionArgs | None = None) -> None:\n+        \"\"\"Run the compaction process for the session.\"\"\"\n+        ...\n+\n+\n+def is_openai_responses_compaction_aware_session(\n+    session: Session | None,\n+) -> TypeGuard[OpenAIResponsesCompactionAwareSession]:\n+    \"\"\"Check if a session supports responses compaction.\"\"\"\n+    return isinstance(session, OpenAIResponsesCompactionAwareSession)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmemory%2Fsession.py",
        "sha": "bb92c8654573d5e977c291e514fdf865e663c9d5",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmodels%2Fchatcmpl_converter.py",
        "changes": 9,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fmodels%2Fchatcmpl_converter.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 1,
        "filename": "src/agents/models/chatcmpl_converter.py",
        "patch": "@@ -669,7 +669,14 @@ def ensure_assistant_message() -> ChatCompletionAssistantMessageParam:\n                     # This preserves the original behavior\n                     pending_thinking_blocks = reconstructed_thinking_blocks\n \n-            # 8) If we haven't recognized it => fail or ignore\n+            # 8) compaction items => reject for chat completions\n+            elif isinstance(item, dict) and item.get(\"type\") == \"compaction\":\n+                raise UserError(\n+                    \"Compaction items are not supported for chat completions. \"\n+                    \"Please use the Responses API to handle compaction.\"\n+                )\n+\n+            # 9) If we haven't recognized it => fail or ignore\n             else:\n                 raise UserError(f\"Unhandled item type or structure: {item}\")\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Fmodels%2Fchatcmpl_converter.py",
        "sha": "99a0e9f1588ed32d9c9deaf2b557b12c028f3ccd",
        "status": "modified"
      },
      {
        "additions": 30,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Frun.py",
        "changes": 37,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frun.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 7,
        "filename": "src/agents/run.py",
        "patch": "@@ -59,7 +59,7 @@\n )\n from .lifecycle import AgentHooksBase, RunHooks, RunHooksBase\n from .logger import logger\n-from .memory import Session, SessionInputCallback\n+from .memory import Session, SessionInputCallback, is_openai_responses_compaction_aware_session\n from .model_settings import ModelSettings\n from .models.interface import Model, ModelProvider\n from .models.multi_provider import MultiProvider\n@@ -740,7 +740,10 @@ async def run(\n                                 for guardrail_result in input_guardrail_results\n                             ):\n                                 await self._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.new_step_items,\n+                                    turn_result.model_response.response_id,\n                                 )\n \n                             return result\n@@ -752,7 +755,10 @@ async def run(\n                                     for guardrail_result in input_guardrail_results\n                                 ):\n                                     await self._save_result_to_session(\n-                                        session, [], turn_result.new_step_items\n+                                        session,\n+                                        [],\n+                                        turn_result.new_step_items,\n+                                        turn_result.model_response.response_id,\n                                     )\n                             current_agent = cast(Agent[TContext], turn_result.next_step.new_agent)\n                             current_span.finish(reset_current=True)\n@@ -764,7 +770,10 @@ async def run(\n                                 for guardrail_result in input_guardrail_results\n                             ):\n                                 await self._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.new_step_items,\n+                                    turn_result.model_response.response_id,\n                                 )\n                         else:\n                             raise AgentsException(\n@@ -1234,7 +1243,10 @@ async def _start_streaming(\n                             )\n                             if should_skip_session_save is False:\n                                 await AgentRunner._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.new_step_items,\n+                                    turn_result.model_response.response_id,\n                                 )\n \n                         current_agent = turn_result.next_step.new_agent\n@@ -1280,7 +1292,10 @@ async def _start_streaming(\n                             )\n                             if should_skip_session_save is False:\n                                 await AgentRunner._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.new_step_items,\n+                                    turn_result.model_response.response_id,\n                                 )\n \n                         streamed_result._event_queue.put_nowait(QueueCompleteSentinel())\n@@ -1293,7 +1308,10 @@ async def _start_streaming(\n                             )\n                             if should_skip_session_save is False:\n                                 await AgentRunner._save_result_to_session(\n-                                    session, [], turn_result.new_step_items\n+                                    session,\n+                                    [],\n+                                    turn_result.new_step_items,\n+                                    turn_result.model_response.response_id,\n                                 )\n \n                         # Check for soft cancel after turn completion\n@@ -2011,6 +2029,7 @@ async def _save_result_to_session(\n         session: Session | None,\n         original_input: str | list[TResponseInputItem],\n         new_items: list[RunItem],\n+        response_id: str | None = None,\n     ) -> None:\n         \"\"\"\n         Save the conversation turn to session.\n@@ -2030,6 +2049,10 @@ async def _save_result_to_session(\n         items_to_save = input_list + new_items_as_input\n         await session.add_items(items_to_save)\n \n+        # Run compaction if session supports it and we have a response_id\n+        if response_id and is_openai_responses_compaction_aware_session(session):\n+            await session.run_compaction({\"response_id\": response_id})\n+\n     @staticmethod\n     async def _input_guardrail_tripwire_triggered_for_stream(\n         streamed_result: RunResultStreaming,",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/src%2Fagents%2Frun.py",
        "sha": "f9692ff07528b4476899ba58d310e865e8287b19",
        "status": "modified"
      },
      {
        "additions": 278,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc3c67a54208e83c411503ec4377e90ed89e6307/tests%2Fmemory%2Ftest_openai_responses_compaction_session.py",
        "changes": 278,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Fmemory%2Ftest_openai_responses_compaction_session.py?ref=cc3c67a54208e83c411503ec4377e90ed89e6307",
        "deletions": 0,
        "filename": "tests/memory/test_openai_responses_compaction_session.py",
        "patch": "@@ -0,0 +1,278 @@\n+from __future__ import annotations\n+\n+import warnings as warnings_module\n+from typing import Any, cast\n+from unittest.mock import AsyncMock, MagicMock\n+\n+import pytest\n+\n+from agents.items import TResponseInputItem\n+from agents.memory import (\n+    OpenAIResponsesCompactionSession,\n+    Session,\n+    is_openai_responses_compaction_aware_session,\n+)\n+from agents.memory.openai_responses_compaction_session import (\n+    DEFAULT_COMPACTION_THRESHOLD,\n+    is_openai_model_name,\n+    select_compaction_candidate_items,\n+)\n+\n+\n+class TestIsOpenAIModelName:\n+    def test_gpt_models(self) -> None:\n+        assert is_openai_model_name(\"gpt-4o\") is True\n+        assert is_openai_model_name(\"gpt-4o-mini\") is True\n+        assert is_openai_model_name(\"gpt-3.5-turbo\") is True\n+        assert is_openai_model_name(\"gpt-4.1\") is True\n+        assert is_openai_model_name(\"gpt-5\") is True\n+        assert is_openai_model_name(\"gpt-5.2\") is True\n+        assert is_openai_model_name(\"gpt-5-mini\") is True\n+        assert is_openai_model_name(\"gpt-5-nano\") is True\n+\n+    def test_o_models(self) -> None:\n+        assert is_openai_model_name(\"o1\") is True\n+        assert is_openai_model_name(\"o1-preview\") is True\n+        assert is_openai_model_name(\"o3\") is True\n+\n+    def test_fine_tuned_models(self) -> None:\n+        assert is_openai_model_name(\"ft:gpt-4o-mini:org:proj:suffix\") is True\n+        assert is_openai_model_name(\"ft:gpt-4.1:my-org::id\") is True\n+\n+    def test_invalid_models(self) -> None:\n+        assert is_openai_model_name(\"\") is False\n+        assert is_openai_model_name(\"not-openai\") is False\n+\n+\n+class TestSelectCompactionCandidateItems:\n+    def test_excludes_user_messages(self) -> None:\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"user\", \"content\": \"hello\"}),\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"hi\"}),\n+        ]\n+        result = select_compaction_candidate_items(items)\n+        assert len(result) == 1\n+        assert result[0].get(\"role\") == \"assistant\"\n+\n+    def test_excludes_compaction_items(self) -> None:\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"compaction\", \"summary\": \"...\"}),\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"hi\"}),\n+        ]\n+        result = select_compaction_candidate_items(items)\n+        assert len(result) == 1\n+        assert result[0].get(\"type\") == \"message\"\n+\n+    def test_excludes_easy_user_messages_without_type(self) -> None:\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"content\": \"hi\", \"role\": \"user\"}),\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"hello\"}),\n+        ]\n+        result = select_compaction_candidate_items(items)\n+        assert len(result) == 1\n+        assert result[0].get(\"role\") == \"assistant\"\n+\n+\n+class TestOpenAIResponsesCompactionSession:\n+    def create_mock_session(self) -> MagicMock:\n+        mock = MagicMock(spec=Session)\n+        mock.session_id = \"test-session\"\n+        mock.get_items = AsyncMock(return_value=[])\n+        mock.add_items = AsyncMock()\n+        mock.pop_item = AsyncMock(return_value=None)\n+        mock.clear_session = AsyncMock()\n+        return mock\n+\n+    def test_init_validates_model(self) -> None:\n+        mock_session = self.create_mock_session()\n+\n+        with pytest.raises(ValueError, match=\"Unsupported model\"):\n+            OpenAIResponsesCompactionSession(\n+                session_id=\"test\",\n+                underlying_session=mock_session,\n+                model=\"claude-3\",\n+            )\n+\n+    def test_init_accepts_valid_model(self) -> None:\n+        mock_session = self.create_mock_session()\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            model=\"gpt-4.1\",\n+        )\n+        assert session.model == \"gpt-4.1\"\n+\n+    @pytest.mark.asyncio\n+    async def test_add_items_delegates(self) -> None:\n+        mock_session = self.create_mock_session()\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+        )\n+\n+        items: list[TResponseInputItem] = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"test\"})\n+        ]\n+        await session.add_items(items)\n+\n+        mock_session.add_items.assert_called_once_with(items)\n+\n+    @pytest.mark.asyncio\n+    async def test_get_items_delegates(self) -> None:\n+        mock_session = self.create_mock_session()\n+        mock_session.get_items.return_value = [{\"type\": \"message\", \"content\": \"test\"}]\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+        )\n+\n+        result = await session.get_items()\n+        assert len(result) == 1\n+        mock_session.get_items.assert_called_once()\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_requires_response_id(self) -> None:\n+        mock_session = self.create_mock_session()\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+        )\n+\n+        with pytest.raises(ValueError, match=\"requires a response_id\"):\n+            await session.run_compaction()\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_skips_when_below_threshold(self) -> None:\n+        mock_session = self.create_mock_session()\n+        # Return fewer than threshold items\n+        mock_session.get_items.return_value = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": f\"msg{i}\"})\n+            for i in range(DEFAULT_COMPACTION_THRESHOLD - 1)\n+        ]\n+\n+        mock_client = MagicMock()\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+        )\n+\n+        await session.run_compaction({\"response_id\": \"resp-123\"})\n+\n+        # Should not have called the compact API\n+        mock_client.responses.compact.assert_not_called()\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_executes_when_threshold_met(self) -> None:\n+        mock_session = self.create_mock_session()\n+        # Return exactly threshold items (all assistant messages = candidates)\n+        mock_session.get_items.return_value = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": f\"msg{i}\"})\n+            for i in range(DEFAULT_COMPACTION_THRESHOLD)\n+        ]\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = [{\"type\": \"compaction\", \"summary\": \"compacted\"}]\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+            model=\"gpt-4.1\",\n+        )\n+\n+        await session.run_compaction({\"response_id\": \"resp-123\"})\n+\n+        mock_client.responses.compact.assert_called_once_with(\n+            previous_response_id=\"resp-123\",\n+            model=\"gpt-4.1\",\n+        )\n+        mock_session.clear_session.assert_called_once()\n+        mock_session.add_items.assert_called()\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_force_bypasses_threshold(self) -> None:\n+        mock_session = self.create_mock_session()\n+        mock_session.get_items.return_value = []\n+\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = []\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+        )\n+\n+        await session.run_compaction({\"response_id\": \"resp-123\", \"force\": True})\n+\n+        mock_client.responses.compact.assert_called_once()\n+\n+    @pytest.mark.asyncio\n+    async def test_run_compaction_suppresses_model_dump_warnings(self) -> None:\n+        mock_session = self.create_mock_session()\n+        mock_session.get_items.return_value = [\n+            cast(TResponseInputItem, {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"hi\"})\n+            for _ in range(DEFAULT_COMPACTION_THRESHOLD)\n+        ]\n+\n+        class WarningModel:\n+            def __init__(self) -> None:\n+                self.received_warnings_arg: bool | None = None\n+\n+            def model_dump(\n+                self, *, exclude_unset: bool, warnings: bool | None = None\n+            ) -> dict[str, Any]:\n+                self.received_warnings_arg = warnings\n+                if warnings:\n+                    warnings_module.warn(\"unexpected warning\", stacklevel=2)\n+                return {\"type\": \"message\", \"role\": \"assistant\", \"content\": \"ok\"}\n+\n+        warning_model = WarningModel()\n+        mock_compact_response = MagicMock()\n+        mock_compact_response.output = [warning_model]\n+\n+        mock_client = MagicMock()\n+        mock_client.responses.compact = AsyncMock(return_value=mock_compact_response)\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_session,\n+            client=mock_client,\n+        )\n+\n+        with warnings_module.catch_warnings():\n+            warnings_module.simplefilter(\"error\")\n+            await session.run_compaction({\"response_id\": \"resp-123\"})\n+\n+        assert warning_model.received_warnings_arg is False\n+\n+\n+class TestTypeGuard:\n+    def test_is_compaction_aware_session_true(self) -> None:\n+        mock_underlying = MagicMock(spec=Session)\n+        mock_underlying.session_id = \"test\"\n+        mock_underlying.get_items = AsyncMock(return_value=[])\n+        mock_underlying.add_items = AsyncMock()\n+        mock_underlying.pop_item = AsyncMock(return_value=None)\n+        mock_underlying.clear_session = AsyncMock()\n+\n+        session = OpenAIResponsesCompactionSession(\n+            session_id=\"test\",\n+            underlying_session=mock_underlying,\n+        )\n+        assert is_openai_responses_compaction_aware_session(session) is True\n+\n+    def test_is_compaction_aware_session_false(self) -> None:\n+        mock_session = MagicMock(spec=Session)\n+        assert is_openai_responses_compaction_aware_session(mock_session) is False\n+\n+    def test_is_compaction_aware_session_none(self) -> None:\n+        assert is_openai_responses_compaction_aware_session(None) is False",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc3c67a54208e83c411503ec4377e90ed89e6307/tests%2Fmemory%2Ftest_openai_responses_compaction_session.py",
        "sha": "d4363eec7d7284cb56ab7dc36b237c44a3f57701",
        "status": "added"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:53:48.227376Z"
}
