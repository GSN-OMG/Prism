{
  "finished_at": "2026-01-20T04:57:43.546311Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "7827418722325c9e",
    "tag": "rest_pr_files_pr2323_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2323/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "44821",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:57:43 GMT",
      "etag": "\"2ac3ee74483c5d80cb43d77e9a2f790bddda8d225aa3400be4913c0c8b55d3df\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Fri, 16 Jan 2026 11:24:15 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DCDA:37BFB7:1612C01:1F1EA46:696F0B47",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4937",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "63",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 28,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2F_run_impl.py",
        "changes": 32,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2F_run_impl.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 4,
        "filename": "src/agents/_run_impl.py",
        "patch": "@@ -245,7 +245,8 @@ class SingleStepResult:\n     \"\"\"Items generated before the current step.\"\"\"\n \n     new_step_items: list[RunItem]\n-    \"\"\"Items generated during this current step.\"\"\"\n+    \"\"\"Items generated during this current step. May be filtered during handoffs to avoid\n+    duplication in model input.\"\"\"\n \n     next_step: NextStepHandoff | NextStepFinalOutput | NextStepRunAgain\n     \"\"\"The next step to take.\"\"\"\n@@ -256,11 +257,18 @@ class SingleStepResult:\n     tool_output_guardrail_results: list[ToolOutputGuardrailResult]\n     \"\"\"Tool output guardrail results from this step.\"\"\"\n \n+    session_step_items: list[RunItem] | None = None\n+    \"\"\"Full unfiltered items for session history. When set, these are used instead of\n+    new_step_items for session saving and generated_items property.\"\"\"\n+\n     @property\n     def generated_items(self) -> list[RunItem]:\n         \"\"\"Items generated during the agent run (i.e. everything generated after\n-        `original_input`).\"\"\"\n-        return self.pre_step_items + self.new_step_items\n+        `original_input`). Uses session_step_items when available for full observability.\"\"\"\n+        items = (\n+            self.session_step_items if self.session_step_items is not None else self.new_step_items\n+        )\n+        return self.pre_step_items + items\n \n \n def get_model_tracing_impl(\n@@ -1290,6 +1298,12 @@ async def execute_handoffs(\n                 )\n                 pre_step_items = list(filtered.pre_handoff_items)\n                 new_step_items = list(filtered.new_items)\n+                # For custom input filters, use input_items if available, otherwise new_items\n+                if filtered.input_items is not None:\n+                    session_step_items = list(filtered.new_items)\n+                    new_step_items = list(filtered.input_items)\n+                else:\n+                    session_step_items = None\n             elif should_nest_history and handoff_input_data is not None:\n                 nested = nest_handoff_history(\n                     handoff_input_data,\n@@ -1301,7 +1315,16 @@ async def execute_handoffs(\n                     else list(nested.input_history)\n                 )\n                 pre_step_items = list(nested.pre_handoff_items)\n-                new_step_items = list(nested.new_items)\n+                # Keep full new_items for session history.\n+                session_step_items = list(nested.new_items)\n+                # Use input_items (filtered) for model input if available.\n+                if nested.input_items is not None:\n+                    new_step_items = list(nested.input_items)\n+                else:\n+                    new_step_items = session_step_items\n+            else:\n+                # No filtering or nesting - session_step_items not needed\n+                session_step_items = None\n \n         return SingleStepResult(\n             original_input=original_input,\n@@ -1311,6 +1334,7 @@ async def execute_handoffs(\n             next_step=NextStepHandoff(new_agent),\n             tool_input_guardrail_results=[],\n             tool_output_guardrail_results=[],\n+            session_step_items=session_step_items,\n         )\n \n     @classmethod",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2F_run_impl.py",
        "sha": "e3fa2489d0de33aa861282f531fd5c0133e81be0",
        "status": "modified"
      },
      {
        "additions": 12,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Fhandoffs%2F__init__.py",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fhandoffs%2F__init__.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 4,
        "filename": "src/agents/handoffs/__init__.py",
        "patch": "@@ -62,6 +62,13 @@ class HandoffInputData:\n     later on, it is optional for backwards compatibility.\n     \"\"\"\n \n+    input_items: tuple[RunItem, ...] | None = None\n+    \"\"\"\n+    Items to include in the next agent's input. When set, these items are used instead of\n+    new_items for building the input to the next agent. This allows filtering duplicates\n+    from agent input while preserving all items in new_items for session history.\n+    \"\"\"\n+\n     def clone(self, **kwargs: Any) -> HandoffInputData:\n         \"\"\"\n         Make a copy of the handoff input data, with the given arguments changed. For example, you\n@@ -117,10 +124,11 @@ class Handoff(Generic[TContext, TAgent]):\n     filter inputs (for example, to remove older inputs or remove tools from existing inputs). The\n     function receives the entire conversation history so far, including the input item that\n     triggered the handoff and a tool call output item representing the handoff tool's output. You\n-    are free to modify the input history or new items as you see fit. The next agent that runs will\n-    receive ``handoff_input_data.all_items``. IMPORTANT: in streaming mode, we will not stream\n-    anything as a result of this function. The items generated before will already have been\n-    streamed.\n+    are free to modify the input history or new items as you see fit. The next agent receives the\n+    input history plus ``input_items`` when provided, otherwise it receives ``new_items``. Use\n+    ``input_items`` to filter model input while keeping ``new_items`` intact for session history.\n+    IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The\n+    items generated before will already have been streamed.\n     \"\"\"\n \n     nest_handoff_history: bool | None = None",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Fhandoffs%2F__init__.py",
        "sha": "507ca633d1c9a5771e7888bcf8420e8e291fc13d",
        "status": "modified"
      },
      {
        "additions": 45,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Fhandoffs%2Fhistory.py",
        "changes": 58,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fhandoffs%2Fhistory.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 13,
        "filename": "src/agents/handoffs/history.py",
        "patch": "@@ -26,6 +26,13 @@\n _conversation_history_start = _DEFAULT_CONVERSATION_HISTORY_START\n _conversation_history_end = _DEFAULT_CONVERSATION_HISTORY_END\n \n+# Item types that are summarized in the conversation history.\n+# They should not be forwarded verbatim to the next agent to avoid duplication.\n+_SUMMARY_ONLY_INPUT_TYPES = {\n+    \"function_call\",\n+    \"function_call_output\",\n+}\n+\n \n def set_conversation_history_wrappers(\n     *,\n@@ -67,23 +74,34 @@ def nest_handoff_history(\n \n     normalized_history = _normalize_input_history(handoff_input_data.input_history)\n     flattened_history = _flatten_nested_history_messages(normalized_history)\n-    pre_items_as_inputs = [\n-        _run_item_to_plain_input(item) for item in handoff_input_data.pre_handoff_items\n-    ]\n-    new_items_as_inputs = [_run_item_to_plain_input(item) for item in handoff_input_data.new_items]\n+\n+    # Convert items to plain inputs for the transcript summary.\n+    pre_items_as_inputs: list[TResponseInputItem] = []\n+    filtered_pre_items: list[RunItem] = []\n+    for run_item in handoff_input_data.pre_handoff_items:\n+        plain_input = _run_item_to_plain_input(run_item)\n+        pre_items_as_inputs.append(plain_input)\n+        if _should_forward_pre_item(plain_input):\n+            filtered_pre_items.append(run_item)\n+\n+    new_items_as_inputs: list[TResponseInputItem] = []\n+    filtered_input_items: list[RunItem] = []\n+    for run_item in handoff_input_data.new_items:\n+        plain_input = _run_item_to_plain_input(run_item)\n+        new_items_as_inputs.append(plain_input)\n+        if _should_forward_new_item(plain_input):\n+            filtered_input_items.append(run_item)\n+\n     transcript = flattened_history + pre_items_as_inputs + new_items_as_inputs\n \n     mapper = history_mapper or default_handoff_history_mapper\n     history_items = mapper(transcript)\n-    filtered_pre_items = tuple(\n-        item\n-        for item in handoff_input_data.pre_handoff_items\n-        if _get_run_item_role(item) != \"assistant\"\n-    )\n \n     return handoff_input_data.clone(\n         input_history=tuple(deepcopy(item) for item in history_items),\n-        pre_handoff_items=filtered_pre_items,\n+        pre_handoff_items=tuple(filtered_pre_items),\n+        # new_items stays unchanged for session history.\n+        input_items=tuple(filtered_input_items),\n     )\n \n \n@@ -231,6 +249,20 @@ def _split_role_and_name(role_text: str) -> tuple[str, str | None]:\n     return (role_text or \"developer\", None)\n \n \n-def _get_run_item_role(run_item: RunItem) -> str | None:\n-    role_candidate = run_item.to_input_item().get(\"role\")\n-    return role_candidate if isinstance(role_candidate, str) else None\n+def _should_forward_pre_item(input_item: TResponseInputItem) -> bool:\n+    \"\"\"Return False when the previous transcript item is represented in the summary.\"\"\"\n+    role_candidate = input_item.get(\"role\")\n+    if isinstance(role_candidate, str) and role_candidate == \"assistant\":\n+        return False\n+    type_candidate = input_item.get(\"type\")\n+    return not (isinstance(type_candidate, str) and type_candidate in _SUMMARY_ONLY_INPUT_TYPES)\n+\n+\n+def _should_forward_new_item(input_item: TResponseInputItem) -> bool:\n+    \"\"\"Return False for tool or side-effect items that the summary already covers.\"\"\"\n+    # Items with a role should always be forwarded.\n+    role_candidate = input_item.get(\"role\")\n+    if isinstance(role_candidate, str) and role_candidate:\n+        return True\n+    type_candidate = input_item.get(\"type\")\n+    return not (isinstance(type_candidate, str) and type_candidate in _SUMMARY_ONLY_INPUT_TYPES)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Fhandoffs%2Fhistory.py",
        "sha": "9dd164cac51c62517ee989e617a28a6274f339d1",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Fresult.py",
        "changes": 3,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Fresult.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 0,
        "filename": "src/agents/result.py",
        "patch": "@@ -209,6 +209,9 @@ class RunResultStreaming(RunResultBase):\n         default=None,\n     )\n \n+    _model_input_items: list[RunItem] = field(default_factory=list, repr=False)\n+    \"\"\"Filtered items used to build model input between streaming turns.\"\"\"\n+\n     # Queues that the background run_loop writes to\n     _event_queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel] = field(\n         default_factory=asyncio.Queue, repr=False",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Fresult.py",
        "sha": "0a7d2331c9277babd4fa7f4fcdfc4e4bb79d745b",
        "status": "modified"
      },
      {
        "additions": 70,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Frun.py",
        "changes": 97,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frun.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 27,
        "filename": "src/agents/run.py",
        "patch": "@@ -583,7 +583,8 @@ async def run(\n         ):\n             current_turn = 0\n             original_input: str | list[TResponseInputItem] = _copy_str_or_list(prepared_input)\n-            generated_items: list[RunItem] = []\n+            generated_items: list[RunItem] = []  # For model input (may be filtered on handoffs)\n+            session_items: list[RunItem] = []  # For observability (always unfiltered)\n             model_responses: list[ModelResponse] = []\n \n             context_wrapper: RunContextWrapper[TContext] = RunContextWrapper(\n@@ -705,7 +706,15 @@ async def run(\n \n                     model_responses.append(turn_result.model_response)\n                     original_input = turn_result.original_input\n-                    generated_items = turn_result.generated_items\n+                    # For model input, use new_step_items (filtered on handoffs)\n+                    generated_items = turn_result.pre_step_items + turn_result.new_step_items\n+                    # Accumulate unfiltered items for observability\n+                    session_items_for_turn = (\n+                        turn_result.session_step_items\n+                        if turn_result.session_step_items is not None\n+                        else turn_result.new_step_items\n+                    )\n+                    session_items.extend(session_items_for_turn)\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -725,7 +734,7 @@ async def run(\n                             )\n                             result = RunResult(\n                                 input=original_input,\n-                                new_items=generated_items,\n+                                new_items=session_items,  # Use unfiltered items for observability\n                                 raw_responses=model_responses,\n                                 final_output=turn_result.next_step.output,\n                                 _last_agent=current_agent,\n@@ -742,7 +751,9 @@ async def run(\n                                 await self._save_result_to_session(\n                                     session,\n                                     [],\n-                                    turn_result.new_step_items,\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n                                 )\n \n@@ -757,7 +768,9 @@ async def run(\n                                     await self._save_result_to_session(\n                                         session,\n                                         [],\n-                                        turn_result.new_step_items,\n+                                        turn_result.session_step_items\n+                                        if turn_result.session_step_items is not None\n+                                        else turn_result.new_step_items,\n                                         turn_result.model_response.response_id,\n                                     )\n                             current_agent = cast(Agent[TContext], turn_result.next_step.new_agent)\n@@ -772,7 +785,9 @@ async def run(\n                                 await self._save_result_to_session(\n                                     session,\n                                     [],\n-                                    turn_result.new_step_items,\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n                                 )\n                         else:\n@@ -789,7 +804,7 @@ async def run(\n             except AgentsException as exc:\n                 exc.run_data = RunErrorDetails(\n                     input=original_input,\n-                    new_items=generated_items,\n+                    new_items=session_items,  # Use unfiltered items for observability\n                     raw_responses=model_responses,\n                     last_agent=current_agent,\n                     context_wrapper=context_wrapper,\n@@ -1227,7 +1242,17 @@ async def _start_streaming(\n                         turn_result.model_response\n                     ]\n                     streamed_result.input = turn_result.original_input\n-                    streamed_result.new_items = turn_result.generated_items\n+                    # Keep filtered items for building model input on the next turn.\n+                    streamed_result._model_input_items = (\n+                        turn_result.pre_step_items + turn_result.new_step_items\n+                    )\n+                    # Accumulate unfiltered items for observability\n+                    session_items_for_turn = (\n+                        turn_result.session_step_items\n+                        if turn_result.session_step_items is not None\n+                        else turn_result.new_step_items\n+                    )\n+                    streamed_result.new_items.extend(session_items_for_turn)\n \n                     if server_conversation_tracker is not None:\n                         server_conversation_tracker.track_server_items(turn_result.model_response)\n@@ -1245,7 +1270,9 @@ async def _start_streaming(\n                                 await AgentRunner._save_result_to_session(\n                                     session,\n                                     [],\n-                                    turn_result.new_step_items,\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n                                 )\n \n@@ -1294,7 +1321,9 @@ async def _start_streaming(\n                                 await AgentRunner._save_result_to_session(\n                                     session,\n                                     [],\n-                                    turn_result.new_step_items,\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n                                 )\n \n@@ -1310,7 +1339,9 @@ async def _start_streaming(\n                                 await AgentRunner._save_result_to_session(\n                                     session,\n                                     [],\n-                                    turn_result.new_step_items,\n+                                    turn_result.session_step_items\n+                                    if turn_result.session_step_items is not None\n+                                    else turn_result.new_step_items,\n                                     turn_result.model_response.response_id,\n                                 )\n \n@@ -1423,11 +1454,11 @@ async def _run_single_turn_streamed(\n \n         if server_conversation_tracker is not None:\n             input = server_conversation_tracker.prepare_input(\n-                streamed_result.input, streamed_result.new_items\n+                streamed_result.input, streamed_result._model_input_items\n             )\n         else:\n             input = ItemHelpers.input_to_new_input_list(streamed_result.input)\n-            input.extend([item.to_input_item() for item in streamed_result.new_items])\n+            input.extend([item.to_input_item() for item in streamed_result._model_input_items])\n \n         # THIS IS THE RESOLVED CONFLICT BLOCK\n         filtered = await cls._maybe_filter_model_input(\n@@ -1547,7 +1578,7 @@ async def _run_single_turn_streamed(\n         single_step_result = await cls._get_single_step_result_from_response(\n             agent=agent,\n             original_input=streamed_result.input,\n-            pre_step_items=streamed_result.new_items,\n+            pre_step_items=streamed_result._model_input_items,\n             new_response=final_response,\n             output_schema=output_schema,\n             all_tools=all_tools,\n@@ -1561,14 +1592,21 @@ async def _run_single_turn_streamed(\n \n         import dataclasses as _dc\n \n-        # Filter out items that have already been sent to avoid duplicates\n-        items_to_filter = single_step_result.new_step_items\n+        # Stream session items (unfiltered) when available for observability.\n+        streaming_items = (\n+            single_step_result.session_step_items\n+            if single_step_result.session_step_items is not None\n+            else single_step_result.new_step_items\n+        )\n+\n+        # Filter out items that have already been sent to avoid duplicates.\n+        items_to_stream = streaming_items\n \n         if emitted_tool_call_ids:\n             # Filter out tool call items that were already emitted during streaming\n-            items_to_filter = [\n+            items_to_stream = [\n                 item\n-                for item in items_to_filter\n+                for item in items_to_stream\n                 if not (\n                     isinstance(item, ToolCallItem)\n                     and (\n@@ -1582,9 +1620,9 @@ async def _run_single_turn_streamed(\n \n         if emitted_reasoning_item_ids:\n             # Filter out reasoning items that were already emitted during streaming\n-            items_to_filter = [\n+            items_to_stream = [\n                 item\n-                for item in items_to_filter\n+                for item in items_to_stream\n                 if not (\n                     isinstance(item, ReasoningItem)\n                     and (reasoning_id := getattr(item.raw_item, \"id\", None))\n@@ -1593,12 +1631,12 @@ async def _run_single_turn_streamed(\n             ]\n \n         # Filter out HandoffCallItem to avoid duplicates (already sent earlier)\n-        items_to_filter = [\n-            item for item in items_to_filter if not isinstance(item, HandoffCallItem)\n+        items_to_stream = [\n+            item for item in items_to_stream if not isinstance(item, HandoffCallItem)\n         ]\n \n         # Create filtered result and send to queue\n-        filtered_result = _dc.replace(single_step_result, new_step_items=items_to_filter)\n+        filtered_result = _dc.replace(single_step_result, new_step_items=items_to_stream)\n         RunImpl.stream_step_result_to_queue(filtered_result, streamed_result._event_queue)\n         return single_step_result\n \n@@ -1738,7 +1776,7 @@ async def _get_single_step_result_from_streamed_response(\n         tool_use_tracker: AgentToolUseTracker,\n     ) -> SingleStepResult:\n         original_input = streamed_result.input\n-        pre_step_items = streamed_result.new_items\n+        pre_step_items = streamed_result._model_input_items\n         event_queue = streamed_result._event_queue\n \n         processed_response = RunImpl.process_model_response(\n@@ -1763,10 +1801,15 @@ async def _get_single_step_result_from_streamed_response(\n             context_wrapper=context_wrapper,\n             run_config=run_config,\n         )\n+        # Use session_step_items (unfiltered) if available for streaming observability,\n+        # otherwise fall back to new_step_items.\n+        streaming_items = (\n+            single_step_result.session_step_items\n+            if single_step_result.session_step_items is not None\n+            else single_step_result.new_step_items\n+        )\n         new_step_items = [\n-            item\n-            for item in single_step_result.new_step_items\n-            if item not in new_items_processed_response\n+            item for item in streaming_items if item not in new_items_processed_response\n         ]\n         RunImpl.stream_step_items_to_queue(new_step_items, event_queue)\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/src%2Fagents%2Frun.py",
        "sha": "ab527bde74fc76d87f65e251f92fe0dcdcd7bef8",
        "status": "modified"
      },
      {
        "additions": 66,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_agent_runner.py",
        "changes": 66,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_agent_runner.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 0,
        "filename": "tests/test_agent_runner.py",
        "patch": "@@ -29,6 +29,7 @@\n     handoff,\n )\n from agents.agent import ToolsToFinalOutputResult\n+from agents.items import HandoffOutputItem, ToolCallOutputItem\n from agents.tool import FunctionToolResult, function_tool\n \n from .fake_model import FakeModel\n@@ -179,6 +180,71 @@ async def test_handoffs():\n     assert result.last_agent == agent_1, \"should have handed off to agent_1\"\n \n \n+@pytest.mark.asyncio\n+async def test_nested_handoff_filters_model_input_but_preserves_session_items():\n+    model = FakeModel()\n+    delegate = Agent(\n+        name=\"delegate\",\n+        model=model,\n+    )\n+    triage = Agent(\n+        name=\"triage\",\n+        model=model,\n+        handoffs=[delegate],\n+        tools=[get_function_tool(\"some_function\", \"result\")],\n+    )\n+\n+    model.add_multiple_turn_outputs(\n+        [\n+            # First turn: a tool call.\n+            [get_function_tool_call(\"some_function\", json.dumps({\"a\": \"b\"}))],\n+            # Second turn: a message and a handoff.\n+            [get_text_message(\"a_message\"), get_handoff_tool_call(delegate)],\n+            # Third turn: final message.\n+            [get_text_message(\"done\")],\n+        ]\n+    )\n+\n+    model_input_types: list[list[str]] = []\n+\n+    def capture_model_input(data):\n+        types: list[str] = []\n+        for item in data.model_data.input:\n+            if isinstance(item, dict):\n+                item_type = item.get(\"type\")\n+                if isinstance(item_type, str):\n+                    types.append(item_type)\n+        model_input_types.append(types)\n+        return data.model_data\n+\n+    session = SimpleListSession()\n+    result = await Runner.run(\n+        triage,\n+        input=\"user_message\",\n+        run_config=RunConfig(\n+            nest_handoff_history=True,\n+            call_model_input_filter=capture_model_input,\n+        ),\n+        session=session,\n+    )\n+\n+    assert result.final_output == \"done\"\n+    assert len(model_input_types) >= 3\n+    handoff_input_types = model_input_types[2]\n+    assert \"function_call\" not in handoff_input_types\n+    assert \"function_call_output\" not in handoff_input_types\n+\n+    assert any(isinstance(item, ToolCallOutputItem) for item in result.new_items)\n+    assert any(isinstance(item, HandoffOutputItem) for item in result.new_items)\n+\n+    session_items = await session.get_items()\n+    has_function_call_output = any(\n+        isinstance(item, dict) and item.get(\"type\") == \"function_call_output\"\n+        for item in session_items\n+    )\n+    assert has_function_call_output\n+\n+\n class Foo(TypedDict):\n     bar: str\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_agent_runner.py",
        "sha": "6a0962327325e8969fb8b736ff2cf85b57918219",
        "status": "modified"
      },
      {
        "additions": 67,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_agent_runner_streamed.py",
        "changes": 69,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_agent_runner_streamed.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 2,
        "filename": "tests/test_agent_runner_streamed.py",
        "patch": "@@ -22,7 +22,7 @@\n     function_tool,\n     handoff,\n )\n-from agents.items import RunItem\n+from agents.items import HandoffOutputItem, RunItem, ToolCallOutputItem\n from agents.run import RunConfig\n from agents.stream_events import AgentUpdatedStreamEvent\n \n@@ -186,6 +186,70 @@ class Foo(TypedDict):\n     bar: str\n \n \n+@pytest.mark.asyncio\n+async def test_nested_handoff_filters_model_input_streamed():\n+    model = FakeModel()\n+    delegate = Agent(\n+        name=\"delegate\",\n+        model=model,\n+    )\n+    triage = Agent(\n+        name=\"triage\",\n+        model=model,\n+        handoffs=[delegate],\n+        tools=[get_function_tool(\"some_function\", \"result\")],\n+    )\n+\n+    model.add_multiple_turn_outputs(\n+        [\n+            [get_function_tool_call(\"some_function\", json.dumps({\"a\": \"b\"}))],\n+            [get_text_message(\"a_message\"), get_handoff_tool_call(delegate)],\n+            [get_text_message(\"done\")],\n+        ]\n+    )\n+\n+    model_input_types: list[list[str]] = []\n+\n+    def capture_model_input(data):\n+        types: list[str] = []\n+        for item in data.model_data.input:\n+            if isinstance(item, dict):\n+                item_type = item.get(\"type\")\n+                if isinstance(item_type, str):\n+                    types.append(item_type)\n+        model_input_types.append(types)\n+        return data.model_data\n+\n+    session = SimpleListSession()\n+    result = Runner.run_streamed(\n+        triage,\n+        input=\"user_message\",\n+        run_config=RunConfig(\n+            nest_handoff_history=True,\n+            call_model_input_filter=capture_model_input,\n+        ),\n+        session=session,\n+    )\n+    async for _ in result.stream_events():\n+        pass\n+\n+    assert result.final_output == \"done\"\n+    assert len(model_input_types) >= 3\n+    handoff_input_types = model_input_types[2]\n+    assert \"function_call\" not in handoff_input_types\n+    assert \"function_call_output\" not in handoff_input_types\n+\n+    assert any(isinstance(item, ToolCallOutputItem) for item in result.new_items)\n+    assert any(isinstance(item, HandoffOutputItem) for item in result.new_items)\n+\n+    session_items = await session.get_items()\n+    has_function_call_output = any(\n+        isinstance(item, dict) and item.get(\"type\") == \"function_call_output\"\n+        for item in session_items\n+    )\n+    assert has_function_call_output\n+\n+\n @pytest.mark.asyncio\n async def test_structured_output():\n     model = FakeModel()\n@@ -737,7 +801,8 @@ async def test_streaming_events():\n         \"tool_call_output\": 2,\n         \"message\": 2,  # get_text_message(\"a_message\") + get_final_output_message(...)\n         \"handoff\": 1,  # get_handoff_tool_call(agent_1)\n-        \"handoff_output\": 1,  # handoff_output_item\n+        # Handoff output is now emitted as a streamed item for observability.\n+        \"handoff_output\": 1,\n     }\n \n     total_expected_item_count = sum(expected_item_type_map.values())",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_agent_runner_streamed.py",
        "sha": "8348aa9c80122df273acb401dd70c1b5d92d5976",
        "status": "modified"
      },
      {
        "additions": 276,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_handoff_history_duplication.py",
        "changes": 276,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_handoff_history_duplication.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 0,
        "filename": "tests/test_handoff_history_duplication.py",
        "patch": "@@ -0,0 +1,276 @@\n+\"\"\"Tests for handoff history duplication fix (Issue #2171).\n+\n+These tests verify that when nest_handoff_history is enabled,\n+function_call and function_call_output items are NOT duplicated\n+in the input sent to the next agent.\n+\"\"\"\n+\n+from typing import Any, cast\n+\n+from openai.types.responses import (\n+    ResponseFunctionToolCall,\n+    ResponseOutputMessage,\n+    ResponseOutputText,\n+)\n+\n+from agents import Agent\n+from agents.handoffs import HandoffInputData, nest_handoff_history\n+from agents.items import (\n+    HandoffCallItem,\n+    HandoffOutputItem,\n+    MessageOutputItem,\n+    ToolCallItem,\n+    ToolCallOutputItem,\n+)\n+\n+\n+def _create_mock_agent() -> Agent:\n+    \"\"\"Create a mock agent for testing.\"\"\"\n+    return Agent(name=\"test_agent\")\n+\n+\n+def _create_tool_call_item(agent: Agent) -> ToolCallItem:\n+    \"\"\"Create a mock ToolCallItem.\"\"\"\n+    raw_item = ResponseFunctionToolCall(\n+        id=\"call_tool_123\",\n+        call_id=\"call_tool_123\",\n+        name=\"get_weather\",\n+        arguments='{\"city\": \"London\"}',\n+        type=\"function_call\",\n+    )\n+    return ToolCallItem(agent=agent, raw_item=raw_item, type=\"tool_call_item\")\n+\n+\n+def _create_tool_output_item(agent: Agent) -> ToolCallOutputItem:\n+    \"\"\"Create a mock ToolCallOutputItem.\"\"\"\n+    raw_item = {\n+        \"type\": \"function_call_output\",\n+        \"call_id\": \"call_tool_123\",\n+        \"output\": \"Sunny, 22°C\",\n+    }\n+    return ToolCallOutputItem(\n+        agent=agent,\n+        raw_item=raw_item,\n+        output=\"Sunny, 22°C\",\n+        type=\"tool_call_output_item\",\n+    )\n+\n+\n+def _create_handoff_call_item(agent: Agent) -> HandoffCallItem:\n+    \"\"\"Create a mock HandoffCallItem.\"\"\"\n+    raw_item = ResponseFunctionToolCall(\n+        id=\"call_handoff_456\",\n+        call_id=\"call_handoff_456\",\n+        name=\"transfer_to_agent_b\",\n+        arguments=\"{}\",\n+        type=\"function_call\",\n+    )\n+    return HandoffCallItem(agent=agent, raw_item=raw_item, type=\"handoff_call_item\")\n+\n+\n+def _create_handoff_output_item(agent: Agent[Any]) -> HandoffOutputItem:\n+    \"\"\"Create a mock HandoffOutputItem.\"\"\"\n+    raw_item: dict[str, str] = {\n+        \"type\": \"function_call_output\",\n+        \"call_id\": \"call_handoff_456\",\n+        \"output\": '{\"assistant\": \"agent_b\"}',\n+    }\n+    return HandoffOutputItem(\n+        agent=agent,\n+        raw_item=cast(Any, raw_item),\n+        source_agent=agent,\n+        target_agent=agent,\n+        type=\"handoff_output_item\",\n+    )\n+\n+\n+def _create_message_item(agent: Agent) -> MessageOutputItem:\n+    \"\"\"Create a mock MessageOutputItem.\"\"\"\n+    raw_item = ResponseOutputMessage(\n+        id=\"msg_123\",\n+        content=[ResponseOutputText(text=\"Hello!\", type=\"output_text\", annotations=[])],\n+        role=\"assistant\",\n+        status=\"completed\",\n+        type=\"message\",\n+    )\n+    return MessageOutputItem(agent=agent, raw_item=raw_item, type=\"message_output_item\")\n+\n+\n+class TestHandoffHistoryDuplicationFix:\n+    \"\"\"Tests for Issue #2171: nest_handoff_history duplication fix.\"\"\"\n+\n+    def test_pre_handoff_tool_items_are_filtered(self):\n+        \"\"\"Verify ToolCallItem and ToolCallOutputItem in pre_handoff_items are filtered.\n+\n+        These items should NOT appear in the filtered output because they are\n+        already included in the summary message.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(\n+                _create_tool_call_item(agent),\n+                _create_tool_output_item(agent),\n+            ),\n+            new_items=(),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # pre_handoff_items should be empty (tool items filtered)\n+        assert len(nested.pre_handoff_items) == 0, (\n+            \"ToolCallItem and ToolCallOutputItem should be filtered from pre_handoff_items\"\n+        )\n+\n+        # Summary should contain the conversation\n+        assert len(nested.input_history) == 1\n+        first_item = nested.input_history[0]\n+        assert isinstance(first_item, dict)\n+        assert \"<CONVERSATION HISTORY>\" in str(first_item.get(\"content\", \"\"))\n+\n+    def test_new_items_handoff_output_is_filtered_for_input(self):\n+        \"\"\"Verify HandoffOutputItem in new_items is filtered from input_items.\n+\n+        The HandoffOutputItem is a function_call_output which would be duplicated.\n+        It should be filtered from input_items but preserved in new_items.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(),\n+            new_items=(\n+                _create_handoff_call_item(agent),\n+                _create_handoff_output_item(agent),\n+            ),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # new_items should still have both items (for session history)\n+        assert len(nested.new_items) == 2, \"new_items should preserve all items for session history\"\n+\n+        # input_items should be populated and filtered\n+        assert nested.input_items is not None, \"input_items should be populated\"\n+\n+        # input_items should NOT contain HandoffOutputItem (it's function_call_output)\n+        has_handoff_output = any(isinstance(item, HandoffOutputItem) for item in nested.input_items)\n+        assert not has_handoff_output, \"HandoffOutputItem should be filtered from input_items\"\n+\n+    def test_message_items_are_preserved_in_new_items(self):\n+        \"\"\"Verify MessageOutputItem in new_items is preserved.\n+\n+        Message items have a 'role' and should NOT be filtered from input_items.\n+        Note: pre_handoff_items are converted to summary text regardless of type.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(),  # pre_handoff items go into summary\n+            new_items=(_create_message_item(agent),),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # Message items should be preserved in new_items\n+        assert len(nested.new_items) == 1, \"MessageOutputItem should be preserved in new_items\"\n+        # And in input_items (since it has a role)\n+        assert nested.input_items is not None\n+        assert len(nested.input_items) == 1, \"MessageOutputItem should be preserved in input_items\"\n+        assert isinstance(nested.input_items[0], MessageOutputItem)\n+\n+    def test_summary_contains_filtered_items_as_text(self):\n+        \"\"\"Verify the summary message contains the filtered tool items as text.\n+\n+        This ensures observability - the items are not lost, just converted to text.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(\n+                _create_tool_call_item(agent),\n+                _create_tool_output_item(agent),\n+            ),\n+            new_items=(),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        first_item = nested.input_history[0]\n+        assert isinstance(first_item, dict)\n+        summary = str(first_item.get(\"content\", \"\"))\n+\n+        # Summary should contain function_call reference\n+        assert \"function_call\" in summary or \"get_weather\" in summary, (\n+            \"Summary should contain the tool call that was filtered\"\n+        )\n+\n+    def test_input_items_field_exists_after_nesting(self):\n+        \"\"\"Verify the input_items field is populated after nest_handoff_history.\n+\n+        This is the key field that separates model input from session history.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"Hello\"},),\n+            pre_handoff_items=(),\n+            new_items=(_create_handoff_call_item(agent),),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        assert nested.input_items is not None, (\n+            \"input_items should be populated after nest_handoff_history\"\n+        )\n+\n+    def test_full_handoff_scenario_no_duplication(self):\n+        \"\"\"Full end-to-end test of the handoff scenario from Issue #2171.\n+\n+        Simulates: User -> Agent does tool call -> Agent hands off to next agent\n+        Verifies: Next agent receives summary only, no duplicate raw items.\n+        \"\"\"\n+        agent = _create_mock_agent()\n+\n+        # Full scenario: tool call in pre_handoff, handoff in new_items\n+        handoff_data = HandoffInputData(\n+            input_history=({\"role\": \"user\", \"content\": \"What's the weather?\"},),\n+            pre_handoff_items=(\n+                _create_tool_call_item(agent),  # function_call\n+                _create_tool_output_item(agent),  # function_call_output\n+            ),\n+            new_items=(\n+                _create_message_item(agent),  # assistant message\n+                _create_handoff_call_item(agent),  # function_call (handoff)\n+                _create_handoff_output_item(agent),  # function_call_output (handoff)\n+            ),\n+        )\n+\n+        nested = nest_handoff_history(handoff_data)\n+\n+        # Count what would be sent to the model\n+        total_model_items = (\n+            len(nested.input_history)  # Summary\n+            + len(nested.pre_handoff_items)  # Filtered pre-handoff\n+            + len(nested.input_items or [])  # Filtered new items\n+        )\n+\n+        # Before fix: would have 6+ items (summary + raw tool items)\n+        # After fix: should have ~2 items (summary + message)\n+        assert total_model_items <= 3, (\n+            f\"Model should receive at most 3 items (summary + messages), got {total_model_items}\"\n+        )\n+\n+        # Verify no raw function_call_output items in model input\n+        all_input_items = list(nested.pre_handoff_items) + list(nested.input_items or [])\n+        function_call_outputs = [\n+            item\n+            for item in all_input_items\n+            if isinstance(item, (ToolCallOutputItem, HandoffOutputItem))\n+        ]\n+        assert len(function_call_outputs) == 0, (\n+            \"No function_call_output items should be in model input\"\n+        )",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_handoff_history_duplication.py",
        "sha": "617c7ef71064538eb99214d3c5133b00f08c0387",
        "status": "added"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_soft_cancel.py",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Ftest_soft_cancel.py?ref=31d954709db8ee4d659bce58b26ea06c8d1e4150",
        "deletions": 1,
        "filename": "tests/test_soft_cancel.py",
        "patch": "@@ -421,7 +421,7 @@ async def on_invoke_handoff(context, data):\n \n     handoff_seen = False\n     async for event in result.stream_events():\n-        if event.type == \"run_item_stream_event\" and event.name == \"handoff_occured\":\n+        if event.type == \"run_item_stream_event\" and event.name == \"handoff_requested\":\n             handoff_seen = True\n             # Cancel right after handoff\n             result.cancel(mode=\"after_turn\")",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/31d954709db8ee4d659bce58b26ea06c8d1e4150/tests%2Ftest_soft_cancel.py",
        "sha": "ddb51f8f17ff90dbfc544ca7174f761defe353aa",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:57:42.923129Z"
}
