{
  "finished_at": "2026-01-20T04:57:22.482343Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "367f9d53164b058d",
    "tag": "rest_pr_files_pr2316_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2316/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "849107",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:57:21 GMT",
      "etag": "\"95f15022bd5a13c6cb1101ee9351904fea65abaa5cd6c2d197ce8e7795bfcfb2\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Fri, 16 Jan 2026 01:43:54 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DCAA:3E6F74:1601399:1F0C7BA:696F0B30",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4942",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "58",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 37,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fagents.md",
        "changes": 74,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fagents.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 37,
        "filename": "docs/ja/agents.md",
        "patch": "@@ -4,16 +4,16 @@ search:\n ---\n # エージェント\n \n-エージェントはアプリの中心となる構成要素です。エージェントは、指示とツールで構成された大規模言語モデル（ LLM ）です。\n+エージェントはアプリの中核となる基本コンポーネントです。エージェントは、 instructions とツールで構成された大規模言語モデル（ LLM ）です。\n \n-## 基本構成\n+## 基本設定\n \n-エージェントで最も一般的に設定するプロパティは次のとおりです。\n+設定で最も一般的に使用するエージェントのプロパティは次のとおりです。\n \n-- `name`: エージェントを識別する必須の文字列です。\n-- `instructions`: developer message または システムプロンプト とも呼ばれます。\n-- `model`: 使用する LLM と、temperature、top_p などのモデル調整用パラメーターを構成する任意の `model_settings`。\n-- `tools`: エージェントがタスク達成のために使用できるツールです。\n+- `name`: エージェントを識別する必須の文字列。\n+- `instructions`: developer message または system prompt とも呼ばれます。\n+- `model`: 使用する LLM と、temperature、top_p などのモデル調整パラメーターを設定するための任意の `model_settings`。\n+- `tools`: エージェントがタスク達成のために使用できるツール。\n \n ```python\n from agents import Agent, ModelSettings, function_tool\n@@ -33,7 +33,7 @@ agent = Agent(\n \n ## コンテキスト\n \n-エージェントはその `context` 型に対してジェネリックです。コンテキストは依存性注入のためのツールで、あなたが作成して `Runner.run()` に渡すオブジェクトです。これはあらゆるエージェント、ツール、ハンドオフなどに渡され、エージェント実行時の依存関係や状態の入れ物として機能します。コンテキストには任意の Python オブジェクトを提供できます。\n+エージェントは `context` 型に対してジェネリックです。コンテキストは依存性注入のツールで、あなたが作成して `Runner.run()` に渡すオブジェクトです。これはすべてのエージェント、ツール、ハンドオフなどに渡され、エージェント実行のための依存関係と状態をまとめて保持します。任意の Python オブジェクトをコンテキストとして提供できます。\n \n ```python\n @dataclass\n@@ -50,9 +50,9 @@ agent = Agent[UserContext](\n )\n ```\n \n-## 出力タイプ\n+## 出力型\n \n-デフォルトでは、エージェントはプレーンテキスト（つまり `str`）の出力を生成します。特定のタイプの出力を生成させたい場合は、`output_type` パラメーターを使用できます。一般的には [Pydantic](https://docs.pydantic.dev/) オブジェクトを使用しますが、Pydantic の [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) でラップできる任意の型（dataclasses、lists、TypedDict など）に対応しています。\n+デフォルトでは、エージェントはプレーンテキスト（すなわち `str`）を出力します。特定の型の出力を生成させたい場合は、`output_type` パラメーターを使用できます。一般的な選択肢は [Pydantic](https://docs.pydantic.dev/) オブジェクトですが、Pydantic の [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) でラップ可能な任意の型（dataclasses、list、TypedDict など）をサポートします。\n \n ```python\n from pydantic import BaseModel\n@@ -73,20 +73,20 @@ agent = Agent(\n \n !!! note\n \n-    `output_type` を渡すと、通常のプレーンテキスト応答の代わりに [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) を使うようモデルに指示します。\n+    `output_type` を渡すと、モデルに通常のプレーンテキスト応答ではなく [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) を使用するよう指示します。\n \n-## マルチ エージェント システムの設計パターン\n+## マルチエージェントの設計パターン\n \n-マルチ エージェント システムの設計方法は多数ありますが、一般的に広く適用できるパターンを 2 つ紹介します。\n+マルチエージェント システムの設計には多くの方法がありますが、一般的に広く適用できるパターンは次の 2 つです。\n \n-1. マネージャー（ツールとしての エージェント）: 中央のマネージャー／オーケストレーターが、特化したサブ エージェントをツールとして呼び出し、会話の制御を保持します。\n-2. ハンドオフ: ピアのエージェントが、会話を引き継ぐ特化エージェントに制御をハンドオフします。これは分散型です。\n+1. マネージャー（エージェントをツールとして使用）: 中央のマネージャー／オーケストレーターが、専門のサブエージェントをツールとして呼び出し、会話の制御を保持します。\n+2. ハンドオフ: ピアエージェントが、会話を引き継ぐ専門エージェントに制御をハンドオフします。これは分散型です。\n \n-詳細は [エージェント構築の実践ガイド](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf) を参照してください。\n+詳細は [実践的なエージェント構築ガイド](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf) を参照してください。\n \n-### マネージャー（ツールとしての エージェント）\n+### マネージャー（エージェントをツールとして使用）\n \n-`customer_facing_agent` はすべての ユーザー との対話を処理し、ツールとして公開された特化サブ エージェントを呼び出します。詳しくは [ツール](tools.md#agents-as-tools) のドキュメントを参照してください。\n+`customer_facing_agent` はすべてのユーザー対応を処理し、ツールとして公開された専門のサブエージェントを呼び出します。詳しくは [tools](tools.md#agents-as-tools) ドキュメントを参照してください。\n \n ```python\n from agents import Agent\n@@ -115,7 +115,7 @@ customer_facing_agent = Agent(\n \n ### ハンドオフ\n \n-ハンドオフは、エージェントが委任できるサブ エージェントです。ハンドオフが発生すると、委任先のエージェントは会話履歴を受け取り、会話を引き継ぎます。このパターンにより、単一タスクに特化して卓越する、モジュール型の エージェント を実現できます。詳しくは [ハンドオフ](handoffs.md) のドキュメントを参照してください。\n+ハンドオフは、エージェントが委任できるサブエージェントです。ハンドオフが発生すると、委任先のエージェントは会話履歴を受け取り、会話を引き継ぎます。このパターンにより、単一タスクに特化して優れた性能を発揮する、モジュール式かつ専門特化のエージェントが可能になります。詳しくは [handoffs](handoffs.md) ドキュメントを参照してください。\n \n ```python\n from agents import Agent\n@@ -134,9 +134,9 @@ triage_agent = Agent(\n )\n ```\n \n-## 動的な instructions\n+## 動的 instructions\n \n-多くの場合、エージェントの作成時に指示を指定できます。しかし、関数を通じて動的な指示を提供することも可能です。その関数はエージェントとコンテキストを受け取り、プロンプトを返す必要があります。通常の関数と `async` 関数の両方が利用できます。\n+ほとんどの場合、エージェントの作成時に instructions を指定できます。しかし、関数を介して動的な instructions を提供することも可能です。この関数はエージェントとコンテキストを受け取り、プロンプトを返す必要があります。通常の関数と `async` 関数の両方が利用できます。\n \n ```python\n def dynamic_instructions(\n@@ -153,15 +153,15 @@ agent = Agent[UserContext](\n \n ## ライフサイクルイベント（フック）\n \n-エージェントのライフサイクルを観測したい場合があります。たとえば、イベントを記録したり、特定のイベント発生時にデータを事前取得したりするケースです。`hooks` プロパティを使ってエージェントのライフサイクルにフックできます。[`AgentHooks`][agents.lifecycle.AgentHooks] クラスをサブクラス化し、関心のあるメソッドをオーバーライドしてください。\n+場合によっては、エージェントのライフサイクルを観測したいことがあります。例えば、イベントをログに記録したり、特定のイベント発生時にデータを事前取得したい場合です。`hooks` プロパティでエージェントのライフサイクルにフックできます。[`AgentHooks`][agents.lifecycle.AgentHooks] クラスをサブクラス化し、関心のあるメソッドをオーバーライドしてください。\n \n ## ガードレール\n \n-ガードレール により、エージェントの実行と並行して ユーザー 入力に対する検査／検証を行い、さらにエージェントの出力が生成された後にも検査／検証を実行できます。たとえば、ユーザーの入力とエージェントの出力を関連性でスクリーニングできます。詳しくは [ガードレール](guardrails.md) のドキュメントを参照してください。\n+ガードレールにより、エージェントの実行と並行してユーザー入力のチェック／検証を行い、さらにエージェントが出力を生成した後にも検証を実施できます。例えば、ユーザー入力とエージェント出力の関連性をスクリーニングできます。詳しくは [guardrails](guardrails.md) ドキュメントを参照してください。\n \n-## エージェントの複製／コピー\n+## エージェントのクローン／コピー\n \n-エージェントの `clone()` メソッドを使用すると、エージェントを複製し、必要に応じて任意のプロパティを変更できます。\n+エージェントの `clone()` メソッドを使用すると、エージェントを複製し、オプションで任意のプロパティを変更できます。\n \n ```python\n pirate_agent = Agent(\n@@ -178,12 +178,12 @@ robot_agent = pirate_agent.clone(\n \n ## ツール使用の強制\n \n-ツールのリストを指定しても、LLM が必ずしもツールを使用するとは限りません。[`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] を設定してツール使用を強制できます。有効な値は以下のとおりです。\n+ツールのリストを指定しても、 LLM が必ずツールを使用するとは限りません。[`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] を設定してツール使用を強制できます。有効な値は次のとおりです。\n \n-1. `auto`: ツールを使用するかどうかを LLM に判断させます。\n-2. `required`: LLM にツールの使用を必須にします（ただし、どのツールを使うかは賢く判断します）。\n-3. `none`: LLM にツールを使用「しない」ことを必須にします。\n-4. 特定の文字列（例: `my_tool`）を設定し、LLM にその特定のツールの使用を必須にします。\n+1. `auto`: ツールを使用するかどうかを LLM に任せます。\n+2. `required`: LLM にツールの使用を要求します（ただし、どのツールを使うかは賢く判断します）。\n+3. `none`: ツールを _使用しない_ ことを要求します。\n+4. 特定の文字列（例: `my_tool`）を設定すると、LLM にその特定のツールを使用させます。\n \n ```python\n from agents import Agent, Runner, function_tool, ModelSettings\n@@ -201,12 +201,12 @@ agent = Agent(\n )\n ```\n \n-## ツール使用時の挙動\n+## ツール使用時の動作\n \n-`Agent` 構成の `tool_use_behavior` パラメーターは、ツール出力の扱い方を制御します。\n+`Agent` の設定にある `tool_use_behavior` パラメーターは、ツール出力の扱いを制御します。\n \n-- `\"run_llm_again\"`: デフォルト。ツールを実行し、LLM がその結果を処理して最終応答を生成します。\n-- `\"stop_on_first_tool\"`: 最初のツール呼び出しの出力を、以降の LLM 処理なしで最終応答として使用します。\n+- `\"run_llm_again\"`: デフォルト。ツールを実行し、その結果を LLM が処理して最終応答を生成します。\n+- `\"stop_on_first_tool\"`: 最初のツール呼び出しの出力を、以降の LLM 処理なしに最終応答として使用します。\n \n ```python\n from agents import Agent, Runner, function_tool, ModelSettings\n@@ -224,7 +224,7 @@ agent = Agent(\n )\n ```\n \n-- `StopAtTools(stop_at_tool_names=[...])`: 指定したいずれかのツールが呼び出された時点で停止し、その出力を最終応答として使用します。\n+- `StopAtTools(stop_at_tool_names=[...])`: 指定したいずれかのツールが呼び出されたら停止し、その出力を最終応答として使用します。\n \n ```python\n from agents import Agent, Runner, function_tool\n@@ -248,7 +248,7 @@ agent = Agent(\n )\n ```\n \n-- `ToolsToFinalOutputFunction`: ツール結果を処理し、停止するか LLM を続行するかを判断するカスタム関数です。\n+- `ToolsToFinalOutputFunction`: ツール結果を処理し、停止するか LLM を続行するかを決定するカスタム関数です。\n \n ```python\n from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper\n@@ -286,4 +286,4 @@ agent = Agent(\n \n !!! note\n \n-    無限ループを防ぐため、フレームワークはツール呼び出し後に `tool_choice` を自動的に \"auto\" にリセットします。この挙動は [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] で構成可能です。無限ループは、ツール結果が LLM に送られ、`tool_choice` により LLM がさらに別のツール呼び出しを生成し続けることが原因です。\n\\ No newline at end of file\n+    無限ループを防ぐため、フレームワークはツール呼び出し後に `tool_choice` を自動的に \"auto\" にリセットします。この動作は [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] で設定可能です。無限ループは、ツール結果が LLM に送られ、その `tool_choice` によって LLM が再びツール呼び出しを生成し続けるために発生します。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fagents.md",
        "sha": "cf688d18cd76acba3439c85b5d9d72f4be1200de",
        "status": "modified"
      },
      {
        "additions": 13,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fconfig.md",
        "changes": 26,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fconfig.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 13,
        "filename": "docs/ja/config.md",
        "patch": "@@ -6,15 +6,15 @@ search:\n \n ## API キーとクライアント\n \n-デフォルトでは、SDK はインポートされるとすぐに、LLM リクエストとトレーシングのために `OPENAI_API_KEY` 環境変数を探します。アプリの起動前にその環境変数を設定できない場合は、[set_default_openai_key()][agents.set_default_openai_key] 関数でキーを設定できます。\n+デフォルトでは、SDK はインポートされた直後から、LLM リクエストと トレーシング のために `OPENAI_API_KEY` 環境変数を探します。アプリ起動前にその環境変数を設定できない場合は、[set_default_openai_key()][agents.set_default_openai_key] 関数でキーを設定できます。\n \n ```python\n from agents import set_default_openai_key\n \n set_default_openai_key(\"sk-...\")\n ```\n \n-また、使用する OpenAI クライアントを設定することもできます。デフォルトでは、SDK は、環境変数の API キーまたは上で設定したデフォルトキーを用いて `AsyncOpenAI` インスタンスを作成します。これは [set_default_openai_client()][agents.set_default_openai_client] 関数で変更できます。\n+また、使用する OpenAI クライアントを設定することもできます。デフォルトでは、SDK は環境変数または上記で設定したデフォルトキーを用いて `AsyncOpenAI` インスタンスを作成します。これを変更するには、[set_default_openai_client()][agents.set_default_openai_client] 関数を使用します。\n \n ```python\n from openai import AsyncOpenAI\n@@ -24,7 +24,7 @@ custom_client = AsyncOpenAI(base_url=\"...\", api_key=\"...\")\n set_default_openai_client(custom_client)\n ```\n \n-最後に、使用する OpenAI API をカスタマイズすることもできます。デフォルトでは、OpenAI Responses API を使用します。[set_default_openai_api()][agents.set_default_openai_api] 関数を使って、Chat Completions API を使用するように上書きできます。\n+最後に、使用する OpenAI API をカスタマイズすることもできます。デフォルトでは OpenAI Responses API を使用します。これを上書きして Chat Completions API を使用するには、[set_default_openai_api()][agents.set_default_openai_api] 関数を使用します。\n \n ```python\n from agents import set_default_openai_api\n@@ -34,15 +34,15 @@ set_default_openai_api(\"chat_completions\")\n \n ## トレーシング\n \n-トレーシングはデフォルトで有効です。デフォルトでは、上のセクションでの OpenAI API キー（すなわち、環境変数または設定したデフォルトキー）を使用します。トレーシングに使用する API キーを個別に設定するには、[`set_tracing_export_api_key`][agents.set_tracing_export_api_key] 関数を使用します。\n+トレーシング はデフォルトで有効です。デフォルトでは、上記の OpenAI API キー（つまり、環境変数または設定したデフォルトキー）を使用します。トレーシング に使用する API キーを個別に設定するには、[`set_tracing_export_api_key`][agents.set_tracing_export_api_key] 関数を使用します。\n \n ```python\n from agents import set_tracing_export_api_key\n \n set_tracing_export_api_key(\"sk-...\")\n ```\n \n-グローバルエクスポーターを変更せずに、実行ごとにトレーシング用の API キーを設定することもできます。\n+グローバルなエクスポーターを変更せずに、実行ごとに トレーシング 用 API キーを設定することもできます。\n \n ```python\n from agents import Runner, RunConfig\n@@ -54,17 +54,17 @@ await Runner.run(\n )\n ```\n \n-また、[`set_tracing_disabled()`][agents.set_tracing_disabled] 関数でトレーシングを完全に無効化できます。\n+[`set_tracing_disabled()`][agents.set_tracing_disabled] 関数を使用して、トレーシング を完全に無効化することもできます。\n \n ```python\n from agents import set_tracing_disabled\n \n set_tracing_disabled(True)\n ```\n \n-## デバッグログ\n+## デバッグロギング\n \n-SDK には、ハンドラー未設定の Python ロガーが 2 つあります。デフォルトでは、警告とエラーは `stdout` に送られ、それ以外のログは抑制されます。\n+SDK にはハンドラーが設定されていない 2 つの Python ロガーがあります。デフォルトでは、警告とエラーは `stdout` に送信され、それ以外のログは抑制されます。\n \n 詳細なログを有効にするには、[`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] 関数を使用します。\n \n@@ -74,7 +74,7 @@ from agents import enable_verbose_stdout_logging\n enable_verbose_stdout_logging()\n ```\n \n-また、ハンドラー、フィルター、フォーマッターなどを追加してログをカスタマイズできます。詳しくは [Python ロギングガイド](https://docs.python.org/3/howto/logging.html) を参照してください。\n+また、ハンドラー、フィルター、フォーマッターなどを追加してログをカスタマイズできます。詳しくは [Python logging guide](https://docs.python.org/3/howto/logging.html) を参照してください。\n \n ```python\n import logging\n@@ -93,17 +93,17 @@ logger.setLevel(logging.WARNING)\n logger.addHandler(logging.StreamHandler())\n ```\n \n-### ログ内の機微なデータ\n+### ログ中の機微データ\n \n-一部のログには機微なデータ（例: ユーザー データ）が含まれる場合があります。このデータの記録を無効化したい場合は、次の環境変数を設定してください。\n+一部のログには機微なデータ（例: ユーザー データ）が含まれる場合があります。これらのデータがログに出力されないようにするには、次の環境変数を設定してください。\n \n-LLM の入力と出力のロギングを無効化するには:\n+LLM の入力と出力のロギングを無効にするには:\n \n ```bash\n export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1\n ```\n \n-ツールの入力と出力のロギングを無効化するには:\n+ツールの入力と出力のロギングを無効にするには:\n \n ```bash\n export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fconfig.md",
        "sha": "23f81b6d434f89fb66402163ce177c8db9c1b0a5",
        "status": "modified"
      },
      {
        "additions": 30,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fcontext.md",
        "changes": 60,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fcontext.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 30,
        "filename": "docs/ja/context.md",
        "patch": "@@ -4,30 +4,30 @@ search:\n ---\n # コンテキスト管理\n \n-コンテキストという用語には複数の意味があります。考慮すべき主なコンテキストは次の 2 つです。\n+コンテキストは多義的な用語です。ここでは主に 2 つのコンテキストがあります。\n \n-1. コードからローカルに参照できるコンテキスト: ツール関数の実行時、`on_handoff` のようなコールバック中、ライフサイクルフックなどで必要になる可能性があるデータや依存関係です。\n-2. LLM に提供されるコンテキスト: 応答生成時に LLM が参照できるデータです。\n+1. コードからローカルに利用できるコンテキスト: ツール関数の実行時、`on_handoff` のようなコールバック、ライフサイクルフックなどで必要になるデータや依存関係です。\n+2. LLM に利用できるコンテキスト: 応答を生成するときに LLM が参照できるデータです。\n \n ## ローカルコンテキスト\n \n これは [`RunContextWrapper`][agents.run_context.RunContextWrapper] クラスと、その中の [`context`][agents.run_context.RunContextWrapper.context] プロパティで表現されます。仕組みは次のとおりです。\n \n-1. 任意の Python オブジェクトを作成します。一般的には dataclass や Pydantic オブジェクトを使うパターンです。\n-2. そのオブジェクトを各種の実行メソッドに渡します（例: `Runner.run(..., **context=whatever**)`）。\n-3. すべてのツール呼び出しやライフサイクルフックには、ラッパーオブジェクト `RunContextWrapper[T]` が渡されます。`T` はコンテキストオブジェクトの型で、`wrapper.context` からアクセスできます。\n+1. 任意の Python オブジェクトを作成します。一般的なパターンとして dataclass や Pydantic オブジェクトを使います。\n+2. そのオブジェクトを各種 run メソッド（例: `Runner.run(..., **context=whatever**)`）に渡します。\n+3. すべてのツール呼び出しやライフサイクルフックなどには `RunContextWrapper[T]` というラッパーオブジェクトが渡されます。ここで T はあなたのコンテキストオブジェクトの型で、`wrapper.context` からアクセスできます。\n \n-**最も重要** な点: 特定のエージェント実行に関わるすべてのエージェント、ツール関数、ライフサイクルなどは、同じ型のコンテキストを使う必要があります。\n+**最重要** な点: 特定のエージェント実行において、すべてのエージェント、ツール関数、ライフサイクルなどは同じ型のコンテキストを使わなければなりません。\n \n-コンテキストは次のような用途に使えます:\n+コンテキストは次のような用途に使えます。\n \n--   実行のための状況データ（例: ユーザー名 / uid など、ユーザーに関するその他の情報）\n+-   実行に関する状況データ（例: ユーザー名/uid やユーザーに関するその他の情報）\n -   依存関係（例: ロガーオブジェクト、データフェッチャーなど）\n -   ヘルパー関数\n \n-!!! danger \"注意\"\n+!!! danger \"Note\"\n \n-    コンテキストオブジェクトは LLM には送信されません。読み書きやメソッド呼び出しができる、純粋にローカルなオブジェクトです。\n+    コンテキストオブジェクトは LLM には送信されません。あくまでローカルなオブジェクトであり、読み書きやメソッド呼び出しが可能です。\n \n ```python\n import asyncio\n@@ -66,18 +66,18 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-1. これはコンテキストオブジェクトです。ここでは dataclass を使用していますが、任意の型を使えます。\n-2. これはツールです。`RunContextWrapper[UserInfo]` を受け取り、ツール実装はコンテキストから読み取っています。\n-3. エージェントにジェネリクス `UserInfo` を付け、型チェッカーがエラーを検出できるようにしています（たとえば、異なるコンテキスト型を取るツールを渡そうとした場合など）。\n-4. コンテキストは `run` 関数に渡されます。\n+1. これがコンテキストオブジェクトです。ここでは dataclass を使っていますが、任意の型を使えます。\n+2. これはツールです。`RunContextWrapper[UserInfo]` を受け取り、実装はコンテキストから読み取ります。\n+3. エージェントにジェネリクス `UserInfo` を指定して、型チェッカーがエラーを検出できるようにします（たとえば異なるコンテキスト型を受け取るツールを渡そうとした場合など）。\n+4. `run` 関数にコンテキストを渡します。\n 5. エージェントはツールを正しく呼び出し、年齢を取得します。\n \n ---\n \n-### 詳細: `ToolContext`\n+### 上級: `ToolContext`\n \n-場合によっては、実行中のツールに関する追加メタデータ（名前、コール ID、raw 引数文字列など）にアクセスしたいことがあります。  \n-そのために、`RunContextWrapper` を拡張した [`ToolContext`][agents.tool_context.ToolContext] クラスを使用できます。\n+実行中のツールに関する追加メタデータ（名前、呼び出し ID、raw の引数文字列など）へアクセスしたい場合があります。  \n+その場合は、`RunContextWrapper` を拡張した [`ToolContext`][agents.tool_context.ToolContext] クラスを使えます。\n \n ```python\n from typing import Annotated\n@@ -106,22 +106,22 @@ agent = Agent(\n ```\n \n `ToolContext` は `RunContextWrapper` と同じ `.context` プロパティに加えて、  \n-現在のツール呼び出しに固有の追加フィールドを提供します:\n+現在のツール呼び出しに特化した追加フィールドを提供します。\n \n-- `tool_name` – 呼び出されているツールの名前  \n-- `tool_call_id` – このツール呼び出しの一意の識別子  \n-- `tool_arguments` – ツールに渡された raw 引数文字列  \n+- `tool_name` – 呼び出されるツールの名前  \n+- `tool_call_id` – このツール呼び出しの一意な識別子  \n+- `tool_arguments` – ツールに渡された raw の引数文字列  \n \n-実行中にツールレベルのメタデータが必要な場合は `ToolContext` を使用してください。  \n-エージェントとツール間で一般的にコンテキストを共有するには、`RunContextWrapper` で十分です。\n+実行中にツールレベルのメタデータが必要なときは `ToolContext` を使ってください。  \n+エージェントとツール間で一般的なコンテキスト共有を行うだけであれば、`RunContextWrapper` で十分です。\n \n ---\n \n-## エージェント / LLM のコンテキスト\n+## エージェント / LLM コンテキスト\n \n-LLM が呼び出されると、参照できるデータは会話履歴のものだけです。つまり、LLM に新しいデータを利用させたい場合は、その履歴で参照可能になるような方法で提供する必要があります。これにはいくつかの方法があります。\n+LLM が呼び出されると、そのときに見えるデータは会話履歴のみです。つまり、新しいデータを LLM で利用可能にするには、その履歴で参照できるようにしなければなりません。方法はいくつかあります。\n \n-1. エージェントの `instructions` に追加します。これは「システムプロンプト」または「開発者メッセージ」とも呼ばれます。システムプロンプトは静的な文字列でも、コンテキストを受け取って文字列を出力する動的な関数でもかまいません。常に有用な情報（例: ユーザーの名前や現在の日付）に適した一般的な手法です。\n-2. `Runner.run` を呼び出す際の `input` に追加します。これは `instructions` と似た手法ですが、[chain of command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command) の下位にメッセージを配置できます。\n-3. 関数ツールを介して公開します。これはオンデマンドのコンテキストに有用です。LLM が必要に応じてデータ取得のためにツールを呼び出せます。\n-4. リトリーバルや Web 検索を使用します。これらは、ファイルやデータベースから関連データを取得（リトリーバル）したり、Web（Web 検索）から取得したりできる特別なツールです。関連するコンテキストデータに基づいて応答を「グラウンディング」するのに有用です。\n\\ No newline at end of file\n+1. エージェントの `instructions` に追加します。これは「system prompt」または「developer message」とも呼ばれます。system prompts は静的な文字列でも、コンテキストを受け取って文字列を出力する動的関数でもかまいません。常に有用な情報（例: ユーザー名や現在の日付）に適した手法です。\n+2. `Runner.run` 関数を呼び出すときの `input` に追加します。これは `instructions` と似ていますが、[chain of command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command) において下位のメッセージを用意できます。\n+3. 関数ツールで公開します。これはオンデマンドのコンテキストに適しており、LLM が必要に応じてツールを呼び出し、そのデータを取得できます。\n+4. リトリーバルまたは Web 検索を使います。これらは、ファイルやデータベース（リトリーバル）、または Web（Web 検索）から関連データを取得できる特別なツールです。関連するコンテキストデータに基づいて応答を「グラウンディング」するのに有用です。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fcontext.md",
        "sha": "ff4858fe10dfe86841eeaf2ce83003465a0365e6",
        "status": "modified"
      },
      {
        "additions": 27,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fexamples.md",
        "changes": 54,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fexamples.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 27,
        "filename": "docs/ja/examples.md",
        "patch": "@@ -2,51 +2,51 @@\n search:\n   exclude: true\n ---\n-# 例\n+# コード例\n \n-[リポジトリ](https://github.com/openai/openai-agents-python/tree/main/examples) の code examples セクションで、SDK のさまざまな実装例をご覧ください。これらの code examples は、異なるパターンや機能を示す複数の カテゴリー に整理されています。\n+[repo](https://github.com/openai/openai-agents-python/tree/main/examples) の examples セクションで、 SDK の多様なサンプル実装をご覧ください。これらのコード例は、さまざまなパターンや機能を示す複数のカテゴリーに整理されています。\n \n ## カテゴリー\n \n -   **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**\n-    このカテゴリーの例では、一般的な エージェント の設計パターンを紹介します。例:\n+    このカテゴリーのコード例は、以下のような一般的なエージェント設計パターンを示します。\n \n     -   決定的なワークフロー\n-    -   ツールとしての エージェント\n-    -   エージェント の並列実行\n+    -   ツールとしてのエージェント\n+    -   エージェントの並列実行\n     -   条件付きツール使用\n-    -   入出力ガードレール\n-    -   LLM を審判として用いる\n+    -   入出力の ガードレール\n+    -   審査員としての LLM\n     -   ルーティング\n     -   ストリーミング ガードレール\n \n -   **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**\n-    このカテゴリーでは、SDK の基礎的な機能を紹介します。例:\n+    このカテゴリーのコード例は、以下のような SDK の基礎機能を紹介します。\n \n-    -   Hello World の code examples（既定のモデル、GPT-5、open-weight モデル）\n-    -   エージェント のライフサイクル管理\n+    -   Hello World のコード例（デフォルトモデル、 GPT-5、オープンウェイトモデル）\n+    -   エージェントのライフサイクル管理\n     -   動的な システムプロンプト\n-    -   ストリーミング出力（text, items, function call args）\n+    -   ストリーミング出力（テキスト、アイテム、関数呼び出しの引数）\n     -   プロンプトテンプレート\n     -   ファイル処理（ローカルとリモート、画像と PDF）\n     -   利用状況の追跡\n-    -   非厳密な出力タイプ\n+    -   厳密でない出力型\n     -   以前のレスポンス ID の使用\n \n -   **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service):**\n-    航空会社向けのカスタマーサービス システムの例。\n+    航空会社向けのカスタマーサービスシステムの例。\n \n -   **[financial_research_agent](https://github.com/openai/openai-agents-python/tree/main/examples/financial_research_agent):**\n-    金融データ分析のための エージェント とツールを用いた構造化されたリサーチ ワークフローを示す、金融リサーチ エージェント。\n+    金融データ分析のために、エージェントとツールを用いた構造化されたリサーチワークフローを示す金融リサーチ エージェント。\n \n -   **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**\n-    メッセージフィルタリングを伴う エージェント の ハンドオフ の実用例。\n+    メッセージフィルタリングを用いたエージェントのハンドオフの実用例。\n \n -   **[hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp):**\n-    hosted MCP (Model Context Protocol) のコネクタと承認の使い方を示す code examples。\n+    ホスト型 MCP (Model Context Protocol) コネクタと承認の使い方を示すコード例。\n \n -   **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**\n-    MCP (Model Context Protocol) で エージェント を構築する方法。以下を含みます:\n+    MCP (Model Context Protocol) を用いたエージェントの構築方法。以下を含みます。\n \n     -   ファイルシステムの例\n     -   Git の例\n@@ -55,39 +55,39 @@ search:\n     -   ストリーム可能な HTTP の例\n \n -   **[memory](https://github.com/openai/openai-agents-python/tree/main/examples/memory):**\n-    エージェント 向けのさまざまなメモリ実装例。以下を含みます:\n+    エージェント向けのさまざまなメモリ実装の例。以下を含みます。\n \n     -   SQLite セッションストレージ\n     -   高度な SQLite セッションストレージ\n     -   Redis セッションストレージ\n     -   SQLAlchemy セッションストレージ\n-    -   暗号化されたセッションストレージ\n+    -   暗号化セッションストレージ\n     -   OpenAI セッションストレージ\n \n -   **[model_providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**\n-    カスタムプロバイダーや LiteLLM 連携など、非 OpenAI モデルを SDK で活用する方法。\n+    カスタムプロバイダーや LiteLLM 連携を含む、 OpenAI 以外のモデルを SDK で使う方法。\n \n -   **[realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime):**\n-    SDK を使ってリアルタイム体験を構築する方法の例。以下を含みます:\n+    SDK を使ってリアルタイムの体験を構築するコード例。以下を含みます。\n \n     -   Web アプリケーション\n-    -   コマンドラインインターフェース\n+    -   コマンドライン インターフェース\n     -   Twilio 連携\n \n -   **[reasoning_content](https://github.com/openai/openai-agents-python/tree/main/examples/reasoning_content):**\n-    推論コンテンツと structured outputs の扱い方を示す code examples。\n+    推論コンテンツと structured outputs の扱い方を示すコード例。\n \n -   **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**\n-    複雑なマルチ エージェント のリサーチ ワークフローを示す、シンプルな ディープリサーチ クローン。\n+    複雑なマルチエージェントのリサーチワークフローを示す、シンプルな ディープリサーチ クローン。\n \n -   **[tools](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**\n-    次のような OpenAI がホストするツール の実装方法:\n+    次のような OpenAI がホストするツールの実装方法を学べます。\n \n-    -   Web 検索 と フィルタ付き Web 検索\n+    -   Web 検索 と フィルター付きの Web 検索\n     -   ファイル検索\n     -   Code interpreter\n     -   コンピュータ操作\n     -   画像生成\n \n -   **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**\n-    TTS と STT モデルを用いた音声 エージェント の例。ストリーミング音声の code examples を含みます。\n\\ No newline at end of file\n+    TTS と STT モデルを使用した音声エージェントのコード例。ストリーミングされた音声の例も含みます。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fexamples.md",
        "sha": "814f90092b4f22081d5637f021f23b01d1953c04",
        "status": "modified"
      },
      {
        "additions": 68,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fguardrails.md",
        "changes": 132,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fguardrails.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 64,
        "filename": "docs/ja/guardrails.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # ガードレール\n \n-ガードレールは、 ユーザー 入力や エージェント 出力のチェックと検証を可能にします。例えば、顧客からのリクエスト対応に非常に高性能（つまり遅く/高価）なモデルを使う エージェント を想定してください。悪意のある ユーザー が、そのモデルに数学の宿題を手伝わせるよう求めるのは避けたいはずです。この場合、速く/安価なモデルでガードレールを実行できます。もしガードレールが悪意のある利用を検出したら、即座にエラーを送出して高価なモデルの実行を防ぎ、時間とコストを節約できます（ **ブロッキング型のガードレールを使用する場合。並列ガードレールでは、ガードレールの完了前に高価なモデルがすでに実行を開始している可能性があります。詳細は下記「実行モード」を参照してください** ）。\n+ガードレールは、 ユーザー 入力や エージェント 出力のチェックと検証を可能にします。たとえば、非常に賢い（つまり遅く/高価な）モデルを使ってカスタマーリクエストを支援する エージェント があるとします。悪意のある ユーザー がそのモデルに数学の宿題を手伝わせるよう求めることは避けたいでしょう。そのため、速く/安価なモデルでガードレールを実行できます。ガードレールが悪意のある使用を検出した場合、即座にエラーを発生させて高価なモデルの実行を防ぎ、時間と費用を節約できます （**blocking ガードレールを使用する場合。parallel ガードレールでは、ガードレールの完了前に高価なモデルがすでに実行を開始している可能性があります。詳細は下記「実行モード」を参照してください**）。\n \n ガードレールには 2 種類あります。\n \n@@ -13,96 +13,55 @@ search:\n \n ## 入力ガードレール\n \n-入力ガードレールは 3 段階で実行されます。\n+入力ガードレールは 3 ステップで実行されます。\n \n-1. まず、ガードレールは エージェント に渡されるのと同じ入力を受け取ります。\n-2. 次に、ガードレール関数が実行され、[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] を生成し、これを [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult] でラップします\n-3. 最後に、[`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] が true かを確認します。true の場合、[`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] 例外が送出され、 ユーザー への適切な応答や例外処理が可能です。\n+1. まず、ガードレールは エージェント に渡されたのと同じ入力を受け取ります。\n+2. 次に、ガードレール関数が実行され、[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] を生成し、それを [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult] でラップします\n+3. 最後に、[`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] が true かを確認します。true の場合、[`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] 例外が送出され、 ユーザー に適切に応答するか、例外を処理できます。\n \n !!! Note\n \n-    入力ガードレールは ユーザー 入力に対して実行されることを想定しているため、 エージェント のガードレールが動くのは、その エージェント が「最初の」 エージェント の場合のみです。なぜ `guardrails` プロパティが エージェント にあり、`Runner.run` に渡さないのか不思議に思うかもしれません。これは、ガードレールが実際の Agent に密接に関係する傾向があるためです。 エージェント ごとに異なるガードレールを実行するため、コードを同じ場所に置くと可読性が向上します。\n+    入力ガードレールは ユーザー 入力で実行されることを想定しているため、 エージェント のガードレールはその エージェント が「最初の」 エージェント の場合にのみ実行されます。なぜ `guardrails` プロパティが エージェント にあり、`Runner.run` に渡さないのか不思議に思うかもしれません。これは、ガードレールが実際の Agent に密接に関連する傾向があるためです。エージェント ごとに異なるガードレールを実行するので、コードを同じ場所に置くことは可読性に役立ちます。\n \n ### 実行モード\n \n 入力ガードレールは 2 つの実行モードをサポートします。\n \n-- **並列実行**（デフォルト、`run_in_parallel=True`）: ガードレールは エージェント の実行と同時に並行して動作します。両者が同時に開始するため、レイテンシが最も良好です。ただし、ガードレールが失敗した場合、 エージェント はキャンセルされる前にすでにトークンを消費し、ツールを実行している可能性があります。\n+- **Parallel execution**（デフォルト、`run_in_parallel=True`）: ガードレールは エージェント の実行と同時に並行して動作します。両者が同時に開始するため、待ち時間に最も有利です。ただし、ガードレールが失敗した場合、取り消されるまでに エージェント がすでにトークンを消費し、ツールを実行している可能性があります。\n \n-- **ブロッキング実行**（`run_in_parallel=False`）: ガードレールは エージェント が開始する「前に」実行と完了を行います。ガードレールのトリップワイヤーが発火した場合、 エージェント は実行されず、トークン消費やツール実行を防げます。これはコスト最適化や、ツール呼び出しによる副作用を避けたい場合に最適です。\n+- **Blocking execution**（`run_in_parallel=False`）: ガードレールは エージェント の開始「前に」実行と完了を行います。ガードレールのトリップワイヤーが発火した場合、 エージェント は実行されず、トークン消費とツール実行を防ぎます。これはコスト最適化や、ツール呼び出しによる副作用を回避したい場合に理想的です。\n \n ## 出力ガードレール\n \n-出力ガードレールは 3 段階で実行されます。\n+出力ガードレールは 3 ステップで実行されます。\n \n 1. まず、ガードレールは エージェント によって生成された出力を受け取ります。\n-2. 次に、ガードレール関数が実行され、[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] を生成し、これを [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult] でラップします\n-3. 最後に、[`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] が true かを確認します。true の場合、[`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] 例外が送出され、 ユーザー への適切な応答や例外処理が可能です。\n+2. 次に、ガードレール関数が実行され、[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] を生成し、それを [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult] でラップします\n+3. 最後に、[`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] が true かを確認します。true の場合、[`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] 例外が送出され、 ユーザー に適切に応答するか、例外を処理できます。\n \n !!! Note\n \n-    出力ガードレールは最終的な エージェント 出力に対して実行されることを想定しているため、 エージェント のガードレールが動くのは、その エージェント が「最後の」 エージェント の場合のみです。入力ガードレールと同様に、ガードレールは実際の Agent に密接に関係する傾向があるため、 エージェント ごとに異なるガードレールを実行し、コードを同じ場所に置くと可読性が向上します。\n+    出力ガードレールは最終的な エージェント 出力で実行されることを想定しているため、 エージェント のガードレールはその エージェント が「最後の」 エージェント の場合にのみ実行されます。入力ガードレールと同様に、ガードレールは実際の Agent に関連する傾向があるため、コードを同じ場所に置くことは可読性に役立ちます。\n \n     出力ガードレールは常に エージェント の完了後に実行されるため、`run_in_parallel` パラメーターはサポートしません。\n \n-## ツールガードレール\n+## ツール ガードレール\n \n-ツールガードレールは **関数ツール** をラップし、実行の「前後」でツール呼び出しを検証またはブロックできます。これらはツール自体に設定し、そのツールが呼び出されるたびに実行されます。\n+ツール ガードレールは、 **関数ツール** をラップし、実行の前後でツール呼び出しを検証またはブロックできるようにします。これはツール自体に設定され、そのツールが呼び出されるたびに実行されます。\n \n-- 入力ツールガードレールはツール実行前に動作し、呼び出しをスキップしたり、出力をメッセージで置き換えたり、トリップワイヤーを発火させたりできます。\n-- 出力ツールガードレールはツール実行後に動作し、出力の置き換えやトリップワイヤーの発火ができます。\n-- ツールガードレールが適用されるのは、[`function_tool`][agents.function_tool] で作成された関数ツールのみです。ホスト型のツール（`WebSearchTool`、`FileSearchTool`、`HostedMCPTool`、`CodeInterpreterTool`、`ImageGenerationTool`）やローカル実行環境のツール（`ComputerTool`、`ShellTool`、`ApplyPatchTool`、`LocalShellTool`）は、このガードレールのパイプラインを使用しません。\n-\n-```python\n-import json\n-from agents import (\n-    Agent,\n-    Runner,\n-    ToolGuardrailFunctionOutput,\n-    function_tool,\n-    tool_input_guardrail,\n-    tool_output_guardrail,\n-)\n-\n-@tool_input_guardrail\n-def block_secrets(data):\n-    args = json.loads(data.context.tool_arguments or \"{}\")\n-    if \"sk-\" in json.dumps(args):\n-        return ToolGuardrailFunctionOutput.reject_content(\n-            \"Remove secrets before calling this tool.\"\n-        )\n-    return ToolGuardrailFunctionOutput.allow()\n+- 入力ツール ガードレールはツールの実行前に動作し、呼び出しをスキップしたり、出力をメッセージに置き換えたり、トリップワイヤーを発火させたりできます。\n+- 出力ツール ガードレールはツールの実行後に動作し、出力を置き換えたり、トリップワイヤーを発火させたりできます。\n+- ツール ガードレールは、[`function_tool`][agents.function_tool] で作成された 関数ツール にのみ適用されます。ホスト型ツール（`WebSearchTool`、`FileSearchTool`、`HostedMCPTool`、`CodeInterpreterTool`、`ImageGenerationTool`）およびローカルランタイムツール（`ComputerTool`、`ShellTool`、`ApplyPatchTool`、`LocalShellTool`）はこのガードレールのパイプラインを使用しません。\n \n-\n-@tool_output_guardrail\n-def redact_output(data):\n-    text = str(data.output or \"\")\n-    if \"sk-\" in text:\n-        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n-    return ToolGuardrailFunctionOutput.allow()\n-\n-\n-@function_tool(\n-    tool_input_guardrails=[block_secrets],\n-    tool_output_guardrails=[redact_output],\n-)\n-def classify_text(text: str) -> str:\n-    \"\"\"Classify text for internal routing.\"\"\"\n-    return f\"length:{len(text)}\"\n-\n-\n-agent = Agent(name=\"Classifier\", tools=[classify_text])\n-result = Runner.run_sync(agent, \"hello world\")\n-print(result.final_output)\n-```\n+詳細は次のコードスニペットを参照してください。\n \n ## トリップワイヤー\n \n-入力または出力がガードレールに不合格となった場合、ガードレールはトリップワイヤーでそれを通知できます。トリップワイヤーが発火したガードレールを検出するとすぐに、`{Input,Output}GuardrailTripwireTriggered` 例外を送出し、Agent の実行を停止します。\n+入力または出力がガードレールに失敗した場合、ガードレールはトリップワイヤーでそれを通知できます。トリップワイヤーが発火したガードレールを検出するとすぐに、`{Input,Output}GuardrailTripwireTriggered` 例外を送出し、Agent の実行を停止します。\n \n ## ガードレールの実装\n \n-入力を受け取り、[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] を返す関数を用意する必要があります。次の例では、その内部で エージェント を実行して実現します。\n+入力を受け取り、[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput] を返す関数を用意する必要があります。以下の例では、内部的に エージェント を実行してこれを行います。\n \n ```python\n from pydantic import BaseModel\n@@ -155,10 +114,10 @@ async def main():\n         print(\"Math homework guardrail tripped\")\n ```\n \n-1. ガードレール関数内で使用する エージェント です。\n+1. この エージェント をガードレール関数内で使用します。\n 2. これは エージェント の入力/コンテキストを受け取り、結果を返すガードレール関数です。\n 3. ガードレール結果に追加情報を含めることができます。\n-4. これがワークフローを定義する実際の エージェント です。\n+4. これはワークフローを定義する実際の エージェント です。\n \n 出力ガードレールも同様です。\n \n@@ -216,4 +175,49 @@ async def main():\n 1. これは実際の エージェント の出力型です。\n 2. これはガードレールの出力型です。\n 3. これは エージェント の出力を受け取り、結果を返すガードレール関数です。\n-4. これがワークフローを定義する実際の エージェント です。\n\\ No newline at end of file\n+4. これはワークフローを定義する実際の エージェント です。\n+\n+最後に、ツール ガードレールの例を示します。\n+\n+```python\n+import json\n+from agents import (\n+    Agent,\n+    Runner,\n+    ToolGuardrailFunctionOutput,\n+    function_tool,\n+    tool_input_guardrail,\n+    tool_output_guardrail,\n+)\n+\n+@tool_input_guardrail\n+def block_secrets(data):\n+    args = json.loads(data.context.tool_arguments or \"{}\")\n+    if \"sk-\" in json.dumps(args):\n+        return ToolGuardrailFunctionOutput.reject_content(\n+            \"Remove secrets before calling this tool.\"\n+        )\n+    return ToolGuardrailFunctionOutput.allow()\n+\n+\n+@tool_output_guardrail\n+def redact_output(data):\n+    text = str(data.output or \"\")\n+    if \"sk-\" in text:\n+        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n+    return ToolGuardrailFunctionOutput.allow()\n+\n+\n+@function_tool(\n+    tool_input_guardrails=[block_secrets],\n+    tool_output_guardrails=[redact_output],\n+)\n+def classify_text(text: str) -> str:\n+    \"\"\"Classify text for internal routing.\"\"\"\n+    return f\"length:{len(text)}\"\n+\n+\n+agent = Agent(name=\"Classifier\", tools=[classify_text])\n+result = Runner.run_sync(agent, \"hello world\")\n+print(result.final_output)\n+```\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fguardrails.md",
        "sha": "a2f67bca701f8a7c0aca4ee8f044c784b0587bb1",
        "status": "modified"
      },
      {
        "additions": 17,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fhandoffs.md",
        "changes": 34,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fhandoffs.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 17,
        "filename": "docs/ja/handoffs.md",
        "patch": "@@ -4,17 +4,17 @@ search:\n ---\n # ハンドオフ\n \n-ハンドオフは、あるエージェントが別のエージェントにタスクを委譲できる仕組みです。これは、異なるエージェントがそれぞれ異なる分野に特化しているシナリオで特に有用です。たとえば、カスタマーサポートアプリには、注文状況、返金、FAQ などのタスクを個別に担当するエージェントがいる場合があります。\n+ハンドオフにより、ある エージェント が別の エージェント にタスクを委譲できます。これは、異なる エージェント がそれぞれ異なる分野を専門としているシナリオで特に有用です。たとえば、カスタマーサポートアプリでは、注文状況、返金、FAQ などのタスクを個別に担当する エージェント を用意できます。\n \n-ハンドオフは LLM に対してツールとして表現されます。たとえば、`Refund Agent` という名前のエージェントへのハンドオフがある場合、そのツール名は `transfer_to_refund_agent` になります。\n+ハンドオフは LLM からはツールとして表現されます。たとえば、`Refund Agent` へのハンドオフがある場合、ツール名は `transfer_to_refund_agent` になります。\n \n ## ハンドオフの作成\n \n-すべてのエージェントは [`handoffs`][agents.agent.Agent.handoffs] パラメーターを持ち、これは `Agent` を直接渡すか、ハンドオフをカスタマイズする `Handoff` オブジェクトを渡すことができます。\n+すべての エージェント には [`handoffs`][agents.agent.Agent.handoffs] パラメーターがあり、これは `Agent` を直接受け取ることも、ハンドオフをカスタマイズする `Handoff` オブジェクトを受け取ることもできます。\n \n-プレーンな `Agent` インスタンスを渡す場合、その [`handoff_description`][agents.agent.Agent.handoff_description]（設定されているとき）はデフォルトのツール説明に追加されます。完全な `handoff()` オブジェクトを書かずに、モデルがそのハンドオフを選ぶべきタイミングを示すために使用します。\n+プレーンな `Agent` インスタンスを渡す場合、その [`handoff_description`][agents.agent.Agent.handoff_description]（設定されているとき）がデフォルトのツール説明に追加されます。完全な `handoff()` オブジェクトを書かずに、そのハンドオフをモデルが選択すべきタイミングを示唆するために活用してください。\n \n-Agents SDK が提供する [`handoff()`][agents.handoffs.handoff] 関数を使ってハンドオフを作成できます。この関数では、引き継ぎ先のエージェントに加えて、任意のオーバーライドや入力フィルターを指定できます。\n+Agents SDK が提供する [`handoff()`][agents.handoffs.handoff] 関数を使ってハンドオフを作成できます。この関数では、ハンドオフ先の エージェント の指定に加え、任意の上書き設定や入力フィルターを指定できます。\n \n ### 基本的な使い方\n \n@@ -30,19 +30,19 @@ refund_agent = Agent(name=\"Refund agent\")\n triage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n ```\n \n-1. `billing_agent` のようにエージェントを直接使うことも、`handoff()` 関数を使うこともできます。\n+1. `billing_agent` のように エージェント を直接利用することも、`handoff()` 関数を使用することもできます。\n \n ### `handoff()` 関数によるハンドオフのカスタマイズ\n \n-[`handoff()`][agents.handoffs.handoff] 関数では各種カスタマイズが可能です。\n+[`handoff()`][agents.handoffs.handoff] 関数では、さまざまなカスタマイズが可能です。\n \n-- `agent`: 引き継ぎ先のエージェントです。\n+- `agent`: ハンドオフ先の エージェント です。\n - `tool_name_override`: 既定では `Handoff.default_tool_name()` が使用され、`transfer_to_<agent_name>` に解決されます。これを上書きできます。\n - `tool_description_override`: `Handoff.default_tool_description()` によるデフォルトのツール説明を上書きします。\n-- `on_handoff`: ハンドオフが呼び出されたときに実行されるコールバック関数です。ハンドオフが実行されることが分かった時点でデータ取得を開始する、といった用途に有用です。この関数はエージェントコンテキストを受け取り、オプションで LLM が生成した入力も受け取れます。入力データは `input_type` パラメーターで制御します。\n+- `on_handoff`: ハンドオフが呼び出されたときに実行されるコールバック関数です。ハンドオフが実行されると分かったタイミングでデータ取得を開始するなどに役立ちます。この関数は エージェント コンテキストを受け取り、任意で LLM が生成した入力も受け取れます。入力データは `input_type` パラメーターで制御します。\n - `input_type`: ハンドオフが想定する入力の型（任意）。\n-- `input_filter`: 次のエージェントが受け取る入力をフィルタリングできます。詳細は以下を参照してください。\n-- `is_enabled`: ハンドオフを有効にするかどうか。真偽値、または真偽値を返す関数を指定でき、実行時に動的に有効・無効を切り替えられます。\n+- `input_filter`: 次の エージェント が受け取る入力をフィルタリングできます。詳細は後述します。\n+- `is_enabled`: ハンドオフが有効かどうか。真偽値または真偽値を返す関数を指定でき、実行時にハンドオフを動的に有効・無効化できます。\n \n ```python\n from agents import Agent, handoff, RunContextWrapper\n@@ -60,9 +60,9 @@ handoff_obj = handoff(\n )\n ```\n \n-## ハンドオフの入力\n+## ハンドオフ入力\n \n-状況によっては、ハンドオフを呼び出す際に LLM にいくつかのデータを提供してほしいことがあります。たとえば、「エスカレーションエージェント」へのハンドオフを想定してください。ログ記録のために理由を渡したい場合があります。\n+状況によっては、ハンドオフの呼び出し時に LLM からいくつかのデータを提供させたい場合があります。たとえば、「エスカレーション エージェント」へのハンドオフを考えてみましょう。ログ用に理由が提供されると便利です。\n \n ```python\n from pydantic import BaseModel\n@@ -86,11 +86,11 @@ handoff_obj = handoff(\n \n ## 入力フィルター\n \n-ハンドオフが発生すると、新しいエージェントが会話を引き継ぎ、これまでの会話履歴全体を閲覧できるかのように振る舞います。これを変更したい場合は、[`input_filter`][agents.handoffs.Handoff.input_filter] を設定できます。入力フィルターは、既存の入力を [`HandoffInputData`][agents.handoffs.HandoffInputData] として受け取り、新しい `HandoffInputData` を返す関数です。\n+ハンドオフが発生すると、新しい エージェント が会話を引き継ぎ、これまでの会話履歴全体を参照できるかのように振る舞います。これを変更したい場合は、[`input_filter`][agents.handoffs.Handoff.input_filter] を設定できます。入力フィルターは、既存の入力を [`HandoffInputData`][agents.handoffs.HandoffInputData] 経由で受け取り、新しい `HandoffInputData` を返す関数です。\n \n-デフォルトでは、ランナーは直前までの発話録を 1 件の assistant の要約メッセージに折りたたみます（[`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history] を参照）。この要約は、同一の実行内で複数回のハンドオフが起きる場合に新しいターンを追記していく `<CONVERSATION HISTORY>` ブロックの中に現れます。[`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper] を指定すると、完全な `input_filter` を書かずに生成メッセージを置き換えるための独自マッピング関数を提供できます。なお、このデフォルトは、ハンドオフ側と実行側のどちらにも明示的な `input_filter` がない場合にのみ適用されるため、既にペイロードをカスタマイズしているコード（このリポジトリの code examples を含む）は変更なしで現行の挙動を維持します。単一のハンドオフについてネスト動作を上書きしたい場合は、[`handoff(...)`][agents.handoffs.handoff] に `nest_handoff_history=True` または `False` を渡してください。これは [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] を設定します。生成された要約のラッパーテキストだけを変更したい場合は、エージェントを実行する前に [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers]（必要に応じて [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers] も）を呼び出してください。\n+デフォルトでは、Runner は直前の記録（transcript）を 1 つのアシスタント要約メッセージに畳み込みます（[`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history] を参照）。この要約は、同じ実行中に複数のハンドオフが発生する場合に新しいターンが追記され続ける `<CONVERSATION HISTORY>` ブロックの中に表示されます。生成されたメッセージ全体を置き換えるマッピング関数を自前で提供したい場合は、完全な `input_filter` を書かずに [`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper] を指定できます。このデフォルトは、ハンドオフ側と実行側のどちらからも明示的な `input_filter` が提供されない場合にのみ適用されるため、既にペイロードをカスタマイズしている既存コード（このリポジトリの code examples を含む）は、変更なしで現在の動作を維持します。単一のハンドオフについて入れ子化の動作を上書きしたい場合は、[`handoff(...)`][agents.handoffs.handoff] に `nest_handoff_history=True` または `False` を渡して [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] を設定してください。生成された要約のラッパー文言だけを変更する必要がある場合は、エージェントを実行する前に [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers]（必要に応じて [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers] も）を呼び出してください。\n \n-一般的なパターン（たとえば履歴からすべてのツール呼び出しを削除するなど）は、[`agents.extensions.handoff_filters`][] に実装済みです。\n+一般的なパターン（たとえば履歴からすべてのツール呼び出しを除去するなど）は、[`agents.extensions.handoff_filters`][] に実装済みです。\n \n ```python\n from agents import Agent, handoff\n@@ -108,7 +108,7 @@ handoff_obj = handoff(\n \n ## 推奨プロンプト\n \n-LLM がハンドオフを正しく理解できるようにするため、エージェントにハンドオフに関する情報を含めることを推奨します。[`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] に推奨のプレフィックスがあり、あるいは [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] を呼び出して、推奨データをプロンプトに自動追加できます。\n+LLM がハンドオフを正しく理解できるように、エージェント にハンドオフに関する情報を含めることを推奨します。[`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] に推奨のプレフィックスがあり、あるいは [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] を呼び出して、推奨データをプロンプトに自動的に追加できます。\n \n ```python\n from agents import Agent",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fhandoffs.md",
        "sha": "3a463bed6db80103a078f2e3e25b9ac20e7d9e3b",
        "status": "modified"
      },
      {
        "additions": 18,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Findex.md",
        "changes": 36,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 18,
        "filename": "docs/ja/index.md",
        "patch": "@@ -4,39 +4,39 @@ search:\n ---\n # OpenAI Agents SDK\n \n-[OpenAI Agents SDK](https://github.com/openai/openai-agents-python) は、最小限の抽象化で軽量かつ使いやすいパッケージにより、エージェント駆動の AI アプリを構築できるようにするものです。これは、以前のエージェント向け実験である [Swarm](https://github.com/openai/swarm/tree/main) の本番運用向けアップグレード版です。Agents SDK はごく少数の基本コンポーネントから成ります。\n+[OpenAI Agents SDK](https://github.com/openai/openai-agents-python) は、抽象化を極力減らした軽量で使いやすいパッケージで、エージェント型 AI アプリを構築できるようにします。これはエージェントに関する以前の実験プロジェクトである [Swarm](https://github.com/openai/swarm/tree/main) のプロダクション対応版へのアップグレードです。Agents SDK にはごく少数の basic components があります:\n \n--   **エージェント**: instructions と tools を備えた LLMs\n--   **ハンドオフ**: 特定のタスクを他のエージェントに委譲できる機能\n--   **ガードレール**: エージェントの入力と出力の検証を可能にする機能\n--   **セッション**: エージェントの実行をまたいで会話履歴を自動的に保持する機能\n+-   **エージェント**、instructions と tools を備えた LLM\n+-   **ハンドオフ**、特定のタスクで他のエージェントに委任できる機能\n+-   **ガードレール**、エージェントの入力と出力を検証できる機能\n+-   **セッション**、エージェントの実行間で会話履歴を自動的に維持\n \n-Python と組み合わせることで、これらの基本コンポーネントはツールとエージェント間の複雑な関係を表現でき、学習コストをかけずに実運用のアプリケーションを構築できます。さらに、SDK には組み込みの **トレーシング** があり、エージェントのフローを可視化・デバッグできるほか、評価や、アプリケーション向けのモデルのファインチューニングまで行えます。\n+これらの basic components は Python と組み合わせることで、ツールとエージェント間の複雑な関係を表現でき、急な学習コストなしに実世界のアプリケーションを構築できます。さらに、SDK には組み込みの **トレーシング** が付属し、エージェントのフローを可視化・デバッグできるほか、評価したり、アプリケーション向けにモデルをファインチューニングすることさえ可能です。\n \n ## Agents SDK を使う理由\n \n-この SDK は 2 つの設計原則に基づいています。\n+SDK には 2 つの設計原則があります:\n \n-1. 使う価値がある十分な機能を備えつつ、学習を素早くするための最小限の基本コンポーネント。\n-2. すぐに使える体験を提供しつつ、実際に起こることを細部までカスタマイズ可能。\n+1. 使う価値があるだけの十分な機能を提供しつつ、学習を素早くするために basic components は最小限に保つ。\n+2. すぐに使える状態で優れた体験を提供しつつ、挙動を細部までカスタマイズ可能にする。\n \n-SDK の主な機能は次のとおりです。\n+SDK の主な機能は次のとおりです:\n \n--   エージェントループ: ツール呼び出し、結果を LLM に送信、LLM が完了するまでのループを処理する組み込みのエージェントループ。\n--   Python ファースト: 新しい抽象を学ぶのではなく、言語の組み込み機能でエージェントのオーケストレーションや連鎖を実現。\n--   ハンドオフ: 複数のエージェント間での調整や委譲を可能にする強力な機能。\n--   ガードレール: 入力の検証やチェックをエージェントと並行して実行し、失敗時は早期に中断。\n--   セッション: エージェントの実行をまたいだ会話履歴の自動管理により、手動の状態管理が不要。\n--   関数ツール: 任意の Python 関数をツール化し、自動スキーマ生成と Pydantic ベースの検証を提供。\n--   トレーシング: ワークフローの可視化・デバッグ・監視を可能にし、OpenAI の評価、ファインチューニング、蒸留ツール群も活用できる組み込みのトレーシング。\n+-   エージェントループ: ツールの呼び出し、実行結果を LLM に送信し、LLM が完了するまでループする処理を内蔵。\n+-   Python ファースト: 新しい抽象化を学ぶ必要はなく、言語の組み込み機能でエージェントをオーケストレーションし連鎖。\n+-   ハンドオフ: 複数のエージェント間で調整・委任する強力な機能。\n+-   ガードレール: エージェントと並行して入力の検証やチェックを実行し、失敗時は早期に中断。\n+-   セッション: エージェント実行間で会話履歴を自動管理し、手動の状態管理を不要に。\n+-   関数ツール: 任意の Python 関数をツール化し、スキーマ自動生成と Pydantic による検証を提供。\n+-   トレーシング: ワークフローの可視化・デバッグ・監視を可能にし、OpenAI の評価、ファインチューニング、蒸留ツール群も利用可能。\n \n ## インストール\n \n ```bash\n pip install openai-agents\n ```\n \n-## Hello world の例\n+## Hello World の例\n \n ```python\n from agents import Agent, Runner",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Findex.md",
        "sha": "fab18617e18542f722d294118b8fc455a735e9e8",
        "status": "modified"
      },
      {
        "additions": 52,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmcp.md",
        "changes": 104,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fmcp.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 52,
        "filename": "docs/ja/mcp.md",
        "patch": "@@ -4,32 +4,34 @@ search:\n ---\n # Model context protocol (MCP)\n \n-[Model context protocol](https://modelcontextprotocol.io/introduction) (MCP) は、アプリケーションがツールやコンテキストを言語モデルに公開する方法を標準化します。公式ドキュメントより:\n+[Model context protocol](https://modelcontextprotocol.io/introduction) (MCP) は、アプリケーションが ツール とコンテキストを言語モデルに公開する方法を標準化します。公式ドキュメントからの引用です:\n \n-> MCP は、アプリケーションが LLM にコンテキストを提供する方法を標準化するオープンプロトコルです。MCP は AI アプリケーションのための USB-C ポートのようなものだと考えてください。USB-C がさまざまな周辺機器やアクセサリにデバイスを接続する標準化された方法を提供するのと同様に、MCP はさまざまなデータソースやツールに AI モデルを接続する標準化された方法を提供します。\n+> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI\n+> applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP\n+> provides a standardized way to connect AI models to different data sources and tools.\n \n-Agents Python SDK は複数の MCP トランスポートを理解します。これにより、既存の MCP サーバーを再利用したり、独自のサーバーを構築して、ファイルシステム、HTTP、またはコネクタで支えられたツールを エージェント に公開できます。\n+Agents Python SDK は複数の MCP トランスポートを理解します。これにより、既存の MCP サーバーを再利用したり、独自の サーバー を構築して、ファイルシステム、HTTP、またはコネクタで裏付けられた ツール を エージェント に公開できます。\n \n-## MCP 統合の選択\n+## Choosing an MCP integration\n \n-エージェント に MCP サーバーを接続する前に、ツール呼び出しをどこで実行すべきか、どのトランスポートに到達できるかを決めます。以下のマトリクスは、Python SDK がサポートするオプションをまとめたものです。\n+MCP サーバーを エージェント に接続する前に、ツール呼び出しをどこで実行するか、また到達可能なトランスポートはどれかを決めます。以下のマトリクスは、Python SDK がサポートするオプションをまとめたものです。\n \n-| 必要なこと                                                                            | 推奨オプション                                           |\n-| ------------------------------------------------------------------------------------ | -------------------------------------------------------- |\n-| OpenAI の Responses API に、モデルの代わりに公開到達可能な MCP サーバーを呼び出させる | **Hosted MCP server tools**（ホスト型 MCP ツール）経由 [`HostedMCPTool`][agents.tool.HostedMCPTool] |\n-| ローカルまたはリモートで稼働する Streamable な HTTP サーバーに接続する               | **Streamable HTTP MCP servers**（ストリーミング可能 HTTP MCP サーバー）経由 [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |\n-| Server-Sent Events を実装した HTTP サーバーと通信する                                 | **HTTP with SSE MCP servers**（HTTP + SSE MCP サーバー）経由 [`MCPServerSse`][agents.mcp.server.MCPServerSse] |\n-| ローカルプロセスを起動し、stdin/stdout 経由で通信する                                 | **stdio MCP servers**（stdio MCP サーバー）経由 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |\n+| 必要なこと                                                                            | 推奨オプション                                              |\n+| ------------------------------------------------------------------------------------ | ----------------------------------------------------------- |\n+| OpenAI の Responses API に、モデルの代わりに公開到達可能な MCP サーバーを呼び出させたい | **ホスト型 MCP サーバーのツール**（[`HostedMCPTool`][agents.tool.HostedMCPTool] 経由） |\n+| ローカルまたはリモートで実行する Streamable HTTP サーバーに接続したい                  | **Streamable HTTP MCP サーバー**（[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] 経由） |\n+| Server-Sent Events を実装した サーバー と通信したい                                     | **HTTP with SSE MCP サーバー**（[`MCPServerSse`][agents.mcp.server.MCPServerSse] 経由） |\n+| ローカルプロセスを起動して stdin/stdout で通信したい                                   | **stdio MCP サーバー**（[`MCPServerStdio`][agents.mcp.server.MCPServerStdio] 経由） |\n \n-以下のセクションでは、それぞれのオプションの設定方法と、どのトランスポートを優先すべきかを説明します。\n+以下のセクションでは、それぞれのオプションの使い方、設定方法、そしてどのトランスポートを選ぶべきかを説明します。\n \n ## 1. Hosted MCP server tools\n \n-Hosted ツールは、ツールの往復の全体を OpenAI のインフラに任せます。あなたのコードがツールの列挙や呼び出しを行う代わりに、[`HostedMCPTool`][agents.tool.HostedMCPTool] が サーバーラベル（および任意のコネクタ メタデータ）を Responses API に転送します。モデルはリモートサーバーのツールを列挙し、あなたの Python プロセスへの追加のコールバックなしでそれらを呼び出します。Hosted ツールは現在、Responses API の hosted MCP 統合をサポートする OpenAI モデルで動作します。\n+ホスト型 ツール は、ツールの往復処理全体を OpenAI のインフラストラクチャに委譲します。あなたのコードが ツール を列挙・呼び出す代わりに、[`HostedMCPTool`][agents.tool.HostedMCPTool] が サーバー のラベル（およびオプションのコネクタメタデータ）を Responses API に転送します。モデルはリモート サーバー の ツール を一覧し、あなたの Python プロセスへの追加のコールバックなしにそれらを呼び出します。ホスト型 ツール は現在、Responses API の hosted MCP 連携に対応した OpenAI モデルで動作します。\n \n-### 基本の hosted MCP ツール\n+### Basic hosted MCP tool\n \n-エージェント の `tools` リストに [`HostedMCPTool`][agents.tool.HostedMCPTool] を追加して hosted ツールを作成します。`tool_config` の dict は、REST API に送信する JSON を反映します:\n+エージェント の `tools` リストに [`HostedMCPTool`][agents.tool.HostedMCPTool] を追加して、ホスト型 ツール を作成します。`tool_config` の dict は、REST API に送る JSON をそのまま反映します:\n \n ```python\n import asyncio\n@@ -57,11 +59,11 @@ async def main() -> None:\n asyncio.run(main())\n ```\n \n-Hosted サーバーはツールを自動的に公開します。`mcp_servers` に追加する必要はありません。\n+ホストされた サーバー は ツール を自動的に公開します。`mcp_servers` に追加する必要はありません。\n \n-### ストリーミング hosted MCP 実行結果\n+### Streaming hosted MCP results\n \n-Hosted ツールは、関数ツールとまったく同じ方法で ストリーミング をサポートします。`Runner.run_streamed` に `stream=True` を渡して、モデルが処理中の 増分 MCP 出力 を消費します:\n+ホスト型 ツール は、関数ツール とまったく同じ方法で ストリーミング する 実行結果 に対応しています。`Runner.run_streamed` に `stream=True` を渡すと、モデルが処理を続けている間に増分的な MCP 出力を消費できます:\n \n ```python\n result = Runner.run_streamed(agent, \"Summarise this repository's top languages\")\n@@ -71,9 +73,9 @@ async for event in result.stream_events():\n print(result.final_output)\n ```\n \n-### 任意の承認フロー\n+### Optional approval flows\n \n-サーバーが機密性の高い操作を実行できる場合、各ツール実行の前に人間またはプログラムによる承認を要求できます。`tool_config` の `require_approval` を単一のポリシー（`\"always\"`, `\"never\"`）またはツール名からポリシーへの dict で設定します。Python 内で判断したい場合は、`on_approval_request` コールバックを提供します。\n+サーバー が機微な操作を実行できる場合、各ツール実行の前に人間またはプログラムによる承認を必須にできます。`tool_config` の `require_approval` を単一のポリシー（`\"always\"`、`\"never\"`）または ツール 名からポリシーへの dict で設定します。判断を Python 内で行うには、`on_approval_request` コールバックを指定します。\n \n ```python\n from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest\n@@ -101,11 +103,11 @@ agent = Agent(\n )\n ```\n \n-コールバックは同期または非同期のいずれでもよく、モデルが実行を継続するために承認データを必要とするたびに呼び出されます。\n+このコールバックは同期・非同期のどちらでもよく、モデルが継続実行に必要な承認データを求めるたびに呼び出されます。\n \n-### コネクタ対応の hosted サーバー\n+### Connector-backed hosted servers\n \n-Hosted MCP は OpenAI コネクタにも対応しています。`server_url` を指定する代わりに、`connector_id` とアクセストークンを指定します。Responses API が認証を処理し、hosted サーバーがコネクタのツールを公開します。\n+ホスト型 MCP は OpenAI コネクタにも対応しています。`server_url` を指定する代わりに、`connector_id` とアクセストークンを指定します。Responses API が認証を処理し、ホストされた サーバー がコネクタの ツール を公開します。\n \n ```python\n import os\n@@ -121,13 +123,12 @@ HostedMCPTool(\n )\n ```\n \n-ストリーミング、承認、コネクタを含む完全な hosted ツールのサンプルは、\n+ストリーミング、承認、コネクタを含む完全なホスト型 ツール のサンプルは、\n [`examples/hosted_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) にあります。\n \n ## 2. Streamable HTTP MCP servers\n \n-ネットワーク接続を自分で管理したい場合は、\n-[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] を使用します。Streamable な HTTP サーバーは、トランスポートを自分で制御したい場合や、低レイテンシを維持しながら自社インフラ内でサーバーを稼働させたい場合に最適です。\n+ネットワーク接続を自分で管理したい場合は、[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] を使用します。Streamable HTTP サーバーは、トランスポートを自分で制御したい場合や、レイテンシを低く保ちながら自分のインフラ内で サーバー を実行したい場合に最適です。\n \n ```python\n import asyncio\n@@ -162,21 +163,20 @@ async def main() -> None:\n asyncio.run(main())\n ```\n \n-コンストラクタは追加のオプションを受け付けます:\n+コンストラクタは次のオプションを受け付けます:\n \n - `client_session_timeout_seconds` は HTTP の読み取りタイムアウトを制御します。\n - `use_structured_content` は、テキスト出力よりも `tool_result.structured_content` を優先するかどうかを切り替えます。\n-- `max_retry_attempts` と `retry_backoff_seconds_base` は、`list_tools()` および `call_tool()` に自動リトライを追加します。\n-- `tool_filter` は、公開するツールをサブセットに絞り込めます（[ツールフィルタリング](#tool-filtering) を参照）。\n+- `max_retry_attempts` と `retry_backoff_seconds_base` は、`list_tools()` と `call_tool()` に自動リトライを追加します。\n+- `tool_filter` により、一部の ツール のみを公開できます（[ツールのフィルタリング](#tool-filtering) を参照）。\n \n ## 3. HTTP with SSE MCP servers\n \n !!! warning\n \n-    MCP プロジェクトは Server-Sent Events トランスポートを非推奨にしました。新しい統合には Streamable HTTP または stdio を優先し、SSE はレガシーサーバーのみに維持してください。\n+    MCP プロジェクトは Server-Sent Events トランスポートを非推奨としています。新規の連携では Streamable HTTP または stdio を優先し、SSE はレガシー サーバー のみで使用してください。\n \n-MCP サーバーが HTTP with SSE トランスポートを実装している場合は、\n-[`MCPServerSse`][agents.mcp.server.MCPServerSse] をインスタンス化します。トランスポート以外は、API は Streamable HTTP サーバーと同一です。\n+MCP サーバー が HTTP with SSE トランスポートを実装している場合は、[`MCPServerSse`][agents.mcp.server.MCPServerSse] をインスタンス化します。トランスポート以外は、API は Streamable HTTP サーバーと同一です。\n \n ```python\n \n@@ -205,7 +205,7 @@ async with MCPServerSse(\n \n ## 4. stdio MCP servers\n \n-ローカルのサブプロセスとして実行される MCP サーバーには、[`MCPServerStdio`][agents.mcp.server.MCPServerStdio] を使用します。SDK はプロセスを起動し、パイプを開いたまま維持し、コンテキストマネージャを抜けると自動的に閉じます。このオプションは、迅速なプロトタイプや、サーバーがコマンドラインのエントリポイントのみを公開している場合に役立ちます。\n+ローカルのサブプロセスとして実行する MCP サーバーには、[`MCPServerStdio`][agents.mcp.server.MCPServerStdio] を使用します。SDK はプロセスを起動し、パイプを開いたままにし、コンテキストマネージャーの終了時に自動で閉じます。これは、短時間でのプロトタイピングや、サーバーがコマンドラインのエントリポイントのみを公開している場合に役立ちます。\n \n ```python\n from pathlib import Path\n@@ -231,13 +231,13 @@ async with MCPServerStdio(\n     print(result.final_output)\n ```\n \n-## ツールフィルタリング\n+## Tool filtering\n \n-各 MCP サーバーはツールフィルタをサポートしており、エージェント が必要とする機能のみを公開できます。フィルタリングは、構築時または実行ごとに動的に行えます。\n+各 MCP サーバーは ツール フィルターをサポートしており、エージェント に必要な関数だけを公開できます。フィルタリングは構築時にも、実行ごとに動的にも行えます。\n \n-### 静的ツールフィルタリング\n+### Static tool filtering\n \n-[`create_static_tool_filter`][agents.mcp.create_static_tool_filter] を使用して、簡単な許可/ブロックリストを設定します:\n+[`create_static_tool_filter`][agents.mcp.create_static_tool_filter] を使用して、単純な許可/拒否リストを設定します:\n \n ```python\n from pathlib import Path\n@@ -255,11 +255,11 @@ filesystem_server = MCPServerStdio(\n )\n ```\n \n-`allowed_tool_names` と `blocked_tool_names` の両方が指定された場合、SDK は先に許可リストを適用し、その後、残りの集合からブロック対象のツールを取り除きます。\n+`allowed_tool_names` と `blocked_tool_names` が両方指定された場合、SDK はまず許可リストを適用し、その後、残りの集合からブロック対象の ツール を除外します。\n \n-### 動的ツールフィルタリング\n+### Dynamic tool filtering\n \n-より高度なロジックが必要な場合は、[`ToolFilterContext`][agents.mcp.ToolFilterContext] を受け取る呼び出し可能オブジェクトを渡します。呼び出し可能オブジェクトは同期または非同期のいずれでもよく、ツールを公開すべきときに `True` を返します。\n+より詳細なロジックには、[`ToolFilterContext`][agents.mcp.ToolFilterContext] を受け取る呼び出し可能なオブジェクトを渡します。これは同期・非同期どちらでもよく、当該 ツール を公開すべき場合に `True` を返します。\n \n ```python\n from pathlib import Path\n@@ -283,14 +283,14 @@ async with MCPServerStdio(\n     ...\n ```\n \n-フィルタコンテキストは、アクティブな `run_context`、ツールを要求している `agent`、および `server_name` を公開します。\n+フィルターコンテキストは、アクティブな `run_context`、ツールを要求している `agent`、および `server_name` を公開します。\n \n-## プロンプト\n+## Prompts\n \n-MCP サーバーは、エージェントの instructions を動的に生成する プロンプト も提供できます。プロンプトをサポートするサーバーは次の 2 つのメソッドを公開します:\n+MCP サーバーは、エージェントの instructions を動的に生成する Prompts も提供できます。Prompts に対応する サーバー は、次の 2 つのメソッドを公開します:\n \n-- `list_prompts()` は利用可能なプロンプトテンプレートを列挙します。\n-- `get_prompt(name, arguments)` は、必要に応じてパラメーター付きで具体的なプロンプトを取得します。\n+- `list_prompts()` は、利用可能なプロンプトテンプレートを列挙します。\n+- `get_prompt(name, arguments)` は、必要に応じて パラメーター を指定して具体的なプロンプトを取得します。\n \n ```python\n from agents import Agent\n@@ -308,21 +308,21 @@ agent = Agent(\n )\n ```\n \n-## キャッシュ\n+## Caching\n \n-すべての エージェント 実行は、各 MCP サーバーに対して `list_tools()` を呼び出します。リモートサーバーは顕著なレイテンシを引き起こす可能性があるため、すべての MCP サーバークラスは `cache_tools_list` オプションを公開しています。ツール定義が頻繁に変わらないと確信できる場合にのみ、`True` に設定してください。後から新しい一覧を強制するには、サーバーインスタンスで `invalidate_tools_cache()` を呼び出します。\n+すべての エージェント 実行は、各 MCP サーバーに対して `list_tools()` を呼び出します。リモート サーバー は顕著なレイテンシを生む可能性があるため、すべての MCP サーバークラスは `cache_tools_list` オプションを公開しています。ツール定義が頻繁に変わらないと確信できる場合にのみ `True` に設定してください。あとで新しい一覧を強制するには、サーバーインスタンスで `invalidate_tools_cache()` を呼び出します。\n \n-## トレーシング\n+## Tracing\n \n-[Tracing](./tracing.md) は、以下を含む MCP のアクティビティを自動的にキャプチャします:\n+[トレーシング](./tracing.md) は MCP のアクティビティを自動的に捕捉します。含まれる内容:\n \n-1. ツールを列挙するための MCP サーバーへの呼び出し。\n+1. ツール一覧のための MCP サーバーへの呼び出し。\n 2. ツール呼び出しに関する MCP 関連情報。\n \n ![MCP Tracing Screenshot](../assets/images/mcp-tracing.jpg)\n \n-## 参考資料\n+## Further reading\n \n - [Model Context Protocol](https://modelcontextprotocol.io/) – 仕様および設計ガイド。\n - [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) – 実行可能な stdio、SSE、Streamable HTTP のサンプル。\n-- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) – 承認やコネクタを含む完全な hosted MCP デモ。\n\\ No newline at end of file\n+- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) – 承認やコネクタを含む、完全なホスト型 MCP のデモ。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmcp.md",
        "sha": "f17c880ccd0593a278605928bb3583750cdd3a9e",
        "status": "modified"
      },
      {
        "additions": 41,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmodels%2Findex.md",
        "changes": 82,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fmodels%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 41,
        "filename": "docs/ja/models/index.md",
        "patch": "@@ -4,20 +4,20 @@ search:\n ---\n # モデル\n \n-Agents SDK には、すぐに使える 2 種類の OpenAI モデル対応が含まれています:\n+Agents SDK には、OpenAI モデルのサポートが次の 2 種類で付属しています。\n \n--   **推奨**: 新しい [Responses API](https://platform.openai.com/docs/api-reference/responses) を使って OpenAI API を呼び出す [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]\n--   [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) を使って OpenAI API を呼び出す [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]\n+-  **推奨**: 新しい [Responses API](https://platform.openai.com/docs/api-reference/responses) を使って OpenAI API を呼び出す [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]\n+-  [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) を使って OpenAI API を呼び出す [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]\n \n ## OpenAI モデル\n \n-`Agent` を初期化する際にモデルを指定しない場合、デフォルトのモデルが使用されます。現在のデフォルトは互換性と低レイテンシのために [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1) です。アクセス権がある場合は、明示的な `model_settings` を維持しながら、より高品質のために エージェント を [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) に設定することを推奨します。\n+`Agent` を初期化するときにモデルを指定しない場合は、デフォルトのモデルが使用されます。互換性と低レイテンシのため、現在のデフォルトは [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1) です。アクセス権がある場合は、明示的な `model_settings` を維持しつつ、より高品質な [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) をエージェントに設定することをおすすめします。\n \n [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) のような他のモデルに切り替えたい場合は、次のセクションの手順に従ってください。\n \n ### 既定の OpenAI モデル\n \n-カスタムモデルを設定していないすべての エージェント で特定のモデルを一貫して使用したい場合は、エージェント を実行する前に環境変数 `OPENAI_DEFAULT_MODEL` を設定してください。\n+カスタムモデルを設定していないすべてのエージェントで特定のモデルを一貫して使用したい場合は、エージェントを実行する前に `OPENAI_DEFAULT_MODEL` 環境変数を設定します。\n \n ```bash\n export OPENAI_DEFAULT_MODEL=gpt-5\n@@ -26,9 +26,9 @@ python3 my_awesome_agent.py\n \n #### GPT-5 モデル\n \n-この方法で GPT-5 のいずれかの推論モデル（[`gpt-5`](https://platform.openai.com/docs/models/gpt-5)、[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini)、または [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano)）を使用する場合、SDK は既定で妥当な `ModelSettings` を適用します。具体的には、`reasoning.effort` と `verbosity` の両方を `\"low\"` に設定します。これらの設定を自分で構築したい場合は、`agents.models.get_default_model_settings(\"gpt-5\")` を呼び出してください。\n+この方法で GPT-5 のいずれかの推論モデル（[`gpt-5`](https://platform.openai.com/docs/models/gpt-5)、[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini)、または [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano)）を使用すると、SDK は既定で妥当な `ModelSettings` を適用します。具体的には、`reasoning.effort` と `verbosity` の両方を `\"low\"` に設定します。これらの設定を自分で構築したい場合は、`agents.models.get_default_model_settings(\"gpt-5\")` を呼び出してください。\n \n-より低レイテンシや特定の要件がある場合は、別のモデルと設定を選択できます。デフォルトモデルの推論努力度を調整するには、独自の `ModelSettings` を渡してください:\n+より低レイテンシや特定の要件のために、別のモデルと設定を選ぶこともできます。既定モデルの推論強度を調整するには、独自の `ModelSettings` を渡します。\n \n ```python\n from openai.types.shared import Reasoning\n@@ -44,52 +44,52 @@ my_agent = Agent(\n )\n ```\n \n-特に低レイテンシを重視する場合、[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) または [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) に `reasoning.effort=\"minimal\"` を組み合わせると、デフォルト設定よりも高速に応答が返ることがよくあります。ただし、Responses API における一部の組み込みツール（ファイル検索や画像生成など）は `\"minimal\"` の推論努力度をサポートしていないため、本 Agents SDK のデフォルトは `\"low\"` になっています。\n+特に低レイテンシを重視する場合、[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) または [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) において `reasoning.effort=\"minimal\"` を使うと、既定設定よりも高速に応答が返ることがよくあります。ただし、Responses API の一部の組み込みツール（たとえば ファイル検索 と 画像生成）は `\"minimal\"` の推論強度をサポートしていないため、この Agents SDK では既定を `\"low\"` にしています。\n \n #### 非 GPT-5 モデル\n \n-カスタムの `model_settings` なしで GPT-5 以外のモデル名を渡すと、SDK はあらゆるモデルと互換性のある汎用的な `ModelSettings` にフォールバックします。\n+カスタムの `model_settings` なしで GPT-5 以外のモデル名を渡した場合、SDK は任意のモデルと互換性のある汎用的な `ModelSettings` にフォールバックします。\n \n ## 非 OpenAI モデル\n \n-[LiteLLM 連携](./litellm.md)を通じて、ほとんどの他社製（非 OpenAI）モデルを使用できます。まず、litellm の依存関係グループをインストールします:\n+[LiteLLM との連携](./litellm.md)を通じて、他のほとんどの非 OpenAI モデルを使用できます。まず、litellm の依存関係グループをインストールします。\n \n ```bash\n pip install \"openai-agents[litellm]\"\n ```\n \n-次に、`litellm/` プレフィックスを付けて、[サポートされているモデル](https://docs.litellm.ai/docs/providers) を利用します:\n+次に、`litellm/` プレフィックスを付けて、[サポートされているモデル](https://docs.litellm.ai/docs/providers) を使用します。\n \n ```python\n claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\n gemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)\n ```\n \n-### 非 OpenAI モデルの他の利用方法\n+### 非 OpenAI モデルを使う他の方法\n \n-他の LLM プロバイダーはさらに 3 つの方法で統合できます（code examples は[こちら](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)）:\n+他の LLM プロバイダを統合する方法がさらに 3 つあります（code examples は[こちら](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)）。\n \n-1. [`set_default_openai_client`][agents.set_default_openai_client] は、LLM クライアントとして `AsyncOpenAI` のインスタンスをグローバルに使用したい場合に便利です。これは LLM プロバイダーが OpenAI 互換の API エンドポイントを提供し、`base_url` と `api_key` を設定できるケース向けです。設定可能な例は [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py) を参照してください。\n-2. [`ModelProvider`][agents.models.interface.ModelProvider] は `Runner.run` レベルです。これにより、「この実行のすべての エージェント にカスタムモデルプロバイダーを使う」と指定できます。設定可能な例は [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py) を参照してください。\n-3. [`Agent.model`][agents.agent.Agent.model] は、特定の Agent インスタンスでモデルを指定できます。これにより、エージェント ごとに異なるプロバイダーを組み合わせて利用できます。設定可能な例は [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py) を参照してください。最も多くの利用可能モデルを簡単に使う方法は [LiteLLM 連携](./litellm.md) です。\n+1. [`set_default_openai_client`][agents.set_default_openai_client] は、LLM クライアントとして `AsyncOpenAI` のインスタンスをグローバルに使用したい場合に便利です。これは、LLM プロバイダが OpenAI 互換の API エンドポイントを持ち、`base_url` と `api_key` を設定できる場合に該当します。設定可能なサンプルは [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py) を参照してください。\n+2. [`ModelProvider`][agents.models.interface.ModelProvider] は `Runner.run` レベルにあります。これにより、「この実行内のすべてのエージェントにカスタムのモデルプロバイダを使う」と指定できます。設定可能なサンプルは [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py) を参照してください。\n+3. [`Agent.model`][agents.agent.Agent.model] は、特定の Agent インスタンスでモデルを指定できます。これにより、エージェントごとに異なるプロバイダを組み合わせて使用できます。設定可能なサンプルは [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py) を参照してください。最も多くの利用可能なモデルを簡単に使うには、[LiteLLM との連携](./litellm.md)が便利です。\n \n-`platform.openai.com` の API キーをお持ちでない場合は、`set_tracing_disabled()` で トレーシング を無効化するか、[別のトレーシング プロセッサー](../tracing.md) を設定することを推奨します。\n+`platform.openai.com` の API キーをお持ちでない場合は、`set_tracing_disabled()` でトレーシングを無効化するか、[別のトレーシング プロセッサー](../tracing.md) を設定することをおすすめします。\n \n !!! note\n \n-    これらの例では、多くの LLM プロバイダーがまだ Responses API をサポートしていないため、Chat Completions API/モデルを使用しています。LLM プロバイダーが Responses をサポートしている場合は、Responses の使用を推奨します。\n+    これらの code examples では、Responses API をまだサポートしていない LLM プロバイダがほとんどのため、Chat Completions API/モデルを使用しています。もしご利用の LLM プロバイダがサポートしている場合は、Responses の使用をおすすめします。\n \n ## モデルの組み合わせ\n \n-単一のワークフロー内で、エージェント ごとに異なるモデルを使いたい場合があります。たとえば、振り分けには小さく高速なモデルを使用し、複雑なタスクにはより大きく高性能なモデルを使用できます。[`Agent`][agents.Agent] を構成する際、次のいずれかで特定のモデルを選択できます:\n+1 つのワークフロー内で、エージェントごとに異なるモデルを使いたい場合があります。たとえば、振り分けには小型で高速なモデルを使い、複雑なタスクには大型で高性能なモデルを使う、といった使い分けです。[`Agent`][agents.Agent] を設定する際、次のいずれかの方法で特定のモデルを選択できます。\n \n-1. モデル名を渡す。\n-2. 任意のモデル名 + その名前を Model インスタンスにマッピングできる [`ModelProvider`][agents.models.interface.ModelProvider] を渡す。\n-3. 直接 [`Model`][agents.models.interface.Model] 実装を提供する。\n+1. モデル名を直接渡す。\n+2. 任意のモデル名と、それを Model インスタンスへマッピングできる [`ModelProvider`][agents.models.interface.ModelProvider] を渡す。\n+3. [`Model`][agents.models.interface.Model] 実装を直接渡す。\n \n !!!note\n \n-    SDK は [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] と [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] の両方の形状をサポートしますが、それぞれサポートする機能やツールのセットが異なるため、ワークフローごとに単一のモデル形状を使用することを推奨します。ワークフローでモデル形状を混在させる必要がある場合は、使用しているすべての機能が両方で利用可能であることを確認してください。\n+    SDK は [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] と [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] の両方の形状をサポートしますが、両者はサポートする機能やツールが異なるため、各ワークフローでは単一のモデル形状の使用をおすすめします。ワークフローでモデル形状を混在させる必要がある場合は、使用するすべての機能が両方で利用可能であることを確認してください。\n \n ```python\n from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n@@ -122,10 +122,10 @@ async def main():\n     print(result.final_output)\n ```\n \n-1.  OpenAI のモデル名を直接設定します。\n-2.  [`Model`][agents.models.interface.Model] 実装を提供します。\n+1. OpenAI のモデル名を直接設定します。\n+2. [`Model`][agents.models.interface.Model] 実装を提供します。\n \n-エージェント で使用するモデルをさらに構成したい場合は、温度などのオプションのモデル構成パラメーターを提供する [`ModelSettings`][agents.models.interface.ModelSettings] を渡せます。\n+エージェントに使用するモデルをさらに構成したい場合は、`temperature` などの任意のモデル構成パラメーターを提供する [`ModelSettings`][agents.models.interface.ModelSettings] を渡せます。\n \n ```python\n from agents import Agent, ModelSettings\n@@ -138,7 +138,7 @@ english_agent = Agent(\n )\n ```\n \n-また、OpenAI の Responses API を使用する場合、[他にもいくつかのオプションのパラメーター](https://platform.openai.com/docs/api-reference/responses/create)（例: `user`、`service_tier` など）があります。トップレベルで指定できない場合は、`extra_args` を使って渡すこともできます。\n+また、OpenAI の Responses API を使用する場合、[他にもいくつかの任意パラメーター](https://platform.openai.com/docs/api-reference/responses/create)（例: `user`、`service_tier` など）があります。トップレベルで指定できない場合は、`extra_args` を使って渡すことができます。\n \n ```python\n from agents import Agent, ModelSettings\n@@ -154,39 +154,39 @@ english_agent = Agent(\n )\n ```\n \n-## 他社製 LLM プロバイダー利用時の一般的な問題\n+## 他社 LLM プロバイダ利用時の一般的な問題\n \n ### トレーシング クライアントのエラー 401\n \n-トレーシング に関連するエラーが発生する場合、トレースは OpenAI サーバー にアップロードされ、OpenAI の API キーをお持ちでないことが原因です。解決策は次の 3 つです:\n+トレーシング関連のエラーが発生する場合は、トレースが OpenAI のサーバーにアップロードされる一方で、OpenAI の API キーをお持ちでないことが原因です。解決方法は次の 3 つです。\n \n-1. トレーシング を完全に無効化する: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]\n-2. トレーシング 用に OpenAI キーを設定する: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]。この API キーはトレースのアップロードのみに使用され、[platform.openai.com](https://platform.openai.com/) のものが必要です。\n-3. 非 OpenAI のトレース プロセッサーを使用する。[tracing ドキュメント](../tracing.md#custom-tracing-processors) を参照してください。\n+1. トレーシングを完全に無効化する: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]\n+2. トレーシング用に OpenAI のキーを設定する: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]。この API キーはトレースのアップロードにのみ使用され、[platform.openai.com](https://platform.openai.com/) のものが必要です。\n+3. 非 OpenAI のトレース プロセッサーを使用する。[tracing のドキュメント](../tracing.md#custom-tracing-processors) を参照してください。\n \n ### Responses API のサポート\n \n-SDK はデフォルトで Responses API を使用しますが、他の多くの LLM プロバイダーはまだサポートしていません。その結果、404 などの問題が発生することがあります。解決するには次の 2 つの方法があります:\n+SDK は既定で Responses API を使用しますが、他の多くの LLM プロバイダはまだサポートしていません。そのため、404 などの問題が発生する場合があります。解決するには次の 2 つの方法があります。\n \n-1. [`set_default_openai_api(\"chat_completions\")`][agents.set_default_openai_api] を呼び出す。これは環境変数で `OPENAI_API_KEY` と `OPENAI_BASE_URL` を設定している場合に機能します。\n+1. [`set_default_openai_api(\"chat_completions\")`][agents.set_default_openai_api] を呼び出す。これは、環境変数で `OPENAI_API_KEY` と `OPENAI_BASE_URL` を設定している場合に機能します。\n 2. [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] を使用する。code examples は[こちら](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)にあります。\n \n ### structured outputs のサポート\n \n-一部のモデルプロバイダーは [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) をサポートしていません。これにより、次のようなエラーが発生することがあります:\n+一部のモデルプロバイダは [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) をサポートしていません。その結果、次のようなエラーが発生することがあります。\n \n ```\n \n BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n \n ```\n \n-これは一部のモデルプロバイダーの欠点で、JSON 出力はサポートしていても、出力に使用する `json_schema` を指定できないことがあります。現在この問題の修正に取り組んでいますが、JSON スキーマ出力をサポートしているプロバイダーに依存することを推奨します。そうでない場合、不正な JSON によりアプリが頻繁に壊れる可能性があります。\n+これは一部のモデルプロバイダの制限で、JSON 出力はサポートしていても、出力に使用する `json_schema` を指定できません。現在この問題の解決に取り組んでいますが、アプリが不正な JSON によって頻繁に壊れてしまうため、JSON schema 出力をサポートするプロバイダの利用を推奨します。\n \n-## プロバイダーをまたぐモデルの混在\n+## プロバイダをまたぐモデルの混在\n \n-モデルプロバイダー間の機能差異に注意しないと、エラーが発生する可能性があります。たとえば、OpenAI は structured outputs、マルチモーダル入力、ホスト型の ファイル検索 と Web 検索 をサポートしますが、多くの他社プロバイダーはこれらの機能をサポートしていません。次の制限に注意してください:\n+モデルプロバイダ間の機能差に注意しないと、エラーが発生する場合があります。たとえば、OpenAI は structured outputs、マルチモーダル入力、ホスト型の ファイル検索 および Web 検索 をサポートしますが、他の多くのプロバイダはこれらの機能をサポートしていません。次の制約に注意してください。\n \n--   サポートしていない `tools` を理解しないプロバイダーに送らない\n--   テキストのみのモデルを呼び出す前に、マルチモーダル入力を除外する\n--   structured JSON 出力をサポートしていないプロバイダーは無効な JSON を生成することがある点に注意する\n\\ No newline at end of file\n+-  サポートされていない `tools` を理解しないプロバイダに送信しないでください\n+-  テキストのみのモデルを呼び出す前に、マルチモーダル入力をフィルタリングしてください\n+-  structured JSON 出力をサポートしないプロバイダは、無効な JSON を生成することがある点に注意してください\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmodels%2Findex.md",
        "sha": "8d42962bd618c1aad67105a9ea0b0f6cc00c960c",
        "status": "modified"
      },
      {
        "additions": 14,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmodels%2Flitellm.md",
        "changes": 28,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fmodels%2Flitellm.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 14,
        "filename": "docs/ja/models/litellm.md",
        "patch": "@@ -2,33 +2,33 @@\n search:\n   exclude: true\n ---\n-# LiteLLM 経由で任意のモデルの使用\n+# LiteLLM によるあらゆるモデルの利用\n \n !!! note\n \n-    LiteLLM 統合は beta です。特に小規模なモデルプロバイダーでは問題が発生する可能性があります。問題があれば [Github issues](https://github.com/openai/openai-agents-python/issues) からご報告ください。迅速に修正します。\n+    LiteLLM との統合はベータ版です。特に小規模なモデルプロバイダーで問題が発生する可能性があります。問題は [GitHub の issues](https://github.com/openai/openai-agents-python/issues) からご報告ください。すぐに修正します。\n \n-[LiteLLM](https://docs.litellm.ai/docs/) は、単一のインターフェースで 100+ モデルを利用できるライブラリです。Agents SDK に LiteLLM 統合を追加し、任意の AI モデルを使用できるようにしました。\n+[LiteLLM](https://docs.litellm.ai/docs/) は、単一のインターフェースで 100+ のモデルを利用できるライブラリです。Agents SDK では、任意の AI モデルを使えるように LiteLLM との統合を追加しました。\n \n ## セットアップ\n \n-`litellm` を利用可能にする必要があります。オプションの `litellm` 依存関係グループをインストールしてください。\n+`litellm` が利用可能である必要があります。オプションの `litellm` 依存関係グループをインストールしてください。\n \n ```bash\n pip install \"openai-agents[litellm]\"\n ```\n \n-完了したら、任意のエージェントで [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel] を使用できます。\n+完了したら、任意の エージェント で [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel] を使用できます。\n \n ## 例\n \n これは完全に動作する例です。実行すると、モデル名と API キーの入力を求められます。例えば次のように入力できます。\n \n-- `openai/gpt-4.1`（モデル）と OpenAI API キー\n-- `anthropic/claude-3-5-sonnet-20240620`（モデル）と Anthropic API キー\n-- など\n+-   モデルに `openai/gpt-4.1`、API キーに OpenAI の API キー\n+-   モデルに `anthropic/claude-3-5-sonnet-20240620`、API キーに Anthropic の API キー\n+-   など\n \n-LiteLLM でサポートされるモデルの完全な一覧は、[litellm providers docs](https://docs.litellm.ai/docs/providers) を参照してください。\n+LiteLLM でサポートされているモデルの完全な一覧は、[litellm providers docs](https://docs.litellm.ai/docs/providers) を参照してください。\n \n ```python\n from __future__ import annotations\n@@ -76,9 +76,9 @@ if __name__ == \"__main__\":\n     asyncio.run(main(model, api_key))\n ```\n \n-## 使用データの追跡\n+## 使用状況データの追跡\n \n-LiteLLM のレスポンスを Agents SDK の使用状況メトリクスに反映させたい場合は、エージェント作成時に `ModelSettings(include_usage=True)` を渡してください。\n+LiteLLM のレスポンスで Agents SDK の使用状況メトリクスを集計したい場合は、エージェント作成時に `ModelSettings(include_usage=True)` を渡してください。\n \n ```python\n from agents import Agent, ModelSettings\n@@ -91,14 +91,14 @@ agent = Agent(\n )\n ```\n \n-`include_usage=True` を指定すると、LiteLLM リクエストは組み込みの OpenAI モデルと同様に、`result.context_wrapper.usage` 経由でトークン数とリクエスト数を報告します。\n+`include_usage=True` を指定すると、LiteLLM のリクエストは組み込みの OpenAI モデルと同様に、`result.context_wrapper.usage` を通じてトークン数とリクエスト数を報告します。\n \n ## トラブルシューティング\n \n-LiteLLM のレスポンスで Pydantic シリアライザーの警告が表示される場合は、次を設定して小さな互換パッチを有効にしてください。\n+LiteLLM のレスポンスで Pydantic のシリアライザーに関する警告が表示される場合は、次を設定して小さな互換パッチを有効にしてください。\n \n ```bash\n export OPENAI_AGENTS_ENABLE_LITELLM_SERIALIZER_PATCH=true\n ```\n \n-このオプトインフラグは、既知の LiteLLM シリアライザー警告を抑制し、通常の動作は維持します。不要であれば無効（未設定または `false`）にしてください。\n\\ No newline at end of file\n+このオプトインのフラグは、既知の LiteLLM のシリアライザー警告を抑制し、通常の挙動は維持します。不要な場合は無効化（未設定または `false`）してください。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmodels%2Flitellm.md",
        "sha": "157e384ea6e46b49a70188a4028943a364aa246f",
        "status": "modified"
      },
      {
        "additions": 21,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmulti_agent.md",
        "changes": 42,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fmulti_agent.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 21,
        "filename": "docs/ja/multi_agent.md",
        "patch": "@@ -2,40 +2,40 @@\n search:\n   exclude: true\n ---\n-# 複数のエージェントのオーケストレーション\n+# 複数の エージェント のオーケストレーション\n \n-オーケストレーションとは、アプリ内でのエージェントの流れを指します。どのエージェントがどの順序で動き、次に何をするかをどのように決めるのか、ということです。エージェントをオーケストレーションする主な方法は 2 つあります。\n+オーケストレーションとは、アプリ内での エージェント の流れのことです。どの エージェント を、どの順番で実行し、その後何をするかをどのように決めるのか。エージェント をオーケストレーションする主な方法は 2 つあります。\n \n-1. LLM に意思決定を任せる: LLM の知性を用いて計画・推論し、その結果に基づいて次の手順を決めます。\n-2. コードによるオーケストレーション: コードでエージェントの流れを決定します。\n+1. LLM に意思決定させる: LLM の知能を使って、計画・推論し、その上で取るべき手順を決定します。\n+2. コードでオーケストレーションする: コードによって エージェント の流れを決定します。\n \n-これらのパターンは組み合わせて使用できます。それぞれにトレードオフがあります（以下参照）。\n+これらのパターンは組み合わせて使えます。各手法にはそれぞれトレードオフがあります（以下に記載）。\n \n ## LLM によるオーケストレーション\n \n-エージェントは、指示、ツール、ハンドオフを備えた LLM です。つまり、オープンエンドなタスクが与えられたとき、LLM はツールを使って行動やデータ取得を行い、ハンドオフでサブエージェントにタスクを委譲しながら、自律的にタスク達成の計画を立てられます。たとえば、リサーチ用のエージェントには次のようなツールを備えられます。\n+エージェント は、instructions、tools、handoffs を備えた LLM です。これは、オープンエンドなタスクが与えられたときに、LLM が自律的に計画を立て、ツールを使ってアクションを実行してデータを取得し、ハンドオフを使ってサブエージェントにタスクを委譲できることを意味します。たとえば、リサーチ用の エージェント は次のようなツールを備えられます。\n \n - Web 検索でオンライン情報を探す\n-- ファイル検索と取得でプロプライエタリなデータや接続を横断的に検索する\n-- コンピュータ操作でコンピュータ上のアクションを実行する\n+- ファイル検索と取得でプロプライエタリデータや接続を検索する\n+- コンピュータ操作 でコンピュータ上のアクションを実行する\n - コード実行でデータ分析を行う\n-- 計画立案、レポート作成などに長けた特化型エージェントへのハンドオフ\n+- 計画立案、レポート作成などに優れた特化型 エージェント へのハンドオフ\n \n-このパターンは、タスクがオープンエンドで、LLM の知性に依存したい場合に適しています。重要な戦術は次のとおりです。\n+このパターンは、タスクがオープンエンドで、LLM の知能に依存したい場合に適しています。ここで重要な戦術は次のとおりです。\n \n-1. 良いプロンプトに投資しましょう。利用可能なツール、その使い方、どのパラメーターの範囲で動作すべきかを明確にします。\n-2. アプリを監視して反復改善しましょう。どこで問題が起こるかを把握し、プロンプトを改善します。\n-3. エージェントが内省して改善できるようにしましょう。例えば、ループで実行して自己批評させる、またはエラーメッセージを与えて改善させます。\n-4. 何でもこなす汎用エージェントではなく、単一のタスクに秀でた特化型エージェントを用意しましょう。\n-5. [evals](https://platform.openai.com/docs/guides/evals) に投資しましょう。これによりエージェントを訓練し、タスクの遂行能力を高められます。\n+1. 良いプロンプトに投資する。利用可能なツール、使用方法、遵守すべきパラメーターを明確にします。\n+2. アプリを監視して反復する。問題が起きる箇所を把握し、プロンプトを改善していきます。\n+3. エージェント に内省と改善を許可する。例えばループで実行して自己批評させる、あるいはエラーメッセージを与えて改善させます。\n+4. 何でもできる汎用 エージェント を想定するのではなく、1 つのタスクに秀でた特化型 エージェント を用意する。\n+5. [evals](https://platform.openai.com/docs/guides/evals) に投資する。これにより エージェント を訓練して、タスク遂行能力を向上できます。\n \n ## コードによるオーケストレーション\n \n-LLM によるオーケストレーションは強力ですが、コードによるオーケストレーションは速度、コスト、パフォーマンスの観点で、より決定的かつ予測可能にできます。一般的なパターンは次のとおりです。\n+LLM によるオーケストレーションは強力ですが、コードによるオーケストレーションは、速度・コスト・パフォーマンスの面で、より決定論的かつ予測可能にできます。一般的なパターンは次のとおりです。\n \n-- [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) を使って、コードで検査できる適切な形式のデータを生成する。例えば、エージェントにタスクをいくつかのカテゴリーに分類させ、そのカテゴリーに基づいて次のエージェントを選びます。\n-- あるエージェントの出力を変換して次のエージェントの入力にし、複数のエージェントを連鎖させる。ブログ記事の執筆を、リサーチ、アウトライン作成、本文執筆、批評、改善といった一連の手順に分解できます。\n-- タスクを実行するエージェントと、それを評価してフィードバックを返すエージェントを `while` ループで回し、評価者が所定の基準を満たしたと判断するまで繰り返す。\n-- 複数のエージェントを並列実行する（例えば `asyncio.gather` のような Python の基本コンポーネントを使用）。互いに依存しない複数タスクがある場合、速度向上に有効です。\n+- [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) を使って、コードで検査可能な 適切な形式のデータ を生成する。例えば、エージェント にタスクをいくつかの カテゴリー に分類させ、その カテゴリー に基づいて次の エージェント を選ぶ、といった方法です。\n+- 複数の エージェント を、前の出力を次の入力に変換して連鎖させる。ブログ記事の執筆のようなタスクを、リサーチ、アウトライン作成、本文執筆、批評、改善といった一連のステップに分解できます。\n+- タスクを実行する エージェント と、それを評価してフィードバックする エージェント を `while` ループで回し、評価者が所定の基準を満たしたと判断するまで繰り返す。\n+- 複数の エージェント を並列実行する（例: `asyncio.gather` のような Python の基本コンポーネントを使用）。相互依存しない複数タスクがある場合、速度向上に有用です。\n \n-[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns) に複数のコード例があります。\n\\ No newline at end of file\n+[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns) に多数の code examples があります。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fmulti_agent.md",
        "sha": "8ae31312129c705eeabf860db02108264780fe4c",
        "status": "modified"
      },
      {
        "additions": 17,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fquickstart.md",
        "changes": 34,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 17,
        "filename": "docs/ja/quickstart.md",
        "patch": "@@ -6,7 +6,7 @@ search:\n \n ## プロジェクトと仮想環境の作成\n \n-この操作は 1 回だけで大丈夫です。\n+これは最初の一度だけ行います。\n \n ```bash\n mkdir my_project\n@@ -16,7 +16,7 @@ python -m venv .venv\n \n ### 仮想環境の有効化\n \n-新しいターミナルセッションを開始するたびに実行します。\n+新しいターミナル セッションを開始するたびに実行します。\n \n ```bash\n source .venv/bin/activate\n@@ -30,15 +30,15 @@ pip install openai-agents # or `uv add openai-agents`, etc\n \n ### OpenAI API キーの設定\n \n-まだお持ちでない場合は、[こちらの手順](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)に従って OpenAI API キーを作成してください。\n+まだお持ちでない場合は、[これらの手順](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)に従って OpenAI API キーを作成してください。\n \n ```bash\n export OPENAI_API_KEY=sk-...\n ```\n \n ## 最初の エージェント の作成\n \n-エージェントは instructions、名前、そして任意の config（例えば `model_config`）で定義します。\n+エージェント は instructions、名前、任意の設定（例えば `model_config`）で定義します。\n \n ```python\n from agents import Agent\n@@ -49,9 +49,9 @@ agent = Agent(\n )\n ```\n \n-## さらにエージェントの追加\n+## いくつかの エージェント の追加\n \n-追加の エージェント は同様に定義できます。`handoff_descriptions` はハンドオフのルーティングを判断するための追加コンテキストを提供します。\n+追加の エージェント も同じ方法で定義できます。`handoff_descriptions` はハンドオフ ルーティングを判断するための追加コンテキストを提供します。\n \n ```python\n from agents import Agent\n@@ -71,7 +71,7 @@ math_tutor_agent = Agent(\n \n ## ハンドオフの定義\n \n-各 エージェント で、タスクを進める方法を決定するために エージェント が選択できる送信側ハンドオフのオプションのインベントリを定義できます。\n+各 エージェント で、タスクを前進させる方法を選ぶために利用できる、発信側ハンドオフ オプションの在庫を定義できます。\n \n ```python\n triage_agent = Agent(\n@@ -81,9 +81,9 @@ triage_agent = Agent(\n )\n ```\n \n-## エージェントのオーケストレーションの実行\n+## エージェント オーケストレーションの実行\n \n-ワークフローが実行され、トリアージ エージェント が 2 つのスペシャリスト エージェント 間を正しくルーティングすることを確認します。\n+ワークフローが実行され、トリアージ エージェント が 2 つの専門 エージェント 間を正しくルーティングすることを確認しましょう。\n \n ```python\n from agents import Runner\n@@ -95,7 +95,7 @@ async def main():\n \n ## ガードレールの追加\n \n-入力または出力に対して実行するカスタム ガードレールを定義できます。\n+入力または出力に対してカスタム ガードレールを定義できます。\n \n ```python\n from agents import GuardrailFunctionOutput, Agent, Runner\n@@ -121,9 +121,9 @@ async def homework_guardrail(ctx, agent, input_data):\n     )\n ```\n \n-## すべてをまとめる\n+## すべてを組み合わせる\n \n-ハンドオフと入力用ガードレールを使用して、すべてをまとめてワークフロー全体を実行します。\n+ハンドオフと入力 ガードレールを使って、すべてを組み合わせてワークフロー全体を実行しましょう。\n \n ```python\n from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\n@@ -192,12 +192,12 @@ if __name__ == \"__main__\":\n \n ## トレースの表示\n \n-エージェントの実行中に何が起きたかを確認するには、[OpenAI ダッシュボードの Trace viewer](https://platform.openai.com/traces) に移動してエージェント実行のトレースを表示します。\n+エージェント の実行中に何が起きたかを確認するには、[OpenAI ダッシュボードの Trace viewer](https://platform.openai.com/traces)に移動して、エージェント 実行のトレースを表示してください。\n \n ## 次のステップ\n \n-より複雑な エージェント 的フローの作り方を学びます:\n+より複雑なエージェント フローの構築方法を学びましょう:\n \n-- [エージェント](agents.md) の設定方法について学びます。\n-- [エージェントの実行](running_agents.md) について学びます。\n-- [tools](tools.md)、[ガードレール](guardrails.md)、[モデル](models/index.md) について学びます。\n\\ No newline at end of file\n+- Learn about how to configure [エージェント](agents.md).\n+- Learn about [エージェントの実行](running_agents.md).\n+- Learn about [ツール](tools.md)、[ガードレール](guardrails.md)、[モデル](models/index.md)。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fquickstart.md",
        "sha": "dc47a72d1ac10a7fea173ecacb9855d2b0d25a49",
        "status": "modified"
      },
      {
        "additions": 40,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frealtime%2Fguide.md",
        "changes": 80,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Frealtime%2Fguide.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 40,
        "filename": "docs/ja/realtime/guide.md",
        "patch": "@@ -4,65 +4,65 @@ search:\n ---\n # ガイド\n \n-このガイドでは、 OpenAI Agents SDK の realtime 機能を使って音声対応の AI エージェントを構築する方法を詳しく説明します。\n+このガイドでは、OpenAI Agents SDK のリアルタイム機能を使って音声対応の AI エージェントを構築する方法を詳しく説明します。\n \n !!! warning \"ベータ機能\"\n-Realtime エージェントはベータ版です。実装の改善に伴い、後方互換性のない変更が入る可能性があります。\n+リアルタイム エージェントはベータ版です。実装の改善に伴い、破壊的変更が発生する可能性があります。\n \n ## 概要\n \n-Realtime エージェントは、音声とテキストの入力をリアルタイムに処理し、リアルタイム音声で応答する双方向の会話フローを実現します。 OpenAI の Realtime API との永続接続を維持し、低遅延で自然な音声対話と、割り込みへのスムーズな対応を可能にします。\n+リアルタイム エージェントは、音声とテキストの入力をリアルタイムに処理し、リアルタイム音声で応答する会話フローを可能にします。OpenAI の Realtime API との永続的な接続を維持し、低レイテンシで自然な音声会話を実現し、割り込みにも優雅に対応します。\n \n ## アーキテクチャ\n \n ### コアコンポーネント\n \n-realtime システムは、次の主要コンポーネントで構成されます。\n+リアルタイム システムは、いくつかの重要なコンポーネントで構成されます。\n \n--   **RealtimeAgent**: instructions、ツール、ハンドオフで構成されたエージェントです。\n--   **RealtimeRunner**: 設定を管理します。`runner.run()` を呼び出すとセッションを取得できます。\n--   **RealtimeSession**: 単一の対話セッションです。通常、 ユーザー が会話を開始するたびに作成し、会話が終了するまで維持します。\n--   **RealtimeModel**: 基盤となるモデルのインターフェース（通常は OpenAI の WebSocket 実装）です。\n+-   **RealtimeAgent**: instructions、tools、handoffs で構成されたエージェント。\n+-   **RealtimeRunner**: 設定を管理します。`runner.run()` を呼び出してセッションを取得できます。\n+-   **RealtimeSession**: 単一の対話セッション。通常は ユーザー が会話を開始するたびに 1 つ作成し、会話が完了するまで維持します。\n+-   **RealtimeModel**: 基盤となるモデル インターフェース（通常は OpenAI の WebSocket 実装）\n \n ### セッションフロー\n \n-典型的な realtime セッションは次のフローに従います。\n+典型的なリアルタイム セッションの流れは次のとおりです。\n \n-1. instructions、ツール、ハンドオフを用いて **RealtimeAgent を作成** します。\n-2. エージェントと設定オプションを用いて **RealtimeRunner をセットアップ** します。\n-3. `await runner.run()` を使って **セッションを開始** し、 RealtimeSession を受け取ります。\n-4. `send_audio()` または `send_message()` を使って **音声またはテキストのメッセージを送信** します。\n-5. セッションを反復処理して **イベントをリッスン** します。イベントには音声出力、文字起こし、ツール呼び出し、ハンドオフ、エラーなどが含まれます。\n-6. ユーザー がエージェントの発話に被せたときに **割り込みを処理** します。現在の音声生成は自動で停止します。\n+1. **RealtimeAgent を作成** し、instructions、tools、handoffs を設定します。\n+2. **RealtimeRunner をセットアップ** し、エージェントと設定オプションを渡します。\n+3. **セッションを開始** します。`await runner.run()` を使用すると RealtimeSession が返ります。\n+4. **音声またはテキスト メッセージを送信** します。`send_audio()` または `send_message()` を使用します。\n+5. **イベントを監視** します。セッションを反復処理して、音声出力、書き起こし、ツール呼び出し、ハンドオフ、エラーなどのイベントを受信します。\n+6. **割り込みを処理** します。ユーザー がエージェントの発話にかぶせた場合、現在の音声生成は自動的に停止します。\n \n-セッションは会話履歴を保持し、 realtime モデルとの永続接続を管理します。\n+セッションは会話履歴を保持し、リアルタイム モデルとの永続的な接続を管理します。\n \n ## エージェントの設定\n \n-RealtimeAgent は、通常の Agent クラスと同様に動作しますが、いくつか重要な違いがあります。 API の詳細は [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] のリファレンスをご覧ください。\n+RealtimeAgent は通常の Agent クラスと同様に動作しますが、いくつか重要な違いがあります。API の詳細は [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] の API リファレンスをご覧ください。\n \n-通常のエージェントとの差分:\n+通常のエージェントとの主な違い:\n \n--   モデルの選択はエージェント レベルではなくセッション レベルで設定します。\n--   structured outputs はサポートされていません（`outputType` は未対応）。\n--   音声はエージェントごとに設定できますが、最初のエージェントが発話した後は変更できません。\n--   ツール、ハンドオフ、instructions などのその他の機能は同様に動作します。\n+-   モデルの選択はエージェント レベルではなく、セッション レベルで設定します。\n+-   structured outputs はサポートされません（`outputType` はサポートされません）。\n+-   ボイスはエージェントごとに設定できますが、最初のエージェントが話し始めた後は変更できません。\n+-   その他の機能（ツール、ハンドオフ、instructions）は同様に動作します。\n \n ## セッションの設定\n \n ### モデル設定\n \n-セッション設定では、基盤となる realtime モデルの動作を制御できます。モデル名（例: `gpt-realtime`）、音声（alloy、echo、fable、onyx、nova、shimmer）の選択、サポートするモダリティ（テキストや音声）を設定できます。音声フォーマットは入力・出力ともに設定可能で、既定は PCM16 です。\n+セッション設定では、基盤となるリアルタイム モデルの動作を制御できます。モデル名（`gpt-realtime` など）、ボイスの選択（alloy、echo、fable、onyx、nova、shimmer）、対応するモダリティ（テキストおよび/または音声）を設定できます。音声フォーマットは入力と出力の両方で設定でき、デフォルトは PCM16 です。\n \n ### 音声設定\n \n-音声設定では、セッションが音声入力と出力をどのように処理するかを制御します。 Whisper のようなモデルを使って入力音声の文字起こしを設定し、言語の優先設定や、ドメイン固有用語の精度を高めるための文字起こし用プロンプトを指定できます。ターン検出設定では、音声活動検出のしきい値、無音時間、検出された発話前後のパディングなど、エージェントが応答を開始・停止すべきタイミングを調整できます。\n+音声設定では、セッションが音声入力と出力をどのように扱うかを制御します。Whisper などのモデルを使用した入力音声の書き起こし、言語設定、ドメイン特有の用語の精度を高めるための書き起こしプロンプトを設定できます。ターン検出設定では、エージェントが応答を開始・停止すべきタイミングを制御し、音声活動検出しきい値、無音時間、検出された発話の前後に付けるパディングなどのオプションを提供します。\n \n ## ツールと関数\n \n ### ツールの追加\n \n-通常のエージェントと同様に、realtime エージェントは会話中に実行される 関数ツール をサポートします。\n+通常のエージェントと同様に、リアルタイム エージェントは会話中に実行される 関数ツール をサポートします。\n \n ```python\n from agents import function_tool\n@@ -90,7 +90,7 @@ agent = RealtimeAgent(\n \n ### ハンドオフの作成\n \n-ハンドオフにより、会話を専門のエージェント間で引き継ぐことができます。\n+ハンドオフにより、専門化されたエージェント間で会話を移譲できます。\n \n ```python\n from agents.realtime import realtime_handoff\n@@ -119,10 +119,10 @@ main_agent = RealtimeAgent(\n \n ## イベント処理\n \n-セッションはイベントを ストリーミング し、セッションオブジェクトを反復処理してリッスンできます。イベントには、音声出力チャンク、文字起こし結果、ツール実行の開始・終了、エージェント間のハンドオフ、エラーなどが含まれます。特に処理すべき主なイベントは以下です。\n+セッションは、セッション オブジェクトを反復処理することで監視できるイベントをストリーミングします。イベントには、音声出力チャンク、書き起こし結果、ツール実行の開始と終了、エージェントのハンドオフ、エラーが含まれます。主に対応すべきイベントは次のとおりです。\n \n--   **audio**: エージェントの応答からの Raw 音声データ\n--   **audio_end**: エージェントの発話が完了\n+-   **audio**: エージェントの応答からの raw な音声データ\n+-   **audio_end**: エージェントの発話が終了\n -   **audio_interrupted**: ユーザー がエージェントを割り込み\n -   **tool_start/tool_end**: ツール実行のライフサイクル\n -   **handoff**: エージェントのハンドオフが発生\n@@ -132,9 +132,9 @@ main_agent = RealtimeAgent(\n \n ## ガードレール\n \n-Realtime エージェントでは出力の ガードレール のみサポートされます。パフォーマンス低下を避けるため、これらのガードレールはデバウンスされ、（毎語ではなく）定期的に実行されます。既定のデバウンス長は 100 文字ですが、設定可能です。\n+リアルタイム エージェントでサポートされるのは出力 ガードレール のみです。これらのガードレールはデバウンスされ、リアルタイム生成中のパフォーマンス問題を避けるために（毎語ではなく）定期的に実行されます。デフォルトのデバウンス長は 100 文字ですが、変更可能です。\n \n-ガードレールは `RealtimeAgent` に直接アタッチするか、セッションの `run_config` を介して提供できます。両方のソースから提供されたガードレールは併用されます。\n+ガードレールは `RealtimeAgent` に直接付与するか、セッションの `run_config` から提供できます。両方のソースからのガードレールは併せて実行されます。\n \n ```python\n from agents.guardrail import GuardrailFunctionOutput, OutputGuardrail\n@@ -152,19 +152,19 @@ agent = RealtimeAgent(\n )\n ```\n \n-ガードレールがトリガーされると、`guardrail_tripped` イベントが生成され、エージェントの現在の応答を中断することがあります。デバウンス動作により、安全性とリアルタイム性能要件のバランスを取ります。テキストエージェントとは異なり、realtime エージェントはガードレール発火時に例外を発生させることは **ありません**。\n+ガードレールが発火すると、`guardrail_tripped` イベントを生成し、エージェントの現在の応答を割り込むことがあります。デバウンスの動作は、安全性とリアルタイム性能要件のバランスを取るのに役立ちます。テキスト エージェントと異なり、リアルタイム エージェントはガードレールが発火しても **Exception** をスローしません。\n \n ## 音声処理\n \n-[`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio] を使って音声を、[`session.send_message()`][agents.realtime.session.RealtimeSession.send_message] を使ってテキストをセッションへ送信します。\n+[`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio] を使って音声をセッションに送信するか、[`session.send_message()`][agents.realtime.session.RealtimeSession.send_message] を使ってテキストを送信します。\n \n-音声出力については、`audio` イベントをリッスンし、好みの音声ライブラリで再生してください。ユーザー がエージェントを割り込んだ際にすぐ再生を止め、キュー済みの音声をクリアできるよう、`audio_interrupted` イベントも必ず監視してください。\n+音声出力に対しては、`audio` イベントを監視し、任意の音声ライブラリで音声データを再生してください。ユーザー がエージェントを割り込んだ際に即座に再生を停止し、キューにある音声をクリアするため、`audio_interrupted` イベントを必ず監視してください。\n \n ## SIP 連携\n \n-[Realtime Calls API](https://platform.openai.com/docs/guides/realtime-sip) 経由で着信した電話に realtime エージェントを接続できます。 SDK は [`OpenAIRealtimeSIPModel`][agents.realtime.openai_realtime.OpenAIRealtimeSIPModel] を提供しており、 SIP 上でメディアをネゴシエートしつつ、同じエージェントフローを再利用します。\n+[Realtime Calls API](https://platform.openai.com/docs/guides/realtime-sip) 経由で着信する電話にリアルタイム エージェントを接続できます。SDK には [`OpenAIRealtimeSIPModel`][agents.realtime.openai_realtime.OpenAIRealtimeSIPModel] が用意されており、SIP 上でメディアをネゴシエートしつつ、同じエージェント フローを再利用します。\n \n-使用するには、モデルインスタンスを runner に渡し、セッション開始時に SIP の `call_id` を指定します。 Call ID は、着信を通知する Webhook から配信されます。\n+使用するには、モデル インスタンスを runner に渡し、セッション開始時に SIP の `call_id` を指定します。コール ID は、着信を知らせる webhook によって渡されます。\n \n ```python\n from agents.realtime import RealtimeAgent, RealtimeRunner\n@@ -187,19 +187,19 @@ async with await runner.run(\n         ...\n ```\n \n-発信者が電話を切ると、 SIP セッションは終了し、 realtime 接続は自動的にクローズされます。完全なテレフォニーの例は [`examples/realtime/twilio_sip`](https://github.com/openai/openai-agents-python/tree/main/examples/realtime/twilio_sip) を参照してください。\n+発信者が電話を切ると、SIP セッションは終了し、リアルタイム接続は自動的に閉じられます。完全なテレフォニーのサンプルについては、[`examples/realtime/twilio_sip`](https://github.com/openai/openai-agents-python/tree/main/examples/realtime/twilio_sip) を参照してください。\n \n ## モデルへの直接アクセス\n \n-基盤となるモデルにアクセスして、カスタムリスナーの追加や高度な操作を実行できます。\n+基盤となるモデルにアクセスして、カスタム リスナーの追加や高度な操作を行うことができます。\n \n ```python\n # Add a custom listener to the model\n session.model.add_listener(my_custom_listener)\n ```\n \n-これにより、接続を低レベルで制御する必要がある高度なユースケース向けに、[`RealtimeModel`][agents.realtime.model.RealtimeModel] インターフェースへ直接アクセスできます。\n+これにより、高度なユースケース向けに接続をより低レベルで制御できる [`RealtimeModel`][agents.realtime.model.RealtimeModel] インターフェースへ直接アクセスできます。\n \n ## コード例\n \n-完全に動作するサンプルは、 UI コンポーネントあり・なしのデモを含む [examples/realtime ディレクトリ](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) を参照してください。\n\\ No newline at end of file\n+動作する完全なサンプルは、UI コンポーネントの有無それぞれのデモを含む [examples/realtime ディレクトリ](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) を参照してください。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frealtime%2Fguide.md",
        "sha": "7afc04a33d391ac8774c87f960005394ceecfd0a",
        "status": "modified"
      },
      {
        "additions": 24,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frealtime%2Fquickstart.md",
        "changes": 48,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Frealtime%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 24,
        "filename": "docs/ja/realtime/quickstart.md",
        "patch": "@@ -4,20 +4,20 @@ search:\n ---\n # クイックスタート\n \n-リアルタイム エージェントは、OpenAI の Realtime API を使用して AI エージェントとの音声対話を可能にします。このガイドでは、最初のリアルタイム音声エージェントの作成手順を説明します。\n+リアルタイム エージェントは、 OpenAI の Realtime API を使って AI エージェントとの音声会話を可能にします。本ガイドでは、最初のリアルタイム音声エージェントの作成手順を説明します。\n \n !!! warning \"ベータ機能\"\n-リアルタイム エージェントはベータ版です。実装の改善に伴い、破壊的変更が発生する可能性があります。\n+リアルタイム エージェントはベータ版です。実装の改善に伴い、破壊的変更が発生する場合があります。\n \n ## 前提条件\n \n--   Python 3.9 以上\n--   OpenAI API キー\n--   OpenAI Agents SDK の基本的な理解\n+- Python 3.9 以上\n+- OpenAI API キー\n+- OpenAI Agents SDK の基本的な知識\n \n ## インストール\n \n-まだインストールしていない場合は、OpenAI Agents SDK をインストールします:\n+まだの場合は、 OpenAI Agents SDK をインストールします:\n \n ```bash\n pip install openai-agents\n@@ -41,7 +41,7 @@ agent = RealtimeAgent(\n )\n ```\n \n-### 3. ランナーの設定\n+### 3. ランナーのセットアップ\n \n ```python\n runner = RealtimeRunner(\n@@ -111,7 +111,7 @@ def _truncate_str(s: str, max_length: int) -> str:\n \n ## 完全なコード例\n \n-以下は動作する完全な例です:\n+完全に動作するコード例は次のとおりです:\n \n ```python\n import asyncio\n@@ -188,34 +188,34 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-## 設定オプション\n+## 構成オプション\n \n ### モデル設定\n \n--   `model_name`: 利用可能なリアルタイム モデルから選択します (例: `gpt-realtime`)\n--   `voice`: 音声を選択します (`alloy`, `echo`, `fable`, `onyx`, `nova`, `shimmer`)\n--   `modalities`: テキストまたはオーディオを有効化します (`[\"text\"]` または `[\"audio\"]`)\n+- `model_name`: 利用可能なリアルタイム モデルから選択 (例: `gpt-realtime`)\n+- `voice`: 音声の選択 (`alloy`, `echo`, `fable`, `onyx`, `nova`, `shimmer`)\n+- `modalities`: テキストまたは音声を有効化 (`[\"text\"]` または `[\"audio\"]`)\n \n ### オーディオ設定\n \n--   `input_audio_format`: 入力オーディオの形式 (`pcm16`, `g711_ulaw`, `g711_alaw`)\n--   `output_audio_format`: 出力オーディオの形式\n--   `input_audio_transcription`: 文字起こしの設定\n+- `input_audio_format`: 入力オーディオの形式 (`pcm16`, `g711_ulaw`, `g711_alaw`)\n+- `output_audio_format`: 出力オーディオの形式\n+- `input_audio_transcription`: 文字起こしの構成\n \n ### ターン検出\n \n--   `type`: 検出方法 (`server_vad`, `semantic_vad`)\n--   `threshold`: 音声活動のしきい値 (0.0–1.0)\n--   `silence_duration_ms`: ターン終了を検出する無音時間\n--   `prefix_padding_ms`: 発話前のオーディオ パディング\n+- `type`: 検出方法 (`server_vad`, `semantic_vad`)\n+- `threshold`: 音声活動の閾値 ( 0.0-1.0 )\n+- `silence_duration_ms`: ターン終了検出のための無音継続時間\n+- `prefix_padding_ms`: 発話前のオーディオ パディング\n \n ## 次のステップ\n \n--   [リアルタイム エージェントの詳細](guide.md)\n--   [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) フォルダの動作するサンプルを確認\n--   エージェントにツールを追加\n--   エージェント間のハンドオフを実装\n--   安全のためのガードレールを設定\n+- [リアルタイム エージェントについてさらに学ぶ](guide.md)\n+- 動作するサンプルコードは [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) フォルダにあります\n+- エージェントにツールを追加\n+- エージェント間のハンドオフを実装\n+- 安全のためのガードレールを設定\n \n ## 認証\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frealtime%2Fquickstart.md",
        "sha": "974cb16506df3dfbb20f761353de67f170bdb2c2",
        "status": "modified"
      },
      {
        "additions": 14,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frelease.md",
        "changes": 28,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Frelease.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 14,
        "filename": "docs/ja/release.md",
        "patch": "@@ -4,49 +4,49 @@ search:\n ---\n # リリースプロセス/変更履歴\n \n-本プロジェクトは、`0.Y.Z` 形式を用いた、やや修正したセマンティック バージョニングに従います。先頭の `0` は、SDK がなお急速に進化していることを示します。各コンポーネントの増分は以下のとおりです。\n+このプロジェクトは、`0.Y.Z` という形式を用いた、やや変更したセマンティック バージョニングに従います。先頭の `0` は、この SDK がまだ急速に進化していることを示します。各コンポーネントの増分は次のとおりです。\n \n ## マイナー（`Y`）バージョン\n \n-ベータでないパブリック インターフェースに対する**破壊的変更**がある場合、マイナー バージョン `Y` を増やします。例えば、`0.0.x` から `0.1.x` への更新には破壊的変更が含まれる可能性があります。\n+ベータではない公開インターフェースへの **破壊的変更** のために、マイナー バージョン `Y` を増やします。たとえば、`0.0.x` から `0.1.x` への移行には破壊的変更が含まれる場合があります。\n \n-破壊的変更を避けたい場合は、プロジェクトで `0.0.x` バージョンにピン留めすることを推奨します。\n+破壊的変更を避けたい場合は、プロジェクトで `0.0.x` バージョンに固定することをおすすめします。\n \n ## パッチ（`Z`）バージョン\n \n-後方互換な変更については `Z` を増やします:\n+破壊的でない変更では `Z` を増やします。\n \n - バグ修正\n - 新機能\n-- プライベート インターフェースの変更\n+- 非公開インターフェースの変更\n - ベータ機能の更新\n \n ## 破壊的変更の変更履歴\n \n ### 0.6.0\n \n-このバージョンでは、デフォルトの ハンドオフ 履歴が、生の ユーザー/アシスタント のターンを露出するのではなく、1 つのアシスタント メッセージにまとめられるようになり、下流の エージェント に簡潔で予測可能な要約を提供します。\n-- 既存の単一メッセージの ハンドオフ 文字起こしは、デフォルトで `<CONVERSATION HISTORY>` ブロックの前に \"For context, here is the conversation so far between the user and the previous agent:\" で始まるようになったため、下流の エージェント は明確にラベル付けされた要約を受け取れます\n+このバージョンでは、既定のハンドオフ履歴は、生の user/assistant のターンを公開するのではなく、1 つのアシスタント メッセージにまとめてパッケージ化され、下流の エージェント に簡潔で予測可能な要約を提供します。\n+- 既存の単一メッセージによるハンドオフの書き起こしは、既定で `<CONVERSATION HISTORY>` ブロックの前に \"For context, here is the conversation so far between the user and the previous agent:\" で始まるようになったため、下流の エージェント は明確にラベル付けされた要約を受け取れます\n \n ### 0.5.0\n \n-このバージョンは目に見える破壊的変更を導入していませんが、新機能と、内部実装におけるいくつかの重要な更新が含まれます:\n+このバージョンは目に見える破壊的変更を導入していませんが、新機能と内部的にいくつかの重要な更新を含みます。\n \n-- `RealtimeRunner` が [SIP protocol connections](https://platform.openai.com/docs/guides/realtime-sip) を扱えるようサポートを追加\n-- `Runner#run_sync` の内部ロジックを大幅に見直し、Python 3.14 との互換性を向上\n+- `RealtimeRunner` に [SIP プロトコル接続](https://platform.openai.com/docs/guides/realtime-sip) への対応を追加\n+- Python 3.14 との互換性のために `Runner#run_sync` の内部ロジックを大幅に改訂\n \n ### 0.4.0\n \n-このバージョンでは、[openai](https://pypi.org/project/openai/) パッケージの v1.x バージョンはサポートされなくなりました。openai v2.x を本 SDK と併用してください。\n+このバージョンでは、[openai](https://pypi.org/project/openai/) パッケージの v1.x バージョンはサポートされなくなりました。openai v2.x をこの SDK と合わせて使用してください。\n \n ### 0.3.0\n \n-このバージョンでは、Realtime API のサポートが gpt-realtime モデルおよびその API インターフェース（GA バージョン）に移行しました。\n+このバージョンでは、Realtime API のサポートが gpt-realtime モデルおよびその API インターフェース（GA バージョン）に移行します。\n \n ### 0.2.0\n \n-このバージョンでは、これまで `Agent` を引数として受け取っていた箇所の一部が、代わりに `AgentBase` を引数として受け取るようになりました。例えば、MCP サーバーの `list_tools()` 呼び出しです。これは純粋に型付けの変更であり、引き続き `Agent` オブジェクトを受け取ります。更新するには、`Agent` を `AgentBase` に置き換えて型エラーを解消してください。\n+このバージョンでは、以前は `Agent` を引数に取っていた箇所の一部が、代わりに `AgentBase` を引数に取るようになりました。たとえば、MCP サーバー の `list_tools()` 呼び出しなどです。これは純粋に型に関する変更であり、引き続き `Agent` オブジェクトを受け取ります。更新するには、`Agent` を `AgentBase` に置き換えて型エラーを修正してください。\n \n ### 0.1.0\n \n-このバージョンでは、[`MCPServer.list_tools()`][agents.mcp.server.MCPServer] に 2 つの新しいパラメーター `run_context` と `agent` が追加されました。`MCPServer` をサブクラス化するすべてのクラスに、これらのパラメーターを追加する必要があります。\n\\ No newline at end of file\n+このバージョンでは、[`MCPServer.list_tools()`][agents.mcp.server.MCPServer] に新しいパラメーターが 2 つ、`run_context` と `agent` が追加されました。`MCPServer` をサブクラス化するすべてのクラスに、これらのパラメーターを追加する必要があります。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frelease.md",
        "sha": "310a59c8990abeafe54b45ccd0db34fd75f53166",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frepl.md",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Frepl.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 3,
        "filename": "docs/ja/repl.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # REPL ユーティリティ\n \n-この SDK は、ターミナルで直接エージェントの動作を素早く対話的にテストできる `run_demo_loop` を提供します。\n+SDK は `run_demo_loop` を提供しており、ターミナル上で エージェント の挙動を素早く対話的にテストできます。\n \n \n ```python\n@@ -19,6 +19,6 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-`run_demo_loop` はループでユーザー入力を促し、ターン間で会話履歴を保持します。デフォルトでは、生成と同時にモデル出力をストリーミングします。上記の例を実行すると、`run_demo_loop` はインタラクティブなチャットセッションを開始します。継続的に入力を求め、各ターンの間で会話全体の履歴を記憶し（そのためエージェントは何が議論されたかを把握できます）、生成と同時にエージェントの応答をリアルタイムで自動的にストリーミングします。\n+`run_demo_loop` はループで ユーザー 入力を促し、ターン間で会話履歴を保持します。デフォルトでは、生成されると同時にモデルの出力を ストリーミング します。上のサンプルを実行すると、`run_demo_loop` が対話型チャットセッションを開始します。継続的に入力を尋ね、ターン間の会話履歴全体を記憶するため（これにより エージェント は何が話されたかを把握します）、生成と同時に エージェント の応答をリアルタイムで自動 ストリーミング します。\n \n-このチャットセッションを終了するには、`quit` または `exit` と入力して Enter キーを押すか、`Ctrl-D` のキーボードショートカットを使用します。\n\\ No newline at end of file\n+このチャットセッションを終了するには、`quit` または `exit` と入力して（Enter キーを押す）、または `Ctrl-D` キーボードショートカットを使用してください。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frepl.md",
        "sha": "8c55eb8951687c3bc16d36f0019fe11c107ef004",
        "status": "modified"
      },
      {
        "additions": 22,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fresults.md",
        "changes": 44,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fresults.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 22,
        "filename": "docs/ja/results.md",
        "patch": "@@ -2,57 +2,57 @@\n search:\n   exclude: true\n ---\n-# 実行結果\n+# 結果\n \n-`Runner.run` メソッドを呼び出すと、次のいずれかが得られます。\n+`Runner.run` メソッドを呼び出すと、次のいずれかが返ります。\n \n - [`RunResult`][agents.result.RunResult]（`run` または `run_sync` を呼び出した場合）\n - [`RunResultStreaming`][agents.result.RunResultStreaming]（`run_streamed` を呼び出した場合）\n \n-どちらも [`RunResultBase`][agents.result.RunResultBase] を継承しており、ほとんどの有用な情報はそこに含まれます。\n+どちらも [`RunResultBase`][agents.result.RunResultBase] を継承しており、ここに最も有用な情報が含まれます。\n \n ## 最終出力\n \n-[`final_output`][agents.result.RunResultBase.final_output] プロパティには、最後に実行された エージェント の最終出力が含まれます。これは次のいずれかです。\n+[`final_output`][agents.result.RunResultBase.final_output] プロパティには、最後に実行されたエージェントの最終出力が含まれます。これは次のいずれかです。\n \n-- 最後の エージェント に `output_type` が定義されていない場合は `str`\n-- エージェント に出力タイプが定義されている場合は `last_agent.output_type` 型のオブジェクト\n+- 最後のエージェントに `output_type` が定義されていない場合は `str`\n+- エージェントに出力タイプが定義されている場合は、型 `last_agent.output_type` のオブジェクト\n \n !!! note\n \n-    `final_output` の型は `Any` です。ハンドオフ の可能性があるため、静的な型付けはできません。ハンドオフ が発生すると、どの エージェント でも最後になり得るため、可能な出力タイプの集合を静的には把握できません。\n+    `final_output` は型 `Any` です。handoffs の可能性があるため、静的型付けはできません。handoffs が発生すると、どのエージェントが最後になるか分からないため、可能な出力タイプの集合を静的には特定できません。\n \n-## 次ターンの入力\n+## 次のターンへの入力\n \n-[`result.to_input_list()`][agents.result.RunResultBase.to_input_list] を使うと、実行結果を、最初に提供した元の入力に、エージェント 実行中に生成された項目を連結した入力リストへと変換できます。これにより、ある エージェント 実行の出力を別の実行に渡したり、ループで実行して毎回新しい ユーザー 入力を追加したりするのが便利になります。\n+[`result.to_input_list()`][agents.result.RunResultBase.to_input_list] を使うと、提供した元の入力と、エージェントの実行中に生成されたアイテムを連結した入力リストへ結果を変換できます。これにより、あるエージェント実行の出力を別の実行へ渡したり、ループで実行して毎回新しい ユーザー 入力を追加したりしやすくなります。\n \n ## 最後のエージェント\n \n-[`last_agent`][agents.result.RunResultBase.last_agent] プロパティには、最後に実行された エージェント が含まれます。アプリケーションによっては、これは次回 ユーザー が何かを入力する際に有用なことが多いです。たとえば、最前線のトリアージ エージェント から言語特化の エージェント にハンドオフ する場合、最後の エージェント を保存しておき、次回 ユーザー がメッセージを送る際に再利用できます。\n+[`last_agent`][agents.result.RunResultBase.last_agent] プロパティには、最後に実行されたエージェントが含まれます。アプリケーションによっては、次に ユーザー が何か入力する際に役立ちます。たとえば、フロントラインのトリアージ エージェントが言語別のエージェントに handoff する場合、最後のエージェントを保存しておき、次回 ユーザー がエージェントにメッセージを送るときに再利用できます。\n \n ## 新規アイテム\n \n-[`new_items`][agents.result.RunResultBase.new_items] プロパティには、実行中に生成された新しいアイテムが含まれます。アイテムは [`RunItem`][agents.items.RunItem] です。実行アイテムは、 LLM が生成した raw アイテムをラップします。\n+[`new_items`][agents.result.RunResultBase.new_items] プロパティには、実行中に生成された新しいアイテムが含まれます。アイテムは [`RunItem`][agents.items.RunItem] です。実行アイテムは、LLM が生成した生のアイテムをラップします。\n \n-- [`MessageOutputItem`][agents.items.MessageOutputItem] は LLM からのメッセージを示します。raw アイテムは生成されたメッセージです。\n-- [`HandoffCallItem`][agents.items.HandoffCallItem] は LLM がハンドオフ ツールを呼び出したことを示します。raw アイテムは LLM からのツール呼び出しアイテムです。\n-- [`HandoffOutputItem`][agents.items.HandoffOutputItem] はハンドオフ が発生したことを示します。raw アイテムはハンドオフ ツール呼び出しに対するツールの応答です。アイテムからソース/ターゲットの エージェント にアクセスすることもできます。\n-- [`ToolCallItem`][agents.items.ToolCallItem] は LLM がツールを呼び出したことを示します。\n-- [`ToolCallOutputItem`][agents.items.ToolCallOutputItem] はツールが呼び出されたことを示します。raw アイテムはツールの応答です。アイテムからツールの出力にアクセスすることもできます。\n-- [`ReasoningItem`][agents.items.ReasoningItem] は LLM からの推論アイテムを示します。raw アイテムは生成された推論です。\n+- [`MessageOutputItem`][agents.items.MessageOutputItem]: LLM からのメッセージを示します。生のアイテムは生成されたメッセージです。\n+- [`HandoffCallItem`][agents.items.HandoffCallItem]: LLM が handoff ツールを呼び出したことを示します。生のアイテムは LLM からのツール呼び出しアイテムです。\n+- [`HandoffOutputItem`][agents.items.HandoffOutputItem]: handoff が発生したことを示します。生のアイテムは handoff ツール呼び出しへのツール応答です。アイテムからソース/ターゲットのエージェントにもアクセスできます。\n+- [`ToolCallItem`][agents.items.ToolCallItem]: LLM がツールを呼び出したことを示します。\n+- [`ToolCallOutputItem`][agents.items.ToolCallOutputItem]: ツールが呼び出されたことを示します。生のアイテムはツール応答です。アイテムからツール出力にもアクセスできます。\n+- [`ReasoningItem`][agents.items.ReasoningItem]: LLM からの推論アイテムを示します。生のアイテムは生成された推論です。\n \n ## その他の情報\n \n ### ガードレールの結果\n \n-[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] と [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] プロパティには、存在する場合はガードレールの結果が含まれます。ガードレールの結果には、記録や保存に有用な情報が含まれることがあるため、これらを利用できるようにしています。\n+[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] と [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] プロパティには、ガードレールの結果（存在する場合）が含まれます。ガードレールの結果には、記録や保存したい有用な情報が含まれることがあるため、参照できるようにしています。\n \n-ツールのガードレール結果は、[`tool_input_guardrail_results`][agents.result.RunResultBase.tool_input_guardrail_results] と [`tool_output_guardrail_results`][agents.result.RunResultBase.tool_output_guardrail_results] として個別に利用できます。これらのガードレールはツールにアタッチでき、ツール呼び出しは エージェント のワークフロー中にガードレールを実行します。\n+ツールのガードレール結果は、[`tool_input_guardrail_results`][agents.result.RunResultBase.tool_input_guardrail_results] と [`tool_output_guardrail_results`][agents.result.RunResultBase.tool_output_guardrail_results] として個別に利用できます。これらのガードレールはツールに付与でき、エージェントのワークフロー中にそのツール呼び出しがガードレールを実行します。\n \n-### raw 応答\n+### 生の応答\n \n-[`raw_responses`][agents.result.RunResultBase.raw_responses] プロパティには、 LLM によって生成された [`ModelResponse`][agents.items.ModelResponse] が含まれます。\n+[`raw_responses`][agents.result.RunResultBase.raw_responses] プロパティには、LLM によって生成された [`ModelResponse`][agents.items.ModelResponse] が含まれます。\n \n ### 元の入力\n \n-[`input`][agents.result.RunResultBase.input] プロパティには、`run` メソッドに提供した元の入力が含まれます。ほとんどの場合これは不要ですが、必要な場合に備えて利用可能です。\n\\ No newline at end of file\n+[`input`][agents.result.RunResultBase.input] プロパティには、`run` メソッドに提供した元の入力が含まれます。ほとんどの場合は不要ですが、必要なときのために参照できるようになっています。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fresults.md",
        "sha": "db90b8f1fec1676892fff32c6ad47c4379d874db",
        "status": "modified"
      },
      {
        "additions": 51,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frunning_agents.md",
        "changes": 102,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Frunning_agents.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 51,
        "filename": "docs/ja/running_agents.md",
        "patch": "@@ -4,11 +4,11 @@ search:\n ---\n # エージェントの実行\n \n-エージェントは [`Runner`][agents.run.Runner] クラスで実行できます。オプションは 3 つあります。\n+エージェントは [`Runner`][agents.run.Runner] クラスで実行できます。次の 3 つの選択肢があります。\n \n 1. [`Runner.run()`][agents.run.Runner.run]: 非同期で実行し、[`RunResult`][agents.result.RunResult] を返します。\n 2. [`Runner.run_sync()`][agents.run.Runner.run_sync]: 同期メソッドで、内部的には `.run()` を実行します。\n-3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]: 非同期で実行し、[`RunResultStreaming`][agents.result.RunResultStreaming] を返します。LLM をストリーミング モードで呼び出し、受信したイベントをリアルタイムでストリーミングします。\n+3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]: 非同期で実行し、[`RunResultStreaming`][agents.result.RunResultStreaming] を返します。LLM を ストリーミング モードで呼び出し、受信したイベントを逐次 ストリーミング します。\n \n ```python\n from agents import Agent, Runner\n@@ -23,62 +23,62 @@ async def main():\n     # Infinite loop's dance\n ```\n \n-詳細は [results guide](results.md) を参照してください。\n+詳細は [結果ガイド](results.md) を参照してください。\n \n ## エージェントループ\n \n-`Runner` の run メソッドを使うとき、開始エージェントと入力を渡します。入力は文字列（ ユーザー メッセージとして扱われます）か、OpenAI Responses API のアイテムである入力アイテムのリストのいずれかです。\n+`Runner` の run メソッドを使うとき、開始するエージェントと入力を渡します。入力は文字列（ユーザー メッセージとして扱われます）または入力アイテムのリスト（OpenAI Responses API のアイテム）を指定できます。\n \n-runner は次のループを実行します。\n+Runner は次のループを実行します。\n \n 1. 現在のエージェントに対して、現在の入力で LLM を呼び出します。\n 2. LLM が出力を生成します。\n     1. LLM が `final_output` を返した場合、ループを終了し、結果を返します。\n-    2. LLM が ハンドオフ を行った場合、現在のエージェントと入力を更新してループを再実行します。\n-    3. LLM が ツール呼び出し を生成した場合、それらを実行して結果を追加し、ループを再実行します。\n+    2. LLM が ハンドオフ を行った場合、現在のエージェントと入力を更新して、ループを再実行します。\n+    3. LLM が ツール呼び出し を生成した場合、それらを実行して結果を追記し、ループを再実行します。\n 3. 渡された `max_turns` を超えた場合、[`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] 例外を送出します。\n \n !!! note\n \n-    LLM の出力が「最終出力」と見なされる条件は、望ましい型のテキスト出力を生成し、かつ ツール呼び出し が存在しないことです。\n+    LLM の出力が「最終出力」と見なされるルールは、所望の型のテキスト出力を生成し、ツール呼び出しが存在しないことです。\n \n ## ストリーミング\n \n-ストリーミングにより、LLM の実行中にストリーミングイベントを受け取れます。ストリームが完了すると、[`RunResultStreaming`][agents.result.RunResultStreaming] には、生成されたすべての新しい出力を含む、実行に関する完全な情報が含まれます。ストリーミングイベントは `.stream_events()` を呼び出して取得できます。詳細は [streaming guide](streaming.md) を参照してください。\n+ストリーミング を使用すると、LLM の実行中に ストリーミング イベントも受け取れます。ストリームが完了すると、[`RunResultStreaming`][agents.result.RunResultStreaming] に、その実行で生成された新しい出力を含む完全な情報が含まれます。ストリーミング イベントは `.stream_events()` を呼び出してください。詳細は [ストリーミング ガイド](streaming.md) を参照してください。\n \n-## 実行設定 (Run config)\n+## 実行設定\n \n-`run_config` パラメーターで、エージェント実行のグローバル設定を構成できます。\n+`run_config` パラメーターでは、エージェント実行のグローバル設定を構成できます。\n \n--   [`model`][agents.run.RunConfig.model]: 各 Agent の `model` 設定に関係なく、使用するグローバルな LLM モデルを設定できます。\n--   [`model_provider`][agents.run.RunConfig.model_provider]: モデル名を解決するためのモデルプロバイダーで、デフォルトは OpenAI です。\n--   [`model_settings`][agents.run.RunConfig.model_settings]: エージェント固有の設定を上書きします。例えば、グローバルな `temperature` や `top_p` を設定できます。\n+-   [`model`][agents.run.RunConfig.model]: 各 Agent の `model` 設定に関わらず、使用するグローバルな LLM モデルを設定します。\n+-   [`model_provider`][agents.run.RunConfig.model_provider]: モデル名を解決するモデルプロバイダーで、デフォルトは OpenAI です。\n+-   [`model_settings`][agents.run.RunConfig.model_settings]: エージェント固有の設定を上書きします。たとえば、グローバルな `temperature` や `top_p` を設定できます。\n -   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: すべての実行に含める入力または出力の ガードレール のリストです。\n--   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: ハンドオフ に独自のフィルターがない場合に適用する、すべてのハンドオフに対するグローバルな入力フィルターです。入力フィルターにより、新しいエージェントに送信される入力を編集できます。詳細は [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] のドキュメントを参照してください。\n--   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]: `True`（デフォルト）の場合、runner は次のエージェントを呼び出す前に、これまでの発話履歴を 1 つの assistant メッセージにまとめます。ヘルパーは内容を `<CONVERSATION HISTORY>` ブロック内に配置し、その後のハンドオフが発生するたびに新しいターンを追加します。以前の raw な書き起こしをそのまま渡したい場合は、これを `False` に設定するか、カスタムのハンドオフフィルターを指定してください。すべての [`Runner` methods](agents.run.Runner) は、未指定時に自動で `RunConfig` を作成するため、クイックスタートや code examples はこのデフォルトを自動で利用し、明示的な [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] コールバックは引き続きそれを上書きします。個々のハンドオフは、[`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] でこの設定を上書きできます。\n--   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]: `nest_handoff_history` が `True` のときに正規化された書き起こし（履歴 + ハンドオフ項目）を受け取る任意の呼び出し可能オブジェクトです。次のエージェントに転送する入力アイテムの正確なリストを返す必要があり、完全なハンドオフフィルターを書くことなく組み込みの要約を置き換えられます。\n--   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: 実行全体の [tracing](tracing.md) を無効化できます。\n--   [`tracing`][agents.run.RunConfig.tracing]: この実行のエクスポーター、プロセッサー、またはトレーシングメタデータを上書きするために [`TracingConfig`][agents.tracing.TracingConfig] を渡します。\n--   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: LLM や ツール呼び出し の入出力など、潜在的に機微なデータをトレースに含めるかどうかを設定します。\n--   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: この実行のトレーシングのワークフロー名、トレース ID、トレース グループ ID を設定します。少なくとも `workflow_name` の設定を推奨します。グループ ID は任意で、複数の実行にまたがるトレースをリンクできます。\n+-   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: ハンドオフ に既にフィルターがない場合に適用されるグローバルな入力フィルターです。入力フィルターにより、新しいエージェントに送る入力を編集できます。詳細は [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] のドキュメントを参照してください。\n+-   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]: `True`（デフォルト）の場合、Runner は次のエージェントを呼び出す前に、それまでのトランスクリプトを 1 つの assistant メッセージに折りたたみます。ヘルパーは内容を `<CONVERSATION HISTORY>` ブロックに配置し、以後のハンドオフ発生時に新しいターンを追加していきます。生のトランスクリプトをそのまま渡したい場合は、これを `False` にするか、カスタムの handoff フィルターを指定してください。すべての [`Runner` メソッド](agents.run.Runner) は、未指定時に自動で `RunConfig` を作成するため、クイックスタートや code examples はこのデフォルトを自動的に利用し、明示的な [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] コールバックは引き続きそれを上書きします。個々のハンドオフは、[`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] によってこの設定を上書きできます。\n+-   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]: `nest_handoff_history` が `True` のときに正規化済みのトランスクリプト（履歴 + handoff アイテム）を受け取る任意の呼び出し可能オブジェクトです。次のエージェントへ転送する入力アイテムのリストを正確に返す必要があり、完全な handoff フィルターを書かずに組み込み要約を置き換えられます。\n+-   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: 実行全体で [トレーシング](tracing.md) を無効化します。\n+-   [`tracing`][agents.run.RunConfig.tracing]: この実行のエクスポーター、プロセッサー、または トレーシング メタデータを上書きするために [`TracingConfig`][agents.tracing.TracingConfig] を渡します。\n+-   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: LLM やツール呼び出しの入出力など、機微なデータをトレースに含めるかどうかを設定します。\n+-   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: 実行の トレーシング ワークフロー名、トレース ID、トレース グループ ID を設定します。少なくとも `workflow_name` を設定することを推奨します。グループ ID は任意で、複数の実行に跨るトレースをリンクできます。\n -   [`trace_metadata`][agents.run.RunConfig.trace_metadata]: すべてのトレースに含めるメタデータです。\n--   [`session_input_callback`][agents.run.RunConfig.session_input_callback]: Sessions 使用時に、各ターンの前に新しい ユーザー 入力をセッション履歴とどのようにマージするかをカスタマイズします。\n--   [`call_model_input_filter`][agents.run.RunConfig.call_model_input_filter]: モデル呼び出し直前に、完全に準備されたモデル入力（instructions と入力アイテム）を編集するフックです。例えば履歴のトリミングや system prompt の注入に使用します。\n+-   [`session_input_callback`][agents.run.RunConfig.session_input_callback]: Sessions 使用時、各ターン前に新しいユーザー入力をセッション履歴へマージする方法をカスタマイズします。\n+-   [`call_model_input_filter`][agents.run.RunConfig.call_model_input_filter]: モデル呼び出し直前に、完全に準備されたモデル入力（instructions と入力アイテム）を編集するフックです。例: 履歴のトリミングや system prompt の挿入。\n \n-デフォルトでは、SDK はあるエージェントが別のエージェントにハンドオフするたびに、前のターンを 1 つの assistant 要約メッセージ内に入れ子にします。これにより、assistant メッセージの重複を減らし、完全な書き起こしを新しいエージェントが高速にスキャンできる 1 つのブロック内に保持します。従来の挙動に戻したい場合は、`RunConfig(nest_handoff_history=False)` を渡すか、会話を必要なとおりにそのまま転送する `handoff_input_filter`（または `handoff_history_mapper`）を指定してください。特定のハンドオフでのみオプトアウト（またはオプトイン）するには、`handoff(..., nest_handoff_history=False)` または `True` を設定します。カスタムマッパーを書かずに生成される要約のラッパーテキストを変更するには、[`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] を呼び出します（デフォルトに戻すには [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]）。\n+デフォルトでは、SDK はあるエージェントが別のエージェントへ ハンドオフ するたびに、それまでのターンを 1 つの assistant の要約メッセージ内にネストします。これにより、assistant メッセージの重複が減り、完全なトランスクリプトが 1 つのブロックに収まり、新しいエージェントがすばやく参照できます。従来の動作に戻したい場合は、`RunConfig(nest_handoff_history=False)` を渡すか、会話を必要なとおりに転送する `handoff_input_filter`（または `handoff_history_mapper`）を指定してください。特定のハンドオフごとに、`handoff(..., nest_handoff_history=False)` または `True` を設定してオプトアウト（またはオプトイン）できます。カスタム マッパーを書かずに生成される要約で使われるラッパーテキストを変更するには、[`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] を呼び出してください（デフォルトへ戻すには [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]）。\n \n-## 会話／チャットスレッド\n+## 会話/チャットスレッド\n \n-いずれかの run メソッドを呼び出すと、1 つ以上のエージェントが実行される（つまり、1 回以上の LLM 呼び出し）可能性がありますが、チャット会話における 1 つの論理的なターンを表します。例:\n+いずれの run メソッドでも、1 回の呼び出しで 1 つ以上のエージェント（すなわち 1 回以上の LLM 呼び出し）が実行されますが、これはチャット会話の 1 つの論理的なターンを表します。例:\n \n-1. ユーザー のターン: ユーザー がテキストを入力\n-2. Runner の実行: 最初のエージェントが LLM を呼び出し、ツールを実行し、2 番目のエージェントへハンドオフ、2 番目のエージェントがさらにツールを実行し、最終的に出力を生成。\n+1. ユーザーのターン: ユーザーがテキストを入力\n+2. Runner の実行: 最初のエージェントが LLM を呼び出し、ツールを実行し、2 つ目のエージェントへ ハンドオフ、2 つ目のエージェントがさらにツールを実行し、最終的な出力を生成\n \n-エージェント実行の最後に、ユーザー に何を表示するかを選べます。例えば、エージェントによって生成されたすべての新しいアイテムを表示するか、最終出力のみを表示します。どちらの場合でも、ユーザー が追質問をするかもしれないので、その場合は再度 run メソッドを呼び出します。\n+エージェントの実行が終わったら、ユーザーに何を見せるかを選べます。たとえば、エージェントが生成したすべての新しいアイテムを見せる、または最終出力だけを見せる、といったことが可能です。いずれの場合でも、ユーザーが追質問をするかもしれません。その場合は再度 run メソッドを呼び出してください。\n \n-### 手動の会話管理\n+### 手動での会話管理\n \n-次のターンの入力を取得するために、[`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] メソッドを使用して会話履歴を手動で管理できます。\n+次のターンの入力を取得するために、[`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] メソッドを使って、会話履歴を手動で管理できます。\n \n ```python\n async def main():\n@@ -100,7 +100,7 @@ async def main():\n \n ### Sessions による自動会話管理\n \n-より簡単な方法として、[Sessions](sessions/index.md) を使うと、`.to_input_list()` を手動で呼び出すことなく会話履歴を自動処理できます。\n+より簡単な方法として、[Sessions](sessions/index.md) を使えば、`.to_input_list()` を手動で呼び出さずに会話履歴を自動管理できます。\n \n ```python\n from agents import Agent, Runner, SQLiteSession\n@@ -124,24 +124,24 @@ async def main():\n         # California\n ```\n \n-Sessions は自動で次を行います。\n+Sessions は自動的に次を行います。\n \n -   各実行の前に会話履歴を取得\n -   各実行の後に新しいメッセージを保存\n -   セッション ID ごとに独立した会話を維持\n \n-詳細は [Sessions documentation](sessions/index.md) を参照してください。\n+詳細は [Sessions のドキュメント](sessions/index.md) を参照してください。\n \n \n ### サーバー管理の会話\n \n-`to_input_list()` や `Sessions` でローカルに扱う代わりに、OpenAI の会話状態機能により サーバー 側で会話状態を管理することもできます。これにより、過去のメッセージを手動で再送せずに会話履歴を保持できます。詳細は [OpenAI Conversation state guide](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses) を参照してください。\n+`to_input_list()` や `Sessions` でローカルに管理する代わりに、OpenAI の conversation state 機能により、サーバー側で会話状態を管理することもできます。これにより、過去のメッセージをすべて手動で再送せずに会話履歴を保持できます。詳細は [OpenAI Conversation state ガイド](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses) を参照してください。\n \n-OpenAI はターン間で状態を追跡する 2 つの方法を提供します。\n+OpenAI はターン間の状態を追跡する 2 つの方法を提供します。\n \n #### 1. `conversation_id` を使用\n \n-まず OpenAI Conversations API を使って会話を作成し、その ID を以後の呼び出しで使い回します。\n+最初に OpenAI Conversations API で会話を作成し、その ID を以降のすべての呼び出しで再利用します。\n \n ```python\n from agents import Agent, Runner\n@@ -164,7 +164,7 @@ async def main():\n \n #### 2. `previous_response_id` を使用\n \n-もう 1 つの方法は **response chaining** で、各ターンが前のターンのレスポンス ID に明示的にリンクします。\n+もう 1 つの方法は、各ターンが前のターンの response ID に明示的にリンクする **response chaining** です。\n \n ```python\n from agents import Agent, Runner\n@@ -189,9 +189,9 @@ async def main():\n         print(f\"Assistant: {result.final_output}\")\n ```\n \n-## Call model input filter\n+## call_model_input_filter\n \n-モデル呼び出し直前のモデル入力を編集するには `call_model_input_filter` を使用します。フックは現在のエージェント、コンテキスト、（存在する場合はセッション履歴を含む）結合済み入力アイテムを受け取り、新しい `ModelInputData` を返します。\n+モデル呼び出し直前にモデル入力を編集するには `call_model_input_filter` を使用します。フックは現在のエージェント、コンテキスト、結合済みの入力アイテム（セッション履歴があればそれも含む）を受け取り、新しい `ModelInputData` を返します。\n \n ```python\n from agents import Agent, Runner, RunConfig\n@@ -210,20 +210,20 @@ result = Runner.run_sync(\n )\n ```\n \n-`run_config` で実行ごとに、または `Runner` のデフォルトとしてフックを設定して、機微情報の編集、長い履歴のトリミング、追加の system prompt の注入などを行えます。\n+機微情報のマスキング、長い履歴のトリミング、追加のシステムガイダンスの挿入などのために、`run_config` で実行ごとに設定するか、`Runner` のデフォルトとして設定してください。\n \n-## 長時間実行のエージェントと human-in-the-loop\n+## 長時間実行のエージェント & human-in-the-loop\n \n-Agents SDK の [Temporal](https://temporal.io/) 連携を使用すると、human-in-the-loop タスクを含む永続的で長時間実行のワークフローを実行できます。Temporal と Agents SDK が協調して長時間タスクを完了するデモは [この動画](https://www.youtube.com/watch?v=fFBZqzT4DD8) を、ドキュメントは [こちら](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents) を参照してください。\n+Agents SDK の [Temporal](https://temporal.io/) 連携を使うと、human-in-the-loop を含む永続的で長時間実行のワークフローを動かせます。Temporal と Agents SDK が連携して長時間タスクを完了するデモは [この動画](https://www.youtube.com/watch?v=fFBZqzT4DD8) を参照し、ドキュメントは [こちら](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents) を参照してください。\n \n ## 例外\n \n-SDK は特定のケースで例外を送出します。完全な一覧は [`agents.exceptions`][] にあります。概要は以下のとおりです。\n+SDK は特定の状況で例外を送出します。完全な一覧は [`agents.exceptions`][] にあります。概要は以下のとおりです。\n \n--   [`AgentsException`][agents.exceptions.AgentsException]: SDK 内で送出されるすべての例外の基底クラスです。その他すべての特定例外はこの汎用型から派生します。\n--   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: エージェントの実行が `Runner.run`、`Runner.run_sync`、`Runner.run_streamed` に渡された `max_turns` 制限を超えたときに送出されます。指定された対話ターン数内にタスクを完了できなかったことを示します。\n--   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: 基盤のモデル（LLM）が想定外または無効な出力を生成した場合に発生します。例えば次のようなケースです。\n-    -   不正な JSON: 特定の `output_type` が定義されている場合に特に、ツール呼び出しや直接の出力で不正な JSON 構造を返した。\n-    -   想定外のツール関連の失敗: モデルが想定どおりにツールを使用できなかった。\n--   [`UserError`][agents.exceptions.UserError]: SDK を使用する（コードを書く）あなたが誤りを犯した場合に送出されます。これは通常、不正なコード実装、無効な構成、SDK の API の誤用に起因します。\n--   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: 入力 ガードレール または出力 ガードレール の条件が満たされた場合にそれぞれ送出されます。入力 ガードレール は処理前の受信メッセージを検査し、出力 ガードレール はエージェントの最終応答を配信前に検査します。\n\\ No newline at end of file\n+-   [`AgentsException`][agents.exceptions.AgentsException]: SDK 内で送出されるすべての例外の基底クラスです。他の特定の例外はすべてこれを継承します。\n+-   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: エージェントの実行が `Runner.run`、`Runner.run_sync`、`Runner.run_streamed` に渡した `max_turns` の上限を超えた場合に送出されます。指定された対話ターン数内にエージェントがタスクを完了できなかったことを示します。\n+-   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: 基盤となるモデル（LLM）が予期しない、または無効な出力を生成した場合に発生します。これには以下が含まれます。\n+    -   不正な JSON: 特定の `output_type` が定義されている場合に、ツール呼び出しや直接の出力で不正な JSON 構造を返す。\n+    -   予期しないツール関連の失敗: モデルが想定どおりにツールを使用できない場合。\n+-   [`UserError`][agents.exceptions.UserError]: SDK を使用するあなた（SDK を使ってコードを書く人）が誤りを犯した場合に送出されます。これは通常、コードの実装ミス、無効な設定、SDK の API の誤用に起因します。\n+-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: それぞれ、入力ガードレールまたは出力ガードレールの条件に合致した場合に送出されます。入力ガードレールは処理前に受信メッセージを検査し、出力ガードレールはエージェントの最終応答を配送前に検査します。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Frunning_agents.md",
        "sha": "c9cf9712c50fec8252bc840d83392a89bec25f60",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Fadvanced_sqlite_session.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fsessions%2Fadvanced_sqlite_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/ja/sessions/advanced_sqlite_session.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # 高度な SQLite セッション\n \n-`AdvancedSQLiteSession` は、基本の `SQLiteSession` を拡張したもので、会話の分岐、詳細な使用状況分析、構造化された会話クエリなど、高度な会話管理機能を提供します。\n+`AdvancedSQLiteSession` は、基本的な `SQLiteSession` を拡張し、会話の分岐、詳細な使用状況分析、構造化された会話クエリなど、高度な会話管理機能を提供します。\n \n ## 機能\n \n--  **会話の分岐**: 任意の ユーザー メッセージから代替の会話パスを作成\n--  **使用状況の追跡**: JSON の完全な内訳付きで、ターンごとの詳細なトークン使用分析\n--  **構造化クエリ**: ターン単位の会話取得、ツール使用統計など\n--  **ブランチ管理**: 独立したブランチの切り替えと管理\n--  **メッセージ構造メタデータ**: メッセージ種別、ツール使用状況、会話フローを追跡\n+- **会話の分岐**: 任意の ユーザー メッセージから代替の会話パスを作成\n+- **使用状況の追跡**: 各ターンの詳細なトークン使用分析と完全な JSON 内訳\n+- **構造化クエリ**: ターンごとの会話取得、ツール使用統計など\n+- **ブランチ管理**: 独立したブランチ切り替えと管理\n+- **メッセージ構造メタデータ**: メッセージ種別、ツール使用状況、会話フローを追跡\n \n ## クイックスタート\n \n@@ -84,14 +84,14 @@ session = AdvancedSQLiteSession(\n \n ### パラメーター\n \n-- `session_id` (str): 会話セッションの一意の識別子\n-- `db_path` (str | Path): SQLite データベースファイルへのパス。メモリ内保存の場合はデフォルトで `:memory:` です\n+- `session_id` (str): 会話セッションの一意な識別子\n+- `db_path` (str | Path): SQLite データベースファイルへのパス。メモリ内ストレージには `:memory:` がデフォルト\n - `create_tables` (bool): 高度なテーブルを自動作成するかどうか。デフォルトは `False`\n - `logger` (logging.Logger | None): セッション用のカスタムロガー。デフォルトはモジュールロガー\n \n ## 使用状況の追跡\n \n-AdvancedSQLiteSession は、会話の各ターンごとにトークン使用データを保存することで、詳細な使用状況分析を提供します。これは、各 エージェント 実行後に `store_run_usage` メソッドが呼び出されることに完全に依存します。\n+AdvancedSQLiteSession は、会話の各ターンごとにトークン使用データを保存することで、詳細な使用状況分析を提供します。**これは各 エージェント 実行後に `store_run_usage` メソッドが呼び出されることに完全に依存します。**\n \n ### 使用データの保存\n \n@@ -137,7 +137,7 @@ turn_2_usage = await session.get_turn_usage(user_turn_number=2)\n \n ## 会話の分岐\n \n-AdvancedSQLiteSession の主要機能の 1 つは、任意の ユーザー メッセージから会話のブランチを作成し、代替の会話パスを探索できることです。\n+AdvancedSQLiteSession の主要機能の 1 つは、任意の ユーザー メッセージから会話のブランチを作成でき、代替の会話パスを探索できることです。\n \n ### ブランチの作成\n \n@@ -245,17 +245,17 @@ for turn in matching_turns:\n \n ### メッセージ構造\n \n-セッションは次のようなメッセージ構造を自動的に追跡します。\n+セッションは次のようなメッセージ構造を自動的に追跡します:\n \n-- メッセージ種別（user、assistant、tool_call など）\n+- メッセージ種別（user、assistant、tool_call、など）\n - ツール呼び出しのツール名\n - ターン番号とシーケンス番号\n - ブランチの関連付け\n - タイムスタンプ\n \n ## データベーススキーマ\n \n-AdvancedSQLiteSession は、基本の SQLite スキーマを拡張し、2 つの追加テーブルを提供します。\n+AdvancedSQLiteSession は、基本の SQLite スキーマを拡張し、次の 2 つの追加テーブルを提供します。\n \n ### message_structure テーブル\n \n@@ -296,9 +296,9 @@ CREATE TABLE turn_usage (\n );\n ```\n \n-## 完全なサンプルコード\n+## 完全な例\n \n-すべての機能を網羅したデモについては、[完全なサンプルコード](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py)をご覧ください。\n+すべての機能を網羅したデモは、[完全なサンプル](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py)をご覧ください。\n \n \n ## API リファレンス",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Fadvanced_sqlite_session.md",
        "sha": "cf99c131be3bb39c82c52bba7abeeb58791694bc",
        "status": "modified"
      },
      {
        "additions": 29,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Fencrypted_session.md",
        "changes": 58,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fsessions%2Fencrypted_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 29,
        "filename": "docs/ja/sessions/encrypted_session.md",
        "patch": "@@ -2,20 +2,20 @@\n search:\n   exclude: true\n ---\n-# 暗号化セッション\n+# 暗号化されたセッション\n \n-`EncryptedSession` は、任意のセッション実装に対して透過的な暗号化を提供し、会話データを保護しつつ、古いアイテムの自動期限切れを実現します。\n+`EncryptedSession` は、あらゆるセッション実装に対して透過的な暗号化を提供し、会話データを保護するとともに、古い項目を自動的に期限切れにします。\n \n ## 機能\n \n--  **透過的な暗号化** : 任意のセッションを Fernet 暗号化でラップします\n--  **セッションごとの鍵** : HKDF による鍵導出でセッションごとに一意の暗号化を行います\n--  **自動有効期限** : TTL が切れた古いアイテムは何も出力せずにスキップされます\n--  **ドロップイン置き換え** : 既存の任意のセッション実装で動作します\n+- **透過的な暗号化**: 任意のセッションを Fernet 暗号化でラップします\n+- **セッションごとの鍵**: HKDF による鍵導出でセッションごとに一意の暗号化を実現します\n+- **自動期限切れ**: TTL が切れると古い項目は自動的にスキップされます\n+- **ドロップイン置き換え**: 既存のセッション実装とそのまま置き換え可能です\n \n ## インストール\n \n-暗号化セッションには `encrypt` エクストラが必要です:\n+暗号化セッションには `encrypt` extra が必要です:\n \n ```bash\n pip install openai-agents[encrypt]\n@@ -55,9 +55,9 @@ if __name__ == \"__main__\":\n \n ## 設定\n \n-### 暗号化キー\n+### 暗号鍵\n \n-暗号化キーは、Fernet キーまたは任意の文字列を使用できます:\n+暗号鍵は Fernet キーでも任意の文字列でも構いません:\n \n ```python\n from agents.extensions.memory import EncryptedSession\n@@ -79,9 +79,9 @@ session = EncryptedSession(\n )\n ```\n \n-### TTL (Time To Live)\n+### TTL（有効期間）\n \n-暗号化されたアイテムの有効期間を設定します:\n+暗号化された項目が有効な期間を設定します:\n \n ```python\n # Items expire after 1 hour\n@@ -103,7 +103,7 @@ session = EncryptedSession(\n \n ## さまざまなセッションタイプでの使用\n \n-### SQLite セッションでの使用\n+### SQLite セッションで\n \n ```python\n from agents import SQLiteSession\n@@ -119,7 +119,7 @@ session = EncryptedSession(\n )\n ```\n \n-### SQLAlchemy セッションでの使用\n+### SQLAlchemy セッションで\n \n ```python\n from agents.extensions.memory import EncryptedSession, SQLAlchemySession\n@@ -140,30 +140,30 @@ session = EncryptedSession(\n \n !!! warning \"高度なセッション機能\"\n \n-    `EncryptedSession` を `AdvancedSQLiteSession` のような高度なセッション実装で使用する場合は、次に注意してください:\n+    `EncryptedSession` を `AdvancedSQLiteSession` のような高度なセッション実装と併用する場合、次の点に注意してください。\n \n-    - メッセージ内容が暗号化されるため、`find_turns_by_content()` のようなメソッドは有効に機能しません\n-    - コンテンツベースの検索は暗号化データ上で実行されるため、その有効性は制限されます\n+    - メッセージの内容が暗号化されるため、`find_turns_by_content()` のようなメソッドは効果的に機能しません\n+    - コンテンツベースの検索は暗号化済みデータに対して行われるため、効果が制限されます\n \n \n \n-## キー導出\n+## 鍵導出\n \n-EncryptedSession は HKDF (HMAC ベースの Key Derivation Function) を使用して、セッションごとに一意の暗号化キーを導出します:\n+EncryptedSession は HKDF（HMAC-based Key Derivation Function）を使用して、セッションごとに一意の暗号鍵を導出します。\n \n--  **マスターキー** : 指定した暗号化キー\n--  **セッションソルト** : セッション ID\n--  **Info 文字列** : `\"agents.session-store.hkdf.v1\"`\n--  **出力** : 32 バイトの Fernet キー\n+- **マスター鍵**: 提供された暗号鍵\n+- **セッションソルト**: セッション ID\n+- **Info 文字列**: `\"agents.session-store.hkdf.v1\"`\n+- **出力**: 32 バイトの Fernet キー\n \n-これにより、次が保証されます:\n-- 各セッションには一意の暗号化キーがあります\n-- マスターキーがなければ鍵を導出できません\n-- セッションをまたいでデータを復号できません\n+これにより、次のことが保証されます。\n+- 各セッションは一意の暗号鍵を持ちます\n+- マスター鍵がなければ鍵を導出できません\n+- 異なるセッション間でセッションデータを復号できません\n \n-## 自動有効期限\n+## 自動期限切れ\n \n-アイテムが TTL を超えた場合、取得時に自動的にスキップされます:\n+項目が TTL を超えた場合、取得時に自動的にスキップされます。\n \n ```python\n # Items older than TTL are silently ignored\n@@ -176,4 +176,4 @@ result = await Runner.run(agent, \"Continue conversation\", session=session)\n ## API リファレンス\n \n - [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - メインクラス\n-- [`Session`][agents.memory.session.Session] - ベースのセッションプロトコル\n\\ No newline at end of file\n+- [`Session`][agents.memory.session.Session] - ベースとなるセッションプロトコル\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Fencrypted_session.md",
        "sha": "559aa567dc1f6eebe026c71334c3c63c37afd9c2",
        "status": "modified"
      },
      {
        "additions": 34,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Findex.md",
        "changes": 68,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fsessions%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 34,
        "filename": "docs/ja/sessions/index.md",
        "patch": "@@ -4,9 +4,9 @@ search:\n ---\n # セッション\n \n-Agents SDK は、複数のエージェント実行にわたって会話履歴を自動的に維持する組み込みのセッションメモリを提供し、ターン間で手動で `.to_input_list()` を扱う必要をなくします。\n+ Agents SDK は、複数回のエージェント実行にわたって会話履歴を自動的に保持する組み込みのセッションメモリを提供し、ターン間で手動で `.to_input_list()` を扱う必要をなくします。\n \n-セッションは特定のセッションの会話履歴を保存し、明示的な手動メモリ管理なしでエージェントがコンテキストを維持できるようにします。これは、エージェントに以前のやり取りを記憶させたいチャットアプリケーションやマルチターン会話の構築に特に有用です。\n+セッションは特定のセッションに対して会話履歴を保存し、明示的な手動メモリ管理なしでエージェントがコンテキストを維持できるようにします。これは、エージェントに以前のやり取りを記憶させたいチャットアプリケーションやマルチターンの会話を構築する際に特に有用です。\n \n ## クイックスタート\n \n@@ -51,17 +51,17 @@ print(result.final_output)  # \"Approximately 39 million\"\n \n セッションメモリを有効にすると:\n \n-1. **各実行前**: ランナーはセッションの会話履歴を自動的に取得し、入力アイテムの先頭に付加します。\n-2. **各実行後**: 実行中に生成された新しいアイテム（ユーザー入力、アシスタントの応答、ツール呼び出しなど）は、すべて自動的にセッションに保存されます。\n-3. **コンテキストの保持**: 同じセッションでの後続の実行には完全な会話履歴が含まれ、エージェントはコンテキストを維持できます。\n+1. **各実行前**: Runner はそのセッションの会話履歴を自動的に取得し、入力アイテムの先頭に付加します。\n+2. **各実行後**: 実行中に生成されたすべての新しいアイテム（ユーザー入力、アシスタントの応答、ツール呼び出しなど）が自動的にセッションに保存されます。\n+3. **コンテキストの維持**: 同じセッションでの後続の実行には完全な会話履歴が含まれ、エージェントはコンテキストを維持できます。\n \n-これにより、実行間で `.to_input_list()` を手動で呼び出したり、会話状態を管理する必要がなくなります。\n+これにより、`.to_input_list()` を手動で呼び出し、実行間で会話状態を管理する必要がなくなります。\n \n ## メモリ操作\n \n ### 基本操作\n \n-セッションは、会話履歴を管理するためのいくつかの操作をサポートします:\n+セッションは会話履歴を管理するためにいくつかの操作をサポートします:\n \n ```python\n from agents import SQLiteSession\n@@ -88,7 +88,7 @@ await session.clear_session()\n \n ### 修正のための pop_item の使用\n \n-`pop_item` メソッドは、会話内の最後のアイテムを取り消したり修正したりしたい場合に特に有用です:\n+`pop_item` メソッドは、会話の最後のアイテムを取り消したり変更したりしたい場合に特に有用です:\n \n ```python\n from agents import Agent, Runner, SQLiteSession\n@@ -119,11 +119,11 @@ print(f\"Agent: {result.final_output}\")\n \n ## セッションの種類\n \n-SDK は、用途に応じていくつかのセッション実装を提供します:\n+ SDK は、さまざまなユースケース向けに複数のセッション実装を提供します:\n \n ### OpenAI Conversations API セッション\n \n-`OpenAIConversationsSession` を通じて [OpenAI の Conversations API](https://platform.openai.com/docs/api-reference/conversations) を使用します。\n+ `OpenAIConversationsSession` を通じて [OpenAI の Conversations API](https://platform.openai.com/docs/api-reference/conversations) を使用します。\n \n ```python\n from agents import Agent, Runner, OpenAIConversationsSession\n@@ -159,7 +159,7 @@ print(result.final_output)  # \"California\"\n \n ### SQLite セッション\n \n-SQLite を使用するデフォルトの軽量セッション実装です:\n+デフォルトの軽量な SQLite ベースのセッション実装:\n \n ```python\n from agents import SQLiteSession\n@@ -180,7 +180,7 @@ result = await Runner.run(\n \n ### SQLAlchemy セッション\n \n-任意の SQLAlchemy 対応データベースを使用する本番対応のセッションです:\n+任意の SQLAlchemy 対応データベースを使用する本番運用向けのセッション:\n \n ```python\n from agents.extensions.memory import SQLAlchemySession\n@@ -198,13 +198,13 @@ engine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\n session = SQLAlchemySession(\"user_123\", engine=engine, create_tables=True)\n ```\n \n-詳しくは [SQLAlchemy セッション](sqlalchemy_session.md) をご覧ください。\n+詳細は [SQLAlchemy セッション](sqlalchemy_session.md) を参照してください。\n \n \n \n ### 高度な SQLite セッション\n \n-会話の分岐、利用状況分析、構造化クエリを備えた拡張 SQLite セッションです:\n+会話の分岐、利用状況分析、構造化クエリを備えた強化版 SQLite セッション:\n \n ```python\n from agents.extensions.memory import AdvancedSQLiteSession\n@@ -224,11 +224,11 @@ await session.store_run_usage(result)  # Track token usage\n await session.create_branch_from_turn(2)  # Branch from turn 2\n ```\n \n-詳しくは [高度な SQLite セッション](advanced_sqlite_session.md) をご覧ください。\n+詳細は [高度な SQLite セッション](advanced_sqlite_session.md) を参照してください。\n \n ### 暗号化セッション\n \n-任意のセッション実装に対する透過的な暗号化ラッパーです:\n+任意のセッション実装を透過的に暗号化するラッパー:\n \n ```python\n from agents.extensions.memory import EncryptedSession, SQLAlchemySession\n@@ -251,31 +251,31 @@ session = EncryptedSession(\n result = await Runner.run(agent, \"Hello\", session=session)\n ```\n \n-詳しくは [暗号化セッション](encrypted_session.md) をご覧ください。\n+詳細は [暗号化セッション](encrypted_session.md) を参照してください。\n \n ### その他のセッションタイプ\n \n-他にもいくつかの組み込みオプションがあります。`examples/memory/` と `extensions/memory/` 配下のソースコードを参照してください。\n+いくつかの組み込みオプションもあります。`examples/memory/` と `extensions/memory/` 配下のソースコードを参照してください。\n \n ## セッション管理\n \n ### セッション ID の命名\n \n 会話を整理しやすくする意味のあるセッション ID を使用します:\n \n-- ユーザー ベース: `\"user_12345\"`\n-- スレッド ベース: `\"thread_abc123\"`\n-- コンテキスト ベース: `\"support_ticket_456\"`\n+- ユーザー単位: `\"user_12345\"`\n+- スレッド単位: `\"thread_abc123\"`\n+- コンテキスト単位: `\"support_ticket_456\"`\n \n ### メモリの永続化\n \n-- 一時的な会話には、インメモリ SQLite（`SQLiteSession(\"session_id\")`）を使用します\n-- 永続的な会話には、ファイルベースの SQLite（`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`）を使用します\n-- 既存のデータベース（SQLAlchemy がサポートするもの）を使う本番システムには、SQLAlchemy 駆動のセッション（`SQLAlchemySession(\"session_id\", engine=engine, create_tables=True)`）を使用します\n-- クラウドネイティブな本番デプロイには、Dapr ステートストア セッション（`DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")`）を使用します。組み込みのテレメトリー、トレーシング、データ分離を備え、30+ のデータベースバックエンドをサポートします\n-- 履歴を OpenAI Conversations API に保存したい場合は、OpenAI がホストするストレージ（`OpenAIConversationsSession()`）を使用します\n-- 任意のセッションを透過的な暗号化と TTL ベースの有効期限でラップするには、暗号化セッション（`EncryptedSession(session_id, underlying_session, encryption_key)`）を使用します\n-- より高度なユースケース向けに、他の本番システム（Redis、Django など）用のカスタムセッションバックエンドの実装も検討してください\n+- 一時的な会話にはインメモリ SQLite（`SQLiteSession(\"session_id\")`）を使用\n+- 永続的な会話にはファイルベースの SQLite（`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`）を使用\n+- 既存の SQLAlchemy 対応データベースを備えた本番システムには SQLAlchemy ベースのセッション（`SQLAlchemySession(\"session_id\", engine=engine, create_tables=True)`）を使用\n+- クラウドネイティブな本番環境には Dapr ステートストアのセッション（`DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")`）を使用。組み込みのテレメトリー、トレーシング、データ分離を備え、 30+ のデータベースバックエンドをサポート\n+- 履歴を OpenAI Conversations API に保存したい場合は OpenAI ホスト型ストレージ（`OpenAIConversationsSession()`）を使用\n+- 透過的な暗号化と TTL ベースの有効期限で任意のセッションをラップするには暗号化セッション（`EncryptedSession(session_id, underlying_session, encryption_key)`）を使用\n+- より高度なユースケースでは、他の本番システム（Redis、Django など）向けのカスタムセッションバックエンドの実装を検討\n \n ### 複数セッション\n \n@@ -323,7 +323,7 @@ result2 = await Runner.run(\n \n ## 完全な例\n \n-セッションメモリがどのように機能するかを示す完全な例です:\n+次は、セッションメモリの動作を示す完全な例です:\n \n ```python\n import asyncio\n@@ -430,24 +430,24 @@ result = await Runner.run(\n )\n ```\n \n-## コミュニティ製セッション実装\n+## コミュニティ提供のセッション実装\n \n コミュニティによって、追加のセッション実装が開発されています:\n \n | パッケージ | 説明 |\n |---------|-------------|\n | [openai-django-sessions](https://pypi.org/project/openai-django-sessions/) | 任意の Django 対応データベース（PostgreSQL、MySQL、SQLite など）向けの Django ORM ベースのセッション |\n \n-セッション実装を作成された場合は、ぜひドキュメントの PR を送ってここに追加してください。\n+セッション実装を作成した場合は、ぜひドキュメントへの PR を送ってここに追加してください。\n \n ## API リファレンス\n \n-詳細な API ドキュメントは以下をご覧ください:\n+詳細な API ドキュメントは以下を参照してください:\n \n - [`Session`][agents.memory.session.Session] - プロトコルインターフェース\n - [`OpenAIConversationsSession`][agents.memory.OpenAIConversationsSession] - OpenAI Conversations API 実装\n - [`SQLiteSession`][agents.memory.sqlite_session.SQLiteSession] - 基本的な SQLite 実装\n-- [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - SQLAlchemy 駆動の実装\n+- [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - SQLAlchemy ベースの実装\n - [`DaprSession`][agents.extensions.memory.dapr_session.DaprSession] - Dapr ステートストア実装\n - [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - 分岐と分析を備えた拡張 SQLite\n-- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 任意のセッション向け暗号化ラッパー\n\\ No newline at end of file\n+- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 任意のセッション向けの暗号化ラッパー\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Findex.md",
        "sha": "319523b00080d8c8bf0e8d7b9d40ae1c9ebb0516",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Fsqlalchemy_session.md",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fsessions%2Fsqlalchemy_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 3,
        "filename": "docs/ja/sessions/sqlalchemy_session.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # SQLAlchemy セッション\n \n-`SQLAlchemySession` は SQLAlchemy を使用してプロダクション対応のセッション実装を提供し、セッションの保存先として SQLAlchemy がサポートする任意のデータベース (PostgreSQL、MySQL、SQLite など) を利用できます。\n+`SQLAlchemySession` は SQLAlchemy を使用して本番運用可能なセッション実装を提供し、SQLAlchemy がサポートする任意のデータベース（PostgreSQL、MySQL、SQLite など）をセッションストレージに利用できます。\n \n ## インストール\n \n@@ -44,7 +44,7 @@ if __name__ == \"__main__\":\n \n ### 既存のエンジンの使用\n \n-既存の SQLAlchemy エンジンを持つアプリケーション向け:\n+既存の SQLAlchemy エンジンを使用するアプリケーション向け:\n \n ```python\n import asyncio\n@@ -77,4 +77,4 @@ if __name__ == \"__main__\":\n ## API リファレンス\n \n - [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - メインクラス\n-- [`Session`][agents.memory.session.Session] - セッションのベースプロトコル\n\\ No newline at end of file\n+- [`Session`][agents.memory.session.Session] - ベースセッションプロトコル\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fsessions%2Fsqlalchemy_session.md",
        "sha": "2237f3da8d4904c3c0dea2036fab96dd51a7d073",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fstreaming.md",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fstreaming.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 8,
        "filename": "docs/ja/streaming.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # ストリーミング\n \n-ストリーミングを使うと、エージェントの実行が進むにつれて更新を購読できます。これはエンドユーザーへの進行状況の表示や部分的な応答の表示に役立ちます。\n+ストリーミングは、エージェントの実行が進むにつれてその更新を購読できるようにします。これは、エンドユーザーに進捗の更新や部分的な応答を表示するのに有用です。\n \n-ストリーミングするには、[`Runner.run_streamed()`][agents.run.Runner.run_streamed] を呼び出します。これは [`RunResultStreaming`][agents.result.RunResultStreaming] を返します。`result.stream_events()` を呼ぶと、以下で説明する [`StreamEvent`][agents.stream_events.StreamEvent] オブジェクトの非同期ストリームが得られます。\n+ストリーミングするには、[`Runner.run_streamed()`][agents.run.Runner.run_streamed] を呼び出します。これは [`RunResultStreaming`][agents.result.RunResultStreaming] を返します。`result.stream_events()` を呼び出すと、以下で説明する [`StreamEvent`][agents.stream_events.StreamEvent] オブジェクトの非同期ストリームが得られます。\n \n-## raw レスポンスイベント\n+## Raw 応答イベント\n \n-[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent] は、LLM から直接渡される raw なイベントです。OpenAI Responses API の形式であり、各イベントにはタイプ（`response.created`、`response.output_text.delta` など）とデータがあります。生成され次第、レスポンスメッセージをユーザーにストリーミングしたい場合に有用です。\n+[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent] は、 LLM から直接渡される raw なイベントです。これは OpenAI Responses API 形式であり、つまり各イベントはタイプ（`response.created`、`response.output_text.delta` など）とデータを持ちます。これらのイベントは、生成され次第、応答メッセージをユーザーにストリーミングしたい場合に有用です。\n \n-たとえば、次の例は LLM によって生成されたテキストをトークンごとに出力します。\n+例えば、これは LLM が生成するテキストをトークンごとに出力します。\n \n ```python\n import asyncio\n@@ -35,11 +35,11 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-## Run アイテムイベントと エージェントイベント\n+## Run item イベントと エージェント イベント\n \n-[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent] は、より高レベルなイベントです。アイテムが完全に生成されたタイミングを通知します。これにより、各トークンではなく「メッセージが生成された」「ツールを実行した」などのレベルで進行状況を更新できます。同様に、[`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] は現在のエージェントが変更された際（例: handoff の結果）に更新を提供します。\n+[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent] は、より高レベルのイベントです。これは、アイテムが完全に生成されたタイミングを知らせます。これにより、各トークン単位ではなく、「メッセージが生成された」「ツールが実行された」などのレベルで進捗更新をプッシュできます。同様に、[`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] は、現在のエージェントが変更されたとき（例: ハンドオフの結果）に更新を提供します。\n \n-たとえば、次の例は raw イベントを無視し、ユーザーに更新をストリーミングします。\n+例えば、これは raw イベントを無視して、ユーザーに更新をストリーミングします。\n \n ```python\n import asyncio",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fstreaming.md",
        "sha": "4ff6b5970a8b218b87bce891d2fbfea2cbda4024",
        "status": "modified"
      },
      {
        "additions": 57,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Ftools.md",
        "changes": 114,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Ftools.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 57,
        "filename": "docs/ja/tools.md",
        "patch": "@@ -4,20 +4,20 @@ search:\n ---\n # ツール\n \n-ツールは、エージェントがデータ取得、コード実行、外部 API 呼び出し、さらにはコンピュータ操作などのアクションを実行できるようにします。SDK は次の 4 つの カテゴリー をサポートします:\n+ツールは エージェント がアクションを実行できるようにします。たとえばデータの取得、コードの実行、外部 API の呼び出し、さらには コンピュータ操作 などです。SDK は次の 4 つの カテゴリー をサポートします:\n \n-- OpenAI がホストするツール: モデルと同じ OpenAI サーバー 上で実行されます。\n-- ローカル実行ツール: あなたの環境で実行されます（コンピュータ操作、シェル、パッチ適用）。\n-- Function Calling: 任意の Python 関数をツールとしてラップします。\n-- ツールとしてのエージェント: フルな ハンドオフ なしで、エージェントを呼び出し可能なツールとして公開します。\n+- Hosted OpenAI tools: モデルと並行して OpenAI の サーバー 上で実行されます。\n+- Local runtime tools: あなたの環境で実行されます（コンピュータ操作、シェル、パッチ適用）。\n+- Function calling: 任意の Python 関数をツールとしてラップします。\n+- エージェント as tools: 完全な ハンドオフ なしで、エージェント を呼び出し可能なツールとして公開します。\n \n ## ホスト型ツール\n \n-[`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] を使用する場合、OpenAI はいくつかの組み込みツールを提供します:\n+OpenAI は [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] を使用する場合、いくつかの組み込みツールを提供します:\n \n-- [`WebSearchTool`][agents.tool.WebSearchTool]: エージェントが Web 検索 を実行できます。\n-- [`FileSearchTool`][agents.tool.FileSearchTool]: OpenAI ベクトルストア から情報を取得できます。\n-- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool]: LLM がサンドボックス環境でコードを実行できます。\n+- [`WebSearchTool`][agents.tool.WebSearchTool]: エージェント が Web 検索 を行えるようにします。\n+- [`FileSearchTool`][agents.tool.FileSearchTool]: OpenAI の ベクトルストア から情報を取得できます。\n+- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool]: LLM がサンドボックス環境でコードを実行できるようにします。\n - [`HostedMCPTool`][agents.tool.HostedMCPTool]: リモート MCP サーバー のツールをモデルに公開します。\n - [`ImageGenerationTool`][agents.tool.ImageGenerationTool]: プロンプトから画像を生成します。\n \n@@ -42,11 +42,11 @@ async def main():\n \n ## ローカル実行ツール\n \n-ローカル実行ツールはあなたの環境で動作し、実装の提供が必要です:\n+ローカル実行ツールはあなたの環境で実行され、実装の提供が必要です:\n \n-- [`ComputerTool`][agents.tool.ComputerTool]: [`Computer`][agents.computer.Computer] または [`AsyncComputer`][agents.computer.AsyncComputer] インターフェースを実装して、GUI／ブラウザの自動化を有効にします。\n+- [`ComputerTool`][agents.tool.ComputerTool]: GUI/ブラウザ自動化を有効にするために、[`Computer`][agents.computer.Computer] または [`AsyncComputer`][agents.computer.AsyncComputer] インターフェースを実装します。\n - [`ShellTool`][agents.tool.ShellTool] または [`LocalShellTool`][agents.tool.LocalShellTool]: コマンドを実行するシェル実行器を提供します。\n-- [`ApplyPatchTool`][agents.tool.ApplyPatchTool]: [`ApplyPatchEditor`][agents.editor.ApplyPatchEditor] を実装してローカルに差分を適用します。\n+- [`ApplyPatchTool`][agents.tool.ApplyPatchTool]: ローカルで diff を適用するために [`ApplyPatchEditor`][agents.editor.ApplyPatchEditor] を実装します。\n \n ```python\n from agents import Agent, ApplyPatchTool, ShellTool\n@@ -90,14 +90,14 @@ agent = Agent(\n \n ## 関数ツール\n \n-任意の Python 関数をツールとして使えます。Agents SDK が自動的にツールをセットアップします:\n+任意の Python 関数をツールとして使用できます。Agents SDK が自動的にセットアップします:\n \n-- ツール名は Python 関数名になります（任意で名前を指定可能）\n-- ツールの説明は関数の docstring から取得されます（任意で説明を指定可能）\n+- ツール名は Python 関数名になります（または名前を指定できます）\n+- ツールの説明は関数の docstring から取得されます（または説明を指定できます）\n - 関数入力のスキーマは関数の引数から自動生成されます\n-- 各入力の説明は、無効化しない限り関数の docstring から取得されます\n+- 各入力の説明は、無効化しない限り、関数の docstring から取得されます\n \n-Python の `inspect` モジュールを使って関数シグネチャを抽出し、[`griffe`](https://mkdocstrings.github.io/griffe/) で docstring を解析し、スキーマ生成には `pydantic` を使用します。\n+Python の `inspect` モジュールで関数シグネチャを抽出し、[`griffe`](https://mkdocstrings.github.io/griffe/) で docstring を解析し、スキーマ作成には `pydantic` を使用します。\n \n ```python\n import json\n@@ -149,12 +149,12 @@ for tool in agent.tools:\n \n ```\n \n-1. 関数の引数には任意の Python 型を使え、関数は同期／非同期のどちらでも構いません。\n-2. docstring が存在する場合、説明と引数の説明を取得するために使用します。\n-3. 関数は任意で `context` を受け取れます（最初の引数である必要があります）。ツール名、説明、docstring スタイルなどの上書き設定も可能です。\n-4. デコレートした関数をツール一覧に渡せます。\n+1. 関数の引数には任意の Python 型を使用でき、関数は同期/非同期いずれでも構いません。\n+2. docstring があれば、説明や引数の説明を取得するために使用します。\n+3. 関数は任意で `context` を受け取れます（最初の引数である必要があります）。ツール名、説明、docstring スタイルなどのオーバーライドも設定できます。\n+4. デコレートした関数を tools のリストに渡せます。\n \n-??? note \"出力を表示するには展開\"\n+??? note \"出力を表示\"\n \n     ```\n     fetch_weather\n@@ -224,22 +224,22 @@ for tool in agent.tools:\n     }\n     ```\n \n-### 関数ツールから画像やファイルを返す\n+### 関数ツールからの画像またはファイルの返却\n \n-テキスト出力に加えて、関数ツールの出力として 1 つまたは複数の画像やファイルを返せます。次のいずれかを返してください:\n+テキスト出力に加えて、関数ツールの出力として 1 つまたは複数の画像やファイルを返すこともできます。次のいずれかを返せます:\n \n - 画像: [`ToolOutputImage`][agents.tool.ToolOutputImage]（または TypedDict 版の [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict]）\n - ファイル: [`ToolOutputFileContent`][agents.tool.ToolOutputFileContent]（または TypedDict 版の [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict]）\n-- テキスト: 文字列または文字列化可能なオブジェクト、または [`ToolOutputText`][agents.tool.ToolOutputText]（または TypedDict 版の [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict]）\n+- テキスト: 文字列または文字列化可能なオブジェクト、あるいは [`ToolOutputText`][agents.tool.ToolOutputText]（または TypedDict 版の [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict]）\n \n ### カスタム関数ツール\n \n-Python 関数をツールとして使いたくない場合もあります。その場合は [`FunctionTool`][agents.tool.FunctionTool] を直接作成できます。次を指定する必要があります:\n+Python 関数をツールとして使いたくない場合もあります。必要に応じて直接 [`FunctionTool`][agents.tool.FunctionTool] を作成できます。次を提供する必要があります:\n \n - `name`\n - `description`\n-- 引数の JSON スキーマ である `params_json_schema`\n-- [`ToolContext`][agents.tool_context.ToolContext] と引数の JSON 文字列を受け取り、ツールの出力を文字列で返す非同期関数 `on_invoke_tool`\n+- `params_json_schema`（引数の JSON スキーマ）\n+- `on_invoke_tool`（[`ToolContext`][agents.tool_context.ToolContext] と引数の JSON 文字列を受け取り、ツール出力を文字列で返す 非同期 関数）\n \n ```python\n from typing import Any\n@@ -272,18 +272,18 @@ tool = FunctionTool(\n )\n ```\n \n-### 引数と docstring の自動解析\n+### 引数および docstring の自動解析\n \n-前述のとおり、ツールのスキーマを抽出するために関数シグネチャを自動解析し、ツールおよび各引数の説明を抽出するために docstring を解析します。注意点:\n+前述のとおり、ツールのスキーマを抽出するために関数シグネチャを自動解析し、ツールおよび個々の引数の説明を抽出するために docstring を解析します。補足事項:\n \n-1. シグネチャ解析は `inspect` モジュール経由で行います。型アノテーションから引数の型を理解し、全体のスキーマを表す Pydantic モデルを動的に構築します。Python の基本型、Pydantic モデル、TypedDict などほとんどの型をサポートします。\n-2. `griffe` を使って docstring を解析します。サポートする docstring 形式は `google`、`sphinx`、`numpy` です。docstring 形式は自動検出を試みますがベストエフォートのため、`function_tool` 呼び出し時に明示的に設定できます。`use_docstring_info` を `False` に設定して docstring 解析を無効化することもできます。\n+1. シグネチャ解析は `inspect` モジュールで行います。型アノテーションにより引数の型を把握し、全体のスキーマを表現する Pydantic モデルを動的に構築します。Python の基本型、Pydantic モデル、TypedDict など、ほとんどの型をサポートします。\n+2. docstring の解析には `griffe` を使用します。サポートする docstring 形式は `google`、`sphinx`、`numpy` です。docstring 形式は自動検出を試みますがベストエフォートであり、`function_tool` 呼び出し時に明示的に設定できます。`use_docstring_info` を `False` に設定して docstring 解析を無効化することもできます。\n \n スキーマ抽出のコードは [`agents.function_schema`][] にあります。\n \n-## ツールとしてのエージェント\n+## エージェント as tools\n \n-一部のワークフローでは、制御を引き渡す代わりに、中央のエージェントが専門エージェントのネットワークをオーケストレーションしたい場合があります。これは、エージェントをツールとしてモデリングすることで実現できます。\n+一部のワークフローでは、制御を引き渡す代わりに、中央の エージェント が専門 エージェント のネットワークをオーケストレーションしたい場合があります。エージェント をツールとしてモデリングすることで実現できます。\n \n ```python\n from agents import Agent, Runner\n@@ -324,7 +324,7 @@ async def main():\n \n ### ツール化エージェントのカスタマイズ\n \n-`agent.as_tool` 関数は、エージェントを簡単にツール化するための便利メソッドです。ただし、すべての設定をサポートしているわけではありません。例えば `max_turns` は設定できません。高度なユースケースでは、ツール実装内で直接 `Runner.run` を使用してください:\n+`agent.as_tool` 関数は、エージェント をツールに変換しやすくするためのユーティリティです。ただし、すべての設定をサポートしているわけではありません。たとえば `max_turns` は設定できません。高度なユースケースでは、ツール実装内で直接 `Runner.run` を使用してください:\n \n ```python\n @function_tool\n@@ -343,15 +343,15 @@ async def run_my_agent() -> str:\n     return str(result.final_output)\n ```\n \n-### カスタム出力抽出\n+### 出力のカスタム抽出\n \n-場合によっては、中央のエージェントに返す前にツール化したエージェントの出力を変更したいことがあります。例えば次のような場合に有用です:\n+場合によっては、中央の エージェント に返す前にツール化した エージェント の出力を加工したいことがあります。次のような場合に有用です:\n \n-- サブエージェントのチャット履歴から特定の情報（例: JSON ペイロード）を抽出する。\n-- エージェントの最終回答を変換・再整形する（例: Markdown をプレーンテキストや CSV に変換）。\n-- 出力を検証し、応答が欠落または不正な場合にフォールバック値を提供する。\n+- サブ エージェント のチャット履歴から特定の情報（例: JSON ペイロード）を抽出する。\n+- エージェント の最終回答を変換または再フォーマットする（例: Markdown をプレーンテキストや CSV に変換）。\n+- 出力を検証し、エージェント の応答が欠落または不正な場合にフォールバック値を提供する。\n \n-これは、`as_tool` メソッドに `custom_output_extractor` 引数を渡すことで行えます:\n+これは `as_tool` メソッドに `custom_output_extractor` 引数を渡すことで実現できます:\n \n ```python\n async def extract_json_payload(run_result: RunResult) -> str:\n@@ -372,7 +372,7 @@ json_tool = data_agent.as_tool(\n \n ### ネストしたエージェント実行の ストリーミング\n \n-`as_tool` に `on_stream` コールバックを渡すと、ストリーム完了後に最終出力を返しつつ、ネストしたエージェントが発行する ストリーミング イベントを受け取れます。\n+`as_tool` に `on_stream` コールバックを渡すと、ネストした エージェント が発行する ストリーミング イベントを受け取りつつ、ストリーム完了後に最終出力を返せます。\n \n ```python\n from agents import AgentToolStreamEvent\n@@ -390,17 +390,17 @@ billing_agent_tool = billing_agent.as_tool(\n )\n ```\n \n-想定される動作:\n+期待される動作:\n \n-- イベント種別は `StreamEvent[\"type\"]` に対応します: `raw_response_event`、`run_item_stream_event`、`agent_updated_stream_event`。\n-- `on_stream` を指定すると、ネストしたエージェントは自動的に ストリーミング モードで実行され、最終出力を返す前にストリームが排出されます。\n-- ハンドラーは同期／非同期のいずれでも構いません。各イベントは到着順に配信されます。\n-- ツールがモデルのツール呼び出し経由で起動された場合は `tool_call_id` が存在します。直接呼び出しでは `None` の場合があります。\n+- イベントタイプは `StreamEvent[\"type\"]` に対応します: `raw_response_event`、`run_item_stream_event`、`agent_updated_stream_event`。\n+- `on_stream` を指定すると、ネストした エージェント は自動的に ストリーミング モードで実行され、最終出力を返す前にストリームが排出されます。\n+- ハンドラーは同期/非同期いずれでも構いません。各イベントは到着順に配信されます。\n+- ツールがモデルのツール呼び出しで起動された場合は `tool_call_id` が存在します。直接呼び出しでは `None` の場合があります。\n - 完全な実行可能サンプルは `examples/agent_patterns/agents_as_tools_streaming.py` を参照してください。\n \n ### 条件付きツール有効化\n \n-実行時に `is_enabled` パラメーター を使って、エージェントのツールを条件付きで有効化または無効化できます。これにより、コンテキスト、ユーザー の設定、実行時条件に基づいて、LLM に提供するツールを動的にフィルタリングできます。\n+実行時に `is_enabled` パラメーター を使用して、エージェント のツールを条件付きで有効化/無効化できます。これにより、コンテキスト、ユーザー の希望、または実行時条件に基づき、LLM に利用可能なツールを動的に絞り込めます。\n \n ```python\n import asyncio\n@@ -457,24 +457,24 @@ asyncio.run(main())\n \n `is_enabled` パラメーター は次を受け付けます:\n \n-- **ブール値**: `True`（常に有効）または `False`（常に無効）\n-- **呼び出し可能関数**: `(context, agent)` を受け取り、真偽値を返す関数\n-- **非同期関数**: 複雑な条件ロジック向けの async 関数\n+-  **ブール値**: `True`（常に有効）または `False`（常に無効）\n+-  **呼び出し可能関数**: `(context, agent)` を受け取り、真偽値を返す関数\n+-  **非同期関数**: 複雑な条件ロジック向けの async 関数\n \n-無効化されたツールは実行時に LLM から完全に隠れるため、次の用途に有用です:\n+無効化されたツールは実行時に LLM から完全に隠されるため、次の用途に有用です:\n \n - ユーザー 権限に基づく機能ゲーティング\n-- 環境別のツール可用性（dev と prod）\n-- 異なるツール構成の A/B テスト\n+- 環境別のツール可用性（開発 vs 本番）\n+- ツール構成の A/B テスト\n - 実行時状態に基づく動的ツールフィルタリング\n \n ## 関数ツールでのエラー処理\n \n `@function_tool` で関数ツールを作成する際、`failure_error_function` を渡せます。これは、ツール呼び出しがクラッシュした場合に LLM へエラー応答を提供する関数です。\n \n-- 既定（何も渡さない場合）では、エラーが発生したことを LLM に知らせる `default_tool_error_function` を実行します。\n-- 独自のエラー関数を渡した場合は、それが代わりに実行され、その応答が LLM に送信されます。\n-- 明示的に `None` を渡すと、ツール呼び出しのエラーは再スローされ、あなたが処理することになります。これは、モデルが不正な JSON を生成した場合の `ModelBehaviorError`、あなたのコードがクラッシュした場合の `UserError` などになり得ます。\n+- 既定（何も渡さない場合）では、エラーが発生したことを LLM に伝える `default_tool_error_function` を実行します。\n+- 独自のエラー関数を渡した場合はそれが実行され、その応答が LLM に送信されます。\n+- 明示的に `None` を渡すと、ツール呼び出しエラーはあなたが処理できるように再送出されます。これは、モデルが不正な JSON を生成した場合の `ModelBehaviorError` や、あなたのコードがクラッシュした場合の `UserError` などになり得ます。\n \n ```python\n from agents import function_tool, RunContextWrapper",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Ftools.md",
        "sha": "725ec9711c00fdf58d16ef8fd491a7e8b4f879e0",
        "status": "modified"
      },
      {
        "additions": 74,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Ftracing.md",
        "changes": 148,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Ftracing.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 74,
        "filename": "docs/ja/tracing.md",
        "patch": "@@ -4,52 +4,52 @@ search:\n ---\n # トレーシング\n \n-Agents SDK には組み込みのトレーシングがあり、エージェント実行中に発生するイベント（ LLM 生成、ツール呼び出し、ハンドオフ、ガードレール、さらにはカスタムイベント）を包括的に記録します。[Traces ダッシュボード](https://platform.openai.com/traces)を使って、開発時や本番環境でワークフローのデバッグ、可視化、監視ができます。\n+Agents SDK には組み込みのトレーシングが含まれており、エージェントの実行中に発生するイベントの包括的な記録を収集します。たとえば、 LLM 生成、ツールの呼び出し、ハンドオフ、ガードレール、さらにカスタムイベントなどです。[Traces ダッシュボード](https://platform.openai.com/traces)を使用して、開発時や本番環境でワークフローのデバッグ、可視化、監視ができます。\n \n !!!note\n \n-    トレーシングはデフォルトで有効です。無効にする方法は 2 つあります:\n+    トレーシングはデフォルトで有効です。トレーシングを無効にする方法は 2 つあります:\n \n-    1. 環境変数 `OPENAI_AGENTS_DISABLE_TRACING=1` を設定して、トレーシングをグローバルに無効化できます\n-    2. 単一の実行に対しては、[`agents.run.RunConfig.tracing_disabled`][] を `True` に設定して無効化できます\n+    1. 環境変数 `OPENAI_AGENTS_DISABLE_TRACING=1` を設定して、グローバルにトレーシングを無効化します\n+    2. 1 回の実行に対してのみ無効にするには、[`agents.run.RunConfig.tracing_disabled`][] を `True` に設定します\n \n-***OpenAI の API を用いた Zero Data Retention (ZDR) ポリシー下で運用する組織では、トレーシングは利用できません。***\n+***OpenAI の API を使用し Zero Data Retention (ZDR) ポリシーで運用している組織では、トレーシングは利用できません。***\n \n ## トレースとスパン\n \n--   **トレース** は「ワークフロー」の単一のエンドツーエンド操作を表します。トレースはスパンで構成されます。トレースには次のプロパティがあります:\n-    -   `workflow_name`: 論理的なワークフローまたはアプリです。例: \"Code generation\" や \"Customer service\"\n-    -   `trace_id`: トレースの一意の ID。渡さない場合は自動生成されます。形式は `trace_<32_alphanumeric>` である必要があります。\n-    -   `group_id`: オプションのグループ ID。同じ会話からの複数のトレースを関連付けます。たとえばチャットスレッド ID を使用できます。\n-    -   `disabled`: True の場合、このトレースは記録されません。\n-    -   `metadata`: トレースの任意のメタデータ。\n--   **スパン** は開始時刻と終了時刻を持つ操作を表します。スパンには次の情報があります:\n-    -   `started_at` と `ended_at` のタイムスタンプ\n-    -   そのスパンが属するトレースを表す `trace_id`\n-    -   親スパン（ある場合）を指す `parent_id`\n-    -   スパンに関する情報である `span_data`。たとえば、`AgentSpanData` にはエージェントに関する情報、`GenerationSpanData` には LLM 生成に関する情報などが含まれます。\n+- **トレース (Traces)** は「ワークフロー」の単一のエンドツーエンド操作を表します。トレースはスパンで構成されます。トレースには以下のプロパティがあります:\n+    - `workflow_name`: 論理的なワークフローまたはアプリです。例: \"Code generation\" や \"Customer service\"\n+    - `trace_id`: トレースの一意の ID。渡さない場合は自動生成されます。形式は `trace_<32_alphanumeric>` である必要があります。\n+    - `group_id`: 同じ会話からの複数のトレースをリンクするための任意のグループ ID。たとえばチャットスレッド ID を使用できます。\n+    - `disabled`: True の場合、トレースは記録されません。\n+    - `metadata`: トレースの任意のメタデータ。\n+- **スパン (Spans)** は開始時刻と終了時刻を持つ操作を表します。スパンには以下があります:\n+    - `started_at` と `ended_at` のタイムスタンプ\n+    - 所属するトレースを表す `trace_id`\n+    - このスパンの親スパン (ある場合) を指す `parent_id`\n+    - スパンに関する情報である `span_data`。たとえば、`AgentSpanData` にはエージェントに関する情報、`GenerationSpanData` には LLM 生成に関する情報が含まれます。\n \n-## デフォルトのトレーシング\n+## 既定のトレーシング\n \n-デフォルトで、 SDK は次の内容をトレースします:\n+デフォルトでは、 SDK は以下をトレースします:\n \n--   `Runner.{run, run_sync, run_streamed}()` 全体が `trace()` でラップされます\n--   エージェントが実行されるたびに、`agent_span()` でラップされます\n--   LLM 生成は `generation_span()` でラップされます\n--   関数ツールの呼び出しはそれぞれ `function_span()` でラップされます\n--   ガードレールは `guardrail_span()` でラップされます\n--   ハンドオフは `handoff_span()` でラップされます\n--   音声入力（音声認識）は `transcription_span()` でラップされます\n--   音声出力（テキスト読み上げ）は `speech_span()` でラップされます\n--   関連する音声スパンは `speech_group_span()` の下に親子付けされる場合があります\n+- 全体の `Runner.{run, run_sync, run_streamed}()` は `trace()` でラップされます。\n+- エージェントが実行されるたびに `agent_span()` でラップされます\n+- LLM の生成は `generation_span()` でラップされます\n+- 関数ツールの呼び出しはそれぞれ `function_span()` でラップされます\n+- ガードレールは `guardrail_span()` でラップされます\n+- ハンドオフは `handoff_span()` でラップされます\n+- 音声入力 (音声認識) は `transcription_span()` でラップされます\n+- 音声出力 (音声合成) は `speech_span()` でラップされます\n+- 関連する音声スパンは `speech_group_span()` の下に親子関係で配置される場合があります\n \n-デフォルトでは、トレース名は \"Agent workflow\" です。`trace` を使用する場合はこの名前を設定できますし、[`RunConfig`][agents.run.RunConfig] を使って名前やその他のプロパティを構成することもできます。\n+デフォルトでは、トレース名は \"Agent workflow\" です。`trace` を使用する場合はこの名前を設定できますし、[`RunConfig`][agents.run.RunConfig] を使用して名前やその他のプロパティを構成することもできます。\n \n-さらに、[カスタム トレース プロセッサー](#custom-tracing-processors) を設定して、他の送信先にトレースを送ることもできます（置き換えまたはセカンダリ送信先として）。\n+さらに、[カスタムトレースプロセッサー](#custom-tracing-processors) を設定して、トレースを他の送信先にプッシュできます (置き換え、またはセカンダリ送信先として)。\n \n-## 高レベルのトレース\n+## 上位レベルのトレース\n \n-`run()` への複数回の呼び出しを 1 つのトレースにまとめたい場合があります。その場合は、コード全体を `trace()` でラップします。\n+`run()` への複数回の呼び出しを 1 つのトレースの一部にしたい場合があります。これには、コード全体を `trace()` でラップします。\n \n ```python\n from agents import Agent, Runner, trace\n@@ -64,48 +64,48 @@ async def main():\n         print(f\"Rating: {second_result.final_output}\")\n ```\n \n-1. `with trace()` の中で 2 回の `Runner.run` 呼び出しを行っているため、個々の実行は 2 つのトレースを作成するのではなく、全体のトレースの一部になります。\n+1. `Runner.run` への 2 回の呼び出しが `with trace()` でラップされているため、個々の実行は 2 つのトレースを作成するのではなく、全体のトレースの一部になります。\n \n ## トレースの作成\n \n-[`trace()`][agents.tracing.trace] 関数を使ってトレースを作成できます。トレースは開始と終了が必要です。方法は 2 つあります:\n+[`trace()`][agents.tracing.trace] 関数を使用してトレースを作成できます。トレースは開始と終了が必要です。次の 2 つの方法があります:\n \n-1. 【推奨】トレースをコンテキストマネージャとして使用します（例: `with trace(...) as my_trace`）。これにより、適切なタイミングで自動的にトレースが開始・終了されます。\n-2. [`trace.start()`][agents.tracing.Trace.start] と [`trace.finish()`][agents.tracing.Trace.finish] を手動で呼び出すこともできます。\n+1. 推奨: トレースをコンテキストマネージャーとして使用します。つまり `with trace(...) as my_trace`。これにより、適切なタイミングでトレースが自動的に開始・終了します。\n+2. 手動で [`trace.start()`][agents.tracing.Trace.start] と [`trace.finish()`][agents.tracing.Trace.finish] を呼ぶこともできます。\n \n-現在のトレースは Python の [`contextvar`](https://docs.python.org/3/library/contextvars.html) で追跡されます。これは、自動的に並行処理と連携することを意味します。トレースを手動で開始/終了する場合は、現在のトレースを更新するために `start()`/`finish()` に `mark_as_current` と `reset_current` を渡す必要があります。\n+現在のトレースは Python の [`contextvar`](https://docs.python.org/3/library/contextvars.html) を通じて追跡されます。つまり、並行処理でも自動的に機能します。トレースを手動で開始/終了する場合、現在のトレースを更新するために `start()`/`finish()` に `mark_as_current` と `reset_current` を渡す必要があります。\n \n ## スパンの作成\n \n-さまざまな [`*_span()`][agents.tracing.create] メソッドを使用してスパンを作成できます。一般に、スパンを手動で作成する必要はありません。カスタムのスパン情報を追跡するための [`custom_span()`][agents.tracing.custom_span] 関数も利用できます。\n+さまざまな [`*_span()`][agents.tracing.create] メソッドを使ってスパンを作成できます。一般的には、手動でスパンを作成する必要はありません。カスタムのスパン情報を追跡するための [`custom_span()`][agents.tracing.custom_span] 関数が利用可能です。\n \n-スパンは自動的に現在のトレースの一部となり、 Python の [`contextvar`](https://docs.python.org/3/library/contextvars.html) で追跡される、最も近い現在のスパンの下にネストされます。\n+スパンは自動的に現在のトレースの一部となり、Python の [`contextvar`](https://docs.python.org/3/library/contextvars.html) によって追跡される最も近い現在のスパンの下にネストされます。\n \n ## 機微なデータ\n \n 一部のスパンは、機微なデータを取得する可能性があります。\n \n-`generation_span()` は LLM 生成の入力/出力を、`function_span()` は関数呼び出しの入力/出力を保存します。これらには機微なデータが含まれる可能性があるため、[`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] でそのデータの取得を無効化できます。\n+`generation_span()` は LLM 生成の入力/出力を保存し、`function_span()` は関数呼び出しの入力/出力を保存します。これらには機微なデータが含まれる可能性があるため、[`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] を使用してそのデータの取得を無効化できます。\n \n 同様に、音声スパンにはデフォルトで入力および出力音声の base64 エンコードされた PCM データが含まれます。[`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] を構成して、この音声データの取得を無効化できます。\n \n-デフォルトでは、`trace_include_sensitive_data` は `True` です。アプリを実行する前に環境変数 `OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA` を `true/1` または `false/0` に設定することで、コードなしでデフォルト値を変更できます。\n+デフォルトでは、`trace_include_sensitive_data` は `True` です。アプリを実行する前に環境変数 `OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA` を `true/1` または `false/0` に設定することで、コードなしでデフォルトを変更できます。\n \n-## カスタム トレーシング プロセッサー\n+## カスタムトレーシングプロセッサー\n \n トレーシングの高レベルなアーキテクチャは次のとおりです:\n \n--   初期化時にグローバルな [`TraceProvider`][agents.tracing.setup.TraceProvider] を作成し、トレースの作成を担当させます。\n--   [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] を `TraceProvider` に設定し、バッチで [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter] にトレース/スパンを送信します。エクスポーターはスパンとトレースを OpenAI のバックエンドへバッチ送信します。\n+- 初期化時に、トレースを作成する役割を持つグローバルな [`TraceProvider`][agents.tracing.setup.TraceProvider] を作成します。\n+- [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] を使用して `TraceProvider` を構成し、トレース/スパンをバッチで [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter] に送信します。これがスパンとトレースを OpenAI のバックエンドへバッチでエクスポートします。\n \n-このデフォルト設定をカスタマイズし、別のバックエンドへの送信や追加のバックエンドへの送信、あるいはエクスポーター動作の変更を行うには、次の 2 つの方法があります:\n+このデフォルト設定をカスタマイズして、別のバックエンドや追加のバックエンドに送信したり、エクスポーターの動作を変更したりするには、次の 2 つの方法があります:\n \n-1. [`add_trace_processor()`][agents.tracing.add_trace_processor] は、トレースやスパンが準備でき次第受け取る、**追加の** トレース プロセッサーを追加できます。これにより、 OpenAI のバックエンドへの送信に加えて、独自の処理を実行できます。\n-2. [`set_trace_processors()`][agents.tracing.set_trace_processors] は、デフォルトのプロセッサーを独自のトレース プロセッサーに**置き換え**られます。これを行うと、 OpenAI のバックエンドにトレースが送信されなくなります（その役割を果たす `TracingProcessor` を含めない限り）。\n+1. [`add_trace_processor()`][agents.tracing.add_trace_processor] は、トレースやスパンが準備でき次第受け取る、**追加の** トレースプロセッサーを追加できます。これにより、OpenAI のバックエンドへの送信に加えて独自の処理を行えます。\n+2. [`set_trace_processors()`][agents.tracing.set_trace_processors] は、デフォルトのプロセッサーを独自のトレースプロセッサーに**置き換え**られます。つまり、 OpenAI のバックエンドにトレースを送信する `TracingProcessor` を含めない限り、トレースは送信されません。\n \n-## 非 OpenAI モデルでのトレーシング\n+## OpenAI 以外のモデルでのトレーシング\n \n-トレーシングを無効化することなく、 OpenAI 以外のモデルでも OpenAI の API キーを使用して、 OpenAI Traces ダッシュボードで無料のトレーシングを有効にできます。\n+OpenAI の API キーを、 OpenAI 以外のモデルと併用して、トレーシングを無効化することなく、 OpenAI の Traces ダッシュボードで無料のトレーシングを有効にできます。\n \n ```python\n import os\n@@ -126,7 +126,7 @@ agent = Agent(\n )\n ```\n \n-単一の実行でのみ別のトレーシングキーが必要な場合は、グローバル エクスポーターを変更せずに、 `RunConfig` 経由で渡してください。\n+単一の実行でのみ別のトレーシングキーが必要な場合は、グローバルなエクスポーターを変更するのではなく、`RunConfig` 経由で渡してください。\n \n ```python\n from agents import Runner, RunConfig\n@@ -138,28 +138,28 @@ await Runner.run(\n )\n ```\n \n-## 注意\n-- 無料のトレースは Openai Traces ダッシュボードで表示できます。\n-\n-## 外部トレーシング プロセッサー一覧\n-\n--   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)\n--   [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)\n--   [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)\n--   [MLflow (self-hosted/OSS)](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)\n--   [MLflow (Databricks hosted)](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)\n--   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)\n--   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)\n--   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)\n--   [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)\n--   [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)\n--   [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)\n--   [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)\n--   [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)\n--   [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)\n--   [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)\n--   [Okahu-Monocle](https://github.com/monocle2ai/monocle)\n--   [Galileo](https://v2docs.galileo.ai/integrations/openai-agent-integration#openai-agent-integration)\n--   [Portkey AI](https://portkey.ai/docs/integrations/agents/openai-agents)\n--   [LangDB AI](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-openai-agents-sdk)\n--   [Agenta](https://docs.agenta.ai/observability/integrations/openai-agents)\n\\ No newline at end of file\n+## 備考\n+- OpenAI Traces ダッシュボードで無料のトレースを表示します。\n+\n+## 外部トレーシングプロセッサー一覧\n+\n+- [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)\n+- [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)\n+- [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)\n+- [MLflow (self-hosted/OSS)](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)\n+- [MLflow (Databricks hosted)](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)\n+- [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)\n+- [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)\n+- [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)\n+- [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)\n+- [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)\n+- [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)\n+- [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)\n+- [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)\n+- [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)\n+- [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)\n+- [Okahu-Monocle](https://github.com/monocle2ai/monocle)\n+- [Galileo](https://v2docs.galileo.ai/integrations/openai-agent-integration#openai-agent-integration)\n+- [Portkey AI](https://portkey.ai/docs/integrations/agents/openai-agents)\n+- [LangDB AI](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-openai-agents-sdk)\n+- [Agenta](https://docs.agenta.ai/observability/integrations/openai-agents)\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Ftracing.md",
        "sha": "3f82fb15cca258271042c1fc7fc63026eac2d757",
        "status": "modified"
      },
      {
        "additions": 14,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fusage.md",
        "changes": 28,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fusage.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 14,
        "filename": "docs/ja/usage.md",
        "patch": "@@ -4,13 +4,13 @@ search:\n ---\n # 使用状況\n \n-Agents SDK は、すべての実行ごとにトークンの使用状況を自動で追跡します。実行コンテキストから参照でき、コストの監視、制限の適用、分析の記録に利用できます。\n+Agents SDK は、すべての実行におけるトークン使用状況を自動で追跡します。実行コンテキストから参照でき、コストの監視、制限の適用、分析の記録に利用できます。\n \n ## 追跡対象\n \n-- **requests**: 実行された LLM API 呼び出し数\n-- **input_tokens**: 送信された入力トークンの合計\n-- **output_tokens**: 受信した出力トークンの合計\n+- **requests**: 実行された LLM API 呼び出しの回数\n+- **input_tokens**: 送信された入力トークン合計\n+- **output_tokens**: 受信した出力トークン合計\n - **total_tokens**: 入力 + 出力\n - **request_usage_entries**: リクエストごとの使用状況内訳のリスト\n - **details**:\n@@ -35,7 +35,7 @@ print(\"Total tokens:\", usage.total_tokens)\n \n ### LiteLLM モデルでの使用状況の有効化\n \n-LiteLLM プロバイダーは、デフォルトでは使用状況メトリクスを報告しません。[`LitellmModel`](models/litellm.md) を使用する場合、エージェントに `ModelSettings(include_usage=True)` を渡すと、LiteLLM のレスポンスが `result.context_wrapper.usage` を埋めるようになります。\n+LiteLLM プロバイダーはデフォルトでは使用状況メトリクスを報告しません。[`LitellmModel`](models/litellm.md) を使用する場合、エージェントに `ModelSettings(include_usage=True)` を渡すことで、LiteLLM のレスポンスが `result.context_wrapper.usage` に反映されます。\n \n ```python\n from agents import Agent, ModelSettings, Runner\n@@ -51,9 +51,9 @@ result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n print(result.context_wrapper.usage.total_tokens)\n ```\n \n-## リクエスト単位の使用状況トラッキング\n+## リクエスト単位の使用状況追跡\n \n-SDK は、`request_usage_entries` 内で各 API リクエストの使用状況を自動追跡します。詳細なコスト計算やコンテキストウィンドウ消費の監視に有用です。\n+SDK は `request_usage_entries` 内で各 API リクエストの使用状況を自動追跡します。詳細なコスト計算やコンテキストウィンドウ消費の監視に便利です。\n \n ```python\n result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n@@ -64,7 +64,7 @@ for i, request in enumerate(result.context_wrapper.usage.request_usage_entries):\n \n ## セッションでの使用状況へのアクセス\n \n-`Session`（例: `SQLiteSession`）を使用する場合、`Runner.run(...)` の各呼び出しは、その実行に固有の使用状況を返します。セッションはコンテキストのために会話履歴を保持しますが、各実行の使用状況は独立しています。\n+`Session`（例: `SQLiteSession`）を使用する場合、`Runner.run(...)` の各呼び出しはその実行に特化した使用状況を返します。セッションは文脈のための会話履歴を保持しますが、各実行の使用状況は独立しています。\n \n ```python\n session = SQLiteSession(\"my_conversation\")\n@@ -76,11 +76,11 @@ second = await Runner.run(agent, \"Can you elaborate?\", session=session)\n print(second.context_wrapper.usage.total_tokens)  # Usage for second run\n ```\n \n-セッションは実行間で会話コンテキストを保持しますが、各 `Runner.run()` 呼び出しで返される使用状況メトリクスは、その実行単体のみを表します。セッションでは、前のメッセージが各実行の入力として再投入されることがあり、その結果として後続ターンの入力トークン数に影響します。\n+セッションは実行間で会話コンテキストを保持しますが、各 `Runner.run()` 呼び出しで返される使用状況メトリクスはその実行のみを表します。セッションでは、前のメッセージが各実行の入力として再投入される場合があり、その結果、後続ターンの入力トークン数に影響します。\n \n ## フックでの使用状況の利用\n \n-`RunHooks` を使用している場合、各フックに渡される `context` オブジェクトには `usage` が含まれます。これにより、ライフサイクルの重要な時点で使用状況を記録できます。\n+`RunHooks` を使用している場合、各フックに渡される `context` オブジェクトは `usage` を含みます。これにより、重要なライフサイクル時点で使用状況を記録できます。\n \n ```python\n class MyHooks(RunHooks):\n@@ -93,7 +93,7 @@ class MyHooks(RunHooks):\n \n 詳細な API ドキュメントは以下を参照してください。\n \n-- [`Usage`][agents.usage.Usage] - 使用状況を追跡するデータ構造\n-- [`RequestUsage`][agents.usage.RequestUsage] - リクエストごとの使用状況詳細\n-- [`RunContextWrapper`][agents.run.RunContextWrapper] - 実行コンテキストから使用状況にアクセス\n-- [`RunHooks`][agents.run.RunHooks] - 使用状況トラッキングのライフサイクルにフック\n\\ No newline at end of file\n+-   [`Usage`][agents.usage.Usage] - 使用状況追跡のデータ構造\n+-   [`RequestUsage`][agents.usage.RequestUsage] - リクエスト単位の使用状況の詳細\n+-   [`RunContextWrapper`][agents.run.RunContextWrapper] - 実行コンテキストからの使用状況へのアクセス\n+-   [`RunHooks`][agents.run.RunHooks] - 使用状況追跡ライフサイクルへのフック\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fusage.md",
        "sha": "273b0b164efd356c34a4f0089603bcb0d63b870f",
        "status": "modified"
      },
      {
        "additions": 20,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvisualization.md",
        "changes": 40,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fvisualization.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 20,
        "filename": "docs/ja/visualization.md",
        "patch": "@@ -4,24 +4,24 @@ search:\n ---\n # エージェントの可視化\n \n-エージェントの可視化では、 **Graphviz** を使用してエージェントとその関係の構造化されたグラフィカル表現を生成できます。これは、アプリケーション内でエージェント、ツール、ハンドオフがどのように相互作用するかを理解するのに役立ちます。\n+エージェントの可視化では、 **Graphviz** を使用してエージェントとその関係を構造化されたグラフィカル表現として生成できます。これは、アプリケーション内でエージェント、ツール、ハンドオフがどのように相互作用するかを理解するのに役立ちます。\n \n ## インストール\n \n-オプションの `viz` 依存関係グループをインストールします:\n+オプションの `viz` 依存関係グループをインストールします。\n \n ```bash\n pip install \"openai-agents[viz]\"\n ```\n \n ## グラフの生成\n \n-`draw_graph` 関数を使用してエージェントの可視化を生成できます。この関数は次のような有向グラフを作成します:\n+`draw_graph` 関数を使用してエージェントの可視化を生成できます。この関数は次のような有向グラフを作成します。\n \n-- **エージェント** は黄色のボックスで表されます。\n-- **MCP サーバー** は灰色のボックスで表されます。\n-- **ツール** は緑色の楕円で表されます。\n-- **ハンドオフ** は、あるエージェントから別のエージェントへの有向エッジとして表されます。\n+- **エージェント** は黄色のボックスで表現されます。\n+- **MCP サーバー** は灰色のボックスで表現されます。\n+- **ツール** は緑の楕円で表現されます。\n+- **ハンドオフ** は、あるエージェントから別のエージェントへの有向エッジです。\n \n ### 使用例\n \n@@ -67,38 +67,38 @@ triage_agent = Agent(\n draw_graph(triage_agent)\n ```\n \n-![エージェント グラフ](../assets/images/graph.png)\n+![Agent Graph](../assets/images/graph.png)\n \n-これは、 **トリアージ エージェント** の構造と、サブエージェントやツールとの接続を視覚的に表すグラフを生成します。\n+これは、 **トリアージ エージェント** の構造と、サブエージェントやツールへの接続を視覚的に表すグラフを生成します。\n \n \n ## 可視化の理解\n \n-生成されるグラフには以下が含まれます:\n+生成されるグラフには次が含まれます。\n \n-- エントリーポイントを示す **開始ノード** (`__start__`)。\n-- 黄塗りの **長方形** で表されるエージェント。\n-- 緑塗りの **楕円** で表されるツール。\n-- 灰塗りの **長方形** で表される MCP サーバー。\n+- エントリポイントを示す **開始ノード** (`__start__`)。\n+- 黄色で塗りつぶされた **長方形** で表現されるエージェント。\n+- 緑で塗りつぶされた **楕円** で表現されるツール。\n+- 灰色で塗りつぶされた **長方形** で表現される MCP サーバー。\n - 相互作用を示す有向エッジ:\n-  - エージェント間のハンドオフには **実線の矢印**。\n-  - ツール呼び出しには **点線の矢印**。\n-  - MCP サーバー呼び出しには **破線の矢印**。\n+  - エージェント間のハンドオフを示す **実線の矢印**。\n+  - ツール呼び出しを示す **点線の矢印**。\n+  - MCP サーバー呼び出しを示す **破線の矢印**。\n - 実行の終了地点を示す **終了ノード** (`__end__`)。\n \n-**注:** MCP サーバーは最近の `agents` パッケージのバージョンでレンダリングされます（ **v0.2.8** で確認済み）。可視化に MCP ボックスが表示されない場合は、最新リリースにアップグレードしてください。\n+**Note:** MCP サーバーは最近の `agents` パッケージのバージョンでレンダリングされます（ **v0.2.8** で確認済み）。可視化に MCP のボックスが表示されない場合は、最新リリースにアップグレードしてください。\n \n ## グラフのカスタマイズ\n \n ### グラフの表示\n-デフォルトでは、`draw_graph` はグラフをインラインで表示します。別ウィンドウでグラフを表示するには、次のように記述します:\n+既定では、`draw_graph` はグラフをインライン表示します。別ウィンドウに表示するには、次を記述します。\n \n ```python\n draw_graph(triage_agent).view()\n ```\n \n ### グラフの保存\n-デフォルトでは、`draw_graph` はグラフをインラインで表示します。ファイルとして保存するには、ファイル名を指定します:\n+既定では、`draw_graph` はグラフをインライン表示します。ファイルとして保存するには、ファイル名を指定します。\n \n ```python\n draw_graph(triage_agent, filename=\"agent_graph\")",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvisualization.md",
        "sha": "be9d70a9543a68044c3599b0c298e504e745ff71",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvoice%2Fpipeline.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fvoice%2Fpipeline.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/ja/voice/pipeline.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # パイプラインとワークフロー\n \n-[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] は、エージェント ベースのワークフローを音声アプリに変換しやすくするクラスです。実行するワークフローを渡すと、パイプラインが入力音声の文字起こし、音声の終了検出、適切なタイミングでのワークフロー呼び出し、そしてワークフロー出力の音声への変換までを処理します。\n+[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] は、エージェントのワークフローを音声アプリに簡単に変換できるクラスです。実行するワークフローを渡すと、パイプラインが入力音声の文字起こし、音声の終了検知、適切なタイミングでのワークフロー呼び出し、そしてワークフロー出力の音声化までを処理します。\n \n ```mermaid\n graph LR\n@@ -36,27 +36,27 @@ graph LR\n \n パイプラインを作成する際、次の項目を設定できます。\n \n-1. [`workflow`][agents.voice.workflow.VoiceWorkflowBase]: 新しい音声が文字起こしされるたびに実行されるコードです。\n-2. 使用する [`speech-to-text`][agents.voice.model.STTModel] と [`text-to-speech`][agents.voice.model.TTSModel] のモデル\n-3. [`config`][agents.voice.pipeline_config.VoicePipelineConfig]: 次のような設定ができます。\n-    - モデルプロバイダー。モデル名をモデルにマッピングできます\n-    - トレーシング（トレーシングの無効化可否、音声ファイルのアップロード有無、ワークフロー名、トレース ID など）\n-    - TTS と STT のモデルに関する各種設定（プロンプト、言語、使用するデータ型など）\n+1. 新しい音声が文字起こしされるたびに実行されるコードである [`workflow`][agents.voice.workflow.VoiceWorkflowBase]\n+2. 使用する [`speech-to-text`][agents.voice.model.STTModel] および [`text-to-speech`][agents.voice.model.TTSModel] のモデル\n+3. 次のような項目を設定できる [`config`][agents.voice.pipeline_config.VoicePipelineConfig]\n+    - モデル名をモデルにマッピングできるモデルプロバイダー\n+    - トレーシング（無効化の可否、音声ファイルのアップロード可否、ワークフロー名、Trace ID など）\n+    - TTS および STT モデルの設定（プロンプト、言語、使用するデータ型など）\n \n ## パイプラインの実行\n \n-パイプラインは [`run()`][agents.voice.pipeline.VoicePipeline.run] メソッドで実行できます。音声入力は 2 つの形式で渡せます。\n+[`run()`][agents.voice.pipeline.VoicePipeline.run] メソッドでパイプラインを実行できます。音声入力は次の 2 つの形式で渡せます。\n \n-1. [`AudioInput`][agents.voice.input.AudioInput]: 完全な音声の書き起こしがあり、それに対する結果の生成だけを行いたい場合に使います。あらかじめ録音された音声や、ユーザーが話し終えるタイミングが明確なプッシュトゥトーク アプリなど、話者の終了検出が不要なケースに適しています。\n-2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]: ユーザーが話し終えたタイミングを検出する必要がある場合に使います。検出された音声チャンクをプッシュでき、音声パイプラインは「アクティビティ検出 (activity detection)」と呼ばれるプロセスによって、適切なタイミングでエージェントのワークフローを自動的に実行します。\n+1. [`AudioInput`][agents.voice.input.AudioInput] は、完全な音声の書き起こし結果があり、その結果に対する出力だけを生成したい場合に使用します。これは、話者の発話終了を検知する必要がないケース、たとえば事前録音の音声や、ユーザーの発話終了が明確なプッシュトゥトークのアプリで有用です。\n+2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] は、ユーザーの発話終了を検知する必要がある場合に使用します。検出された音声チャンクを随時プッシュでき、音声パイプラインは「アクティビティ検出」と呼ばれるプロセスを通じて、適切なタイミングで自動的にエージェントのワークフローを実行します。\n \n ## 結果\n \n-音声パイプラインの実行結果は [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult] です。これは、発生したイベントをストリーミングできるオブジェクトです。いくつかの種類の [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent] があります。\n+音声パイプライン実行の結果は [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult] です。これは、発生したイベントをストリーミングで受け取れるオブジェクトです。いくつかの種類の [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent] があり、次を含みます。\n \n-1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]: 音声チャンクを含みます。\n-2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]: ターンの開始や終了といったライフサイクルイベントを通知します。\n-3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]: エラーイベントです。\n+1. 音声チャンクを含む [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]\n+2. ターンの開始や終了などのライフサイクルイベントを通知する [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]\n+3. エラーイベントである [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]\n \n ```python\n \n@@ -76,4 +76,4 @@ async for event in result.stream():\n \n ### 割り込み\n \n-Agents SDK は現在、[`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] に対するビルトインの割り込み処理をサポートしていません。検出された各ターンごとに、ワークフローの個別の実行がトリガーされます。アプリケーション内で割り込みを扱いたい場合は、[`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] イベントを監視してください。`turn_started` は新しいターンが文字起こしされ処理が開始したことを示し、`turn_ended` は該当するターンの音声がすべて送出された後に発火します。これらのイベントを用いて、モデルがターンを開始した際に話者のマイクをミュートし、ターンに関連する音声をすべてフラッシュした後にミュート解除する、といった制御が可能です。\n\\ No newline at end of file\n+Agents SDK は現在、[`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] に対する組み込みの割り込みサポートを提供していません。代わりに、検出された各ターンごとに、ワークフローの個別の実行がトリガーされます。アプリケーション内で割り込みを扱いたい場合は、[`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] イベントをリッスンしてください。`turn_started` は新しいターンが文字起こしされ、処理が開始されたことを示します。`turn_ended` は該当ターンのすべての音声がディスパッチされた後に発火します。モデルがターンを開始したときに話者のマイクをミュートし、そのターンに関連する音声をすべてフラッシュした後にアンミュートする、といった用途にこれらのイベントを利用できます。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvoice%2Fpipeline.md",
        "sha": "4d71556a7a9b8f5b50a21ed523650d256365b0be",
        "status": "modified"
      },
      {
        "additions": 10,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvoice%2Fquickstart.md",
        "changes": 23,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fvoice%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 13,
        "filename": "docs/ja/voice/quickstart.md",
        "patch": "@@ -6,19 +6,19 @@ search:\n \n ## 前提条件\n \n-Agents SDK の基本 [クイックスタート手順](../quickstart.md) に従い、仮想環境を設定してください。次に、SDK から音声用のオプション依存関係をインストールします。\n+Agents SDK の基本的な[クイックスタート手順](../quickstart.md)に従い、仮想環境をセットアップしてください。次に、SDK から音声関連のオプション依存関係をインストールします。\n \n ```bash\n pip install 'openai-agents[voice]'\n ```\n \n ## 概念\n \n-主に知っておくべき概念は、[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] です。これは 3 ステップのプロセスです。\n+中心となる概念は [`VoicePipeline`][agents.voice.pipeline.VoicePipeline] で、これは 3 ステップのプロセスです。\n \n-1. 音声をテキストに変換するために、音声認識モデルを実行します。\n-2. 通常はエージェント的なワークフローであるあなたのコードを実行し、結果を生成します。\n-3. 結果のテキストを音声に戻すために、音声合成モデルを実行します。\n+1. 音声をテキストに変換するために音声認識（speech-to-text）モデルを実行します。\n+2. 通常はエージェントによるワークフローであるあなたのコードを実行して結果を生成します。\n+3. 結果のテキストを音声に戻すために音声合成（text-to-speech）モデルを実行します。\n \n ```mermaid\n graph LR\n@@ -48,7 +48,7 @@ graph LR\n \n ## エージェント\n \n-まず、いくつかのエージェントを設定しましょう。これは、この SDK でエージェントを構築したことがあれば馴染みがあるはずです。ここでは、複数のエージェント、ハンドオフ、そしてツールを用意します。\n+まず、いくつかのエージェントを設定します。これは、この SDK でエージェントを作成したことがある方にはおなじみのはずです。ここでは、2 つのエージェント、ハンドオフ、そして 1 つのツールを用意します。\n \n ```python\n import asyncio\n@@ -92,12 +92,9 @@ agent = Agent(\n \n ## 音声パイプライン\n \n-ワークフローとして [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] を使用して、シンプルな音声パイプラインを設定します。\n+ワークフローとして [`SingleAgentVoiceWorkflow`][agents.voice.workflow.SingleAgentVoiceWorkflow] を使用し、シンプルな音声パイプラインを設定します。\n \n-```python\n-from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline\n-pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n-```\n+CODE_BLOCK_3\n \n ## パイプラインの実行\n \n@@ -124,7 +121,7 @@ async for event in result.stream():\n \n ```\n \n-## まとめ\n+## まとめて実行\n \n ```python\n import asyncio\n@@ -195,4 +192,4 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-このサンプルを実行すると、エージェントがあなたに話しかけます。自分でエージェントに話しかけられるデモは、[examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) をご覧ください。\n\\ No newline at end of file\n+このサンプルを実行すると、エージェントがあなたに話しかけます。自分でエージェントに話しかけられるデモは [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) をご覧ください。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvoice%2Fquickstart.md",
        "sha": "cfc2228ed8fe06af1563e298792a79e236ce5aef",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvoice%2Ftracing.md",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fja%2Fvoice%2Ftracing.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 8,
        "filename": "docs/ja/voice/tracing.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # トレーシング\n \n-[エージェントのトレーシング](../tracing.md) と同様に、音声パイプラインも自動的にトレーシングされます。\n+[エージェント](../tracing.md) がトレースされるのと同様に、音声パイプラインも自動的にトレースされます。\n \n-基本的な情報は上記のトレーシングドキュメントをご参照ください。加えて、[`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig] を使ってパイプラインのトレーシングを構成できます。\n+基本的なトレーシング情報については上記のトレーシングドキュメントを参照できますが、[`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig] を通じてパイプラインのトレーシングを追加で設定できます。\n \n-主なトレーシング関連フィールドは次のとおりです:\n+トレーシング関連の主なフィールドは次のとおりです。\n \n--   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: トレーシングを無効にするかどうかを制御します。既定では有効です。\n--   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: 音声書き起こしのような潜在的に機微なデータをトレースに含めるかどうかを制御します。これは音声パイプライン専用であり、ワークフロー内部で起こることには適用されません。\n--   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: 音声データをトレースに含めるかどうかを制御します。\n--   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: トレースのワークフロー名です。\n--   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: 複数のトレースをリンクできるようにする、このトレースの `group_id` です。\n+-   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: トレーシングを無効にするかどうかを制御します。デフォルトではトレーシングは有効です。\n+-   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: 音声書き起こしのような潜在的に機微なデータをトレースに含めるかどうかを制御します。これは音声パイプライン専用であり、Workflow 内部で行われることには適用されません。\n+-   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: トレースに音声データを含めるかどうかを制御します。\n+-   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: トレースワークフローの名前です。\n+-   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: 複数のトレースを関連付けるためのトレースの `group_id` です。\n -   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: トレースに含める追加のメタデータです。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fja%2Fvoice%2Ftracing.md",
        "sha": "2319bbc6f6ea586c072f09adff5f317ce6a0ad66",
        "status": "modified"
      },
      {
        "additions": 29,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fagents.md",
        "changes": 58,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fagents.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 29,
        "filename": "docs/ko/agents.md",
        "patch": "@@ -4,16 +4,16 @@ search:\n ---\n # 에이전트\n \n-에이전트는 앱의 핵심 기본 구성 요소입니다. 에이전트는 instructions 및 도구로 구성된 대규모 언어 모델(LLM)입니다.\n+에이전트는 앱의 핵심 빌딩 블록입니다. 에이전트는 instructions 와 tools 로 구성된 대규모 언어 모델(LLM)입니다.\n \n ## 기본 구성\n \n-에이전트에서 가장 자주 구성하는 속성은 다음과 같습니다:\n+에이전트를 구성할 때 가장 흔히 설정하는 속성은 다음과 같습니다:\n \n -   `name`: 에이전트를 식별하는 필수 문자열\n--   `instructions`: developer message 또는 system prompt 라고도 부름\n--   `model`: 사용할 LLM 및 temperature, top_p 등 모델 튜닝 매개변수를 구성하는 선택적 `model_settings`\n--   `tools`: 에이전트가 작업을 달성하기 위해 사용할 수 있는 도구\n+-   `instructions`: 개발자 메시지 또는 시스템 프롬프트(system prompt)라고도 함\n+-   `model`: 사용할 LLM과, temperature, top_p 등 모델 튜닝 매개변수를 설정하는 선택적 `model_settings`\n+-   `tools`: 에이전트가 작업을 수행하기 위해 사용할 수 있는 도구들\n \n ```python\n from agents import Agent, ModelSettings, function_tool\n@@ -33,7 +33,7 @@ agent = Agent(\n \n ## 컨텍스트\n \n-에이전트는 `context` 타입에 대해 제네릭입니다. 컨텍스트는 의존성 주입 도구로, 여러분이 생성하여 `Runner.run()`에 전달하는 객체이며 모든 에이전트, 도구, 핸드오프 등으로 전달되어 에이전트 실행을 위한 의존성과 상태를 담는 잡동사니 역할을 합니다. 컨텍스트로는 어떤 Python 객체든 제공할 수 있습니다.\n+에이전트는 `context` 타입에 대해 제네릭합니다. 컨텍스트는 의존성 주입 도구로, 사용자가 생성하여 `Runner.run()` 에 전달하는 객체이며, 모든 에이전트, 도구, 핸드오프 등으로 전달되어 에이전트 실행을 위한 의존성과 상태를 담는 보관함 역할을 합니다. 컨텍스트로는 어떤 Python 객체든 제공할 수 있습니다.\n \n ```python\n @dataclass\n@@ -50,9 +50,9 @@ agent = Agent[UserContext](\n )\n ```\n \n-## 출력 유형\n+## 출력 타입\n \n-기본적으로, 에이전트는 일반 텍스트(즉, `str`) 출력을 생성합니다. 에이전트가 특정 유형의 출력을 생성하도록 하려면 `output_type` 매개변수를 사용할 수 있습니다. 일반적인 선택은 [Pydantic](https://docs.pydantic.dev/) 객체를 사용하는 것이지만, Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/)로 래핑할 수 있는 모든 유형(데이터클래스, 리스트, TypedDict 등)을 지원합니다.\n+기본적으로 에이전트는 일반 텍스트(즉, `str`) 출력을 생성합니다. 에이전트가 특정 타입의 출력을 생성하도록 하려면 `output_type` 매개변수를 사용할 수 있습니다. 흔히 [Pydantic](https://docs.pydantic.dev/) 객체를 사용하지만, Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) 로 감쌀 수 있는 어떤 타입이든 지원합니다 — dataclasses, lists, TypedDict 등.\n \n ```python\n from pydantic import BaseModel\n@@ -73,20 +73,20 @@ agent = Agent(\n \n !!! note\n \n-    `output_type`을 전달하면 모델이 일반 텍스트 응답 대신 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)을 사용하도록 지시합니다.\n+    `output_type` 을 전달하면, 모델이 일반 텍스트 응답 대신 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) 를 사용하도록 지시합니다.\n \n ## 멀티 에이전트 시스템 설계 패턴\n \n-멀티 에이전트 시스템을 설계하는 방법은 다양하지만, 일반적으로 두 가지 널리 적용 가능한 패턴이 있습니다:\n+멀티 에이전트 시스템을 설계하는 방법은 다양하지만, 일반적으로 두 가지 폭넓게 적용 가능한 패턴이 있습니다:\n \n-1. 매니저(도구로서의 에이전트): 중앙 매니저/오케스트레이터가 특화된 하위 에이전트를 도구로 호출하고 대화를 제어합니다\n-2. 핸드오프: 동등한 에이전트 간에 제어를 특화된 에이전트로 넘겨 그 에이전트가 대화를 인수합니다. 이는 탈중앙화된 방식입니다\n+1. 매니저(도구로서의 에이전트): 중앙 매니저/오케스트레이터가 특화된 하위 에이전트를 도구처럼 호출하며 대화의 제어권을 유지\n+2. 핸드오프: 동등한 에이전트들 간에 제어권을 특화된 에이전트에게 넘기며, 해당 에이전트가 대화를 인수함. 분산형임\n \n-자세한 내용은 [에이전트 구축 실무 가이드](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf)를 참고하세요.\n+자세한 내용은 [에이전트 구축 실용 가이드](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf) 를 참고하세요.\n \n ### 매니저(도구로서의 에이전트)\n \n-`customer_facing_agent`는 모든 사용자 상호작용을 처리하고 도구로 노출된 특화 하위 에이전트를 호출합니다. 자세한 내용은 [도구](tools.md#agents-as-tools) 문서를 참고하세요.\n+`customer_facing_agent` 가 모든 사용자 상호작용을 처리하고, 도구로 노출된 특화 하위 에이전트를 호출합니다. 자세한 내용은 [도구](tools.md#agents-as-tools) 문서를 참고하세요.\n \n ```python\n from agents import Agent\n@@ -115,7 +115,7 @@ customer_facing_agent = Agent(\n \n ### 핸드오프\n \n-핸드오프는 에이전트가 위임할 수 있는 하위 에이전트입니다. 핸드오프가 발생하면 위임된 에이전트가 대화 기록을 받고 대화를 인수합니다. 이 패턴은 단일 작업에 뛰어난 모듈식 특화 에이전트를 가능하게 합니다. 자세한 내용은 [핸드오프](handoffs.md) 문서를 참고하세요.\n+핸드오프는 에이전트가 위임할 수 있는 하위 에이전트입니다. 핸드오프가 발생하면, 위임받은 에이전트가 대화 기록을 전달받아 대화를 인수합니다. 이 패턴은 단일 작업에 뛰어난 모듈형 특화 에이전트를 가능하게 합니다. 자세한 내용은 [핸드오프](handoffs.md) 문서를 참고하세요.\n \n ```python\n from agents import Agent\n@@ -136,7 +136,7 @@ triage_agent = Agent(\n \n ## 동적 instructions\n \n-대부분의 경우, 에이전트를 생성할 때 instructions를 제공할 수 있습니다. 그러나 함수로 동적 instructions를 제공할 수도 있습니다. 해당 함수는 에이전트와 컨텍스트를 전달받으며, 프롬프트를 반환해야 합니다. 일반 함수와 `async` 함수 모두 허용됩니다.\n+대부분의 경우 에이전트를 생성할 때 instructions 를 제공하면 됩니다. 그러나 함수로 동적 instructions 를 제공할 수도 있습니다. 해당 함수는 에이전트와 컨텍스트를 입력으로 받아 프롬프트를 반환해야 합니다. 동기 함수와 `async` 함수 모두 허용됩니다.\n \n ```python\n def dynamic_instructions(\n@@ -153,15 +153,15 @@ agent = Agent[UserContext](\n \n ## 라이프사이클 이벤트(훅)\n \n-때로는 에이전트의 라이프사이클을 관찰하고 싶을 수 있습니다. 예를 들어, 이벤트를 로깅하거나 특정 이벤트가 발생할 때 데이터를 미리 가져오고 싶을 수 있습니다. `hooks` 속성을 사용해 에이전트 라이프사이클에 훅을 걸 수 있습니다. [`AgentHooks`][agents.lifecycle.AgentHooks] 클래스를 서브클래싱하고, 필요한 메서드를 오버라이드하세요.\n+때때로 에이전트의 라이프사이클을 관찰하고 싶을 수 있습니다. 예를 들어 특정 이벤트가 발생할 때 이벤트를 로깅하거나 데이터를 미리 가져오고 싶을 수 있습니다. `hooks` 속성으로 에이전트 라이프사이클에 훅을 연결할 수 있습니다. [`AgentHooks`][agents.lifecycle.AgentHooks] 클래스를 상속하고, 관심 있는 메서드를 오버라이드하세요.\n \n ## 가드레일\n \n-가드레일은 에이전트가 실행되는 동안 사용자 입력에 대한 검사/검증을 병렬로 수행하고, 에이전트 출력이 생성된 후에도 검사/검증을 수행할 수 있게 해줍니다. 예를 들어, 사용자 입력과 에이전트 출력을 관련성 기준으로 선별할 수 있습니다. 자세한 내용은 [가드레일](guardrails.md) 문서를 참고하세요.\n+가드레일은 에이전트가 실행되는 동안 사용자 입력에 대한 검사/검증을 병렬로 수행하고, 에이전트 출력이 생성된 후에도 검사를 수행할 수 있게 합니다. 예를 들어 사용자 입력과 에이전트 출력을 관련성 기준으로 필터링할 수 있습니다. 자세한 내용은 [가드레일](guardrails.md) 문서를 참고하세요.\n \n-## 에이전트 클로닝/복사\n+## 에이전트 복제/복사\n \n-에이전트의 `clone()` 메서드를 사용하여 에이전트를 복제하고, 원하는 속성을 선택적으로 변경할 수 있습니다.\n+에이전트에서 `clone()` 메서드를 사용하면 에이전트를 복제하고, 필요에 따라 원하는 속성을 변경할 수 있습니다.\n \n ```python\n pirate_agent = Agent(\n@@ -178,12 +178,12 @@ robot_agent = pirate_agent.clone(\n \n ## 도구 사용 강제\n \n-도구 목록을 제공하더라도 LLM이 항상 도구를 사용하는 것은 아닙니다. [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice]를 설정하여 도구 사용을 강제할 수 있습니다. 유효한 값은 다음과 같습니다:\n+도구 목록을 제공한다고 해서 LLM 이 항상 도구를 사용하는 것은 아닙니다. [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] 를 설정하여 도구 사용을 강제할 수 있습니다. 유효한 값은 다음과 같습니다:\n \n-1. `auto`: LLM이 도구 사용 여부를 스스로 결정\n-2. `required`: LLM이 반드시 도구를 사용해야 함(단, 어떤 도구를 사용할지는 지능적으로 결정)\n-3. `none`: LLM이 도구를 사용하지 않도록 요구\n-4. 특정 문자열 설정(예: `my_tool`): 해당 특정 도구를 반드시 사용하도록 요구\n+1. `auto`: LLM 이 도구 사용 여부를 결정함\n+2. `required`: LLM 이 반드시 도구를 사용해야 함(단, 어떤 도구를 사용할지는 지능적으로 결정)\n+3. `none`: LLM 이 도구를 _사용하지 않도록_ 요구함\n+4. 특정 문자열 설정(예: `my_tool`): 해당 특정 도구를 사용하도록 LLM 에 요구함\n \n ```python\n from agents import Agent, Runner, function_tool, ModelSettings\n@@ -205,7 +205,7 @@ agent = Agent(\n \n `Agent` 구성의 `tool_use_behavior` 매개변수는 도구 출력이 처리되는 방식을 제어합니다:\n \n-- `\"run_llm_again\"`: 기본값. 도구를 실행하고, LLM이 결과를 처리하여 최종 응답을 생성\n+- `\"run_llm_again\"`: 기본값. 도구를 실행하고, LLM 이 결과를 처리하여 최종 응답을 생성\n - `\"stop_on_first_tool\"`: 첫 번째 도구 호출의 출력을 추가 LLM 처리 없이 최종 응답으로 사용\n \n ```python\n@@ -224,7 +224,7 @@ agent = Agent(\n )\n ```\n \n-- `StopAtTools(stop_at_tool_names=[...])`: 지정한 도구 중 하나가 호출되면 중지하고 해당 출력 을 최종 응답으로 사용\n+- `StopAtTools(stop_at_tool_names=[...])`: 지정된 도구 중 하나가 호출되면 중지하고, 해당 도구의 출력을 최종 응답으로 사용\n \n ```python\n from agents import Agent, Runner, function_tool\n@@ -248,7 +248,7 @@ agent = Agent(\n )\n ```\n \n-- `ToolsToFinalOutputFunction`: 도구 결과를 처리하고 중지할지 LLM으로 계속할지 결정하는 사용자 정의 함수\n+- `ToolsToFinalOutputFunction`: 도구 결과를 처리하고 중지할지 LLM 을 계속 사용할지 결정하는 사용자 정의 함수\n \n ```python\n from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper\n@@ -286,4 +286,4 @@ agent = Agent(\n \n !!! note\n \n-    무한 루프를 방지하기 위해, 프레임워크는 도구 호출 후 자동으로 `tool_choice`를 \"auto\"로 리셋합니다. 이 동작은 [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice]로 구성할 수 있습니다. 무한 루프는 도구 결과가 LLM으로 전달되고, `tool_choice`로 인해 LLM이 다시 도구 호출을 생성하는 과정이 무한히 반복되기 때문에 발생합니다.\n\\ No newline at end of file\n+    무한 루프를 방지하기 위해, 프레임워크는 도구 호출 후 `tool_choice` 를 자동으로 \"auto\" 로 재설정합니다. 이 동작은 [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] 로 구성할 수 있습니다. 무한 루프는 도구 결과가 LLM 에 전달되고, `tool_choice` 로 인해 LLM 이 다시 도구 호출을 생성하는 과정이 반복되기 때문에 발생합니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fagents.md",
        "sha": "31387b16d24696afe457af04b57e61f70d89f6ec",
        "status": "modified"
      },
      {
        "additions": 9,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fconfig.md",
        "changes": 18,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fconfig.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 9,
        "filename": "docs/ko/config.md",
        "patch": "@@ -6,15 +6,15 @@ search:\n \n ## API 키와 클라이언트\n \n-기본적으로 SDK는 가져오자마자 LLM 요청과 트레이싱을 위해 `OPENAI_API_KEY` 환경 변수를 찾습니다. 앱이 시작되기 전에 해당 환경 변수를 설정할 수 없다면 [set_default_openai_key()][agents.set_default_openai_key] 함수를 사용해 키를 설정할 수 있습니다.\n+기본적으로 SDK는 가져오자마자 LLM 요청 및 트레이싱을 위해 `OPENAI_API_KEY` 환경 변수를 찾습니다. 앱 시작 전에 해당 환경 변수를 설정할 수 없다면 [set_default_openai_key()][agents.set_default_openai_key] 함수를 사용해 키를 설정할 수 있습니다.\n \n ```python\n from agents import set_default_openai_key\n \n set_default_openai_key(\"sk-...\")\n ```\n \n-또한 사용할 OpenAI 클라이언트를 구성할 수도 있습니다. 기본적으로 SDK는 환경 변수 또는 위에서 설정한 기본 키를 사용하여 `AsyncOpenAI` 인스턴스를 생성합니다. [set_default_openai_client()][agents.set_default_openai_client] 함수를 사용하여 이를 변경할 수 있습니다.\n+또는 사용할 OpenAI 클라이언트를 구성할 수도 있습니다. 기본적으로 SDK는 환경 변수의 API 키 또는 위에서 설정한 기본 키를 사용하여 `AsyncOpenAI` 인스턴스를 생성합니다. [set_default_openai_client()][agents.set_default_openai_client] 함수를 사용해 이를 변경할 수 있습니다.\n \n ```python\n from openai import AsyncOpenAI\n@@ -24,7 +24,7 @@ custom_client = AsyncOpenAI(base_url=\"...\", api_key=\"...\")\n set_default_openai_client(custom_client)\n ```\n \n-마지막으로, 사용하는 OpenAI API를 사용자 지정할 수도 있습니다. 기본적으로 OpenAI Responses API를 사용합니다. [set_default_openai_api()][agents.set_default_openai_api] 함수를 사용하여 Chat Completions API를 사용하도록 재정의할 수 있습니다.\n+마지막으로, 사용되는 OpenAI API를 사용자 지정할 수도 있습니다. 기본적으로 OpenAI Responses API를 사용합니다. [set_default_openai_api()][agents.set_default_openai_api] 함수를 사용해 Chat Completions API로 오버라이드할 수 있습니다.\n \n ```python\n from agents import set_default_openai_api\n@@ -34,15 +34,15 @@ set_default_openai_api(\"chat_completions\")\n \n ## 트레이싱\n \n-트레이싱은 기본적으로 활성화되어 있습니다. 기본적으로 위 섹션의 OpenAI API 키(즉, 환경 변수 또는 설정한 기본 키)를 사용합니다. 트레이싱에 사용할 API 키를 명시적으로 설정하려면 [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] 함수를 사용하세요.\n+트레이싱은 기본적으로 활성화되어 있습니다. 기본적으로 위 섹션의 OpenAI API 키(즉, 환경 변수 또는 설정한 기본 키)를 사용합니다. 트레이싱에 사용되는 API 키를 지정하려면 [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] 함수를 사용하세요.\n \n ```python\n from agents import set_tracing_export_api_key\n \n set_tracing_export_api_key(\"sk-...\")\n ```\n \n-전역 익스포터를 변경하지 않고 실행 단위로 트레이싱 API 키를 설정할 수도 있습니다.\n+글로벌 내보내기를 변경하지 않고 실행마다 트레이싱 API 키를 설정할 수도 있습니다.\n \n ```python\n from agents import Runner, RunConfig\n@@ -54,7 +54,7 @@ await Runner.run(\n )\n ```\n \n-[`set_tracing_disabled()`][agents.set_tracing_disabled] 함수를 사용하여 트레이싱을 완전히 비활성화할 수도 있습니다.\n+[`set_tracing_disabled()`][agents.set_tracing_disabled] 함수를 사용해 트레이싱을 완전히 비활성화할 수도 있습니다.\n \n ```python\n from agents import set_tracing_disabled\n@@ -64,7 +64,7 @@ set_tracing_disabled(True)\n \n ## 디버그 로깅\n \n-SDK에는 핸들러가 설정되지 않은 두 개의 Python 로거가 있습니다. 기본적으로 이는 경고와 오류가 `stdout`으로 전송되고 다른 로그는 억제됨을 의미합니다.\n+SDK에는 핸들러가 설정되지 않은 두 개의 Python 로거가 있습니다. 기본적으로 이는 경고와 오류는 `stdout`으로 전송되지만, 다른 로그는 억제됨을 의미합니다.\n \n 자세한 로깅을 활성화하려면 [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] 함수를 사용하세요.\n \n@@ -74,7 +74,7 @@ from agents import enable_verbose_stdout_logging\n enable_verbose_stdout_logging()\n ```\n \n-또는 핸들러, 필터, 포매터 등을 추가하여 로그를 사용자 지정할 수 있습니다. 자세한 내용은 [Python logging guide](https://docs.python.org/3/howto/logging.html)를 참고하세요.\n+또는 핸들러, 필터, 포매터 등을 추가하여 로그를 사용자 지정할 수 있습니다. 자세한 내용은 [Python 로깅 가이드](https://docs.python.org/3/howto/logging.html)를 참고하세요.\n \n ```python\n import logging\n@@ -95,7 +95,7 @@ logger.addHandler(logging.StreamHandler())\n \n ### 로그의 민감한 데이터\n \n-일부 로그에는 민감한 데이터(예: 사용자 데이터)가 포함될 수 있습니다. 이러한 데이터가 로그에 기록되지 않도록 하려면 다음 환경 변수를 설정하세요.\n+일부 로그에는 민감한 데이터(예: 사용자 데이터)가 포함될 수 있습니다. 이러한 데이터가 로깅되지 않도록 하려면 다음 환경 변수를 설정하세요.\n \n LLM 입력과 출력을 로깅하지 않으려면:\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fconfig.md",
        "sha": "f537aed5513389bee84e2c92b240145933444582",
        "status": "modified"
      },
      {
        "additions": 22,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fcontext.md",
        "changes": 44,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fcontext.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 22,
        "filename": "docs/ko/context.md",
        "patch": "@@ -4,9 +4,9 @@ search:\n ---\n # 컨텍스트 관리\n \n-컨텍스트는 여러 의미로 사용됩니다. 여기서 중요한 컨텍스트는 두 가지 범주가 있습니다:\n+컨텍스트는 다양한 의미로 사용됩니다. 여기에서 신경 써야 할 컨텍스트는 두 가지 주요 범주가 있습니다:\n \n-1. 코드에서 로컬로 사용할 수 있는 컨텍스트: 도구 함수가 실행될 때, `on_handoff` 같은 콜백 동안, 라이프사이클 훅에서 필요할 수 있는 데이터와 의존성\n+1. 코드에서 로컬로 사용할 수 있는 컨텍스트: 도구 함수가 실행될 때, `on_handoff` 같은 콜백 동안, 라이프사이클 훅 등에서 필요한 데이터와 의존성\n 2. LLM 에서 사용할 수 있는 컨텍스트: LLM 이 응답을 생성할 때 볼 수 있는 데이터\n \n ## 로컬 컨텍스트\n@@ -15,19 +15,19 @@ search:\n \n 1. 원하는 어떤 Python 객체든 생성합니다. 일반적으로 dataclass 또는 Pydantic 객체를 사용합니다\n 2. 해당 객체를 다양한 실행 메서드에 전달합니다(예: `Runner.run(..., **context=whatever**)`)\n-3. 모든 도구 호출, 라이프사이클 훅 등은 `RunContextWrapper[T]` 래퍼 객체를 전달받습니다. 여기서 `T` 는 컨텍스트 객체 타입을 나타내며 `wrapper.context` 로 접근할 수 있습니다\n+3. 모든 도구 호출, 라이프사이클 훅 등에는 `RunContextWrapper[T]` 래퍼 객체가 전달되며, 여기서 `T` 는 컨텍스트 객체 타입을 나타내며 `wrapper.context` 를 통해 액세스할 수 있습니다\n \n-가장 중요한 점: 특정 에이전트 실행에서 모든 에이전트, 도구 함수, 라이프사이클 등은 동일한 유형의 컨텍스트를 사용해야 합니다.\n+가장 **중요한** 점: 특정 에이전트 실행에 대해 해당 에이전트의 모든 에이전트, 도구 함수, 라이프사이클 등은 동일한 _type_ 의 컨텍스트를 사용해야 합니다.\n \n 컨텍스트는 다음과 같은 용도로 사용할 수 있습니다:\n \n--   실행에 필요한 컨텍스트 데이터(예: 사용자 이름/uid 또는 사용자에 대한 기타 정보)\n+-   실행에 필요한 문맥 데이터(예: 사용자 이름/UID 또는 사용자에 대한 기타 정보)\n -   의존성(예: 로거 객체, 데이터 페처 등)\n -   헬퍼 함수\n \n-!!! danger \"주의\"\n+!!! danger \"Note\"\n \n-    컨텍스트 객체는 LLM 에게 전송되지 않습니다. 순수하게 로컬 객체이며 읽고, 쓰고, 메서드를 호출할 수 있습니다.\n+    컨텍스트 객체는 LLM 에게 **전송되지 않습니다**. 순수한 로컬 객체로서 읽고, 쓰고, 메서드를 호출할 수 있습니다.\n \n ```python\n import asyncio\n@@ -66,17 +66,17 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-1. 이것이 컨텍스트 객체입니다. 여기서는 dataclass 를 사용했지만, 어떤 타입이든 사용할 수 있습니다\n-2. 이것은 도구입니다. `RunContextWrapper[UserInfo]` 를 받는 것을 볼 수 있습니다. 도구 구현은 컨텍스트에서 값을 읽습니다\n-3. 에이전트에 제네릭 `UserInfo` 를 표시하여, 타입 체커가 오류를 잡을 수 있게 합니다(예를 들어, 다른 컨텍스트 타입을 받는 도구를 전달하려고 하면)\n-4. 컨텍스트가 `run` 함수에 전달됩니다\n-5. 에이전트가 도구를 올바르게 호출하여 나이를 가져옵니다\n+1. 이것이 컨텍스트 객체입니다. 여기서는 dataclass 를 사용했지만 어떤 타입이든 사용할 수 있습니다\n+2. 이것은 도구입니다. `RunContextWrapper[UserInfo]` 를 받는 것을 볼 수 있습니다. 도구 구현은 컨텍스트에서 읽습니다\n+3. 에이전트를 제네릭 `UserInfo` 로 표시하여 타입 체커가 오류를 잡을 수 있도록 합니다(예를 들어, 다른 컨텍스트 타입을 받는 도구를 전달하려고 할 때)\n+4. 컨텍스트는 `run` 함수에 전달됩니다\n+5. 에이전트는 도구를 올바르게 호출하고 나이를 가져옵니다\n \n ---\n \n ### 고급: `ToolContext`\n \n-경우에 따라 실행 중인 도구의 추가 메타데이터(예: 이름, 호출 ID, 원문 인수 문자열)에 접근하고 싶을 수 있습니다.  \n+일부 경우에는 실행 중인 도구에 대한 추가 메타데이터(이름, 호출 ID, 원문 인자 문자열 등)에 접근하고 싶을 수 있습니다  \n 이를 위해 `RunContextWrapper` 를 확장한 [`ToolContext`][agents.tool_context.ToolContext] 클래스를 사용할 수 있습니다.\n \n ```python\n@@ -106,22 +106,22 @@ agent = Agent(\n ```\n \n `ToolContext` 는 `RunContextWrapper` 와 동일한 `.context` 속성을 제공하며,  \n-현재 도구 호출에 특화된 추가 필드를 제공합니다:\n+현재 도구 호출에 특화된 추가 필드가 있습니다:\n \n - `tool_name` – 호출 중인 도구의 이름  \n - `tool_call_id` – 이 도구 호출의 고유 식별자  \n-- `tool_arguments` – 도구에 전달된 원문 인수 문자열  \n+- `tool_arguments` – 도구에 전달된 원문 인자 문자열  \n \n-실행 중 도구 수준의 메타데이터가 필요할 때 `ToolContext` 를 사용하세요.  \n-에이전트와 도구 간의 일반적인 컨텍스트 공유에는 `RunContextWrapper` 만으로 충분합니다.\n+실행 중 도구 수준의 메타데이터가 필요할 때 `ToolContext` 를 사용하세요  \n+에이전트와 도구 간 일반적인 컨텍스트 공유에는 `RunContextWrapper` 로 충분합니다\n \n ---\n \n ## 에이전트/LLM 컨텍스트\n \n-LLM 이 호출될 때 볼 수 있는 데이터는 대화 기록뿐입니다. 따라서 LLM 에 새 데이터를 제공하려면, 해당 데이터가 대화 기록에 포함되도록 해야 합니다. 방법은 몇 가지가 있습니다:\n+LLM 이 호출될 때 볼 수 있는 **유일한** 데이터는 대화 히스토리에서 옵니다. 따라서 LLM 에게 새로운 데이터를 제공하려면, 반드시 그 히스토리에 포함되도록 해야 합니다. 이를 수행하는 방법은 다음과 같습니다:\n \n-1. 에이전트 `instructions` 에 추가합니다. 이는 \"system prompt\" 또는 \"developer message\" 라고도 합니다. 시스템 프롬프트는 정적 문자열일 수도 있고, 컨텍스트를 받아 문자열을 출력하는 동적 함수일 수도 있습니다. 항상 유용한 정보(예: 사용자 이름이나 현재 날짜)에 일반적으로 사용됩니다\n-2. `Runner.run` 함수를 호출할 때 `input` 에 추가합니다. 이는 `instructions` 전략과 유사하지만, [chain of command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command) 에서 더 낮은 위치의 메시지를 사용할 수 있습니다\n-3. 함수 도구를 통해 노출합니다. 이는 _온디맨드_ 컨텍스트에 유용합니다 — LLM 이 데이터가 필요할 때를 스스로 판단하고, 해당 데이터를 가져오기 위해 도구를 호출할 수 있습니다\n-4. retrieval 또는 웹 검색을 사용합니다. 이는 파일이나 데이터베이스에서 관련 데이터를 가져오거나(retrieval), 웹에서 가져올 수 있는(웹 검색) 특수 도구입니다. 관련 컨텍스트 데이터로 응답을 그라운딩(grounding) 하는 데 유용합니다\n\\ No newline at end of file\n+1. 에이전트 `instructions` 에 추가합니다. 이는 \"시스템 프롬프트(system prompt)\" 또는 \"개발자 메시지\"라고도 불립니다. 시스템 프롬프트는 정적 문자열일 수도 있고, 컨텍스트를 받아 문자열을 출력하는 동적 함수일 수도 있습니다. 사용자 이름이나 현재 날짜처럼 항상 유용한 정보를 제공할 때 흔히 사용되는 방식입니다\n+2. `Runner.run` 함수를 호출할 때 `input` 에 추가합니다. 이는 `instructions` 방식과 유사하지만, [지휘 체계](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command)에서 더 아래에 위치한 메시지를 제공할 수 있습니다\n+3. 함수 도구를 통해 노출합니다. 이는 필요 시(on-demand) 컨텍스트에 유용합니다. LLM 이 언제 데이터가 필요한지 판단하고, 해당 데이터를 가져오기 위해 도구를 호출할 수 있습니다\n+4. retrieval 또는 웹 검색을 사용합니다. 이는 파일이나 데이터베이스에서 관련 데이터를 가져오거나(retrieval), 웹에서 데이터를 가져오는(웹 검색) 특수한 도구입니다. 관련된 컨텍스트 데이터에 기반하여 응답을 \"그라운딩\"하는 데 유용합니다\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fcontext.md",
        "sha": "c8275ef129661deea7eac204656a4e50a891af7e",
        "status": "modified"
      },
      {
        "additions": 30,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fexamples.md",
        "changes": 60,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fexamples.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 30,
        "filename": "docs/ko/examples.md",
        "patch": "@@ -2,9 +2,9 @@\n search:\n   exclude: true\n ---\n-# 코드 예제\n+# 예제\n \n-[repo](https://github.com/openai/openai-agents-python/tree/main/examples)의 examples 섹션에서 SDK의 다양한 샘플 구현을 확인하세요. 이 code examples는 다양한 패턴과 기능을 보여주는 여러 카테고리로 구성되어 있습니다.\n+[repo](https://github.com/openai/openai-agents-python/tree/main/examples)의 examples 섹션에서 SDK의 다양한 샘플 구현을 확인하세요. 이 예제들은 여러 패턴과 기능을 보여주는 여러 카테고리로 구성되어 있습니다.\n \n ## 카테고리\n \n@@ -13,22 +13,22 @@ search:\n \n     -   결정적 워크플로\n     -   도구로서의 에이전트\n-    -   병렬 에이전트 실행\n+    -   에이전트 병렬 실행\n     -   조건부 도구 사용\n     -   입력/출력 가드레일\n-    -   LLM 심사관 역할\n+    -   LLM 판정자\n     -   라우팅\n     -   스트리밍 가드레일\n \n -   **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**\n-    이 예제들은 SDK의 기본 기능을 보여줍니다\n+    이 예제들은 다음과 같은 SDK의 기본 기능을 보여줍니다\n \n-    -   Hello world code examples (기본 모델, GPT-5, 오픈 가중치 모델)\n-    -   에이전트 수명 주기 관리\n+    -   Hello World 예제(기본 모델, GPT-5, 오픈 가중치 모델)\n+    -   에이전트 라이프사이클 관리\n     -   동적 시스템 프롬프트\n-    -   스트리밍 출력 (텍스트, 아이템, 함수 호출 인자)\n+    -   스트리밍 출력(텍스트, 항목, 함수 호출 인자)\n     -   프롬프트 템플릿\n-    -   파일 처리 (로컬 및 원격, 이미지 및 PDF)\n+    -   파일 처리(로컬 및 원격, 이미지와 PDF)\n     -   사용량 추적\n     -   비엄격 출력 타입\n     -   이전 응답 ID 사용\n@@ -37,57 +37,57 @@ search:\n     항공사를 위한 고객 서비스 시스템 예제\n \n -   **[financial_research_agent](https://github.com/openai/openai-agents-python/tree/main/examples/financial_research_agent):**\n-    금융 데이터 분석을 위한 에이전트와 도구로 구성된 구조적 리서치 워크플로를 보여주는 금융 리서치 에이전트\n+    금융 데이터 분석을 위한 에이전트와 도구로 구조화된 리서치 워크플로를 시연하는 금융 리서치 에이전트\n \n -   **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**\n-    메시지 필터링을 활용한 에이전트 핸드오프의 실용적인 예제\n+    메시지 필터링이 포함된 에이전트 핸드오프의 실용적인 예제\n \n -   **[hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp):**\n-    호스티드 MCP (Model Context Protocol) 커넥터와 승인 절차 사용법을 보여주는 예제\n+    hosted MCP (Model Context Protocol) 커넥터와 승인을 사용하는 방법을 보여주는 예제\n \n -   **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**\n-    MCP (Model Context Protocol)를 사용해 에이전트를 만드는 방법을 다룹니다\n+    MCP (Model Context Protocol)로 에이전트를 만드는 방법, 포함 내용:\n \n     -   파일시스템 예제\n     -   Git 예제\n     -   MCP 프롬프트 서버 예제\n-    -   SSE (Server-Sent Events) 예제\n+    -   SSE(서버 전송 이벤트) 예제\n     -   스트리밍 가능한 HTTP 예제\n \n -   **[memory](https://github.com/openai/openai-agents-python/tree/main/examples/memory):**\n-    에이전트를 위한 다양한 메모리 구현 예제\n+    에이전트를 위한 다양한 메모리 구현 예제, 포함 내용:\n \n-    -   SQLite 세션 저장소\n-    -   고급 SQLite 세션 저장소\n-    -   Redis 세션 저장소\n-    -   SQLAlchemy 세션 저장소\n-    -   암호화된 세션 저장소\n-    -   OpenAI 세션 저장소\n+    -   SQLite 세션 스토리지\n+    -   고급 SQLite 세션 스토리지\n+    -   Redis 세션 스토리지\n+    -   SQLAlchemy 세션 스토리지\n+    -   암호화된 세션 스토리지\n+    -   OpenAI 세션 스토리지\n \n -   **[model_providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**\n-    커스텀 프로바이더와 LiteLLM 연동을 포함하여, OpenAI 이외의 모델을 SDK에서 사용하는 방법을 살펴보세요\n+    커스텀 제공자와 LiteLLM 통합을 포함해, OpenAI가 아닌 모델을 SDK와 함께 사용하는 방법을 확인하세요\n \n -   **[realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime):**\n-    SDK를 사용해 실시간 경험을 구축하는 방법을 보여주는 예제\n+    SDK로 실시간 경험을 구축하는 방법을 보여주는 예제, 포함 내용:\n \n     -   웹 애플리케이션\n     -   커맨드라인 인터페이스\n-    -   Twilio 연동\n+    -   Twilio 통합\n \n -   **[reasoning_content](https://github.com/openai/openai-agents-python/tree/main/examples/reasoning_content):**\n-    reasoning content 및 structured outputs를 다루는 방법을 보여주는 예제\n+    추론 콘텐츠와 structured outputs을 다루는 방법을 보여주는 예제\n \n -   **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**\n-    복잡한 멀티 에이전트 리서치 워크플로를 보여주는 간단한 딥 리서치 클론\n+    복잡한 멀티 에이전트 리서치 워크플로를 시연하는 간단한 딥 리서치 클론\n \n -   **[tools](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**\n-    다음과 같은 OpenAI 호스트하는 도구를 구현하는 방법을 알아보세요\n+    다음과 같은 OpenAI 호스트하는 도구를 구현하는 방법을 배워보세요\n \n-    -   웹 검색 및 필터가 포함된 웹 검색\n+    -   웹 검색 및 필터가 있는 웹 검색\n     -   파일 검색\n-    -   Code interpreter\n+    -   Code Interpreter\n     -   컴퓨터 사용\n     -   이미지 생성\n \n -   **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**\n-    TTS 및 STT 모델을 사용한 보이스 에이전트 예제를 확인하세요. 스트리밍 보이스 예제도 포함됩니다.\n\\ No newline at end of file\n+    TTS 및 STT 모델을 사용하는 음성 에이전트 예제와 스트리밍 음성 예제를 확인하세요\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fexamples.md",
        "sha": "a4aee1a58d7d7bd8692c1daad729b6ccdefbe7e6",
        "status": "modified"
      },
      {
        "additions": 73,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fguardrails.md",
        "changes": 141,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fguardrails.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 68,
        "filename": "docs/ko/guardrails.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # 가드레일\n \n-가드레일은 사용자 입력과 에이전트 출력에 대한 검사와 검증을 가능하게 합니다. 예를 들어, 고객 요청을 도와주는 매우 똑똑한(따라서 느리고 비싼) 모델을 사용하는 에이전트가 있다고 가정해 보겠습니다. 악의적인 사용자가 수학 숙제를 도와 달라고 모델에 요구하는 것을 원치 않을 것입니다. 이럴 때 빠르고 저렴한 모델로 가드레일을 실행할 수 있습니다. 가드레일이 악의적 사용을 감지하면 즉시 오류를 발생시키고 고가 모델의 실행을 막아 시간과 비용을 절약할 수 있습니다(**블로킹 가드레일 사용 시; 병렬 가드레일의 경우 가드레일이 완료되기 전에 고가 모델이 이미 실행을 시작했을 수 있습니다. 자세한 내용은 아래 \"실행 모드\"를 참조하세요**).\n+가드레일은 사용자 입력과 에이전트 출력에 대한 점검과 검증을 수행할 수 있게 해줍니다. 예를 들어, 고객 요청을 처리하기 위해 매우 똑똑한(따라서 느리고 비싼) 모델을 사용하는 에이전트를 상상해 보세요. 악의적인 사용자가 모델에게 수학 숙제를 도와달라고 요청하는 것을 원치 않을 것입니다. 이때 빠르고 저렴한 모델로 가드레일을 실행할 수 있습니다. 가드레일이 악의적 사용을 감지하면 즉시 오류를 발생시켜 비용이 큰 모델의 실행을 방지하여 시간과 비용을 절약할 수 있습니다 (**블로킹 가드레일을 사용하는 경우에 한함. 병렬 가드레일의 경우, 가드레일 완료 전에 비용이 큰 모델이 이미 실행을 시작했을 수 있습니다. 아래의 \"실행 모드\"를 참고하세요**)\n \n 가드레일에는 두 가지 종류가 있습니다:\n \n@@ -13,87 +13,47 @@ search:\n \n ## 입력 가드레일\n \n-입력 가드레일은 3단계로 실행됩니다:\n+입력 가드레일은 다음 3단계로 실행됩니다:\n \n-1. 먼저, 가드레일은 에이전트에 전달된 것과 동일한 입력을 받습니다\n-2. 다음으로, 가드레일 함수가 실행되어 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]을 생성하고, 이는 [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]로 래핑됩니다\n-3. 마지막으로 [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]가 true인지 확인합니다. true이면 [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] 예외가 발생하며, 이에 따라 사용자에게 적절히 응답하거나 예외를 처리할 수 있습니다\n+1. 먼저, 가드레일이 에이전트에 전달된 것과 동일한 입력을 받습니다.\n+2. 다음으로, 가드레일 함수가 실행되어 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]을 생성하고, 이를 [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]로 래핑합니다\n+3. 마지막으로 [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]가 true 인지 확인합니다. true 이면, [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] 예외를 발생시켜 사용자에게 적절히 응답하거나 예외를 처리할 수 있습니다.\n \n !!! Note\n \n-    입력 가드레일은 사용자 입력에서 실행되도록 설계되었으므로, 에이전트의 가드레일은 그 에이전트가 첫 번째 에이전트일 때만 실행됩니다. 왜 `guardrails` 속성이 `Runner.run`에 전달되지 않고 에이전트에 있느냐고 궁금할 수 있습니다. 가드레일은 실제 에이전트와 밀접히 관련되는 경우가 많기 때문입니다. 에이전트마다 다른 가드레일을 실행하므로, 코드를 같은 위치에 두면 가독성이 좋아집니다.\n+    입력 가드레일은 사용자 입력에서 실행되도록 설계되었으므로, 에이전트의 가드레일은 해당 에이전트가 *첫 번째* 에이전트일 때만 실행됩니다. 왜 `guardrails` 속성이 `Runner.run`에 전달되는 대신 에이전트에 있는지 궁금할 수 있습니다. 이는 가드레일이 실제 에이전트와 밀접히 관련되는 경향이 있기 때문입니다. 에이전트마다 서로 다른 가드레일을 실행하게 되므로, 코드를 같은 위치에 두면 가독성에 유리합니다.\n \n ### 실행 모드\n \n 입력 가드레일은 두 가지 실행 모드를 지원합니다:\n \n-- **병렬 실행**(기본값, `run_in_parallel=True`): 가드레일이 에이전트 실행과 동시에 실행됩니다. 둘이 동시에 시작되므로 지연 시간이 가장 좋습니다. 다만 가드레일이 실패하면, 에이전트는 취소되기 전에 이미 토큰을 소모하고 도구를 실행했을 수 있습니다\n-- **블로킹 실행**(`run_in_parallel=False`): 가드레일이 에이전트가 시작되기 전에 실행을 완료합니다. 가드레일의 트립와이어가 트리거되면 에이전트는 전혀 실행되지 않아 토큰 소모와 도구 실행을 방지합니다. 비용 최적화와 도구 호출의 잠재적 부작용을 피하고 싶을 때 적합합니다\n+- **병렬 실행**(기본값, `run_in_parallel=True`): 가드레일이 에이전트 실행과 동시에 실행됩니다. 둘 다 동시에 시작하므로 지연 시간이 가장 좋습니다. 다만 가드레일이 실패할 경우, 에이전트가 취소되기 전에 이미 토큰을 소비하고 도구를 실행했을 수 있습니다.\n+\n+- **블로킹 실행**(`run_in_parallel=False`): 가드레일이 에이전트가 시작되기 *전*에 실행을 완료합니다. 가드레일 트립와이어가 트리거되면 에이전트는 전혀 실행되지 않아 토큰 소비와 도구 실행을 방지합니다. 비용 최적화와 도구 호출에 의한 잠재적 부작용을 피하고자 할 때 이상적입니다.\n \n ## 출력 가드레일\n \n-출력 가드레일은 3단계로 실행됩니다:\n+출력 가드레일은 다음 3단계로 실행됩니다:\n \n-1. 먼저, 가드레일은 에이전트가 생성한 출력을 받습니다\n-2. 다음으로, 가드레일 함수가 실행되어 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]을 생성하고, 이는 [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]로 래핑됩니다\n-3. 마지막으로 [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]가 true인지 확인합니다. true이면 [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] 예외가 발생하며, 이에 따라 사용자에게 적절히 응답하거나 예외를 처리할 수 있습니다\n+1. 먼저, 가드레일이 에이전트가 생성한 출력을 받습니다.\n+2. 다음으로, 가드레일 함수가 실행되어 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]을 생성하고, 이를 [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]로 래핑합니다\n+3. 마지막으로 [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]가 true 인지 확인합니다. true 이면, [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] 예외를 발생시켜 사용자에게 적절히 응답하거나 예외를 처리할 수 있습니다.\n \n !!! Note\n \n-    출력 가드레일은 최종 에이전트 출력에서 실행되도록 설계되었으므로, 에이전트의 가드레일은 그 에이전트가 마지막 에이전트일 때만 실행됩니다. 입력 가드레일과 마찬가지로, 가드레일은 실제 에이전트와 밀접히 관련되는 경우가 많기 때문에 가독성을 위해 코드를 같은 위치에 둡니다.\n+    출력 가드레일은 최종 에이전트 출력에서 실행되도록 설계되었으므로, 에이전트의 가드레일은 해당 에이전트가 *마지막* 에이전트일 때만 실행됩니다. 입력 가드레일과 마찬가지로, 가드레일은 실제 에이전트와 밀접히 관련되는 경향이 있으므로 코드를 같은 위치에 두면 가독성에 유리합니다.\n \n-    출력 가드레일은 항상 에이전트가 완료된 후 실행되므로 `run_in_parallel` 매개변수를 지원하지 않습니다.\n+    출력 가드레일은 항상 에이전트 완료 후에 실행되므로 `run_in_parallel` 매개변수를 지원하지 않습니다.\n \n ## 도구 가드레일\n \n-도구 가드레일은 **함수 도구**를 감싸며, 실행 전후에 도구 호출을 검증하거나 차단할 수 있도록 합니다. 도구 자체에 설정되며, 해당 도구가 호출될 때마다 실행됩니다.\n-\n-- 입력 도구 가드레일은 도구가 실행되기 전에 실행되며 호출을 건너뛰거나, 메시지로 출력을 대체하거나, 트립와이어를 발생시킬 수 있음\n-- 출력 도구 가드레일은 도구가 실행된 후에 실행되며 출력을 대체하거나 트립와이어를 발생시킬 수 있음\n-- 도구 가드레일은 [`function_tool`][agents.function_tool]로 생성된 함수 도구에만 적용됩니다. 호스티드 툴(`WebSearchTool`, `FileSearchTool`, `HostedMCPTool`, `CodeInterpreterTool`, `ImageGenerationTool`)과 로컬 런타임 도구(`ComputerTool`, `ShellTool`, `ApplyPatchTool`, `LocalShellTool`)는 이 가드레일 파이프라인을 사용하지 않습니다\n-\n-```python\n-import json\n-from agents import (\n-    Agent,\n-    Runner,\n-    ToolGuardrailFunctionOutput,\n-    function_tool,\n-    tool_input_guardrail,\n-    tool_output_guardrail,\n-)\n-\n-@tool_input_guardrail\n-def block_secrets(data):\n-    args = json.loads(data.context.tool_arguments or \"{}\")\n-    if \"sk-\" in json.dumps(args):\n-        return ToolGuardrailFunctionOutput.reject_content(\n-            \"Remove secrets before calling this tool.\"\n-        )\n-    return ToolGuardrailFunctionOutput.allow()\n+도구 가드레일은 **함수 도구**를 래핑하여 실행 전후에 도구 호출을 검증하거나 차단할 수 있게 합니다. 이는 도구 자체에서 구성되며 해당 도구가 호출될 때마다 실행됩니다.\n \n+- 입력 도구 가드레일은 도구 실행 전 실행되며 호출을 건너뛰거나, 메시지로 출력을 대체하거나, 트립와이어를 발생시킬 수 있습니다.\n+- 출력 도구 가드레일은 도구 실행 후 실행되며 출력을 대체하거나 트립와이어를 발생시킬 수 있습니다.\n+- 도구 가드레일은 [`function_tool`][agents.function_tool]로 생성된 함수 도구에만 적용됩니다. 호스티드 도구(`WebSearchTool`, `FileSearchTool`, `HostedMCPTool`, `CodeInterpreterTool`, `ImageGenerationTool`)와 로컬 런타임 도구(`ComputerTool`, `ShellTool`, `ApplyPatchTool`, `LocalShellTool`)는 이 가드레일 파이프라인을 사용하지 않습니다.\n \n-@tool_output_guardrail\n-def redact_output(data):\n-    text = str(data.output or \"\")\n-    if \"sk-\" in text:\n-        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n-    return ToolGuardrailFunctionOutput.allow()\n-\n-\n-@function_tool(\n-    tool_input_guardrails=[block_secrets],\n-    tool_output_guardrails=[redact_output],\n-)\n-def classify_text(text: str) -> str:\n-    \"\"\"Classify text for internal routing.\"\"\"\n-    return f\"length:{len(text)}\"\n-\n-\n-agent = Agent(name=\"Classifier\", tools=[classify_text])\n-result = Runner.run_sync(agent, \"hello world\")\n-print(result.final_output)\n-```\n+자세한 내용은 아래 코드 스니펫을 참고하세요.\n \n ## 트립와이어\n \n@@ -154,10 +114,10 @@ async def main():\n         print(\"Math homework guardrail tripped\")\n ```\n \n-1. 이 에이전트를 가드레일 함수에서 사용합니다\n-2. 이것은 에이전트의 입력/컨텍스트를 받아 결과를 반환하는 가드레일 함수입니다\n-3. 가드레일 결과에 추가 정보를 포함할 수 있습니다\n-4. 이것이 워크플로를 정의하는 실제 에이전트입니다\n+1. 이 에이전트를 가드레일 함수에서 사용합니다.\n+2. 에이전트의 입력/컨텍스트를 받아 결과를 반환하는 가드레일 함수입니다.\n+3. 가드레일 결과에 추가 정보를 포함할 수 있습니다.\n+4. 워크플로를 정의하는 실제 에이전트입니다.\n \n 출력 가드레일도 유사합니다.\n \n@@ -212,7 +172,52 @@ async def main():\n         print(\"Math output guardrail tripped\")\n ```\n \n-1. 이것은 실제 에이전트의 출력 타입입니다\n-2. 이것은 가드레일의 출력 타입입니다\n-3. 이것은 에이전트의 출력을 받아 결과를 반환하는 가드레일 함수입니다\n-4. 이것이 워크플로를 정의하는 실제 에이전트입니다\n\\ No newline at end of file\n+1. 실제 에이전트의 출력 타입입니다.\n+2. 가드레일의 출력 타입입니다.\n+3. 에이전트의 출력을 받아 결과를 반환하는 가드레일 함수입니다.\n+4. 워크플로를 정의하는 실제 에이전트입니다.\n+\n+마지막으로, 도구 가드레일의 예시입니다.\n+\n+```python\n+import json\n+from agents import (\n+    Agent,\n+    Runner,\n+    ToolGuardrailFunctionOutput,\n+    function_tool,\n+    tool_input_guardrail,\n+    tool_output_guardrail,\n+)\n+\n+@tool_input_guardrail\n+def block_secrets(data):\n+    args = json.loads(data.context.tool_arguments or \"{}\")\n+    if \"sk-\" in json.dumps(args):\n+        return ToolGuardrailFunctionOutput.reject_content(\n+            \"Remove secrets before calling this tool.\"\n+        )\n+    return ToolGuardrailFunctionOutput.allow()\n+\n+\n+@tool_output_guardrail\n+def redact_output(data):\n+    text = str(data.output or \"\")\n+    if \"sk-\" in text:\n+        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n+    return ToolGuardrailFunctionOutput.allow()\n+\n+\n+@function_tool(\n+    tool_input_guardrails=[block_secrets],\n+    tool_output_guardrails=[redact_output],\n+)\n+def classify_text(text: str) -> str:\n+    \"\"\"Classify text for internal routing.\"\"\"\n+    return f\"length:{len(text)}\"\n+\n+\n+agent = Agent(name=\"Classifier\", tools=[classify_text])\n+result = Runner.run_sync(agent, \"hello world\")\n+print(result.final_output)\n+```\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fguardrails.md",
        "sha": "bbf87ca5f95e7cb1d6c818c7c2867da4fcad0734",
        "status": "modified"
      },
      {
        "additions": 23,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fhandoffs.md",
        "changes": 46,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fhandoffs.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 23,
        "filename": "docs/ko/handoffs.md",
        "patch": "@@ -4,21 +4,21 @@ search:\n ---\n # 핸드오프\n \n-핸드오프는 에이전트가 다른 에이전트에게 작업을 위임하도록 합니다. 이는 서로 다른 에이전트가 각기 다른 영역을 전문으로 하는 시나리오에서 특히 유용합니다. 예를 들어 고객 지원 앱에서는 주문 상태, 환불, FAQ 등과 같은 작업을 각각 전담하는 에이전트가 있을 수 있습니다.\n+핸드오프는 한 에이전트가 다른 에이전트에게 작업을 위임할 수 있게 합니다. 이는 각 에이전트가 서로 다른 분야에 특화되어 있는 시나리오에서 특히 유용합니다. 예를 들어, 고객 지원 앱은 주문 상태, 환불, FAQ 등 특정 작업을 각각 처리하는 에이전트를 가질 수 있습니다.\n \n-핸드오프는 LLM 에게 도구로 표현됩니다. 예를 들어 `Refund Agent` 라는 에이전트로의 핸드오프가 있다면, 도구 이름은 `transfer_to_refund_agent` 가 됩니다.\n+핸드오프는 LLM 에게 도구로 표현됩니다. 예를 들어 `Refund Agent`라는 에이전트로의 핸드오프가 있다면, 도구 이름은 `transfer_to_refund_agent`가 됩니다.\n \n ## 핸드오프 생성\n \n-모든 에이전트는 [`handoffs`][agents.agent.Agent.handoffs] 매개변수를 가지며, 여기에 직접 `Agent` 를 전달하거나, 핸드오프를 커스터마이즈하는 `Handoff` 객체를 전달할 수 있습니다.\n+모든 에이전트는 [`handoffs`][agents.agent.Agent.handoffs] 매개변수를 가지며, 여기에 `Agent` 자체를 직접 전달하거나, 핸드오프를 커스터마이즈하는 `Handoff` 객체를 전달할 수 있습니다.\n \n-일반 `Agent` 인스턴스를 전달하면, 해당 에이전트의 [`handoff_description`][agents.agent.Agent.handoff_description] (설정된 경우)이 기본 도구 설명에 추가됩니다. 전체 `handoff()` 객체를 작성하지 않고도 모델이 해당 핸드오프를 선택해야 하는 상황을 암시하는 데 사용할 수 있습니다.\n+일반 `Agent` 인스턴스를 전달하면, 해당 에이전트의 [`handoff_description`][agents.agent.Agent.handoff_description] (설정된 경우)이 기본 도구 설명에 추가됩니다. 전체 `handoff()` 객체를 작성하지 않고도, 모델이 그 핸드오프를 선택해야 하는 시점을 힌트하는 용도로 사용하세요.\n \n-Agents SDK 가 제공하는 [`handoff()`][agents.handoffs.handoff] 함수를 사용해 핸드오프를 생성할 수 있습니다. 이 함수는 핸드오프 대상 에이전트를 지정하고, 선택적으로 오버라이드 및 입력 필터를 설정할 수 있습니다.\n+Agents SDK 에서 제공하는 [`handoff()`][agents.handoffs.handoff] 함수를 사용해 핸드오프를 생성할 수 있습니다. 이 함수는 핸드오프 대상 에이전트와 함께 선택적인 오버라이드 및 입력 필터를 지정할 수 있습니다.\n \n-### 기본 사용\n+### 기본 사용법\n \n-간단한 핸드오프를 만드는 방법은 다음과 같습니다:\n+간단한 핸드오프를 생성하는 방법은 다음과 같습니다:\n \n ```python\n from agents import Agent, handoff\n@@ -30,19 +30,19 @@ refund_agent = Agent(name=\"Refund agent\")\n triage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n ```\n \n-1. `billing_agent` 처럼 에이전트를 직접 사용할 수도 있고, `handoff()` 함수를 사용할 수도 있습니다.\n+1. 에이전트를 직접 사용할 수 있으며(예: `billing_agent`), 또는 `handoff()` 함수를 사용할 수 있습니다\n \n-### `handoff()` 함수를 통한 핸드오프 커스터마이징\n+### `handoff()` 함수를 통한 핸드오프 커스터마이즈\n \n-[`handoff()`][agents.handoffs.handoff] 함수로 다양한 커스터마이징이 가능합니다.\n+[`handoff()`][agents.handoffs.handoff] 함수로 다양한 설정을 커스터마이즈할 수 있습니다.\n \n-- `agent`: 핸드오프 대상 에이전트입니다.\n-- `tool_name_override`: 기본적으로 `Handoff.default_tool_name()` 함수가 사용되며, 이는 `transfer_to_<agent_name>` 로 해석됩니다. 이를 오버라이드할 수 있습니다.\n-- `tool_description_override`: `Handoff.default_tool_description()` 의 기본 도구 설명을 오버라이드합니다\n-- `on_handoff`: 핸드오프가 호출될 때 실행되는 콜백 함수입니다. 핸드오프가 호출되는 즉시 데이터 페칭을 시작하는 등의 용도로 유용합니다. 이 함수는 에이전트 컨텍스트를 받고, 선택적으로 LLM 이 생성한 입력도 받을 수 있습니다. 입력 데이터는 `input_type` 매개변수로 제어합니다.\n-- `input_type`: 핸드오프가 기대하는 입력의 타입입니다(선택 사항).\n-- `input_filter`: 다음 에이전트로 전달되는 입력을 필터링합니다. 아래를 참조하세요.\n-- `is_enabled`: 핸드오프 활성화 여부입니다. 불리언 또는 불리언을 반환하는 함수일 수 있어, 런타임에 동적으로 핸드오프를 활성화하거나 비활성화할 수 있습니다.\n+- `agent`: 작업을 넘길 대상 에이전트\n+- `tool_name_override`: 기본적으로 `Handoff.default_tool_name()` 함수가 사용되며, 이는 `transfer_to_<agent_name>` 으로 결정됩니다. 이를 오버라이드할 수 있습니다\n+- `tool_description_override`: `Handoff.default_tool_description()` 의 기본 도구 설명을 오버라이드\n+- `on_handoff`: 핸드오프가 호출될 때 실행되는 콜백 함수. 핸드오프가 호출되는 즉시 데이터 패칭을 시작하는 등의 용도로 유용합니다. 이 함수는 에이전트 컨텍스트를 전달받고, 선택적으로 LLM 이 생성한 입력도 전달받을 수 있습니다. 입력 데이터는 `input_type` 매개변수로 제어됩니다\n+- `input_type`: 핸드오프가 기대하는 입력의 타입(선택)\n+- `input_filter`: 다음 에이전트가 받는 입력을 필터링할 수 있습니다. 아래 내용을 참고하세요\n+- `is_enabled`: 핸드오프 활성화 여부. 불리언 또는 불리언을 반환하는 함수가 될 수 있어, 런타임에 동적으로 활성/비활성화할 수 있습니다\n \n ```python\n from agents import Agent, handoff, RunContextWrapper\n@@ -62,7 +62,7 @@ handoff_obj = handoff(\n \n ## 핸드오프 입력\n \n-특정 상황에서는 LLM 이 핸드오프를 호출할 때 일부 데이터를 제공하길 원할 수 있습니다. 예를 들어 \"에스컬레이션 에이전트\"로의 핸드오프를 상상해 보세요. 로깅을 위해 사유를 제공받고 싶을 수 있습니다.\n+특정 상황에서는, LLM 이 핸드오프를 호출할 때 일부 데이터를 제공하길 원할 수 있습니다. 예를 들어, \"에스컬레이션 에이전트\"로의 핸드오프를 생각해 봅시다. 이때 이유를 제공받아 로깅할 수 있으면 좋습니다.\n \n ```python\n from pydantic import BaseModel\n@@ -86,11 +86,11 @@ handoff_obj = handoff(\n \n ## 입력 필터\n \n-핸드오프가 발생하면, 마치 새로운 에이전트가 대화를 인수하고 이전 대화 내역 전체를 볼 수 있는 것과 같습니다. 이를 변경하려면 [`input_filter`][agents.handoffs.Handoff.input_filter] 를 설정할 수 있습니다. 입력 필터는 [`HandoffInputData`][agents.handoffs.HandoffInputData] 를 통해 기존 입력을 받고, 새로운 `HandoffInputData` 를 반환해야 하는 함수입니다.\n+핸드오프가 발생하면, 마치 새로운 에이전트가 대화를 인수한 것처럼 동작하며, 이전 대화 이력을 모두 볼 수 있습니다. 이를 변경하려면 [`input_filter`][agents.handoffs.Handoff.input_filter]를 설정할 수 있습니다. 입력 필터는 기존 입력을 [`HandoffInputData`][agents.handoffs.HandoffInputData]로 받아 새로운 `HandoffInputData`를 반환해야 하는 함수입니다.\n \n-기본적으로 러너는 이전 기록을 하나의 assistant 요약 메시지로 축약합니다([`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history] 참조). 이 요약은 동일한 실행 중 여러 번 핸드오프가 발생할 때 새 턴이 계속 추가되는 `<CONVERSATION HISTORY>` 블록 내부에 표시됩니다. 생성된 메시지를 완전한 `input_filter` 없이 대체하려면 [`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper] 를 통해 매핑 함수를 제공할 수 있습니다. 이 기본 동작은 핸드오프나 실행에서 명시적으로 `input_filter` 를 제공하지 않았을 때에만 적용되므로, 이미 페이로드를 커스터마이즈하는 기존 코드(이 저장소의 code examples 포함)는 변경 없이 현재 동작을 유지합니다. 단일 핸드오프에 대해 중첩 동작을 오버라이드하려면 [`handoff(...)`][agents.handoffs.handoff] 호출 시 `nest_handoff_history=True` 또는 `False` 를 전달하여 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] 를 설정하면 됩니다. 생성된 요약의 래퍼 텍스트만 변경이 필요하다면, 에이전트를 실행하기 전에 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] (선택적으로 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers])를 호출하세요.\n+기본적으로 러너는 이전 대화 기록을 하나의 assistant 요약 메시지로 축약합니다([`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history] 참조). 요약은 `<CONVERSATION HISTORY>` 블록 안에 표시되며, 동일한 실행(run) 동안 여러 번의 핸드오프가 일어날 경우 새 턴이 계속 추가됩니다. 전체 `input_filter`를 작성하지 않고도 생성된 메시지를 교체하려면 [`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]를 통해 자체 매핑 함수를 제공할 수 있습니다. 이 기본 동작은 핸드오프와 실행 모두에서 명시적인 `input_filter`를 제공하지 않은 경우에만 적용되므로, 이미 페이로드를 커스터마이즈하는 기존 코드(이 저장소의 code examples 포함)는 변경 없이 현재 동작을 유지합니다. 단일 핸드오프에 대해 중첩 동작을 오버라이드하려면 [`handoff(...)`][agents.handoffs.handoff]에 `nest_handoff_history=True` 또는 `False`를 전달하여 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history]를 설정하세요. 생성된 요약의 래퍼 텍스트만 변경하면 되는 경우, 에이전트를 실행하기 전에 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers](필요한 경우 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers])를 호출하세요.\n \n-일반적인 패턴 몇 가지(예: 히스토리에서 모든 도구 호출 제거)는 [`agents.extensions.handoff_filters`][] 에 구현되어 있습니다\n+일반적인 패턴들이 있으며(예: 히스토리에서 모든 도구 호출 제거), 이는 [`agents.extensions.handoff_filters`][]에 구현되어 있습니다\n \n ```python\n from agents import Agent, handoff\n@@ -104,11 +104,11 @@ handoff_obj = handoff(\n )\n ```\n \n-1. 이것은 `FAQ agent` 가 호출될 때 히스토리에서 모든 도구를 자동으로 제거합니다.\n+1. 이는 `FAQ agent`가 호출될 때 히스토리에서 모든 도구를 자동으로 제거합니다\n \n ## 권장 프롬프트\n \n-LLM 이 핸드오프를 올바르게 이해하도록 하려면, 에이전트에 핸드오프에 관한 정보를 포함하는 것을 권장합니다. [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] 의 권장 프리픽스가 준비되어 있으며, 또는 [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] 를 호출해 권장 데이터를 프롬프트에 자동으로 추가할 수 있습니다.\n+LLM 이 핸드오프를 올바르게 이해하도록 하려면, 에이전트에 핸드오프 관련 정보를 포함하는 것을 권장합니다. [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][]에 권장 접두사가 있으며, 또는 [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][]를 호출하여 권장 데이터를 프롬프트에 자동으로 추가할 수 있습니다.\n \n ```python\n from agents import Agent",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fhandoffs.md",
        "sha": "caeed4104c735d718429e8c785e7f33f167e222d",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Findex.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/ko/index.md",
        "patch": "@@ -4,31 +4,31 @@ search:\n ---\n # OpenAI Agents SDK\n \n-[OpenAI Agents SDK](https://github.com/openai/openai-agents-python)는 소수의 추상화만으로 가볍고 사용하기 쉬운 패키지에서 에이전트형 AI 앱을 구축할 수 있도록 합니다. 이는 이전 에이전트 실험 프레임워크인 [Swarm](https://github.com/openai/swarm/tree/main)의 프로덕션 레디 업그레이드입니다. Agents SDK에는 매우 작은 기본 구성 요소 집합이 있습니다:\n+[OpenAI Agents SDK](https://github.com/openai/openai-agents-python)는 최소한의 추상화로 가볍고 사용하기 쉬운 패키지에서 에이전트형 AI 앱을 만들 수 있도록 해줍니다. 이는 이전의 에이전트 실험 프로젝트인 [Swarm](https://github.com/openai/swarm/tree/main)의 프로덕션 준비가 된 업그레이드입니다. Agents SDK는 매우 작은 범위의 기본 구성요소를 제공합니다:\n \n -   **에이전트**: instructions와 tools를 갖춘 LLM\n--   **핸드오프**: 에이전트가 특정 작업을 다른 에이전트에 위임할 수 있게 함\n--   **가드레일**: 에이전트 입력 및 출력의 검증을 가능하게 함\n--   **세션**: 에이전트 실행 전반에서 대화 기록을 자동으로 유지 관리함\n+-   **핸드오프**: 특정 작업에 대해 다른 에이전트에 위임할 수 있게 함\n+-   **가드레일**: 에이전트의 입력과 출력을 검증할 수 있게 함\n+-   **세션**: 에이전트 실행 전반에 걸쳐 대화 기록을 자동으로 유지 관리함\n \n-Python과 결합하면, 이 기본 구성 요소들은 도구와 에이전트 간의 복잡한 관계를 표현할 만큼 강력하며, 가파른 학습 곡선 없이 실제 애플리케이션을 구축할 수 있게 합니다. 또한 SDK에는 에이전트 흐름을 시각화하고 디버그할 수 있는 내장 **트레이싱**이 포함되어 있어, 이를 평가하고 심지어 애플리케이션에 맞게 모델을 파인튜닝할 수도 있습니다.\n+Python과 결합하면, 이러한 기본 구성요소만으로도 도구와 에이전트 간의 복잡한 관계를 표현할 수 있으며, 가파른 학습 곡선 없이 실제 애플리케이션을 구축할 수 있습니다. 또한 SDK에는 에이전트 플로우를 시각화하고 디버깅할 수 있는 내장 **트레이싱**이 포함되어 있으며, 이를 평가하고 애플리케이션에 맞게 모델을 파인튜닝하는 것도 가능합니다.\n \n ## Agents SDK를 사용하는 이유\n \n-SDK의 설계 원칙은 두 가지입니다:\n+SDK의 설계 원칙은 다음 두 가지입니다:\n \n-1. 사용할 가치가 있을 만큼 충분한 기능을 제공하되, 빠르게 배울 수 있을 만큼 기본 구성 요소는 적게 유지\n-2. 기본 설정만으로도 훌륭히 작동하지만, 동작을 정확히 원하는 대로 커스터마이즈 가능\n+1. 사용할 가치가 있을 만큼 충분한 기능을 제공하되, 빠르게 학습할 수 있을 만큼 기본 구성요소는 적게 유지\n+2. 기본 설정만으로도 훌륭하게 동작하지만, 내부 동작을 정확히 원하는 대로 커스터마이즈 가능\n \n SDK의 주요 기능은 다음과 같습니다:\n \n--   에이전트 루프: 도구 호출, 결과를 LLM에 전달, LLM이 완료될 때까지 루프를 처리하는 내장 에이전트 루프\n--   파이썬 우선: 새로운 추상화를 학습할 필요 없이, 내장 언어 기능으로 에이전트를 오케스트레이션하고 체이닝\n+-   에이전트 루프: 도구 호출, 결과를 LLM에 전달, LLM이 완료될 때까지 반복을 처리하는 내장 에이전트 루프\n+-   파이썬 우선: 새로운 추상화를 배울 필요 없이 언어의 기본 기능으로 에이전트를 오케스트레이션하고 체이닝\n -   핸드오프: 여러 에이전트 간의 조정과 위임을 위한 강력한 기능\n--   가드레일: 에이전트와 병렬로 입력 검증과 점검을 실행하며, 점검이 실패하면 조기에 중단\n--   세션: 에이전트 실행 전반의 대화 기록을 자동 관리하여 수동 상태 처리를 제거\n--   함수 도구: 어떤 Python 함수든 도구로 전환, 자동 스키마 생성과 Pydantic 기반 검증 제공\n--   트레이싱: 워크플로를 시 visualize, 디버그, 모니터링할 수 있는 내장 트레이싱과 함께 OpenAI의 평가, 파인튜닝, 디스틸레이션 도구 활용\n+-   가드레일: 에이전트와 병렬로 입력 검증과 점검을 실행하고, 점검 실패 시 조기에 중단\n+-   세션: 에이전트 실행 전반에 걸친 대화 기록 자동 관리로 수동 상태 처리를 제거\n+-   함수 도구: 어떤 Python 함수든 도구로 전환하며, 스키마 자동 생성과 Pydantic 기반 검증 제공\n+-   트레이싱: 워크플로를 시각화, 디버깅, 모니터링할 수 있는 내장 트레이싱과 OpenAI의 평가, 파인튜닝, 지식 증류 도구 제품군 활용\n \n ## 설치\n \n@@ -51,7 +51,7 @@ print(result.final_output)\n # Infinite loop's dance.\n ```\n \n-(_실행 시 `OPENAI_API_KEY` 환경 변수를 설정했는지 확인하세요_)\n+(_If running this, ensure you set the `OPENAI_API_KEY` environment variable_)\n \n ```bash\n export OPENAI_API_KEY=sk-...",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Findex.md",
        "sha": "409ca2d269b51e44c75bfe81acbed5a13a9e08ee",
        "status": "modified"
      },
      {
        "additions": 53,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmcp.md",
        "changes": 110,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fmcp.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 57,
        "filename": "docs/ko/mcp.md",
        "patch": "@@ -4,34 +4,34 @@ search:\n ---\n # Model context protocol (MCP)\n \n-[Model context protocol](https://modelcontextprotocol.io/introduction) (MCP)는 애플리케이션이 도구와 컨텍스트를 언어 모델에 노출하는 방식을 표준화합니다. 공식 문서에서 발췌:\n+[Model context protocol](https://modelcontextprotocol.io/introduction) (MCP)은 애플리케이션이 도구와 컨텍스트를 언어 모델에 노출하는 방식을 표준화합니다. 공식 문서에서:\n \n-> MCP는 애플리케이션이 LLM에 컨텍스트를 제공하는 방식을 표준화하는 개방형 프로토콜입니다. MCP를 AI 애플리케이션을 위한 USB-C 포트라고 생각해 보세요. USB-C가 다양한 주변기기와 액세서리에 기기를 연결하는 표준화된 방식을 제공하듯, MCP는 AI 모델을 다양한 데이터 소스와 도구에 연결하는 표준화된 방식을 제공합니다.\n+> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI\n+> applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP\n+> provides a standardized way to connect AI models to different data sources and tools.\n \n-Agents Python SDK는 여러 MCP 전송 방식을 이해합니다. 이를 통해 기존 MCP 서버를 재사용하거나 직접 구축하여 파일시스템, HTTP, 또는 커넥터 기반 도구를 에이전트에 노출할 수 있습니다.\n+Agents Python SDK는 여러 MCP 전송 방식을 이해합니다. 이를 통해 기존 MCP 서버를 재사용하거나 자체 서버를 구축하여 파일 시스템, HTTP 또는 커넥터 기반 도구를 에이전트에 노출할 수 있습니다.\n \n-## MCP 통합 선택\n+## Choosing an MCP integration\n \n-MCP 서버를 에이전트에 연결하기 전에 도구 호출이 어디에서 실행되어야 하는지와 도달 가능한 전송 방식을 결정하세요. 아래 매트릭스는 Python SDK가 지원하는 옵션을 요약합니다.\n+MCP 서버를 에이전트에 연결하기 전에 도구 호출을 어디에서 실행할지와 어떤 전송 방식을 사용할 수 있는지를 결정하세요. 아래 매트릭스는 Python SDK가 지원하는 옵션을 요약한 것입니다.\n \n-| 필요한 것                                                                            | 권장 옵션                                             |\n+| 필요한 것                                                                           | 권장 옵션                                             |\n | ------------------------------------------------------------------------------------ | ----------------------------------------------------- |\n-| OpenAI의 Responses API가 모델을 대신해 공개적으로 접근 가능한 MCP 서버를 호출        | **호스티드 MCP 서버 도구** via [`HostedMCPTool`][agents.tool.HostedMCPTool] |\n-| 로컬 또는 원격에서 실행하는 Streamable HTTP 서버에 연결                              | **Streamable HTTP MCP 서버** via [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |\n-| Server-Sent Events 를 구현한 서버와 통신                                             | **HTTP with SSE MCP 서버** via [`MCPServerSse`][agents.mcp.server.MCPServerSse] |\n-| 로컬 프로세스를 실행하고 stdin/stdout으로 통신                                       | **stdio MCP 서버** via [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |\n+| 모델을 대신해 OpenAI의 Responses API가 공개적으로 접근 가능한 MCP 서버를 호출하도록 하기 | **Hosted MCP server tools** via [`HostedMCPTool`][agents.tool.HostedMCPTool] |\n+| 로컬 또는 원격으로 실행하는 Streamable HTTP 서버에 연결                               | **Streamable HTTP MCP servers** via [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |\n+| Server-Sent Events를 구현하는 서버와 통신                                            | **HTTP with SSE MCP servers** via [`MCPServerSse`][agents.mcp.server.MCPServerSse] |\n+| 로컬 프로세스를 실행하고 stdin/stdout으로 통신                                       | **stdio MCP servers** via [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |\n \n-아래 섹션에서는 각 옵션과 구성 방법, 그리고 언제 어떤 전송 방식을 선호해야 하는지 설명합니다.\n+아래 섹션에서는 각 옵션을 설정하는 방법과 언제 어떤 전송 방식을 선호해야 하는지를 설명합니다.\n \n ## 1. Hosted MCP server tools\n \n-호스티드 툴은 전체 도구 왕복 호출을 OpenAI 인프라로 이동합니다. 사용자의 코드가 도구를 나열하고 호출하는 대신,\n-[`HostedMCPTool`][agents.tool.HostedMCPTool]이 서버 레이블(및 선택적 커넥터 메타데이터)을 Responses API로 전달합니다. 모델은 원격 서버의 도구를 나열하고, 사용자의 Python 프로세스로 추가 콜백 없이 이를 호출합니다. 호스티드 툴은 현재 Responses API의 호스티드 MCP 통합을 지원하는 OpenAI 모델에서 동작합니다.\n+호스티드 툴은 전체 도구 왕복 과정을 OpenAI 인프라 내로 이동시킵니다. 코드에서 도구를 나열하고 호출하는 대신, [`HostedMCPTool`][agents.tool.HostedMCPTool]이 서버 레이블(및 선택적인 커넥터 메타데이터)을 Responses API로 전달합니다. 모델은 원격 서버의 도구를 나열하고 Python 프로세스로 추가 콜백 없이 이를 호출합니다. 호스티드 툴은 현재 Responses API의 호스티드 MCP 통합을 지원하는 OpenAI 모델에서 동작합니다.\n \n-### 기본 호스티드 MCP 도구\n+### Basic hosted MCP tool\n \n-에이전트의 `tools` 목록에 [`HostedMCPTool`][agents.tool.HostedMCPTool]을 추가하여 호스티드 툴을 생성합니다. `tool_config`\n-dict는 REST API로 전송할 JSON과 동일합니다:\n+에이전트의 `tools` 목록에 [`HostedMCPTool`][agents.tool.HostedMCPTool]을 추가하여 호스티드 툴을 생성합니다. `tool_config` 딕셔너리는 REST API로 보낼 JSON과 동일합니다:\n \n ```python\n import asyncio\n@@ -59,11 +59,11 @@ async def main() -> None:\n asyncio.run(main())\n ```\n \n-호스티드 서버는 도구를 자동으로 노출하므로 `mcp_servers`에 추가할 필요가 없습니다.\n+호스티드 서버는 도구를 자동으로 노출합니다. `mcp_servers`에 추가할 필요가 없습니다.\n \n-### 스트리밍 호스티드 MCP 결과\n+### Streaming hosted MCP results\n \n-호스티드 툴은 함수 도구와 완전히 동일한 방식으로 스트리밍 결과를 지원합니다. `Runner.run_streamed`에 `stream=True`를 전달하여 모델이 아직 작업 중일 때도 MCP 출력을 점진적으로 소비하세요:\n+호스티드 툴은 함수 도구와 정확히 동일한 방식으로 스트리밍 결과를 지원합니다. 모델이 여전히 작업 중일 때 점진적인 MCP 출력을 소비하려면 `Runner.run_streamed`에 `stream=True`를 전달하세요:\n \n ```python\n result = Runner.run_streamed(agent, \"Summarise this repository's top languages\")\n@@ -73,9 +73,9 @@ async for event in result.stream_events():\n print(result.final_output)\n ```\n \n-### 선택적 승인 플로우\n+### Optional approval flows\n \n-서버가 민감한 작업을 수행할 수 있는 경우 각 도구 실행 전에 사람 또는 프로그램에 의한 승인을 요구할 수 있습니다. `tool_config`의 `require_approval`을 단일 정책(`\"always\"`, `\"never\"`) 또는 도구 이름을 정책에 매핑하는 dict로 구성하세요. Python 내부에서 결정을 내리려면 `on_approval_request` 콜백을 제공하세요.\n+서버가 민감한 작업을 수행할 수 있는 경우 각 도구 실행 전에 사람 또는 프로그램의 승인을 요구할 수 있습니다. `tool_config`에서 `require_approval`을 단일 정책(`\"always\"`, `\"never\"`) 또는 도구 이름을 정책에 매핑하는 딕셔너리로 구성하세요. Python 내부에서 결정을 내리려면 `on_approval_request` 콜백을 제공하세요.\n \n ```python\n from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest\n@@ -103,12 +103,11 @@ agent = Agent(\n )\n ```\n \n-콜백은 동기 또는 비동기로 작성할 수 있으며, 모델이 계속 실행되는 데 필요한 승인 데이터가 필요할 때마다 호출됩니다.\n+콜백은 동기 또는 비동기로 구현할 수 있으며, 모델이 실행을 계속하기 위해 승인 데이터가 필요할 때마다 호출됩니다.\n \n-### 커넥터 기반 호스티드 서버\n+### Connector-backed hosted servers\n \n-호스티드 MCP는 OpenAI 커넥터도 지원합니다. `server_url`을 지정하는 대신 `connector_id`와 액세스 토큰을 제공하세요.\n-Responses API가 인증을 처리하며, 호스티드 서버가 커넥터의 도구를 노출합니다.\n+호스티드 MCP는 OpenAI 커넥터도 지원합니다. `server_url`을 지정하는 대신 `connector_id`와 액세스 토큰을 제공하세요. Responses API가 인증을 처리하고 호스티드 서버가 커넥터의 도구를 노출합니다.\n \n ```python\n import os\n@@ -127,10 +126,9 @@ HostedMCPTool(\n 스트리밍, 승인, 커넥터를 포함한 완전한 호스티드 툴 샘플은\n [`examples/hosted_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp)에 있습니다.\n \n-## 2. Streamable HTTP MCP 서버\n+## 2. Streamable HTTP MCP servers\n \n-네트워크 연결을 직접 관리하려면\n-[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]를 사용하세요. Streamable HTTP 서버는 전송을 직접 제어하거나, 낮은 지연 시간을 유지하면서 자체 인프라 내에서 서버를 실행하고자 할 때 이상적입니다.\n+네트워크 연결을 직접 관리하려면 [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]를 사용하세요. 스트리머블 HTTP 서버는 전송을 제어하거나 낮은 지연 시간을 유지하면서 자체 인프라 내에서 서버를 실행하려는 경우에 이상적입니다.\n \n ```python\n import asyncio\n@@ -167,19 +165,18 @@ asyncio.run(main())\n \n 생성자는 다음 추가 옵션을 허용합니다:\n \n-- `client_session_timeout_seconds`는 HTTP 읽기 타임아웃을 제어합니다.\n-- `use_structured_content`는 `tool_result.structured_content`를 텍스트 출력보다 선호할지 여부를 전환합니다.\n-- `max_retry_attempts` 및 `retry_backoff_seconds_base`는 `list_tools()`와 `call_tool()`에 대한 자동 재시도를 추가합니다.\n-- `tool_filter`를 사용하면 노출할 도구의 하위 집합만 공개할 수 있습니다([도구 필터링](#tool-filtering) 참조)\n+- `client_session_timeout_seconds`는 HTTP 읽기 타임아웃을 제어합니다\n+- `use_structured_content`는 `tool_result.structured_content`를 텍스트 출력보다 우선시할지 여부를 토글합니다\n+- `max_retry_attempts` 및 `retry_backoff_seconds_base`는 `list_tools()`와 `call_tool()`에 자동 재시도를 추가합니다\n+- `tool_filter`는 노출할 도구의 하위 집합만 선택할 수 있게 합니다([도구 필터링](#tool-filtering) 참조)\n \n-## 3. HTTP with SSE MCP 서버\n+## 3. HTTP with SSE MCP servers\n \n !!! warning\n \n-    MCP 프로젝트는 Server-Sent Events 전송을 더 이상 권장하지 않습니다. 새로운 통합에는 Streamable HTTP 또는 stdio를 선호하고, SSE는 레거시 서버에만 유지하세요.\n+    MCP 프로젝트는 Server-Sent Events 전송을 더 이상 사용하지 않습니다. 새로운 통합에는 Streamable HTTP 또는 stdio를 선호하고, SSE는 레거시 서버에만 유지하세요.\n \n-MCP 서버가 HTTP with SSE 전송을 구현하는 경우,\n-[`MCPServerSse`][agents.mcp.server.MCPServerSse]를 인스턴스화하세요. 전송 방식을 제외하면 API는 Streamable HTTP 서버와 동일합니다.\n+MCP 서버가 HTTP with SSE 전송을 구현하는 경우 [`MCPServerSse`][agents.mcp.server.MCPServerSse]를 인스턴스화하세요. 전송 방식을 제외하면 API는 Streamable HTTP 서버와 동일합니다.\n \n ```python\n \n@@ -206,9 +203,9 @@ async with MCPServerSse(\n     print(result.final_output)\n ```\n \n-## 4. stdio MCP 서버\n+## 4. stdio MCP servers\n \n-로컬 하위 프로세스로 실행되는 MCP 서버의 경우 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]를 사용하세요. SDK는 프로세스를 생성하고 파이프를 열어 유지하며, 컨텍스트 매니저가 종료될 때 자동으로 닫습니다. 이 옵션은 빠른 프로토타입 제작이나 서버가 커맨드라인 엔트리 포인트만 노출하는 경우에 유용합니다.\n+로컬 하위 프로세스로 실행되는 MCP 서버의 경우 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]를 사용하세요. SDK는 프로세스를 생성하고 파이프를 열어 두며 컨텍스트 관리자가 종료될 때 이를 자동으로 닫습니다. 이 옵션은 빠른 프로토타입을 만들거나 서버가 커맨드라인 엔트리 포인트만 노출할 때 유용합니다.\n \n ```python\n from pathlib import Path\n@@ -234,11 +231,11 @@ async with MCPServerStdio(\n     print(result.final_output)\n ```\n \n-## 도구 필터링\n+## Tool filtering\n \n-각 MCP 서버는 에이전트에 필요한 기능만 노출할 수 있도록 도구 필터를 지원합니다. 필터링은 생성 시점 또는 실행별로 동적으로 수행할 수 있습니다.\n+각 MCP 서버는 에이전트에 필요한 기능만 노출할 수 있도록 도구 필터를 지원합니다. 필터링은 생성 시점 또는 실행마다 동적으로 수행할 수 있습니다.\n \n-### 정적 도구 필터링\n+### Static tool filtering\n \n [`create_static_tool_filter`][agents.mcp.create_static_tool_filter]를 사용하여 간단한 허용/차단 목록을 구성하세요:\n \n@@ -258,11 +255,11 @@ filesystem_server = MCPServerStdio(\n )\n ```\n \n-`allowed_tool_names`와 `blocked_tool_names`가 모두 제공된 경우 SDK는 먼저 허용 목록을 적용한 뒤, 남은 집합에서 차단된 도구를 제거합니다.\n+`allowed_tool_names`와 `blocked_tool_names`가 모두 제공되면 SDK는 먼저 허용 목록을 적용한 다음 남은 집합에서 차단된 도구를 제거합니다.\n \n-### 동적 도구 필터링\n+### Dynamic tool filtering\n \n-좀 더 정교한 로직이 필요하다면 [`ToolFilterContext`][agents.mcp.ToolFilterContext]를 받는 호출 가능 객체를 전달하세요. 이 호출 가능 객체는 동기 또는 비동기로 작성할 수 있으며, 도구를 노출해야 하는 경우 `True`를 반환합니다.\n+더 정교한 로직이 필요하면 [`ToolFilterContext`][agents.mcp.ToolFilterContext]를 받는 호출 가능 객체를 전달하세요. 이 호출 가능 객체는 동기 또는 비동기일 수 있으며, 도구를 노출해야 할 때 `True`를 반환합니다.\n \n ```python\n from pathlib import Path\n@@ -286,15 +283,14 @@ async with MCPServerStdio(\n     ...\n ```\n \n-필터 컨텍스트는 활성 `run_context`, 도구를 요청하는 `agent`, 그리고 `server_name`을 노출합니다.\n+필터 컨텍스트는 활성 `run_context`, 도구를 요청하는 `agent`, 그리고 `server_name`을 제공합니다.\n \n-## 프롬프트\n+## Prompts\n \n-MCP 서버는 에이전트 instructions를 동적으로 생성하는 프롬프트도 제공할 수 있습니다. 프롬프트를 지원하는 서버는 두 가지\n-메서드를 노출합니다:\n+MCP 서버는 에이전트 instructions를 동적으로 생성하는 프롬프트도 제공할 수 있습니다. 프롬프트를 지원하는 서버는 두 가지 메서드를 노출합니다:\n \n-- `list_prompts()`는 사용 가능한 프롬프트 템플릿을 열거합니다.\n-- `get_prompt(name, arguments)`는 선택적 매개변수와 함께 구체적인 프롬프트를 가져옵니다.\n+- `list_prompts()`는 사용 가능한 프롬프트 템플릿을 나열합니다\n+- `get_prompt(name, arguments)`는 선택적 매개변수와 함께 구체적인 프롬프트를 가져옵니다\n \n ```python\n from agents import Agent\n@@ -312,21 +308,21 @@ agent = Agent(\n )\n ```\n \n-## 캐싱\n+## Caching\n \n-모든 에이전트 실행은 각 MCP 서버에서 `list_tools()`를 호출합니다. 원격 서버는 눈에 띄는 지연 시간을 초래할 수 있으므로, 모든 MCP 서버 클래스는 `cache_tools_list` 옵션을 노출합니다. 도구 정의가 자주 변경되지 않는다고 확신할 때만 `True`로 설정하세요. 이후 새 목록을 강제로 가져오려면 서버 인스턴스에서 `invalidate_tools_cache()`를 호출하세요.\n+모든 에이전트 실행은 각 MCP 서버에서 `list_tools()`를 호출합니다. 원격 서버는 눈에 띄는 지연 시간을 유발할 수 있으므로 모든 MCP 서버 클래스는 `cache_tools_list` 옵션을 제공합니다. 도구 정의가 자주 변경되지 않는다고 확신하는 경우에만 `True`로 설정하세요. 나중에 새 목록을 강제로 가져오려면 서버 인스턴스에서 `invalidate_tools_cache()`를 호출하세요.\n \n-## 트레이싱\n+## Tracing\n \n-[트레이싱](./tracing.md)은 MCP 활동을 자동으로 캡처합니다. 다음을 포함합니다:\n+[트레이싱](./tracing.md)은 다음을 포함하여 MCP 활동을 자동으로 캡처합니다:\n \n-1. 도구 목록을 가져오기 위한 MCP 서버 호출\n+1. 도구를 나열하기 위한 MCP 서버 호출\n 2. 도구 호출에 대한 MCP 관련 정보\n \n ![MCP Tracing Screenshot](../assets/images/mcp-tracing.jpg)\n \n-## 추가 자료\n+## Further reading\n \n-- [Model Context Protocol](https://modelcontextprotocol.io/) – 사양 및 설계 가이드\n+- [Model Context Protocol](https://modelcontextprotocol.io/) – 사양과 설계 가이드\n - [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) – 실행 가능한 stdio, SSE, Streamable HTTP 샘플\n - [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) – 승인 및 커넥터를 포함한 완전한 호스티드 MCP 데모\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmcp.md",
        "sha": "a928babd5e5dbdd05595ed6cd935d1f7a306f276",
        "status": "modified"
      },
      {
        "additions": 39,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmodels%2Findex.md",
        "changes": 78,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fmodels%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 39,
        "filename": "docs/ko/models/index.md",
        "patch": "@@ -4,16 +4,16 @@ search:\n ---\n # 모델\n \n-Agents SDK는 다음 두 가지 방식으로 OpenAI 모델을 즉시 지원합니다:\n+Agents SDK 는 두 가지 방식으로 OpenAI 모델을 즉시 지원합니다:\n \n--   **권장**: 새로운 [Responses API](https://platform.openai.com/docs/api-reference/responses)를 사용하는 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]\n--   [Chat Completions API](https://platform.openai.com/docs/api-reference/chat)를 사용하는 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]\n+-   **권장**: 새로운 [Responses API](https://platform.openai.com/docs/api-reference/responses)를 사용해 OpenAI API 를 호출하는 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]\n+-   [Chat Completions API](https://platform.openai.com/docs/api-reference/chat)를 사용해 OpenAI API 를 호출하는 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]\n \n ## OpenAI 모델\n \n-`Agent`를 초기화할 때 모델을 지정하지 않으면 기본 모델이 사용됩니다. 현재 기본값은 호환성과 낮은 지연 시간을 위해 [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1)입니다. 사용 권한이 있다면, 명시적으로 `model_settings`를 유지하면서 더 높은 품질을 위해 에이전트를 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2)로 설정하는 것을 권장합니다.\n+`Agent`를 초기화할 때 모델을 지정하지 않으면 기본 모델이 사용됩니다. 현재 기본값은 호환성과 낮은 지연 시간을 위해 [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1) 입니다. 접근 권한이 있다면, 명시적인 `model_settings` 를 유지하면서 더 높은 품질을 위해 에이전트를 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) 로 설정하는 것을 권장합니다.\n \n-[`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2)와 같은 다른 모델로 전환하려면 다음 섹션의 단계를 따르세요.\n+[`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) 같은 다른 모델로 전환하려면 다음 섹션의 단계를 따르세요.\n \n ### 기본 OpenAI 모델\n \n@@ -26,9 +26,9 @@ python3 my_awesome_agent.py\n \n #### GPT-5 모델\n \n-GPT-5의 reasoning 모델들([`gpt-5`](https://platform.openai.com/docs/models/gpt-5), [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini), [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano))을 이 방식으로 사용할 때, SDK는 합리적인 `ModelSettings`를 기본으로 적용합니다. 구체적으로 `reasoning.effort`와 `verbosity`를 모두 `\"low\"`로 설정합니다. 이러한 설정을 직접 구성하려면 `agents.models.get_default_model_settings(\"gpt-5\")`를 호출하세요.\n+이 방식으로 GPT-5 의 reasoning 모델들([`gpt-5`](https://platform.openai.com/docs/models/gpt-5), [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini), [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano))을 사용할 때, SDK 는 합리적인 기본 `ModelSettings` 를 적용합니다. 구체적으로 `reasoning.effort` 와 `verbosity` 를 모두 `\"low\"` 로 설정합니다. 이러한 설정을 직접 구성하고 싶다면 `agents.models.get_default_model_settings(\"gpt-5\")` 를 호출하세요.\n \n-더 낮은 지연 시간이나 특정 요구 사항이 있다면, 다른 모델과 설정을 선택할 수 있습니다. 기본 모델의 reasoning effort를 조정하려면, 사용자 정의 `ModelSettings`를 전달하세요:\n+더 낮은 지연 시간이나 특정 요구 사항이 있다면, 다른 모델과 설정을 선택할 수 있습니다. 기본 모델의 reasoning effort 를 조정하려면, 사용자 지정 `ModelSettings` 를 전달하세요:\n \n ```python\n from openai.types.shared import Reasoning\n@@ -44,52 +44,52 @@ my_agent = Agent(\n )\n ```\n \n-특히 지연 시간을 낮추기 위해서는 [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) 또는 [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) 모델을 `reasoning.effort=\"minimal\"`과 함께 사용하면 기본 설정보다 더 빠르게 응답하는 경우가 많습니다. 다만 Responses API의 일부 내장 도구(예: 파일 검색과 이미지 생성)는 `\"minimal\"` reasoning effort를 지원하지 않기 때문에, 이 Agents SDK는 기본값을 `\"low\"`로 둡니다.\n+특히 더 낮은 지연 시간이 필요하다면, [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) 또는 [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) 모델을 `reasoning.effort=\"minimal\"` 과 함께 사용하는 것이 기본 설정보다 더 빠른 응답을 반환하는 경우가 많습니다. 다만 Responses API 의 일부 내장 도구(예: 파일 검색과 이미지 생성)는 `\"minimal\"` reasoning effort 를 지원하지 않으므로, 이 Agents SDK 는 기본값으로 `\"low\"` 를 사용합니다.\n \n #### 비 GPT-5 모델\n \n-사용자 지정 `model_settings` 없이 비 GPT-5 모델 이름을 전달하면, SDK는 모든 모델과 호환되는 일반적인 `ModelSettings`로 되돌립니다.\n+사용자 지정 `model_settings` 없이 비 GPT-5 모델 이름을 전달하는 경우, SDK 는 모든 모델과 호환되는 일반적인 `ModelSettings` 로 되돌립니다.\n \n ## 비 OpenAI 모델\n \n-[LiteLLM 통합](./litellm.md)을 통해 대부분의 비 OpenAI 모델을 사용할 수 있습니다. 먼저 litellm 의존성 그룹을 설치하세요:\n+[LiteLLM 통합](./litellm.md)을 통해 대부분의 비 OpenAI 모델을 사용할 수 있습니다. 먼저, litellm 의존성 그룹을 설치하세요:\n \n ```bash\n pip install \"openai-agents[litellm]\"\n ```\n \n-그런 다음 `litellm/` 접두사를 사용하여 [지원되는 모델](https://docs.litellm.ai/docs/providers) 중 아무거나 사용하세요:\n+그런 다음, `litellm/` 접두사를 사용하여 [지원되는 모델](https://docs.litellm.ai/docs/providers)을 사용하세요:\n \n ```python\n claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\n gemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)\n ```\n \n-### 비 OpenAI 모델을 사용하는 다른 방법\n+### 비 OpenAI 모델 사용의 기타 방법\n \n-다른 LLM 제공자를 통합하는 방법은 3가지가 더 있습니다(예시는 [여기](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)):\n+다른 LLM 제공자를 통합하는 방법은 3가지가 더 있습니다(예시는 [여기](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)에 있음):\n \n-1. [`set_default_openai_client`][agents.set_default_openai_client]는 LLM 클라이언트로 `AsyncOpenAI` 인스턴스를 전역적으로 사용하려는 경우 유용합니다. 이는 LLM 제공자가 OpenAI 호환 API 엔드포인트를 제공하고, `base_url`과 `api_key`를 설정할 수 있는 경우에 해당합니다. 구성 가능한 예시는 [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py)를 참고하세요.\n-2. [`ModelProvider`][agents.models.interface.ModelProvider]는 `Runner.run` 레벨에서 사용합니다. 이를 통해 \"이 실행의 모든 에이전트에 대해 사용자 지정 모델 제공자를 사용\"하도록 할 수 있습니다. 구성 가능한 예시는 [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py)를 참고하세요.\n-3. [`Agent.model`][agents.agent.Agent.model]을 사용하면 특정 Agent 인스턴스에 대해 모델을 지정할 수 있습니다. 이를 통해 에이전트마다 서로 다른 제공자를 혼용할 수 있습니다. 구성 가능한 예시는 [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py)를 참고하세요. 대부분의 사용 가능한 모델을 쉽게 사용하는 방법은 [LiteLLM 통합](./litellm.md)을 이용하는 것입니다.\n+1. [`set_default_openai_client`][agents.set_default_openai_client] 는 전역적으로 `AsyncOpenAI` 인스턴스를 LLM 클라이언트로 사용하려는 경우에 유용합니다. 이는 LLM 제공자가 OpenAI 호환 API 엔드포인트를 가지고 있고, `base_url` 과 `api_key` 를 설정할 수 있는 경우에 해당합니다. 구성 가능한 예시는 [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py) 를 참조하세요.\n+2. [`ModelProvider`][agents.models.interface.ModelProvider] 는 `Runner.run` 레벨에 있습니다. 이를 통해 \"이 실행에서 모든 에이전트에 대해 사용자 지정 모델 제공자를 사용\"하도록 지정할 수 있습니다. 구성 가능한 예시는 [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py) 를 참조하세요.\n+3. [`Agent.model`][agents.agent.Agent.model] 을 사용하면 특정 Agent 인스턴스에 모델을 지정할 수 있습니다. 이를 통해 에이전트별로 서로 다른 제공자를 혼합하여 사용할 수 있습니다. 구성 가능한 예시는 [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py) 를 참조하세요. 대부분의 사용 가능한 모델을 쉽게 사용하는 방법은 [LiteLLM 통합](./litellm.md) 입니다.\n \n-`platform.openai.com`의 API 키가 없는 경우, `set_tracing_disabled()`를 통해 트레이싱을 비활성화하거나, [다른 트레이싱 프로세서](../tracing.md)를 설정하는 것을 권장합니다.\n+`platform.openai.com` 의 API 키가 없는 경우, `set_tracing_disabled()` 를 통해 트레이싱을 비활성화하거나, [다른 트레이싱 프로세서](../tracing.md) 를 설정하는 것을 권장합니다.\n \n !!! note\n \n-    이 예시들에서는 대부분의 LLM 제공자가 아직 Responses API를 지원하지 않기 때문에 Chat Completions API/모델을 사용합니다. 사용 중인 LLM 제공자가 이를 지원한다면 Responses 사용을 권장합니다.\n+    이 예시들에서는 대부분의 LLM 제공자가 아직 Responses API 를 지원하지 않기 때문에 Chat Completions API/모델을 사용합니다. LLM 제공자가 이를 지원한다면 Responses 사용을 권장합니다.\n \n ## 모델 혼합 및 매칭\n \n-단일 워크플로 내에서 에이전트마다 서로 다른 모델을 사용하고 싶을 수 있습니다. 예를 들어, 분류(트리아지)에는 더 작고 빠른 모델을 사용하고, 복잡한 작업에는 더 크고 강력한 모델을 사용할 수 있습니다. [`Agent`][agents.Agent]를 구성할 때 다음 중 하나로 특정 모델을 선택할 수 있습니다:\n+단일 워크플로 내에서 에이전트마다 다른 모델을 사용하고 싶을 수 있습니다. 예를 들어, 분류(트리아지)에는 더 작고 빠른 모델을, 복잡한 작업에는 더 크고 강력한 모델을 사용할 수 있습니다. [`Agent`][agents.Agent] 를 구성할 때 다음 중 하나의 방법으로 특정 모델을 선택할 수 있습니다:\n \n 1. 모델 이름을 전달\n-2. 임의의 모델 이름 + 그 이름을 Model 인스턴스로 매핑할 수 있는 [`ModelProvider`][agents.models.interface.ModelProvider]를 전달\n-3. [`Model`][agents.models.interface.Model] 구현체를 직접 제공\n+2. 모델 이름 + 해당 이름을 Model 인스턴스로 매핑할 수 있는 [`ModelProvider`][agents.models.interface.ModelProvider] 전달\n+3. [`Model`][agents.models.interface.Model] 구현을 직접 제공\n \n !!!note\n \n-    SDK는 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]과 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] 두 형태를 모두 지원하지만, 두 형태는 지원하는 기능과 도구 세트가 다르므로 각 워크플로마다 단일 모델 형태를 사용할 것을 권장합니다. 워크플로에서 모델 형태를 혼용해야 한다면 사용 중인 모든 기능이 두 형태에서 모두 지원되는지 확인하세요.\n+    우리 SDK 는 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] 과 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] 두 형태를 모두 지원하지만, 두 형태가 지원하는 기능과 도구 세트가 다르므로 각 워크플로에는 단일 모델 형태를 사용할 것을 권장합니다. 워크플로가 모델 형태의 혼합/매칭을 요구한다면, 사용하는 모든 기능이 두 형태 모두에서 사용 가능한지 확인하세요.\n \n ```python\n from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n@@ -122,10 +122,10 @@ async def main():\n     print(result.final_output)\n ```\n \n-1.  OpenAI 모델 이름을 직접 설정합니다\n-2.  [`Model`][agents.models.interface.Model] 구현체를 제공합니다\n+1.  OpenAI 모델의 이름을 직접 설정합니다.\n+2.  [`Model`][agents.models.interface.Model] 구현을 제공합니다.\n \n-에이전트에 사용되는 모델을 더 세부적으로 구성하려면, temperature 같은 선택적 모델 구성 매개변수를 제공하는 [`ModelSettings`][agents.models.interface.ModelSettings]를 전달할 수 있습니다.\n+에이전트에 사용되는 모델을 추가로 구성하려면, temperature 와 같은 선택적 모델 구성 매개변수를 제공하는 [`ModelSettings`][agents.models.interface.ModelSettings] 를 전달할 수 있습니다.\n \n ```python\n from agents import Agent, ModelSettings\n@@ -138,7 +138,7 @@ english_agent = Agent(\n )\n ```\n \n-또한 OpenAI의 Responses API를 사용할 때 [몇 가지 다른 선택적 매개변수](https://platform.openai.com/docs/api-reference/responses/create)도 있습니다(예: `user`, `service_tier` 등). 최상위에 없으면 `extra_args`를 사용해 함께 전달할 수 있습니다.\n+또한 OpenAI 의 Responses API 를 사용할 때는 [몇 가지 다른 선택적 매개변수](https://platform.openai.com/docs/api-reference/responses/create)(예: `user`, `service_tier` 등)가 있습니다. 이러한 매개변수가 최상위 수준에 없으면 `extra_args` 를 사용해 함께 전달할 수 있습니다.\n \n ```python\n from agents import Agent, ModelSettings\n@@ -154,39 +154,39 @@ english_agent = Agent(\n )\n ```\n \n-## 다른 LLM 제공자 사용 시 흔한 문제\n+## 다른 LLM 제공자 사용 시 일반적인 문제\n \n ### 트레이싱 클라이언트 오류 401\n \n-트레이싱 관련 오류가 발생하는 경우, 트레이스가 OpenAI 서버로 업로드되는데 OpenAI API 키가 없기 때문입니다. 다음 세 가지 옵션 중 하나로 해결할 수 있습니다:\n+트레이싱 관련 오류가 발생한다면, 이는 트레이스가 OpenAI 서버로 업로드되는데 OpenAI API 키가 없기 때문입니다. 이를 해결하는 방법은 다음 세 가지입니다:\n \n-1. 트레이싱 완전 비활성화: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]\n-2. 트레이싱용 OpenAI 키 설정: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]. 이 API 키는 트레이스 업로드에만 사용되며, [platform.openai.com](https://platform.openai.com/)에서 발급된 것이어야 합니다\n-3. 비 OpenAI 트레이스 프로세서 사용. [트레이싱 문서](../tracing.md#custom-tracing-processors)를 참고하세요\n+1. 트레이싱을 완전히 비활성화: [`set_tracing_disabled(True)`][agents.set_tracing_disabled]\n+2. 트레이싱용 OpenAI 키 설정: [`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]. 이 API 키는 트레이스 업로드에만 사용되며, [platform.openai.com](https://platform.openai.com/) 의 키여야 합니다.\n+3. 비 OpenAI 트레이스 프로세서를 사용. [트레이싱 문서](../tracing.md#custom-tracing-processors) 를 참조하세요.\n \n ### Responses API 지원\n \n-SDK는 기본적으로 Responses API를 사용하지만, 대부분의 다른 LLM 제공자는 아직 이를 지원하지 않습니다. 그 결과 404 등 유사한 문제가 발생할 수 있습니다. 해결 방법은 두 가지입니다:\n+SDK 는 기본적으로 Responses API 를 사용하지만, 대부분의 다른 LLM 제공자는 아직 이를 지원하지 않습니다. 이로 인해 404 또는 유사한 문제가 발생할 수 있습니다. 해결 방법은 다음 두 가지입니다:\n \n-1. [`set_default_openai_api(\"chat_completions\")`][agents.set_default_openai_api]를 호출하세요. 이는 환경 변수로 `OPENAI_API_KEY`와 `OPENAI_BASE_URL`을 설정한 경우에 작동합니다\n-2. [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]을 사용하세요. 예시는 [여기](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)에 있습니다\n+1. [`set_default_openai_api(\"chat_completions\")`][agents.set_default_openai_api] 를 호출하세요. 환경 변수로 `OPENAI_API_KEY` 와 `OPENAI_BASE_URL` 를 설정하는 경우에 동작합니다.\n+2. [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] 을 사용하세요. 예시는 [여기](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/) 에 있습니다.\n \n ### Structured outputs 지원\n \n-일부 모델 제공자는 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)을 지원하지 않습니다. 이로 인해 다음과 유사한 오류가 발생할 수 있습니다:\n+일부 모델 제공자는 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) 를 지원하지 않습니다. 이로 인해 다음과 유사한 오류가 발생할 수 있습니다:\n \n ```\n \n BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n \n ```\n \n-이는 일부 모델 제공자의 한계로, JSON 출력을 지원하더라도 출력에 사용할 `json_schema`를 지정하도록 허용하지 않습니다. 이에 대한 해결책을 마련 중이지만, JSON 스키마 출력을 지원하는 제공자를 사용하는 것을 권장합니다. 그렇지 않으면 잘못된 JSON 때문에 앱이 자주 실패할 수 있습니다.\n+이는 일부 모델 제공자의 한계입니다. 이들은 JSON 출력을 지원하지만, 출력에 사용할 `json_schema` 를 지정할 수 없습니다. 이에 대한 해결책을 마련 중이지만, JSON schema 출력을 지원하는 제공자에 의존할 것을 권장합니다. 그렇지 않으면 잘못된 형식의 JSON 때문에 앱이 자주 깨질 수 있습니다.\n \n ## 제공자 간 모델 혼합\n \n-모델 제공자 간 기능 차이를 인지하지 못하면 오류가 발생할 수 있습니다. 예를 들어, OpenAI는 structured outputs, 멀티모달 입력, 호스티드 파일 검색 및 웹 검색을 지원하지만, 다른 많은 제공자는 이러한 기능을 지원하지 않습니다. 다음 제한 사항에 유의하세요:\n+모델 제공자 간의 기능 차이를 인지하지 못하면 오류가 발생할 수 있습니다. 예를 들어, OpenAI 는 structured outputs, 멀티모달 입력, 호스티드 파일 검색과 웹 검색을 지원하지만, 다른 많은 제공자는 이러한 기능을 지원하지 않습니다. 다음 제한 사항에 유의하세요:\n \n--   지원되지 않는 `tools`를 이해하지 못하는 제공자에게 보내지 않기\n+-   지원되지 않는 `tools` 를 이해하지 못하는 제공자에게 보내지 않기\n -   텍스트 전용 모델을 호출하기 전에 멀티모달 입력을 필터링하기\n--   structured JSON 출력을 지원하지 않는 제공자는 때때로 잘못된 JSON을 생성할 수 있음을 인지하기\n\\ No newline at end of file\n+-   structured JSON 출력을 지원하지 않는 제공자는 때때로 잘못된 JSON 을 생성할 수 있음을 인지하기\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmodels%2Findex.md",
        "sha": "9a2bafeb643f204e68a9a4b58a9467d0f1bb1720",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmodels%2Flitellm.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fmodels%2Flitellm.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/ko/models/litellm.md",
        "patch": "@@ -2,33 +2,33 @@\n search:\n   exclude: true\n ---\n-# LiteLLM를 통한 모든 모델 사용\n+# LiteLLM을 통한 모든 모델 사용\n \n !!! note\n \n-    LiteLLM 통합은 베타 단계입니다. 특히 소규모 모델 제공업체에서 일부 문제가 발생할 수 있습니다. 문제가 있으면 [Github issues](https://github.com/openai/openai-agents-python/issues)로 보고해 주시면 신속히 수정하겠습니다.\n+    LiteLLM 통합은 베타 버전입니다. 특히 소규모 모델 제공업체에서 문제가 발생할 수 있습니다. [GitHub 이슈](https://github.com/openai/openai-agents-python/issues)를 통해 문제를 보고해 주시면 신속히 해결하겠습니다.\n \n-[LiteLLM](https://docs.litellm.ai/docs/)은 단일 인터페이스로 100개 이상의 모델을 사용할 수 있게 해주는 라이브러리입니다. Agents SDK에서는 LiteLLM 통합을 통해 어떤 AI 모델이든 사용할 수 있습니다.\n+[LiteLLM](https://docs.litellm.ai/docs/)은 단일 인터페이스로 100개 이상의 모델을 사용할 수 있게 해주는 라이브러리입니다. Agents SDK에서 어떤 AI 모델이든 사용할 수 있도록 LiteLLM 통합을 추가했습니다.\n \n ## 설정\n \n-`litellm`이 사용 가능한지 확인해야 합니다. 선택적 `litellm` 종속성 그룹을 설치하면 됩니다:\n+`litellm`이 사용 가능해야 합니다. 선택적 `litellm` 종속성 그룹을 설치하여 설정할 수 있습니다:\n \n ```bash\n pip install \"openai-agents[litellm]\"\n ```\n \n-설치가 완료되면 어떤 에이전트에서도 [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel]을 사용할 수 있습니다.\n+설치가 완료되면, 어떤 에이전트에서든 [`LitellmModel`][agents.extensions.models.litellm_model.LitellmModel]을 사용할 수 있습니다.\n \n-## 예제\n+## 예시\n \n-다음은 완전한 동작 예제입니다. 실행하면 모델 이름과 API 키 입력을 요청합니다. 예를 들어 다음과 같이 입력할 수 있습니다:\n+다음은 완전히 동작하는 예시입니다. 실행하면 모델 이름과 API 키를 입력하라는 프롬프트가 표시됩니다. 예를 들어 다음과 같이 입력할 수 있습니다:\n \n-- `openai/gpt-4.1` 모델과 OpenAI API 키\n-- `anthropic/claude-3-5-sonnet-20240620` 모델과 Anthropic API 키\n+- `openai/gpt-4.1` 를 모델로, 그리고 OpenAI API 키\n+- `anthropic/claude-3-5-sonnet-20240620` 를 모델로, 그리고 Anthropic API 키\n - 등\n \n-LiteLLM에서 지원하는 전체 모델 목록은 [litellm providers docs](https://docs.litellm.ai/docs/providers)를 참조하세요.\n+LiteLLM이 지원하는 전체 모델 목록은 [litellm providers 문서](https://docs.litellm.ai/docs/providers)를 참고하세요.\n \n ```python\n from __future__ import annotations\n@@ -76,9 +76,9 @@ if __name__ == \"__main__\":\n     asyncio.run(main(model, api_key))\n ```\n \n-## 사용 데이터 추적\n+## 사용량 데이터 트래킹\n \n-LiteLLM 응답을 Agents SDK 사용량 메트릭에 반영하려면 에이전트를 생성할 때 `ModelSettings(include_usage=True)`를 전달하세요.\n+LiteLLM 응답이 Agents SDK 사용 메트릭에 반영되도록 하려면, 에이전트를 생성할 때 `ModelSettings(include_usage=True)` 를 전달하세요.\n \n ```python\n from agents import Agent, ModelSettings\n@@ -91,14 +91,14 @@ agent = Agent(\n )\n ```\n \n-`include_usage=True`를 사용하면, LiteLLM 요청은 기본 OpenAI 모델과 마찬가지로 `result.context_wrapper.usage`를 통해 토큰 및 요청 수를 보고합니다.\n+`include_usage=True` 를 사용하면, LiteLLM 요청은 기본 제공 OpenAI 모델과 동일하게 `result.context_wrapper.usage` 를 통해 토큰 및 요청 수를 보고합니다.\n \n ## 문제 해결\n \n-LiteLLM 응답에서 Pydantic 직렬화기 경고가 보인다면 다음 설정으로 작은 호환성 패치를 활성화하세요:\n+LiteLLM 응답에서 Pydantic 직렬화기 경고가 보인다면, 다음과 같이 설정하여 작은 호환성 패치를 활성화하세요:\n \n ```bash\n export OPENAI_AGENTS_ENABLE_LITELLM_SERIALIZER_PATCH=true\n ```\n \n-이 옵트인 플래그는 정상 동작을 유지하면서 알려진 LiteLLM 직렬화기 경고를 억제합니다. 필요 없다면 해제(설정 해제 또는 `false`)할 수 있습니다.\n\\ No newline at end of file\n+이 옵트인 플래그는 정상 동작을 유지하면서 알려진 LiteLLM 직렬화기 경고를 억제합니다. 필요하지 않다면 비활성화하세요(미설정 또는 `false`).\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmodels%2Flitellm.md",
        "sha": "175207975b0ce6a1aeec7d3e59afcc6eaea89218",
        "status": "modified"
      },
      {
        "additions": 20,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmulti_agent.md",
        "changes": 40,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fmulti_agent.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 20,
        "filename": "docs/ko/multi_agent.md",
        "patch": "@@ -4,38 +4,38 @@ search:\n ---\n # 멀티 에이전트 오케스트레이션\n \n-오케스트레이션은 앱에서 에이전트가 흐르는 방식을 의미합니다. 어떤 에이전트를 어떤 순서로 실행하며, 다음에 무엇을 할지 어떻게 결정할까요? 에이전트를 오케스트레이션하는 방법은 크게 두 가지입니다:\n+오케스트레이션은 앱 내 에이전트의 흐름을 의미합니다. 어떤 에이전트를 어떤 순서로 실행하고, 다음에 무엇을 할지 어떻게 결정할까요? 에이전트를 오케스트레이션하는 방법은 두 가지가 있습니다:\n \n-1. LLM 에게 결정을 맡기기: LLM 의 지능을 활용해 계획·추론하고 그에 따라 수행 단계를 결정\n+1. LLM 에게 의사결정을 맡기기: LLM 의 지능을 활용해 계획하고 추론하며 그에 따라 다음 단계를 결정\n 2. 코드로 오케스트레이션하기: 코드로 에이전트의 흐름을 결정\n \n-이 패턴들은 혼합해서 사용할 수 있습니다. 각 방식은 아래처럼 고유한 트레이드오프가 있습니다.\n+이 패턴들은 혼합해 사용할 수 있습니다. 각각의 트레이드오프는 아래에 설명되어 있습니다.\n \n ## LLM 기반 오케스트레이션\n \n-에이전트는 instructions, tools, 그리고 핸드오프로 무장한 LLM 입니다. 이는 개방형 과제가 주어졌을 때 LLM 이 스스로 도구를 사용해 작업을 수행하고 데이터를 획득하며, 핸드오프를 사용해 하위 에이전트에 작업을 위임하는 계획을 세울 수 있음을 의미합니다. 예를 들어, 리서치 에이전트는 다음과 같은 도구를 갖출 수 있습니다:\n+에이전트는 지시문, 도구, 핸드오프를 갖춘 LLM 입니다. 즉, 개방형 태스크가 주어지면 LLM 이 스스로 계획을 세워 도구로 행동하고 데이터를 수집하며, 핸드오프로 하위 에이전트에 태스크를 위임할 수 있습니다. 예를 들어, 리서치 에이전트는 다음과 같은 도구를 갖출 수 있습니다:\n \n-- 웹 검색으로 온라인에서 정보 찾기\n-- 파일 검색 및 검색으로 사내 데이터와 연결을 탐색하기\n-- 컴퓨터 사용으로 컴퓨터에서 작업 수행\n-- 코드 실행으로 데이터 분석 수행\n-- 계획 수립, 보고서 작성 등에 특화된 에이전트로의 핸드오프\n+- 웹 검색을 통한 온라인 정보 탐색\n+- 파일 검색 및 검색을 통한 사내 데이터와 연결 탐색\n+- 컴퓨터 사용을 통한 컴퓨터 상의 액션 수행\n+- 데이터 분석을 위한 코드 실행\n+- 기획, 보고서 작성 등 특정 작업에 특화된 에이전트로의 핸드오프\n \n-이 패턴은 과제가 개방형이고 LLM 의 지능에 의존하고자 할 때 적합합니다. 여기서 가장 중요한 전술은 다음과 같습니다:\n+이 패턴은 태스크가 개방형이고 LLM 의 지능에 의존하고자 할 때 적합합니다. 여기서 가장 중요한 전술은 다음과 같습니다:\n \n 1. 좋은 프롬프트에 투자하세요. 사용 가능한 도구, 사용 방법, 운영해야 할 매개변수를 명확히 하세요.\n-2. 앱을 모니터링하고 반복 개선하세요. 문제가 생기는 지점을 파악하고 프롬프트를 개선하세요.\n-3. 에이전트가 자기 성찰하고 개선할 수 있게 하세요. 예를 들어 루프에서 실행하여 스스로 비판하게 하거나, 오류 메시지를 제공해 스스로 개선하게 하세요.\n-4. 모든 것을 잘하는 범용 에이전트보다는 하나의 작업에 특화된 에이전트를 두세요.\n-5. [평가(evals)](https://platform.openai.com/docs/guides/evals)에 투자하세요. 이를 통해 에이전트를 훈련하여 작업 성능을 향상시킬 수 있습니다.\n+2. 앱을 모니터링하고 반복 개선하세요. 문제가 발생하는 지점을 파악하고 프롬프트를 개선하세요.\n+3. 에이전트가 성찰하고 개선하도록 하세요. 예를 들어 루프로 실행해 스스로 비판하게 하거나, 오류 메시지를 제공해 개선하도록 하세요.\n+4. 범용 에이전트 하나에 모든 것을 기대하기보다, 하나의 작업에 뛰어난 특화된 에이전트를 두세요.\n+5. [evals](https://platform.openai.com/docs/guides/evals)에 투자하세요. 이를 통해 에이전트를 훈련하여 작업 능력을 향상시킬 수 있습니다.\n \n ## 코드 기반 오케스트레이션\n \n-LLM 기반 오케스트레이션이 강력하긴 하지만, 코드 기반 오케스트레이션은 속도, 비용, 성능 측면에서 작업을 더 결정적이고 예측 가능하게 만듭니다. 대표적인 패턴은 다음과 같습니다:\n+LLM 기반 오케스트레이션이 강력하긴 하지만, 코드 기반 오케스트레이션은 속도, 비용, 성능 측면에서 더 결정적이고 예측 가능하게 만듭니다. 일반적인 패턴은 다음과 같습니다:\n \n-- [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)를 사용해 코드로 검사할 수 있는 적절한 형식의 데이터를 생성하기. 예를 들어, 에이전트에게 작업을 몇 가지 카테고리로 분류하게 한 뒤 그 카테고리에 따라 다음 에이전트를 선택할 수 있습니다.\n-- 한 에이전트의 출력을 다음 에이전트의 입력으로 변환하여 여러 에이전트를 체이닝하기. 블로그 글쓰기를 리서치, 개요 작성, 본문 작성, 비판, 개선의 일련의 단계로 분해할 수 있습니다.\n-- 평가와 피드백을 제공하는 에이전트와 작업을 수행하는 에이전트를 `while` 루프에서 함께 실행하여, 평가자가 출력이 특정 기준을 통과했다고 말할 때까지 반복하기\n-- 파이썬 기본 컴포넌트인 `asyncio.gather` 등을 통해 여러 에이전트를 병렬로 실행하기. 상호 의존하지 않는 여러 작업이 있을 때 속도에 유리합니다.\n+- [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)를 사용해 코드로 검사할 수 있는 적절한 형식의 데이터를 생성. 예를 들어, 에이전트에게 작업을 몇 개의 카테고리로 분류하게 한 다음, 해당 카테고리에 따라 다음 에이전트를 선택할 수 있습니다.\n+- 한 에이전트의 출력을 다음 에이전트의 입력으로 변환하여 여러 에이전트를 체이닝. 블로그 글쓰기를 리서치 → 아웃라인 작성 → 본문 작성 → 비판 → 개선의 일련의 단계로 분해할 수 있습니다.\n+- 작업을 수행하는 에이전트와 평가 및 피드백을 제공하는 에이전트를 `while` 루프로 함께 실행하고, 평가자가 출력이 특정 기준을 통과했다고 말할 때까지 반복\n+- 여러 에이전트를 병렬로 실행 (예: Python 기본 컴포넌트인 `asyncio.gather` 사용). 서로 의존하지 않는 여러 작업을 빠르게 처리할 때 유용합니다.\n \n-[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)에는 여러 code examples 이 있습니다.\n\\ No newline at end of file\n+[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)에 다양한 code examples 가 있습니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fmulti_agent.md",
        "sha": "6c096ce7f81bb2caed372f5f2e97d9f3b20a665e",
        "status": "modified"
      },
      {
        "additions": 12,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fquickstart.md",
        "changes": 24,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 12,
        "filename": "docs/ko/quickstart.md",
        "patch": "@@ -30,7 +30,7 @@ pip install openai-agents # or `uv add openai-agents`, etc\n \n ### OpenAI API 키 설정\n \n-아직 없다면 [이 안내](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)를 따라 OpenAI API 키를 생성하세요.\n+키가 없다면 OpenAI API 키를 생성하려면 [이 안내](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)를 따르세요.\n \n ```bash\n export OPENAI_API_KEY=sk-...\n@@ -51,7 +51,7 @@ agent = Agent(\n \n ## 에이전트 추가\n \n-추가 에이전트도 동일한 방식으로 정의할 수 있습니다. `handoff_descriptions`는 핸드오프 라우팅을 결정하는 데 추가 컨텍스트를 제공합니다\n+추가 에이전트도 같은 방식으로 정의할 수 있습니다. `handoff_descriptions`는 핸드오프 라우팅을 결정하는 데 필요한 추가 컨텍스트를 제공합니다\n \n ```python\n from agents import Agent\n@@ -71,7 +71,7 @@ math_tutor_agent = Agent(\n \n ## 핸드오프 정의\n \n-각 에이전트에서 작업을 진행하는 방법을 결정하기 위해 에이전트가 선택할 수 있는 아웃바운드 핸드오프 옵션 목록을 정의할 수 있습니다.\n+각 에이전트에서, 작업을 진행하는 방법을 결정할 때 선택할 수 있는 아웃바운드 핸드오프 옵션 인벤토리를 정의할 수 있습니다.\n \n ```python\n triage_agent = Agent(\n@@ -83,7 +83,7 @@ triage_agent = Agent(\n \n ## 에이전트 오케스트레이션 실행\n \n-워크플로가 실행되고 분류 에이전트가 두 전문 에이전트 사이를 올바르게 라우팅하는지 확인해 보세요.\n+워크플로가 실행되고 분류 에이전트가 두 전문 에이전트 사이를 올바르게 라우팅하는지 확인해 봅시다.\n \n ```python\n from agents import Runner\n@@ -95,7 +95,7 @@ async def main():\n \n ## 가드레일 추가\n \n-입력 또는 출력에 대해 실행할 사용자 지정 가드레일을 정의할 수 있습니다.\n+입력 또는 출력에 대해 실행되는 커스텀 가드레일을 정의할 수 있습니다.\n \n ```python\n from agents import GuardrailFunctionOutput, Agent, Runner\n@@ -121,9 +121,9 @@ async def homework_guardrail(ctx, agent, input_data):\n     )\n ```\n \n-## 통합 실행\n+## 전체 통합\n \n-모든 것을 합쳐 핸드오프와 입력 가드레일을 사용해 전체 워크플로를 실행해 봅시다.\n+핸드오프와 입력 가드레일을 사용해 전체 워크플로를 통합하여 실행해 봅시다.\n \n ```python\n from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\n@@ -192,12 +192,12 @@ if __name__ == \"__main__\":\n \n ## 트레이스 보기\n \n-에이전트 실행 중 발생한 내용을 검토하려면 [OpenAI 대시보드의 Trace viewer](https://platform.openai.com/traces)에서 에이전트 실행의 트레이스를 확인하세요.\n+에이전트 실행 중에 발생한 내용을 검토하려면 OpenAI 대시보드의 [Trace viewer](https://platform.openai.com/traces)로 이동해 에이전트 실행의 트레이스를 확인하세요.\n \n ## 다음 단계\n \n-더 복잡한 에이전트 기반 플로우를 만드는 방법을 학습하세요:\n+더 복잡한 에이전트 기반 플로우를 만드는 방법:\n \n-- 에이전트 구성에 대해 알아보기: [Agents](agents.md)\n-- 에이전트 실행에 대해 알아보기: [running agents](running_agents.md)\n-- 도구, 가드레일, 모델에 대해 알아보기: [tools](tools.md), [guardrails](guardrails.md), [models](models/index.md)\n\\ No newline at end of file\n+- [에이전트](agents.md) 구성 방법 알아보기\n+- [에이전트 실행](running_agents.md) 알아보기\n+- [도구](tools.md), [가드레일](guardrails.md), [모델](models/index.md) 알아보기\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fquickstart.md",
        "sha": "56b551847a7d9dac9666fe9d37f2248689ae3839",
        "status": "modified"
      },
      {
        "additions": 40,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frealtime%2Fguide.md",
        "changes": 80,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Frealtime%2Fguide.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 40,
        "filename": "docs/ko/realtime/guide.md",
        "patch": "@@ -4,59 +4,59 @@ search:\n ---\n # 가이드\n \n-이 가이드는 OpenAI Agents SDK의 실시간 기능을 사용해 음성 기반 AI 에이전트를 구축하는 방법을 자세히 설명합니다.\n+이 가이드는 OpenAI Agents SDK의 실시간 기능을 사용해 음성 지원 AI 에이전트를 구축하는 방법을 심도 있게 설명합니다.\n \n !!! warning \"베타 기능\"\n-실시간 에이전트는 베타 단계입니다. 구현을 개선하는 과정에서 호환성에 영향을 주는 변경이 있을 수 있습니다.\n+실시간 에이전트는 베타 단계입니다. 구현을 개선하는 동안 호환성에 영향을 주는 변경이 발생할 수 있습니다.\n \n ## 개요\n \n-실시간 에이전트는 오디오와 텍스트 입력을 실시간으로 처리하고 실시간 오디오로 응답하는 대화형 흐름을 제공합니다. OpenAI의 Realtime API와 지속 연결을 유지하여 낮은 지연으로 자연스러운 음성 대화를 가능하게 하며, 인터럽션(중단 처리)도 우아하게 처리합니다.\n+실시간 에이전트는 오디오와 텍스트 입력을 실시간으로 처리하고 실시간 오디오로 응답하는 대화형 흐름을 제공합니다. OpenAI의 Realtime API와 지속적인 연결을 유지하여 낮은 지연으로 자연스러운 음성 대화를 가능하게 하고, 인터럽션(중단 처리)을 우아하게 처리합니다.\n \n ## 아키텍처\n \n ### 핵심 구성 요소\n \n-실시간 시스템은 다음의 핵심 구성 요소로 이루어져 있습니다:\n+실시간 시스템은 다음과 같은 주요 구성 요소로 이루어져 있습니다:\n \n--   **RealtimeAgent**: instructions, tools, 핸드오프로 구성된 에이전트\n--   **RealtimeRunner**: 구성 관리. `runner.run()`을 호출해 세션을 가져올 수 있습니다\n--   **RealtimeSession**: 단일 상호작용 세션. 일반적으로 사용자가 대화를 시작할 때마다 하나를 생성하고, 대화가 끝날 때까지 유지합니다\n+-   **RealtimeAgent**: instructions, tools, handoffs로 구성된 에이전트\n+-   **RealtimeRunner**: 구성을 관리합니다. `runner.run()`을 호출해 세션을 얻을 수 있습니다.\n+-   **RealtimeSession**: 단일 상호작용 세션입니다. 일반적으로 사용자가 대화를 시작할 때마다 하나를 생성하고 대화가 끝날 때까지 유지합니다.\n -   **RealtimeModel**: 기본 모델 인터페이스(일반적으로 OpenAI의 WebSocket 구현)\n \n ### 세션 흐름\n \n 일반적인 실시간 세션은 다음 흐름을 따릅니다:\n \n-1. instructions, tools, 핸드오프로 **RealtimeAgent(들)를 생성**합니다\n-2. 에이전트와 구성 옵션으로 **RealtimeRunner를 설정**합니다\n-3. `await runner.run()`을 사용해 **세션을 시작**하고 RealtimeSession을 받습니다\n-4. `send_audio()` 또는 `send_message()`를 사용해 **오디오 또는 텍스트 메시지 전송**합니다\n-5. 세션을 순회(iterate)하며 **이벤트 수신** — 오디오 출력, 전사, tool 호출, 핸드오프, 에러 등이 포함됩니다\n-6. 사용자가 에이전트의 말 위로 말할 때 **인터럽션 처리** — 현재 오디오 생성을 자동으로 중단합니다\n+1. **RealtimeAgent 생성**: instructions, tools, handoffs를 포함해 구성합니다\n+2. **RealtimeRunner 설정**: 에이전트와 구성 옵션으로 설정합니다\n+3. `await runner.run()`을 사용해 **세션 시작**: RealtimeSession을 반환합니다\n+4. `send_audio()` 또는 `send_message()`를 사용해 **오디오 또는 텍스트 메시지 전송**\n+5. 세션을 순회(iterate)하며 **이벤트 수신**: 오디오 출력, 전사, 도구 호출, 핸드오프, 오류 등\n+6. 사용자가 에이전트 말 중에 말하면 **인터럽션 처리**: 현재 오디오 생성이 자동으로 중지됩니다\n \n-세션은 대화 기록을 유지하고 실시간 모델과의 지속 연결을 관리합니다.\n+세션은 대화 내역을 유지하고 실시간 모델과의 지속 연결을 관리합니다.\n \n ## 에이전트 구성\n \n-RealtimeAgent는 일반 Agent 클래스와 유사하게 동작하지만 몇 가지 주요 차이가 있습니다. 전체 API는 [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API 레퍼런스를 참고하세요.\n+RealtimeAgent는 일반 Agent 클래스와 유사하게 동작하지만 몇 가지 중요한 차이가 있습니다. 전체 API 세부 정보는 [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API 레퍼런스를 참고하세요.\n \n 일반 에이전트와의 주요 차이점:\n \n--   모델 선택은 에이전트 수준이 아닌 세션 수준에서 구성합니다\n--   structured outputs 미지원(`outputType` 미지원)\n--   음성은 에이전트별로 구성할 수 있으나 첫 번째 에이전트가 말한 후에는 변경할 수 없습니다\n--   그 외 tools, 핸드오프, instructions 등은 동일하게 동작합니다\n+-   모델 선택은 에이전트 수준이 아니라 세션 수준에서 구성합니다\n+-   structured outputs는 지원되지 않습니다(`outputType`은 지원하지 않음)\n+-   음성은 에이전트별로 설정할 수 있으나 첫 번째 에이전트가 말한 이후에는 변경할 수 없음\n+-   그 외 tools, handoffs, instructions 등은 동일하게 동작\n \n ## 세션 구성\n \n ### 모델 설정\n \n-세션 구성으로 기본 실시간 모델 동작을 제어할 수 있습니다. 모델 이름(예: `gpt-realtime`), 음성 선택(alloy, echo, fable, onyx, nova, shimmer), 지원 모달리티(텍스트 및/또는 오디오)를 설정할 수 있습니다. 오디오 형식은 입력과 출력 모두에 대해 설정 가능하며, 기본값은 PCM16입니다.\n+세션 구성으로 기본 실시간 모델 동작을 제어할 수 있습니다. 모델 이름(예: `gpt-realtime`), 음성 선택(alloy, echo, fable, onyx, nova, shimmer), 지원 모달리티(텍스트 및/또는 오디오)를 설정할 수 있습니다. 오디오 형식은 입력과 출력 모두에 대해 설정할 수 있으며, 기본값은 PCM16입니다.\n \n ### 오디오 구성\n \n-오디오 설정은 세션이 음성 입력과 출력을 처리하는 방식을 제어합니다. Whisper와 같은 모델을 사용해 입력 오디오 전사를 구성하고, 언어 선호도를 설정하며, 도메인 특화 용어의 정확성을 높이기 위한 전사 프롬프트를 제공할 수 있습니다. 턴 감지 설정을 통해 음성 활동 감지 임계값, 무음 지속 시간, 감지된 음성 주변 패딩 등의 옵션으로 에이전트가 언제 응답을 시작하고 멈출지를 제어합니다.\n+오디오 설정은 세션이 음성 입력과 출력을 처리하는 방식을 제어합니다. Whisper와 같은 모델을 사용해 입력 오디오 전사를 구성하고, 언어 기본값을 설정하며, 도메인 특화 용어의 정확도를 높이기 위한 전사 프롬프트를 제공할 수 있습니다. 턴 감지 설정으로 에이전트가 언제 응답을 시작하고 멈춰야 하는지 제어하며, 음성 활동 감지 임계값, 무음 지속 시간, 감지된 음성 주변 패딩 옵션을 제공합니다.\n \n ## 도구와 함수\n \n@@ -90,7 +90,7 @@ agent = RealtimeAgent(\n \n ### 핸드오프 생성\n \n-핸드오프를 통해 전문화된 에이전트 간 대화를 전환할 수 있습니다.\n+핸드오프를 통해 특화된 에이전트 간에 대화를 전환할 수 있습니다.\n \n ```python\n from agents.realtime import realtime_handoff\n@@ -119,20 +119,20 @@ main_agent = RealtimeAgent(\n \n ## 이벤트 처리\n \n-세션은 세션 객체를 순회함으로써 수신할 수 있는 이벤트를 스트리밍합니다. 이벤트에는 오디오 출력 청크, 전사 결과, 도구 실행 시작/종료, 에이전트 핸드오프, 에러가 포함됩니다. 처리해야 할 핵심 이벤트는 다음과 같습니다:\n+세션은 세션 객체를 순회하며 청취할 수 있는 이벤트를 스트리밍합니다. 이벤트에는 오디오 출력 청크, 전사 결과, 도구 실행 시작 및 종료, 에이전트 핸드오프, 오류 등이 포함됩니다. 처리해야 할 주요 이벤트는 다음과 같습니다:\n \n--   **audio**: 에이전트 응답의 원시 오디오 데이터\n--   **audio_end**: 에이전트가 말하기를 완료함\n--   **audio_interrupted**: 사용자가 에이전트를 중단(interrupt)함\n+-   **audio**: 에이전트 응답의 원문 오디오 데이터\n+-   **audio_end**: 에이전트 음성 출력 종료\n+-   **audio_interrupted**: 사용자가 에이전트를 중단함\n -   **tool_start/tool_end**: 도구 실행 라이프사이클\n -   **handoff**: 에이전트 핸드오프 발생\n--   **error**: 처리 중 에러 발생\n+-   **error**: 처리 중 오류 발생\n \n-전체 이벤트 상세는 [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent]를 참고하세요.\n+전체 이벤트 세부 사항은 [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent]를 참고하세요.\n \n ## 가드레일\n \n-실시간 에이전트는 출력 가드레일만 지원합니다. 성능 문제를 방지하기 위해 가드레일은 디바운스되어 주기적으로 실행되며(모든 단어마다 실행되지 않음), 기본 디바운스 길이는 100자이며 구성 가능합니다.\n+실시간 에이전트는 출력 가드레일만 지원합니다. 성능 문제를 피하기 위해 단어마다 실행하지 않고 디바운스되어 주기적으로 실행됩니다. 기본 디바운스 길이는 100자이며, 구성 가능합니다.\n \n 가드레일은 `RealtimeAgent`에 직접 연결하거나 세션의 `run_config`를 통해 제공할 수 있습니다. 두 소스의 가드레일은 함께 실행됩니다.\n \n@@ -152,19 +152,19 @@ agent = RealtimeAgent(\n )\n ```\n \n-가드레일이 트리거되면 `guardrail_tripped` 이벤트를 생성하고 에이전트의 현재 응답을 인터럽트할 수 있습니다. 디바운스 동작은 안전성과 실시간 성능 요구 간 균형을 맞추는 데 도움이 됩니다. 텍스트 에이전트와 달리, 실시간 에이전트는 가드레일이 트리거되어도 Exception을 발생시키지 않습니다.\n+가드레일이 트리거되면 `guardrail_tripped` 이벤트를 생성하고 에이전트의 현재 응답을 인터럽트할 수 있습니다. 디바운스 동작은 안전성과 실시간 성능 요구 사항 간의 균형을 맞추는 데 도움이 됩니다. 텍스트 에이전트와 달리, 실시간 에이전트는 가드레일이 작동해도 예외를 발생시키지 **않습니다**.\n \n ## 오디오 처리\n \n-[`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio]로 세션에 오디오를 전송하거나, [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message]로 텍스트를 전송하세요.\n+[`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio]를 사용해 오디오를 세션으로 전송하거나, [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message]를 사용해 텍스트를 전송하세요.\n \n-오디오 출력의 경우 `audio` 이벤트를 수신하고 선호하는 오디오 라이브러리로 데이터를 재생하세요. 사용자가 에이전트를 중단할 때 재생을 즉시 중지하고 대기 중인 오디오를 모두 비우기 위해 `audio_interrupted` 이벤트를 반드시 수신해야 합니다.\n+오디오 출력의 경우 `audio` 이벤트를 청취해 선호하는 오디오 라이브러리로 재생하세요. 사용자가 에이전트를 중단할 때 즉시 재생을 중지하고 대기 중인 오디오를 모두 비우기 위해 `audio_interrupted` 이벤트를 반드시 청취하세요.\n \n ## SIP 통합\n \n-[Realtime Calls API](https://platform.openai.com/docs/guides/realtime-sip)를 통해 도착하는 전화 통화에 실시간 에이전트를 연결할 수 있습니다. SDK는 미디어를 SIP로 협상하면서 동일한 에이전트 흐름을 재사용하는 [`OpenAIRealtimeSIPModel`][agents.realtime.openai_realtime.OpenAIRealtimeSIPModel]을 제공합니다.\n+[Realtime Calls API](https://platform.openai.com/docs/guides/realtime-sip)로 들어오는 전화 통화에 실시간 에이전트를 연결할 수 있습니다. SDK는 SIP를 통해 미디어를 협상하면서 동일한 에이전트 플로우를 재사용하는 [`OpenAIRealtimeSIPModel`][agents.realtime.openai_realtime.OpenAIRealtimeSIPModel]을 제공합니다.\n \n-사용하려면 모델 인스턴스를 러너에 전달하고 세션 시작 시 SIP `call_id`를 제공하세요. 콜 ID는 수신 전화를 알리는 웹훅으로 전달됩니다.\n+사용하려면 모델 인스턴스를 러너에 전달하고, 세션 시작 시 SIP `call_id`를 제공하세요. 통화 ID는 수신 전화를 알리는 웹훅을 통해 전달됩니다.\n \n ```python\n from agents.realtime import RealtimeAgent, RealtimeRunner\n@@ -187,19 +187,19 @@ async with await runner.run(\n         ...\n ```\n \n-발신자가 전화를 끊으면 SIP 세션이 종료되고 실시간 연결이 자동으로 닫힙니다. 완전한 전화 예시는 [`examples/realtime/twilio_sip`](https://github.com/openai/openai-agents-python/tree/main/examples/realtime/twilio_sip)를 참고하세요.\n+발신자가 전화를 끊으면 SIP 세션이 종료되고 실시간 연결이 자동으로 닫힙니다. 전체 전화(텔레포니) code examples는 [`examples/realtime/twilio_sip`](https://github.com/openai/openai-agents-python/tree/main/examples/realtime/twilio_sip)을 참고하세요.\n \n-## 모델 직접 액세스\n+## 직접 모델 접근\n \n-기저 모델에 접근해 커스텀 리스너를 추가하거나 고급 작업을 수행할 수 있습니다:\n+사용자 지정 리스너를 추가하거나 고급 작업을 수행하기 위해 기본 모델에 접근할 수 있습니다:\n \n ```python\n # Add a custom listener to the model\n session.model.add_listener(my_custom_listener)\n ```\n \n-이 방법을 통해 연결에 대한 저수준 제어가 필요한 고급 사용 사례를 위해 [`RealtimeModel`][agents.realtime.model.RealtimeModel] 인터페이스에 직접 접근할 수 있습니다.\n+이 방법을 통해 연결에 대한 하위 수준 제어가 필요한 고급 사용 사례를 위해 [`RealtimeModel`][agents.realtime.model.RealtimeModel] 인터페이스에 직접 접근할 수 있습니다.\n \n-## 예시\n+## 예제\n \n-완전한 동작 code examples는 UI 구성 요소가 있는 버전과 없는 버전을 포함하여 [examples/realtime 디렉터리](https://github.com/openai/openai-agents-python/tree/main/examples/realtime)를 확인하세요.\n\\ No newline at end of file\n+완전한 동작 code examples는 [examples/realtime 디렉터리](https://github.com/openai/openai-agents-python/tree/main/examples/realtime)를 참고하세요. UI 구성 요소가 있는 데모와 없는 데모가 모두 포함되어 있습니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frealtime%2Fguide.md",
        "sha": "553a3bc2fdd9fc79e0c1db0d49e3298c15ecfbb4",
        "status": "modified"
      },
      {
        "additions": 12,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frealtime%2Fquickstart.md",
        "changes": 24,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Frealtime%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 12,
        "filename": "docs/ko/realtime/quickstart.md",
        "patch": "@@ -4,16 +4,16 @@ search:\n ---\n # 빠른 시작\n \n-실시간 에이전트는 OpenAI의 Realtime API를 사용해 AI 에이전트와 음성 대화를 가능하게 합니다. 이 가이드는 첫 실시간 음성 에이전트를 만드는 과정을 안내합니다.\n+실시간 에이전트는 OpenAI의 Realtime API를 사용해 AI 에이전트와 음성 대화를 가능하게 합니다. 이 가이드는 첫 실시간 음성 에이전트를 만드는 방법을 안내합니다.\n \n !!! warning \"베타 기능\"\n-실시간 에이전트는 베타 단계입니다. 구현을 개선하는 동안 일부 변경이 발생할 수 있습니다.\n+실시간 에이전트는 베타 단계입니다. 구현을 개선하는 동안 일부 호환성 깨짐이 발생할 수 있습니다.\n \n-## 사전 준비 사항\n+## 준비 사항\n \n - Python 3.9 이상\n - OpenAI API 키\n-- OpenAI Agents SDK 에 대한 기본적인 이해\n+- OpenAI Agents SDK에 대한 기본 이해\n \n ## 설치\n \n@@ -23,9 +23,9 @@ search:\n pip install openai-agents\n ```\n \n-## 첫 실시간 에이전트 생성\n+## 첫 실시간 에이전트 만들기\n \n-### 1. 필수 구성요소 가져오기\n+### 1. 필요한 구성 요소 가져오기\n \n ```python\n import asyncio\n@@ -111,7 +111,7 @@ def _truncate_str(s: str, max_length: int) -> str:\n \n ## 전체 예제\n \n-완전한 작동 예제는 다음과 같습니다:\n+다음은 완전한 작동 예제입니다:\n \n ```python\n import asyncio\n@@ -206,16 +206,16 @@ if __name__ == \"__main__\":\n \n - `type`: 감지 방식 (`server_vad`, `semantic_vad`)\n - `threshold`: 음성 활동 임계값 (0.0-1.0)\n-- `silence_duration_ms`: 턴 종료 감지를 위한 무음 지속 시간\n+- `silence_duration_ms`: 턴 종료를 감지할 무음 시간\n - `prefix_padding_ms`: 발화 전 오디오 패딩\n \n ## 다음 단계\n \n - [실시간 에이전트에 대해 더 알아보기](guide.md)\n-- [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) 폴더의 동작 예제를 확인하세요\n-- 에이전트에 도구 추가\n-- 에이전트 간 핸드오프 구현\n-- 안전을 위한 가드레일 설정\n+- [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) 폴더에서 동작하는 예제를 확인하세요\n+- 에이전트에 도구를 추가하세요\n+- 에이전트 간 핸드오프를 구현하세요\n+- 안전을 위한 가드레일을 설정하세요\n \n ## 인증\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frealtime%2Fquickstart.md",
        "sha": "76e2c209354957629b394ffe82f6c0d454c65a1b",
        "status": "modified"
      },
      {
        "additions": 13,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frelease.md",
        "changes": 26,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Frelease.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 13,
        "filename": "docs/ko/release.md",
        "patch": "@@ -4,48 +4,48 @@ search:\n ---\n # 릴리스 프로세스/변경 로그\n \n-이 프로젝트는 `0.Y.Z` 형식의 의미 버전 관리(semantic versioning)를 약간 수정한 버전을 따릅니다. 선행하는 `0`은 SDK가 여전히 빠르게 발전 중임을 나타냅니다. 각 구성 요소는 다음과 같이 증가합니다:\n+이 프로젝트는 `0.Y.Z` 형태의 약간 수정된 시맨틱 버전 규칙을 따릅니다. 선행하는 `0`은 SDK가 아직 빠르게 발전 중임을 의미합니다. 각 구성 요소는 다음과 같이 증가합니다:\n \n ## 마이너(`Y`) 버전\n \n-베타로 표시되지 않은 공개 인터페이스에 **호환성 파괴 변경사항**이 있을 때 마이너 버전 `Y`를 올립니다. 예를 들어 `0.0.x`에서 `0.1.x`로 올라갈 때 브레이킹 체인지가 포함될 수 있습니다.\n+베타로 표시되지 않은 모든 퍼블릭 인터페이스에 **호환성 파괴 변경**이 있을 때 마이너 버전 `Y`를 올립니다. 예를 들어, `0.0.x`에서 `0.1.x`로 올라갈 때 호환성 파괴 변경이 포함될 수 있습니다.\n \n-브레이킹 체인지를 원하지 않는 경우, 프로젝트에서 `0.0.x` 버전에 고정할 것을 권장합니다.\n+호환성 파괴 변경을 원치 않으시면, 프로젝트에서 `0.0.x` 버전에 고정하는 것을 권장합니다.\n \n ## 패치(`Z`) 버전\n \n-호환성을 깨지 않는 변경에 대해 `Z`를 증가시킵니다:\n+다음과 같은 하위 호환 변경에 대해 `Z`를 올립니다:\n \n - 버그 수정\n-- 새로운 기능\n-- 비공개 인터페이스 변경\n+- 신규 기능\n+- 프라이빗 인터페이스 변경\n - 베타 기능 업데이트\n \n ## 호환성 파괴 변경 로그\n \n ### 0.6.0\n \n-이 버전에서는 기본 핸드오프 기록이 원문 사용자/assistant 턴을 노출하는 대신 단일 assistant 메시지로 패키징되어, 다운스트림 에이전트가 간결하고 예측 가능한 요약을 받을 수 있습니다\n-- 기존의 단일 메시지 핸드오프 대화록은 이제 기본적으로 `<CONVERSATION HISTORY>` 블록 앞에 \"For context, here is the conversation so far between the user and the previous agent:\"로 시작하여, 다운스트림 에이전트가 명확한 레이블의 요약을 받습니다\n+이 버전에서는 기본 핸드오프 기록이 원문 사용자/assistant 턴을 노출하는 대신 단일 assistant 메시지로 패키징되어, 다운스트림 에이전트가 간결하고 예측 가능한 요약을 받습니다\n+- 기존의 단일 메시지 핸드오프 대화록은 이제 기본적으로 `<CONVERSATION HISTORY>` 블록 앞에 \"문맥을 위해, 다음은 지금까지 사용자와 이전 에이전트 간의 대화입니다:\"로 시작해, 다운스트림 에이전트가 명확히 라벨링된 요약을 받습니다\n \n ### 0.5.0\n \n-이 버전은 눈에 띄는 브레이킹 체인지는 도입하지 않지만, 새로운 기능과 내부적으로 몇 가지 중요한 업데이트가 포함되어 있습니다:\n+이 버전은 눈에 띄는 호환성 파괴 변경을 도입하지 않지만, 새로운 기능과 내부적으로 몇 가지 중요한 업데이트가 포함됩니다:\n \n - `RealtimeRunner`가 [SIP 프로토콜 연결](https://platform.openai.com/docs/guides/realtime-sip)을 처리하도록 지원 추가\n-- Python 3.14 호환성을 위해 `Runner#run_sync`의 내부 로직을 대폭 개정\n+- Python 3.14 호환성을 위해 `Runner#run_sync`의 내부 로직을 대폭 수정\n \n ### 0.4.0\n \n-이 버전에서는 [openai](https://pypi.org/project/openai/) 패키지 v1.x 버전을 더 이상 지원하지 않습니다. 이 SDK와 함께 openai v2.x를 사용해 주세요.\n+이 버전에서는 [openai](https://pypi.org/project/openai/) 패키지 v1.x 버전을 더 이상 지원하지 않습니다. 이 SDK와 함께 openai v2.x를 사용하세요.\n \n ### 0.3.0\n \n-이 버전에서는 Realtime API 지원이 gpt-realtime 모델 및 해당 API 인터페이스(GA 버전)로 마이그레이션됩니다.\n+이 버전에서는 Realtime API 지원이 gpt-realtime 모델과 해당 API 인터페이스(GA 버전)로 마이그레이션됩니다.\n \n ### 0.2.0\n \n-이 버전에서는 기존에 `Agent`를 인자로 받던 일부 위치에서 이제 `AgentBase`를 인자로 받습니다. 예: MCP 서버의 `list_tools()` 호출. 이는 순수하게 타입 변경이며, 실제로는 계속 `Agent` 객체를 받게 됩니다. 업데이트하려면 `Agent`를 `AgentBase`로 바꿔 타입 오류만 수정하면 됩니다.\n+이 버전에서는 이전에 `Agent`를 인자로 받던 몇몇 위치가 이제 `AgentBase`를 인자로 받습니다. 예: MCP 서버의 `list_tools()` 호출. 이는 타입 관련 변경일 뿐이며, 여전히 `Agent` 객체를 받게 됩니다. 업데이트하려면 타입 오류가 발생하는 곳에서 `Agent`를 `AgentBase`로 교체하세요.\n \n ### 0.1.0\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frelease.md",
        "sha": "c55f98bf2c38854251ba6beeaa0ff95544b5b806",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frepl.md",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Frepl.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 4,
        "filename": "docs/ko/repl.md",
        "patch": "@@ -4,8 +4,7 @@ search:\n ---\n # REPL 유틸리티\n \n-SDK는 터미널에서 에이전트의 동작을 빠르고 상호작용적으로 테스트할 수 있도록 `run_demo_loop`를 제공합니다.\n-\n+SDK는 터미널에서 에이전트의 동작을 빠르고 인터랙티브하게 테스트할 수 있도록 `run_demo_loop`를 제공합니다.\n \n ```python\n import asyncio\n@@ -19,6 +18,6 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-`run_demo_loop`는 루프에서 사용자 입력을 요청하며, 턴 사이의 대화 내역을 유지합니다. 기본적으로 생성되는 즉시 모델 출력이 스트리밍됩니다. 위 예제를 실행하면, `run_demo_loop`가 대화형 채팅 세션을 시작합니다. 이 세션은 계속해서 입력을 요청하고, 턴 사이의 전체 대화 내역을 기억하여(에이전트가 무슨 대화가 오갔는지 알 수 있도록) 에이전트의 응답을 생성되는 대로 실시간으로 자동 스트리밍합니다.\n+`run_demo_loop`는 루프에서 사용자 입력을 요청하며, 턴 사이의 대화 기록을 유지합니다. 기본적으로 생성되는 대로 모델 출력을 스트리밍합니다. 위의 예제를 실행하면, run_demo_loop가 인터랙티브 채팅 세션을 시작합니다. 계속해서 입력을 요청하고, 턴 사이의 전체 대화 기록을 기억해 에이전트가 대화 내용을 파악하도록 하며, 생성되는 즉시 에이전트의 응답을 실시간으로 자동 스트리밍합니다.\n \n-채팅 세션을 종료하려면 `quit` 또는 `exit`를 입력하고 Enter 키를 누르거나 `Ctrl-D` 키보드 단축키를 사용하면 됩니다.\n\\ No newline at end of file\n+이 채팅 세션을 종료하려면 `quit` 또는 `exit`를 입력하고 Enter를 누르거나 `Ctrl-D` 키보드 단축키를 사용하세요.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frepl.md",
        "sha": "b114d2151b5c9f6f7458c869903e8e55bc0f098c",
        "status": "modified"
      },
      {
        "additions": 22,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fresults.md",
        "changes": 44,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fresults.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 22,
        "filename": "docs/ko/results.md",
        "patch": "@@ -6,53 +6,53 @@ search:\n \n `Runner.run` 메서드를 호출하면 다음 중 하나를 받습니다:\n \n-- [`RunResult`][agents.result.RunResult] (`run` 또는 `run_sync` 호출 시)\n-- [`RunResultStreaming`][agents.result.RunResultStreaming] (`run_streamed` 호출 시)\n+-   [`RunResult`][agents.result.RunResult] (`run` 또는 `run_sync` 호출 시)\n+-   [`RunResultStreaming`][agents.result.RunResultStreaming] (`run_streamed` 호출 시)\n \n-둘 다 [`RunResultBase`][agents.result.RunResultBase]를 상속하며, 대부분의 유용한 정보는 여기에 있습니다.\n+두 결과 모두 [`RunResultBase`][agents.result.RunResultBase]를 상속하며, 대부분의 유용한 정보가 여기에 포함됩니다.\n \n ## 최종 출력\n \n-[`final_output`][agents.result.RunResultBase.final_output] 속성에는 마지막으로 실행된 에이전트의 최종 출력이 담깁니다. 이는 다음 중 하나입니다:\n+[`final_output`][agents.result.RunResultBase.final_output] 프로퍼티에는 마지막으로 실행된 에이전트의 최종 출력이 포함됩니다. 다음 중 하나입니다:\n \n-- 마지막 에이전트에 `output_type`이 정의되지 않은 경우 `str`\n-- 에이전트에 출력 타입이 정의된 경우 `last_agent.output_type` 타입의 객체\n+-   마지막 에이전트에 `output_type`이 정의되지 않았다면 `str`\n+-   에이전트에 출력 타입이 정의되어 있다면 `last_agent.output_type` 타입의 객체\n \n !!! note\n \n-    `final_output`의 타입은 `Any`입니다. 핸드오프로 인해 정적으로 타입을 고정할 수 없습니다. 핸드오프가 발생하면 어떤 에이전트도 마지막 에이전트가 될 수 있으므로, 가능한 출력 타입의 집합을 정적으로 알 수 없습니다.\n+    `final_output`의 타입은 `Any`입니다. 핸드오프 때문에 정적으로 타입을 지정할 수 없습니다. 핸드오프가 발생하면 어떤 에이전트든 마지막 에이전트가 될 수 있으므로, 가능한 출력 타입 집합을 정적으로 알 수 없습니다.\n \n-## 다음 턴을 위한 입력\n+## 다음 턴 입력\n \n-[`result.to_input_list()`][agents.result.RunResultBase.to_input_list]를 사용해 결과를 입력 리스트로 변환할 수 있습니다. 이 입력 리스트는 제공한 원래 입력에 에이전트 실행 중 생성된 항목을 이어붙입니다. 이를 통해 한 에이전트 실행의 출력을 다른 실행에 넘기거나, 루프에서 실행하며 매번 새로운 사용자 입력을 추가하기가 편리해집니다.\n+[`result.to_input_list()`][agents.result.RunResultBase.to_input_list]를 사용하면 결과를 입력 리스트로 변환하여, 제공한 원래 입력과 에이전트 실행 중 생성된 항목들을 연결할 수 있습니다. 이를 통해 한 번의 에이전트 실행 결과를 다른 실행으로 전달하거나, 루프에서 실행하며 매번 새로운 사용자 입력을 덧붙이는 작업이 편리해집니다.\n \n ## 마지막 에이전트\n \n-[`last_agent`][agents.result.RunResultBase.last_agent] 속성에는 마지막으로 실행된 에이전트가 담깁니다. 애플리케이션에 따라, 이는 사용자가 다음에 무언가를 입력할 때 유용한 경우가 많습니다. 예를 들어, 프런트라인 트리아지 에이전트가 언어별 에이전트로 핸드오프하는 경우, 마지막 에이전트를 저장해 두었다가 사용자가 다음에 메시지를 보낼 때 재사용할 수 있습니다.\n+[`last_agent`][agents.result.RunResultBase.last_agent] 프로퍼티에는 마지막으로 실행된 에이전트가 포함됩니다. 애플리케이션에 따라, 다음에 사용자가 무언가를 입력할 때 유용한 경우가 많습니다. 예를 들어, 1차 분류 에이전트가 언어별 에이전트로 핸드오프하는 경우, 마지막 에이전트를 저장해 두었다가 다음에 사용자가 메시지를 보낼 때 재사용할 수 있습니다.\n \n ## 새 항목\n \n-[`new_items`][agents.result.RunResultBase.new_items] 속성에는 실행 중 생성된 새 항목이 담깁니다. 항목은 [`RunItem`][agents.items.RunItem]들입니다. 런 아이템은 LLM이 생성한 원문 항목을 감쌉니다.\n+[`new_items`][agents.result.RunResultBase.new_items] 프로퍼티에는 실행 중에 생성된 새 항목이 포함됩니다. 항목은 [`RunItem`][agents.items.RunItem]입니다. 실행 항목은 LLM이 생성한 원문 항목을 래핑합니다.\n \n-- [`MessageOutputItem`][agents.items.MessageOutputItem]: LLM의 메시지를 나타냅니다. 원문 항목은 생성된 메시지입니다\n-- [`HandoffCallItem`][agents.items.HandoffCallItem]: LLM이 핸드오프 도구를 호출했음을 나타냅니다. 원문 항목은 LLM의 도구 호출 아이템입니다\n-- [`HandoffOutputItem`][agents.items.HandoffOutputItem]: 핸드오프가 발생했음을 나타냅니다. 원문 항목은 핸드오프 도구 호출에 대한 도구 응답입니다. 또한 항목에서 소스/타깃 에이전트에 접근할 수 있습니다\n-- [`ToolCallItem`][agents.items.ToolCallItem]: LLM이 도구를 호출했음을 나타냅니다\n-- [`ToolCallOutputItem`][agents.items.ToolCallOutputItem]: 도구가 호출되었음을 나타냅니다. 원문 항목은 도구 응답입니다. 또한 항목에서 도구 출력을 확인할 수 있습니다\n-- [`ReasoningItem`][agents.items.ReasoningItem]: LLM의 추론 항목을 나타냅니다. 원문 항목은 생성된 추론입니다\n+-   [`MessageOutputItem`][agents.items.MessageOutputItem]: LLM의 메시지를 나타냅니다. 원문 항목은 생성된 메시지입니다\n+-   [`HandoffCallItem`][agents.items.HandoffCallItem]: LLM이 핸드오프 도구를 호출했음을 나타냅니다. 원문 항목은 LLM의 도구 호출 항목입니다\n+-   [`HandoffOutputItem`][agents.items.HandoffOutputItem]: 핸드오프가 발생했음을 나타냅니다. 원문 항목은 핸드오프 도구 호출에 대한 도구 응답입니다. 항목에서 소스/타깃 에이전트에도 접근할 수 있습니다\n+-   [`ToolCallItem`][agents.items.ToolCallItem]: LLM이 도구를 호출했음을 나타냅니다\n+-   [`ToolCallOutputItem`][agents.items.ToolCallOutputItem]: 도구가 호출되었음을 나타냅니다. 원문 항목은 도구 응답입니다. 항목에서 도구 출력에도 접근할 수 있습니다\n+-   [`ReasoningItem`][agents.items.ReasoningItem]: LLM의 추론 항목을 나타냅니다. 원문 항목은 생성된 추론입니다\n \n ## 기타 정보\n \n ### 가드레일 결과\n \n-[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results]와 [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] 속성에는 가드레일의 결과(있는 경우)가 담깁니다. 가드레일 결과에는 로깅하거나 저장하고 싶은 유용한 정보가 포함될 수 있어, 이를 제공해 드립니다.\n+[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results]와 [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] 프로퍼티에는 (있는 경우) 가드레일의 결과가 포함됩니다. 가드레일 결과에는 로깅하거나 저장하고 싶은 유용한 정보가 담기는 경우가 있어, 이를 제공해 드립니다.\n \n-도구 가드레일 결과는 [`tool_input_guardrail_results`][agents.result.RunResultBase.tool_input_guardrail_results] 및 [`tool_output_guardrail_results`][agents.result.RunResultBase.tool_output_guardrail_results]로 별도 제공됩니다. 이러한 가드레일은 도구에 부착될 수 있으며, 해당 도구 호출은 에이전트 워크플로 중에 가드레일을 실행합니다.\n+도구 가드레일 결과는 [`tool_input_guardrail_results`][agents.result.RunResultBase.tool_input_guardrail_results] 및 [`tool_output_guardrail_results`][agents.result.RunResultBase.tool_output_guardrail_results]로 별도 제공됩니다. 이러한 가드레일은 도구에 연결될 수 있으며, 해당 도구 호출은 에이전트 워크플로 동안 가드레일을 실행합니다.\n \n ### 원문 응답\n \n-[`raw_responses`][agents.result.RunResultBase.raw_responses] 속성에는 LLM이 생성한 [`ModelResponse`][agents.items.ModelResponse]들이 담깁니다.\n+[`raw_responses`][agents.result.RunResultBase.raw_responses] 프로퍼티에는 LLM이 생성한 [`ModelResponse`][agents.items.ModelResponse]가 포함됩니다.\n \n-### 원래 입력\n+### 원본 입력\n \n-[`input`][agents.result.RunResultBase.input] 속성에는 `run` 메서드에 제공한 원래 입력이 담깁니다. 대부분의 경우 필요하지 않지만, 필요할 경우를 대비해 제공됩니다.\n\\ No newline at end of file\n+[`input`][agents.result.RunResultBase.input] 프로퍼티에는 `run` 메서드에 제공한 원본 입력이 포함됩니다. 대부분의 경우 필요하지 않지만, 필요할 때를 대비해 제공됩니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fresults.md",
        "sha": "fb55bea8da9f7c1392757fba0c6a07225654bf62",
        "status": "modified"
      },
      {
        "additions": 59,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frunning_agents.md",
        "changes": 117,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Frunning_agents.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 58,
        "filename": "docs/ko/running_agents.md",
        "patch": "@@ -4,11 +4,11 @@ search:\n ---\n # 에이전트 실행\n \n-[`Runner`][agents.run.Runner] 클래스를 통해 에이전트를 실행할 수 있습니다. 선택지는 3가지입니다:\n+[`Runner`][agents.run.Runner] 클래스를 통해 에이전트를 실행할 수 있습니다. 다음 3가지 옵션이 있습니다:\n \n-1. [`Runner.run()`][agents.run.Runner.run]: 비동기로 실행되며 [`RunResult`][agents.result.RunResult] 를 반환\n-2. [`Runner.run_sync()`][agents.run.Runner.run_sync]: 동기 메서드로, 내부적으로 `.run()` 을 실행\n-3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]: 비동기로 실행되며 [`RunResultStreaming`][agents.result.RunResultStreaming] 을 반환. LLM 을 스트리밍 모드로 호출하고, 수신되는 대로 이벤트를 스트리밍\n+1. 비동기로 실행되고 [`RunResult`][agents.result.RunResult] 를 반환하는 [`Runner.run()`][agents.run.Runner.run]\n+2. 동기 메서드로 내부적으로 `.run()` 을 실행하는 [`Runner.run_sync()`][agents.run.Runner.run_sync]\n+3. 비동기로 실행되고 [`RunResultStreaming`][agents.result.RunResultStreaming] 을 반환하는 [`Runner.run_streamed()`][agents.run.Runner.run_streamed]. LLM 을 스트리밍 모드로 호출하고, 수신되는 대로 이벤트를 스트리밍합니다\n \n ```python\n from agents import Agent, Runner\n@@ -27,58 +27,58 @@ async def main():\n \n ## 에이전트 루프\n \n-`Runner` 의 run 메서드를 사용할 때 시작 에이전트와 입력을 전달합니다. 입력은 문자열(사용자 메시지로 간주) 또는 OpenAI Responses API 의 입력 아이템 리스트일 수 있습니다.\n+`Runner` 의 run 메서드를 사용할 때 시작 에이전트와 입력을 전달합니다. 입력은 문자열(사용자 메시지로 간주됨) 또는 OpenAI Responses API 의 입력 항목 리스트일 수 있습니다.\n \n-그 다음 러너는 다음과 같은 루프를 수행합니다:\n+런너는 다음 루프를 실행합니다:\n \n-1. 현재 에이전트와 현재 입력으로 LLM 을 호출\n-2. LLM 이 출력을 생성\n-    1. LLM 이 `final_output` 을 반환하면 루프를 종료하고 결과를 반환\n-    2. LLM 이 핸드오프를 수행하면 현재 에이전트와 입력을 업데이트하고 루프 재실행\n-    3. LLM 이 도구 호출을 생성하면 해당 도구 호출을 실행하고 결과를 덧붙인 뒤 루프 재실행\n-3. 전달된 `max_turns` 를 초과하면 [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] 예외를 발생\n+1. 현재 에이전트와 현재 입력으로 LLM 을 호출합니다\n+2. LLM 이 출력을 생성합니다\n+    1. LLM 이 `final_output` 을 반환하면 루프를 종료하고 결과를 반환합니다\n+    2. LLM 이 핸드오프를 수행하면 현재 에이전트와 입력을 업데이트하고 루프를 다시 실행합니다\n+    3. LLM 이 도구 호출을 생성하면 해당 도구 호출을 실행하고 결과를 추가한 뒤 루프를 다시 실행합니다\n+3. 전달된 `max_turns` 를 초과하면 [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] 예외를 발생시킵니다\n \n !!! note\n \n-    LLM 출력이 \"최종 출력\"으로 간주되는 규칙은 원하는 타입의 텍스트 출력을 생성하고 도구 호출이 없을 때입니다.\n+    LLM 출력이 \"최종 출력\"으로 간주되는 규칙은 원하는 타입의 텍스트 출력을 생성하고, 도구 호출이 없을 때입니다.\n \n ## 스트리밍\n \n-스트리밍을 사용하면 LLM 이 실행되는 동안 스트리밍 이벤트를 추가로 받을 수 있습니다. 스트림이 완료되면 [`RunResultStreaming`][agents.result.RunResultStreaming] 에는 실행에 대한 완전한 정보(새로 생성된 모든 출력 포함)가 담깁니다. 스트리밍 이벤트는 `.stream_events()` 를 호출하세요. 자세한 내용은 [스트리밍 가이드](streaming.md)에서 확인하세요.\n+스트리밍을 사용하면 LLM 이 실행되는 동안 스트리밍 이벤트를 추가로 수신할 수 있습니다. 스트림이 끝나면 [`RunResultStreaming`][agents.result.RunResultStreaming] 에 실행에 대한 전체 정보와 새로 생성된 모든 출력이 포함됩니다. 스트리밍 이벤트는 `.stream_events()` 를 호출해 수신할 수 있습니다. 자세한 내용은 [스트리밍 가이드](streaming.md)를 참조하세요.\n \n ## 실행 구성\n \n-`run_config` 매개변수를 사용하면 에이전트 실행에 대한 전역 설정을 구성할 수 있습니다:\n-\n--   [`model`][agents.run.RunConfig.model]: 각 Agent 의 `model` 설정과 무관하게 사용할 전역 LLM 모델을 설정\n--   [`model_provider`][agents.run.RunConfig.model_provider]: 모델 이름 조회를 위한 모델 제공자. 기본값은 OpenAI\n--   [`model_settings`][agents.run.RunConfig.model_settings]: 에이전트별 설정을 오버라이드. 예를 들어 전역 `temperature` 또는 `top_p` 를 설정 가능\n--   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: 모든 실행에 포함할 입력/출력 가드레일 리스트\n--   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: 핸드오프에 이미 필터가 없을 경우 모든 핸드오프에 적용할 전역 입력 필터. 입력 필터를 사용해 새 에이전트로 보내는 입력을 편집할 수 있습니다. 자세한 내용은 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 문서를 참고하세요\n--   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]: `True`(기본값) 인 경우, 러너는 다음 에이전트를 호출하기 전에 이전 대화 내용을 하나의 assistant 메시지로 압축합니다. 도우미는 콘텐츠를 `<CONVERSATION HISTORY>` 블록에 배치하고 이후 핸드오프가 발생할 때마다 새 턴을 계속 추가합니다. 원문 대화(원문 transcript)를 그대로 전달하려면 이를 `False` 로 설정하거나 사용자 지정 핸드오프 필터를 제공하세요. 모든 [`Runner` 메서드](agents.run.Runner)는 `RunConfig` 를 전달하지 않으면 자동으로 생성하므로, 빠른 시작과 code examples 에서도 기본값을 자동으로 사용하며, 명시적인 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 콜백은 계속해서 이를 오버라이드합니다. 개별 핸드오프는 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] 로 이 설정을 오버라이드할 수 있습니다\n--   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]: `nest_handoff_history` 가 `True` 일 때 정규화된 대화(히스토리 + 핸드오프 아이템)를 받아 다음 에이전트로 전달할 정확한 입력 아이템 리스트를 반환하는 선택적 호출자. 전체 핸드오프 필터를 작성하지 않고도 내장 요약을 대체할 수 있습니다\n--   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: 전체 실행에 대해 [tracing](tracing.md) 을 비활성화\n--   [`tracing`][agents.run.RunConfig.tracing]: 이 실행에 대해 내보내기, 프로세서, 트레이싱 메타데이터를 오버라이드하려면 [`TracingConfig`][agents.tracing.TracingConfig] 를 전달\n--   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: LLM 및 도구 호출의 입력/출력과 같은 잠재적 민감 데이터를 트레이스에 포함할지 구성\n--   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: 실행의 트레이싱 워크플로 이름, 트레이스 ID, 트레이스 그룹 ID 설정. 최소한 `workflow_name` 설정을 권장합니다. 그룹 ID 는 여러 실행에 걸쳐 트레이스를 연결할 수 있는 선택 필드입니다\n+`run_config` 매개변수는 에이전트 실행에 대한 전역 설정을 구성할 수 있게 해줍니다:\n+\n+-   [`model`][agents.run.RunConfig.model]: 각 Agent 의 `model` 설정과 무관하게 사용할 전역 LLM 모델 설정\n+-   [`model_provider`][agents.run.RunConfig.model_provider]: 모델 이름 조회를 위한 모델 프로바이더로, 기본값은 OpenAI\n+-   [`model_settings`][agents.run.RunConfig.model_settings]: 에이전트별 설정 오버라이드. 예를 들어 전역 `temperature` 또는 `top_p` 를 설정할 수 있음\n+-   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]: 모든 실행에 포함할 입력 또는 출력 가드레일 리스트\n+-   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]: 핸드오프에 이미 입력 필터가 없는 경우 모든 핸드오프에 적용할 전역 입력 필터. 입력 필터를 사용하면 새 에이전트로 전송되는 입력을 편집할 수 있습니다. 자세한 내용은 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 문서를 참조하세요\n+-   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]: `True`(기본값) 인 경우 다음 에이전트를 호출하기 전에 이전 대화를 단일 assistant 메시지로 접어 넣습니다. 도우미는 콘텐츠를 `<CONVERSATION HISTORY>` 블록 안에 배치하고 이후 핸드오프가 발생할 때마다 새 턴을 계속 추가합니다. 원문(원문) 대화를 그대로 전달하려면 `False` 로 설정하거나 사용자 지정 핸드오프 필터를 제공하세요. 모든 [`Runner` 메서드](agents.run.Runner)는 `RunConfig` 를 전달하지 않으면 자동으로 생성하므로, 퀵스타트와 code examples 는 이 기본값을 자동으로 사용하며, 명시적인 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 콜백은 계속해서 이를 오버라이드합니다. 개별 핸드오프는 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] 를 통해 이 설정을 오버라이드할 수 있습니다\n+-   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]: `nest_handoff_history` 가 `True` 일 때 정규화된 대본(기록 + 핸드오프 항목)을 수신하는 선택적 호출 가능 객체. 다음 에이전트로 전달할 정확한 입력 항목 리스트를 반환해야 하며, 전체 핸드오프 필터를 작성하지 않고도 기본 요약을 교체할 수 있습니다\n+-   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]: 전체 실행에 대해 [트레이싱](tracing.md) 을 비활성화\n+-   [`tracing`][agents.run.RunConfig.tracing]: 이 실행에 대한 내보내기 도구, 프로세서 또는 트레이싱 메타데이터를 오버라이드하기 위해 [`TracingConfig`][agents.tracing.TracingConfig] 전달\n+-   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]: LLM 및 도구 호출의 입력/출력 등 잠재적으로 민감한 데이터를 트레이스에 포함할지 여부 구성\n+-   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]: 실행의 트레이싱 workflow 이름, trace ID, trace group ID 설정. 최소한 `workflow_name` 설정을 권장합니다. group ID 는 여러 실행에 걸쳐 트레이스를 연결할 수 있게 해주는 선택적 필드입니다\n -   [`trace_metadata`][agents.run.RunConfig.trace_metadata]: 모든 트레이스에 포함할 메타데이터\n--   [`session_input_callback`][agents.run.RunConfig.session_input_callback]: Sessions 사용 시 각 턴 전에 새 사용자 입력을 세션 히스토리와 병합하는 방식을 커스터마이즈\n--   [`call_model_input_filter`][agents.run.RunConfig.call_model_input_filter]: 모델 호출 직전에 완전히 준비된 모델 입력(instructions 와 입력 아이템)을 편집하는 훅. 예: 히스토리 자르기 또는 시스템 프롬프트 주입\n+-   [`session_input_callback`][agents.run.RunConfig.session_input_callback]: Sessions 사용 시 각 턴 전에 새 사용자 입력이 세션 히스토리에 병합되는 방식을 사용자 지정\n+-   [`call_model_input_filter`][agents.run.RunConfig.call_model_input_filter]: 모델 호출 직전에 완전히 준비된 모델 입력(instructions 및 입력 항목)을 편집하기 위한 훅. 예: 히스토리 자르기 또는 시스템 프롬프트 주입\n \n-기본적으로, 이제 SDK 는 에이전트가 다른 에이전트로 핸드오프할 때 이전 턴을 하나의 assistant 요약 메시지 안에 중첩합니다. 이는 반복되는 assistant 메시지를 줄이고 전체 대화를 새 에이전트가 빠르게 스캔할 수 있는 단일 블록에 유지합니다. 레거시 동작으로 되돌리려면 `RunConfig(nest_handoff_history=False)` 를 전달하거나, 대화를 원하는 형태로 그대로 전달하는 `handoff_input_filter`(또는 `handoff_history_mapper`) 를 제공하세요. 특정 핸드오프에 대해 옵트아웃(또는 옵트인)하려면 `handoff(..., nest_handoff_history=False)` 또는 `True` 로 설정하세요. 사용자 지정 매퍼를 작성하지 않고 생성된 요약에 사용되는 래퍼 텍스트를 변경하려면 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] 를 호출하세요(기본값 복원은 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]).\n+기본적으로, SDK 는 이제 한 에이전트가 다른 에이전트로 핸드오프할 때 이전 턴을 단일 assistant 요약 메시지로 중첩합니다. 이는 반복되는 assistant 메시지를 줄이고 전체 대화를 새 에이전트가 빠르게 스캔할 수 있는 단일 블록에 유지합니다. 레거시 동작으로 돌아가고 싶다면 `RunConfig(nest_handoff_history=False)` 를 전달하거나, 대화를 원하는 그대로 전달하는 `handoff_input_filter`(또는 `handoff_history_mapper`) 를 제공하세요. 특정 핸드오프에 대해 옵트아웃(또는 옵트인)하려면 `handoff(..., nest_handoff_history=False)` 또는 `True` 로 설정하면 됩니다. 사용자 지정 매퍼를 작성하지 않고 생성된 요약에 사용되는 래퍼 텍스트를 변경하려면 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers] 를 호출하세요(기본값으로 복원하려면 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]).\n \n ## 대화/채팅 스레드\n \n-어떤 run 메서드를 호출하더라도 하나 이상의 에이전트 실행(즉, 하나 이상의 LLM 호출)로 이어질 수 있지만, 이는 채팅 대화에서 하나의 논리적 턴을 의미합니다. 예를 들어:\n+실행 메서드 중 하나를 호출하면 하나 이상의 에이전트가 실행될 수 있으며(따라서 하나 이상의 LLM 호출), 이는 채팅 대화에서 단일 논리적 턴을 나타냅니다. 예를 들어:\n \n 1. 사용자 턴: 사용자가 텍스트 입력\n-2. Runner 실행: 첫 번째 에이전트가 LLM 을 호출하고 도구를 실행한 뒤 두 번째 에이전트로 핸드오프, 두 번째 에이전트가 더 많은 도구를 실행한 후 출력을 생성\n+2. Runner 실행: 첫 번째 에이전트가 LLM 을 호출하고 도구를 실행하며 두 번째 에이전트로 핸드오프, 두 번째 에이전트가 추가 도구를 실행한 뒤 출력을 생성\n \n-에이전트 실행이 끝나면 사용자에게 무엇을 보여줄지 선택할 수 있습니다. 예를 들어 에이전트가 생성한 모든 새 아이템을 사용자에게 보여줄 수도 있고, 최종 출력만 보여줄 수도 있습니다. 어느 쪽이든 사용자가 후속 질문을 할 수 있으며, 이 경우 run 메서드를 다시 호출하면 됩니다.\n+에이전트 실행이 끝나면 사용자에게 무엇을 보여줄지 선택할 수 있습니다. 예를 들어, 에이전트가 생성한 모든 새 항목을 보여주거나 최종 출력만 보여줄 수 있습니다. 어느 쪽이든, 사용자가 후속 질문을 할 수 있으며, 이 경우 run 메서드를 다시 호출하면 됩니다.\n \n-### 수동 대화 관리\n+### 대화 수동 관리\n \n-[`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] 메서드를 사용해 다음 턴의 입력을 얻고, 대화 히스토리를 수동으로 관리할 수 있습니다:\n+다음 턴의 입력을 얻기 위해 [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] 메서드를 사용하여 대화 히스토리를 수동으로 관리할 수 있습니다:\n \n ```python\n async def main():\n@@ -98,9 +98,9 @@ async def main():\n         # California\n ```\n \n-### Sessions 를 통한 자동 대화 관리\n+### Sessions 를 통한 대화 자동 관리\n \n-더 간단한 접근법으로, [Sessions](sessions/index.md) 를 사용하면 `.to_input_list()` 를 수동 호출하지 않고도 대화 히스토리를 자동으로 처리할 수 있습니다:\n+더 간단한 접근 방식으로, [Sessions](sessions/index.md) 를 사용해 `.to_input_list()` 를 수동으로 호출하지 않고도 대화 히스토리를 자동으로 처리할 수 있습니다:\n \n ```python\n from agents import Agent, Runner, SQLiteSession\n@@ -124,23 +124,24 @@ async def main():\n         # California\n ```\n \n-세션은 다음을 자동으로 수행합니다:\n+Sessions 는 다음을 자동으로 수행합니다:\n \n--   각 실행 전에 대화 히스토리를 조회\n--   각 실행 후 새 메시지를 저장\n--   서로 다른 세션 ID 에 대해 별도의 대화를 유지\n+-   각 실행 전에 대화 히스토리 조회\n+-   각 실행 후 새 메시지 저장\n+-   서로 다른 세션 ID 에 대해 별도의 대화 유지\n \n-자세한 내용은 [세션 문서](sessions/index.md)에서 확인하세요.\n+자세한 내용은 [Sessions 문서](sessions/index.md)에서 확인하세요.\n \n-### 서버 관리형 대화\n \n-OpenAI 의 대화 상태 기능을 사용해 `to_input_list()` 또는 `Sessions` 로 로컬에서 처리하는 대신 서버 측에서 대화 상태를 관리할 수도 있습니다. 이렇게 하면 과거 모든 메시지를 수동으로 다시 보내지 않고도 대화 히스토리를 보존할 수 있습니다. 자세한 내용은 [OpenAI Conversation state 가이드](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)를 참고하세요.\n+### 서버 관리 대화\n+\n+로컬에서 `to_input_list()` 또는 `Sessions` 로 처리하는 대신, OpenAI 대화 상태 기능에 서버 측 대화 상태 관리를 맡길 수도 있습니다. 이를 통해 과거 메시지를 모두 수동으로 다시 보내지 않고도 대화 히스토리를 보존할 수 있습니다. 자세한 내용은 [OpenAI Conversation state 가이드](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)를 참조하세요.\n \n OpenAI 는 턴 간 상태를 추적하는 두 가지 방법을 제공합니다:\n \n #### 1. `conversation_id` 사용\n \n-먼저 OpenAI Conversations API 를 사용해 대화를 생성한 다음, 이후 모든 호출에 해당 ID 를 재사용합니다:\n+먼저 OpenAI Conversations API 를 사용해 대화를 생성한 후, 이후 모든 호출에서 해당 ID 를 재사용합니다:\n \n ```python\n from agents import Agent, Runner\n@@ -163,7 +164,7 @@ async def main():\n \n #### 2. `previous_response_id` 사용\n \n-또 다른 옵션은 **response chaining** 으로, 각 턴이 이전 턴의 응답 ID 에 명시적으로 연결됩니다.\n+다른 옵션은 각 턴이 이전 턴의 response ID 에 명시적으로 연결되는 **response chaining** 입니다.\n \n ```python\n from agents import Agent, Runner\n@@ -188,9 +189,9 @@ async def main():\n         print(f\"Assistant: {result.final_output}\")\n ```\n \n-## 모델 호출 입력 필터\n+## Call model input filter\n \n-모델 호출 직전에 모델 입력을 편집하려면 `call_model_input_filter` 를 사용하세요. 이 훅은 현재 에이전트, 컨텍스트, 결합된 입력 아이템(세션 히스토리가 있는 경우 이를 포함) 을 받아 새로운 `ModelInputData` 를 반환합니다.\n+`call_model_input_filter` 를 사용하여 모델 호출 직전에 모델 입력을 편집할 수 있습니다. 이 훅은 현재 에이전트, 컨텍스트, 결합된 입력 항목(세션 히스토리가 있는 경우 포함)을 받아 새로운 `ModelInputData` 를 반환합니다.\n \n ```python\n from agents import Agent, Runner, RunConfig\n@@ -209,20 +210,20 @@ result = Runner.run_sync(\n )\n ```\n \n-실행별로 `run_config` 에 설정하거나 `Runner` 의 기본값으로 설정해, 민감 데이터 마스킹, 긴 히스토리 절단, 추가 시스템 가이드 주입 등을 수행하세요.\n+`run_config` 을 통해 실행별로 또는 `Runner` 의 기본값으로 설정하여 민감한 데이터 마스킹, 긴 히스토리 자르기, 추가 시스템 가이드 주입 등에 사용할 수 있습니다.\n \n-## 장기 실행 에이전트 및 휴먼인더루프 (HITL)\n+## 장시간 실행 에이전트 & 휴먼인더루프\n \n-Agents SDK 의 [Temporal](https://temporal.io/) 통합을 사용하면 내구성이 있는 장기 실행 워크플로(휴먼인더루프 작업 포함) 를 실행할 수 있습니다. Temporal 과 Agents SDK 가 장기 실행 작업을 완료하는 데 함께 작동하는 데모는 [이 동영상](https://www.youtube.com/watch?v=fFBZqzT4DD8)과 [이 문서](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents)에서 확인하세요.\n+Agents SDK 의 [Temporal](https://temporal.io/) 통합을 사용하면 내구성이 있는 장시간 실행 워크플로를 실행할 수 있으며, 휴먼인더루프 작업도 포함할 수 있습니다. Temporal 과 Agents SDK 가 함께 작동하여 장시간 작업을 완료하는 데모는 [이 동영상](https://www.youtube.com/watch?v=fFBZqzT4DD8)에서 확인하고, [문서는 여기](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents)에서 확인하세요.\n \n ## 예외\n \n SDK 는 특정 상황에서 예외를 발생시킵니다. 전체 목록은 [`agents.exceptions`][] 에 있습니다. 개요는 다음과 같습니다:\n \n--   [`AgentsException`][agents.exceptions.AgentsException]: SDK 내에서 발생하는 모든 예외의 기본 클래스입니다. 다른 모든 구체적 예외의 상위 일반 타입 역할을 합니다\n--   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: 에이전트 실행이 `Runner.run`, `Runner.run_sync`, `Runner.run_streamed` 메서드에 전달된 `max_turns` 제한을 초과할 때 발생합니다. 지정된 상호작용 턴 수 내에 에이전트가 작업을 완료하지 못했음을 의미합니다\n--   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: 기반 모델(LLM) 이 예기치 않거나 잘못된 출력을 생성할 때 발생합니다. 예시는 다음과 같습니다:\n-    -   잘못된 JSON: 모델이 도구 호출 또는 직접 출력에 대해 잘못된 JSON 구조를 제공하는 경우, 특히 특정 `output_type` 이 정의된 경우\n-    -   예기치 않은 도구 관련 실패: 모델이 예상된 방식으로 도구를 사용하지 못하는 경우\n--   [`UserError`][agents.exceptions.UserError]: SDK 를 사용하는 여러분(코드를 작성하는 사람) 이 SDK 사용 중 오류를 범했을 때 발생합니다. 일반적으로 잘못된 코드 구현, 잘못된 구성, SDK API 오용으로 인해 발생합니다\n--   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: 각각 입력 가드레일 또는 출력 가드레일의 조건이 충족될 때 발생합니다. 입력 가드레일은 처리 전에 들어오는 메시지를 검사하고, 출력 가드레일은 에이전트의 최종 응답을 전달하기 전에 검사합니다\n\\ No newline at end of file\n+-   [`AgentsException`][agents.exceptions.AgentsException]: SDK 내에서 발생하는 모든 예외의 기본 클래스입니다. 다른 모든 구체적인 예외가 파생되는 일반 타입 역할을 합니다\n+-   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]: 에이전트 실행이 `Runner.run`, `Runner.run_sync`, `Runner.run_streamed` 메서드에 전달된 `max_turns` 제한을 초과했을 때 발생합니다. 이는 에이전트가 지정된 상호작용 턴 수 내에 작업을 완료하지 못했음을 나타냅니다\n+-   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]: 기본 모델(LLM) 이 예상치 못한 또는 잘못된 출력을 생성할 때 발생합니다. 다음을 포함할 수 있습니다:\n+    -   잘못된 JSON: 특히 특정 `output_type` 이 정의된 경우, 도구 호출 또는 직접 출력에서 잘못된 JSON 구조를 제공하는 경우\n+    -   예상치 못한 도구 관련 실패: 모델이 예상된 방식으로 도구를 사용하지 못하는 경우\n+-   [`UserError`][agents.exceptions.UserError]: SDK 를 사용하는 개발자(코드를 작성하는 사람)가 SDK 사용 중 오류를 발생시킬 때 던져집니다. 일반적으로 잘못된 코드 구현, 유효하지 않은 구성, SDK API 오용에서 비롯됩니다\n+-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]: 각각 입력 가드레일 또는 출력 가드레일 조건이 충족될 때 발생합니다. 입력 가드레일은 처리 전에 들어오는 메시지를 검사하고, 출력 가드레일은 에이전트의 최종 응답을 전달하기 전에 검사합니다\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Frunning_agents.md",
        "sha": "7bd54ddcf459b47a9bb0296601892eb47e7aa9b2",
        "status": "modified"
      },
      {
        "additions": 18,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Fadvanced_sqlite_session.md",
        "changes": 37,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fsessions%2Fadvanced_sqlite_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 19,
        "filename": "docs/ko/sessions/advanced_sqlite_session.md",
        "patch": "@@ -8,11 +8,11 @@ search:\n \n ## 기능\n \n-- **대화 분기**: 임의의 사용자 메시지로부터 대체 대화 경로를 생성\n-- **사용량 추적**: 각 턴별 토큰 사용량을 상세 JSON으로 분석\n-- **구조화된 쿼리**: 턴 단위 대화 조회, 도구 사용 통계 등 제공\n+- **대화 분기**: 임의의 사용자 메시지에서 대체 대화 경로 생성\n+- **사용량 추적**: 각 턴별 상세 토큰 사용 분석 및 전체 JSON 분해 제공\n+- **구조화된 쿼리**: 턴별 대화, 도구 사용 통계 등 조회\n - **분기 관리**: 독립적인 분기 전환 및 관리\n-- **메시지 구조 메타데이터**: 메시지 타입, 도구 사용, 대화 흐름 추적\n+- **메시지 구조 메타데이터**: 메시지 유형, 도구 사용, 대화 흐름 추적\n \n ## 빠른 시작\n \n@@ -85,15 +85,15 @@ session = AdvancedSQLiteSession(\n ### 매개변수\n \n - `session_id` (str): 대화 세션의 고유 식별자\n-- `db_path` (str | Path): SQLite 데이터베이스 파일 경로. 메모리 저장은 `:memory:`가 기본값\n-- `create_tables` (bool): 고급 테이블을 자동 생성할지 여부. 기본값은 `False`\n-- `logger` (logging.Logger | None): 세션에 사용할 커스텀 로거. 기본값은 모듈 로거\n+- `db_path` (str | Path): SQLite 데이터베이스 파일 경로. 메모리 저장을 위한 `:memory:`가 기본값\n+- `create_tables` (bool): 고급 테이블을 자동으로 생성할지 여부. 기본값은 `False`\n+- `logger` (logging.Logger | None): 세션에 사용할 커스텀 로거. 모듈 로거가 기본값\n \n ## 사용량 추적\n \n-AdvancedSQLiteSession은 대화의 각 턴별 토큰 사용 데이터를 저장하여 상세 사용량 분석을 제공합니다. **이는 각 에이전트 실행 후 `store_run_usage` 메서드를 호출하는 것에 전적으로 의존합니다.**\n+AdvancedSQLiteSession은 대화의 각 턴별 토큰 사용 데이터를 저장하여 상세한 사용량 분석을 제공합니다. **이는 각 에이전트 실행 후 `store_run_usage` 메서드가 호출되는 것에 전적으로 의존합니다.**\n \n-### 사용 데이터 저장\n+### 사용량 데이터 저장\n \n ```python\n # After each agent run, store the usage data\n@@ -137,7 +137,7 @@ turn_2_usage = await session.get_turn_usage(user_turn_number=2)\n \n ## 대화 분기\n \n-AdvancedSQLiteSession의 핵심 기능 중 하나는 임의의 사용자 메시지로부터 대화 분기를 생성하여, 대체 대화 경로를 탐색할 수 있다는 점입니다.\n+AdvancedSQLiteSession의 핵심 기능 중 하나는 임의의 사용자 메시지에서 대화 분기를 생성하여, 대체 대화 경로를 탐색할 수 있게 하는 것입니다.\n \n ### 분기 생성\n \n@@ -217,7 +217,7 @@ await session.store_run_usage(result)\n \n ## 구조화된 쿼리\n \n-AdvancedSQLiteSession은 대화의 구조와 내용을 분석하기 위한 여러 메서드를 제공합니다.\n+AdvancedSQLiteSession은 대화 구조와 내용을 분석하기 위한 여러 메서드를 제공합니다.\n \n ### 대화 분석\n \n@@ -247,15 +247,15 @@ for turn in matching_turns:\n \n 세션은 다음을 포함한 메시지 구조를 자동으로 추적합니다:\n \n-- 메시지 타입 (user, assistant, tool_call 등)\n-- 도구 호출 시 도구 이름\n+- 메시지 유형(user, assistant, tool_call 등)\n+- 도구 호출의 도구 이름\n - 턴 번호와 시퀀스 번호\n-- 분기 연관 관계\n+- 분기 연관\n - 타임스탬프\n \n ## 데이터베이스 스키마\n \n-AdvancedSQLiteSession은 기본 SQLite 스키마를 확장하여 두 개의 추가 테이블을 제공합니다:\n+AdvancedSQLiteSession은 기본 SQLite 스키마를 두 개의 추가 테이블로 확장합니다:\n \n ### message_structure 테이블\n \n@@ -296,12 +296,11 @@ CREATE TABLE turn_usage (\n );\n ```\n \n-## 전체 예제\n+## 전체 예시\n \n-모든 기능을 종합적으로 보여주는 [완전한 예제](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py)를 확인하세요.\n+모든 기능을 포괄적으로 시연한 [완전한 예시](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py)를 확인하세요.\n \n-\n-## API 레퍼런스\n+## API 참조\n \n - [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - 메인 클래스\n - [`Session`][agents.memory.session.Session] - 기본 세션 프로토콜\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Fadvanced_sqlite_session.md",
        "sha": "d5050a544a9a5cc2db9b9bf91176a6d9bfe2d491",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Fencrypted_session.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fsessions%2Fencrypted_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/ko/sessions/encrypted_session.md",
        "patch": "@@ -4,18 +4,18 @@ search:\n ---\n # 암호화된 세션\n \n-`EncryptedSession`은 모든 세션 구현에 대해 투명한 암호화를 제공하여, 대화 데이터를 보호하고 오래된 항목을 자동으로 만료시킵니다.\n+`EncryptedSession`은 모든 세션 구현에 대해 투명한 암호화를 제공하며, 자동 만료로 오래된 항목을 보호합니다.\n \n ## 기능\n \n-- **투명한 암호화**: 모든 세션을 Fernet 암호화로 래핑\n+- **투명한 암호화**: Fernet 암호화로 어떤 세션이든 래핑\n - **세션별 키**: 세션마다 고유한 암호화를 위해 HKDF 키 파생 사용\n-- **자동 만료**: TTL이 만료되면 오래된 항목을 조용히 건너뜀\n-- **대체 가능**: 기존 세션 구현과 바로 호환\n+- **자동 만료**: TTL이 만료되면 오래된 항목은 조용히 건너뜀\n+- **교체형 구성**: 기존 세션 구현과 함께 바로 사용 가능\n \n ## 설치\n \n-암호화된 세션에는 `encrypt` extra가 필요합니다:\n+암호화된 세션을 사용하려면 `encrypt` extra가 필요합니다:\n \n ```bash\n pip install openai-agents[encrypt]\n@@ -81,7 +81,7 @@ session = EncryptedSession(\n \n ### TTL (Time To Live)\n \n-암호화된 항목이 유효하게 유지되는 기간을 설정합니다:\n+암호화된 항목이 유효한 기간을 설정합니다:\n \n ```python\n # Items expire after 1 hour\n@@ -101,7 +101,7 @@ session = EncryptedSession(\n )\n ```\n \n-## 다양한 세션 유형과 함께 사용\n+## 다양한 세션 타입과 함께 사용\n \n ### SQLite 세션과 함께 사용\n \n@@ -140,30 +140,30 @@ session = EncryptedSession(\n \n !!! warning \"고급 세션 기능\"\n \n-    `EncryptedSession`을 `AdvancedSQLiteSession`과 같은 고급 세션 구현과 함께 사용할 때는 다음을 유의하세요:\n+    `EncryptedSession`을 `AdvancedSQLiteSession`과 같은 고급 세션 구현과 함께 사용할 때에는 다음 사항에 유의하세요:\n \n-    - 메시지 콘텐츠가 암호화되므로 `find_turns_by_content()`와 같은 메서드는 효과적으로 동작하지 않습니다\n-    - 콘텐츠 기반 검색은 암호화된 데이터에서 수행되어 효과가 제한됩니다\n+    - 메시지 콘텐츠가 암호화되므로 find_turns_by_content() 같은 메서드는 효과적으로 동작하지 않습니다\n+    - 콘텐츠 기반 검색은 암호화된 데이터에서 수행되므로 효과가 제한됩니다\n \n \n \n ## 키 파생\n \n-EncryptedSession은 HKDF(HMAC 기반 키 파생 함수)를 사용하여 세션별로 고유한 암호화 키를 파생합니다:\n+EncryptedSession은 HKDF (HMAC 기반 키 파생 함수)를 사용하여 세션별로 고유한 암호화 키를 파생합니다:\n \n - **마스터 키**: 사용자가 제공한 암호화 키\n - **세션 솔트**: 세션 ID\n-- **정보 문자열**: `\"agents.session-store.hkdf.v1\"`\n+- **정보 문자열**: \"agents.session-store.hkdf.v1\"\n - **출력**: 32바이트 Fernet 키\n \n-이로써 다음이 보장됩니다:\n+이를 통해 다음이 보장됩니다:\n - 각 세션은 고유한 암호화 키를 가짐\n - 마스터 키 없이는 키를 파생할 수 없음\n - 서로 다른 세션 간에는 세션 데이터를 복호화할 수 없음\n \n ## 자동 만료\n \n-항목이 TTL을 초과하면 검색 중 자동으로 건너뜁니다:\n+항목이 TTL을 초과하면, 조회 중 자동으로 건너뜁니다:\n \n ```python\n # Items older than TTL are silently ignored\n@@ -175,5 +175,5 @@ result = await Runner.run(agent, \"Continue conversation\", session=session)\n \n ## API 레퍼런스\n \n-- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 기본 클래스\n+- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 주 클래스\n - [`Session`][agents.memory.session.Session] - 기본 세션 프로토콜\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Fencrypted_session.md",
        "sha": "0d2bca26ddc03e256798144c2e15053eae80a170",
        "status": "modified"
      },
      {
        "additions": 35,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Findex.md",
        "changes": 69,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fsessions%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 34,
        "filename": "docs/ko/sessions/index.md",
        "patch": "@@ -4,9 +4,9 @@ search:\n ---\n # 세션\n \n-Agents SDK 는 여러 에이전트 실행에 걸쳐 대화 기록을 자동으로 유지하는 내장 세션 메모리를 제공합니다. 이를 통해 턴 사이에 수동으로 `.to_input_list()` 를 처리할 필요가 없습니다.\n+Agents SDK는 내장 세션 메모리를 제공하여 여러 에이전트 실행 간에 대화 이력을 자동으로 유지하므로, 턴 사이에 `.to_input_list()`를 수동으로 처리할 필요가 없습니다.\n \n-세션은 특정 세션의 대화 기록을 저장하여, 명시적인 수동 메모리 관리 없이도 에이전트가 컨텍스트를 유지할 수 있도록 합니다. 이는 에이전트가 이전 상호작용을 기억해야 하는 채팅 애플리케이션이나 멀티턴 대화에 특히 유용합니다.\n+Sessions는 특정 세션의 대화 이력을 저장하여, 에이전트가 명시적인 수동 메모리 관리 없이 컨텍스트를 유지할 수 있게 합니다. 이는 에이전트가 이전 상호작용을 기억하길 원하는 채팅 애플리케이션이나 멀티 턴 대화에 특히 유용합니다.\n \n ## 빠른 시작\n \n@@ -51,17 +51,17 @@ print(result.final_output)  # \"Approximately 39 million\"\n \n 세션 메모리가 활성화되면:\n \n-1. **각 실행 전**: 러너가 세션의 대화 기록을 자동으로 가져와 입력 항목 앞에 추가합니다\n-2. **각 실행 후**: 실행 중 생성된 모든 새 항목(사용자 입력, 어시스턴트 응답, 도구 호출 등)이 세션에 자동으로 저장됩니다\n-3. **컨텍스트 유지**: 동일한 세션에서의 이후 실행은 전체 대화 기록을 포함하므로 에이전트가 컨텍스트를 유지할 수 있습니다\n+1. **각 실행 전**: 러너가 세션의 대화 이력을 자동으로 가져와 입력 아이템 앞에 추가합니다\n+2. **각 실행 후**: 실행 중 생성된 모든 새 아이템(사용자 입력, 어시스턴트 응답, 도구 호출 등)이 세션에 자동으로 저장됩니다\n+3. **컨텍스트 보존**: 동일한 세션으로 이어지는 각 실행에는 전체 대화 이력이 포함되어, 에이전트가 컨텍스트를 유지합니다\n \n-이를 통해 실행 사이에서 `.to_input_list()` 를 수동으로 호출하고 대화 상태를 관리할 필요가 없어집니다.\n+이는 실행 간에 `.to_input_list()`를 수동으로 호출하고 대화 상태를 관리해야 하는 필요를 제거합니다.\n \n ## 메모리 작업\n \n ### 기본 작업\n \n-세션은 대화 기록을 관리하기 위한 여러 작업을 지원합니다:\n+Sessions는 대화 이력 관리를 위한 여러 작업을 지원합니다:\n \n ```python\n from agents import SQLiteSession\n@@ -86,9 +86,9 @@ print(last_item)  # {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n await session.clear_session()\n ```\n \n-### 수정용 pop_item 사용\n+### 수정을 위한 pop_item 사용\n \n-대화에서 마지막 항목을 되돌리거나 수정하고자 할 때 `pop_item` 메서드가 특히 유용합니다:\n+`pop_item` 메서드는 대화에서 마지막 아이템을 되돌리거나 수정하고 싶을 때 특히 유용합니다:\n \n ```python\n from agents import Agent, Runner, SQLiteSession\n@@ -119,11 +119,11 @@ print(f\"Agent: {result.final_output}\")\n \n ## 세션 유형\n \n-SDK 는 다양한 사용 사례를 위한 여러 세션 구현을 제공합니다:\n+SDK는 다양한 사용 사례를 위한 여러 세션 구현을 제공합니다:\n \n ### OpenAI Conversations API 세션\n \n-`OpenAIConversationsSession` 을 통해 [OpenAI's Conversations API](https://platform.openai.com/docs/api-reference/conversations) 를 사용합니다.\n+`OpenAIConversationsSession`을 통해 [OpenAI의 Conversations API](https://platform.openai.com/docs/api-reference/conversations)를 사용하세요.\n \n ```python\n from agents import Agent, Runner, OpenAIConversationsSession\n@@ -159,7 +159,7 @@ print(result.final_output)  # \"California\"\n \n ### SQLite 세션\n \n-기본의 가벼운 SQLite 기반 세션 구현:\n+기본 제공되는 경량 SQLite 기반 세션 구현:\n \n ```python\n from agents import SQLiteSession\n@@ -180,7 +180,7 @@ result = await Runner.run(\n \n ### SQLAlchemy 세션\n \n-SQLAlchemy 가 지원하는 모든 데이터베이스를 사용하는 프로덕션 준비 완료 세션:\n+SQLAlchemy가 지원하는 모든 데이터베이스를 사용하는 프로덕션 준비 세션:\n \n ```python\n from agents.extensions.memory import SQLAlchemySession\n@@ -198,13 +198,13 @@ engine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\n session = SQLAlchemySession(\"user_123\", engine=engine, create_tables=True)\n ```\n \n-자세한 문서는 [SQLAlchemy 세션](sqlalchemy_session.md) 을 참고하세요.\n+자세한 문서는 [SQLAlchemy Sessions](sqlalchemy_session.md)를 참고하세요.\n \n \n \n ### 고급 SQLite 세션\n \n-대화 분기, 사용량 분석, 구조화된 쿼리를 제공하는 향상된 SQLite 세션:\n+대화 브랜칭, 사용량 분석, 구조화된 쿼리를 제공하는 향상된 SQLite 세션:\n \n ```python\n from agents.extensions.memory import AdvancedSQLiteSession\n@@ -224,11 +224,11 @@ await session.store_run_usage(result)  # Track token usage\n await session.create_branch_from_turn(2)  # Branch from turn 2\n ```\n \n-자세한 문서는 [고급 SQLite 세션](advanced_sqlite_session.md) 을 참고하세요.\n+자세한 문서는 [Advanced SQLite Sessions](advanced_sqlite_session.md)를 참고하세요.\n \n-### 암호화된 세션\n+### 암호화 세션\n \n-모든 세션 구현에 적용 가능한 투명한 암호화 래퍼:\n+어떤 세션 구현에도 적용 가능한 투명한 암호화 래퍼:\n \n ```python\n from agents.extensions.memory import EncryptedSession, SQLAlchemySession\n@@ -251,31 +251,32 @@ session = EncryptedSession(\n result = await Runner.run(agent, \"Hello\", session=session)\n ```\n \n-자세한 문서는 [암호화된 세션](encrypted_session.md) 을 참고하세요.\n+자세한 문서는 [Encrypted Sessions](encrypted_session.md)를 참고하세요.\n \n ### 기타 세션 유형\n \n-추가로 몇 가지 내장 옵션이 있습니다. `examples/memory/` 와 `extensions/memory/` 아래의 소스 코드를 참고하세요.\n+더 몇 가지 내장 옵션이 있습니다. `examples/memory/`와 `extensions/memory/` 아래의 소스 코드를 참고하세요.\n \n ## 세션 관리\n \n ### 세션 ID 명명\n \n-대화를 체계적으로 구성할 수 있도록 의미 있는 세션 ID 를 사용하세요:\n+대화를 체계적으로 구성하는 데 도움이 되는 의미 있는 세션 ID를 사용하세요:\n \n - User 기반: `\"user_12345\"`\n - 스레드 기반: `\"thread_abc123\"`\n - 컨텍스트 기반: `\"support_ticket_456\"`\n \n ### 메모리 지속성\n \n-- 임시 대화에는 인메모리 SQLite (`SQLiteSession(\"session_id\")`) 사용\n-- 지속적인 대화에는 파일 기반 SQLite (`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`) 사용\n-- SQLAlchemy 가 지원하는 기존 데이터베이스를 사용하는 프로덕션 시스템에는 SQLAlchemy 기반 세션 (`SQLAlchemySession(\"session_id\", engine=engine, create_tables=True)`) 사용\n-- 클라우드 네이티브 프로덕션 배포에는 Dapr 상태 저장소 세션 (`DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")`) 사용. 내장 텔레메트리, 트레이싱, 데이터 격리와 함께 30+ 데이터베이스 백엔드를 지원\n-- 기록을 OpenAI Conversations API 에 저장하길 원할 때는 OpenAI 호스트하는 저장소 (`OpenAIConversationsSession()`) 사용\n-- 투명한 암호화와 TTL 기반 만료가 필요한 경우 암호화된 세션 (`EncryptedSession(session_id, underlying_session, encryption_key)`) 사용\n-- 더 고급 사용 사례를 위해 다른 프로덕션 시스템(Redis, Django 등)에 대한 커스텀 세션 백엔드 구현 고려\n+- 임시 대화에는 인메모리 SQLite(`SQLiteSession(\"session_id\")`) 사용\n+- 지속적인 대화에는 파일 기반 SQLite(`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`) 사용\n+- SQLAlchemy가 지원하는 기존 데이터베이스를 사용하는 프로덕션 시스템에는 SQLAlchemy 기반 세션(`SQLAlchemySession(\"session_id\", engine=engine, create_tables=True\")`) 사용\n+- 클라우드 네이티브 프로덕션 배포에는 Dapr 상태 저장소 세션(`DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")`) 사용\n+30+ 데이터베이스 백엔드를 지원하며, 텔레메트리, 트레이싱, 데이터 격리가 내장됨\n+- 기록을 OpenAI Conversations API에 저장하길 원한다면 OpenAI가 호스트하는 스토리지(`OpenAIConversationsSession()`) 사용\n+- 투명한 암호화와 TTL 기반 만료가 필요하면 암호화 세션(`EncryptedSession(session_id, underlying_session, encryption_key\")`) 사용\n+- 고급 사용 사례를 위해 기타 프로덕션 시스템(Redis, Django 등)에 대한 커스텀 세션 백엔드 구현을 고려\n \n ### 다중 세션\n \n@@ -323,7 +324,7 @@ result2 = await Runner.run(\n \n ## 전체 예제\n \n-다음은 세션 메모리가 실제로 동작하는 전체 예제입니다:\n+다음은 세션 메모리가 동작하는 전체 예제입니다:\n \n ```python\n import asyncio\n@@ -387,7 +388,7 @@ if __name__ == \"__main__\":\n \n ## 사용자 정의 세션 구현\n \n-[`Session`][agents.memory.session.Session] 프로토콜을 따르는 클래스를 만들어 자체 세션 메모리를 구현할 수 있습니다:\n+[`Session`][agents.memory.session.Session] 프로토콜을 따르는 클래스를 생성하여 고유한 세션 메모리를 구현할 수 있습니다:\n \n ```python\n from agents.memory.session import SessionABC\n@@ -436,11 +437,11 @@ result = await Runner.run(\n \n | 패키지 | 설명 |\n |---------|-------------|\n-| [openai-django-sessions](https://pypi.org/project/openai-django-sessions/) | Django 가 지원하는 모든 데이터베이스(PostgreSQL, MySQL, SQLite 등)를 위한 Django ORM 기반 세션 |\n+| [openai-django-sessions](https://pypi.org/project/openai-django-sessions/) | Django가 지원하는 모든 데이터베이스(PostgreSQL, MySQL, SQLite 등)를 위한 Django ORM 기반 세션 |\n \n-세션 구현을 제작하셨다면, 이곳에 추가될 수 있도록 문서 PR 을 제출해 주세요!\n+세션 구현을 만들었다면, 이곳에 추가될 수 있도록 문서 PR을 자유롭게 제출해 주세요!\n \n-## API 레퍼런스\n+## API 참조\n \n 자세한 API 문서는 다음을 참고하세요:\n \n@@ -449,5 +450,5 @@ result = await Runner.run(\n - [`SQLiteSession`][agents.memory.sqlite_session.SQLiteSession] - 기본 SQLite 구현\n - [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - SQLAlchemy 기반 구현\n - [`DaprSession`][agents.extensions.memory.dapr_session.DaprSession] - Dapr 상태 저장소 구현\n-- [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - 분기와 분석이 포함된 향상된 SQLite\n+- [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - 브랜칭과 분석을 지원하는 향상된 SQLite\n - [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 어떤 세션에도 적용 가능한 암호화 래퍼\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Findex.md",
        "sha": "bd112458db260b1e3a5f642bc744149bcac09264",
        "status": "modified"
      },
      {
        "additions": 3,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Fsqlalchemy_session.md",
        "changes": 6,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fsessions%2Fsqlalchemy_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 3,
        "filename": "docs/ko/sessions/sqlalchemy_session.md",
        "patch": "@@ -4,11 +4,11 @@ search:\n ---\n # SQLAlchemy 세션\n \n-`SQLAlchemySession`은 SQLAlchemy를 사용하여 프로덕션 준비가 된 세션 구현을 제공합니다. 이를 통해 SQLAlchemy가 지원하는 모든 데이터베이스(PostgreSQL, MySQL, SQLite 등)를 세션 스토리지로 사용할 수 있습니다.\n+`SQLAlchemySession`은 SQLAlchemy를 사용해 프로덕션 준비가 된 세션 구현을 제공하며, 세션 저장소로 SQLAlchemy가 지원하는 모든 데이터베이스(PostgreSQL, MySQL, SQLite 등)를 사용할 수 있습니다.\n \n ## 설치\n \n-SQLAlchemy 세션에는 `sqlalchemy` extra가 필요합니다:\n+SQLAlchemy 세션을 사용하려면 `sqlalchemy` extra가 필요합니다:\n \n ```bash\n pip install openai-agents[sqlalchemy]\n@@ -74,7 +74,7 @@ if __name__ == \"__main__\":\n ```\n \n \n-## API 레퍼런스\n+## API 참조\n \n - [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - 주요 클래스\n - [`Session`][agents.memory.session.Session] - 기본 세션 프로토콜\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fsessions%2Fsqlalchemy_session.md",
        "sha": "0086b473f387e821a647b998420ed81e8f96e870",
        "status": "modified"
      },
      {
        "additions": 6,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fstreaming.md",
        "changes": 12,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fstreaming.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 6,
        "filename": "docs/ko/streaming.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # 스트리밍\n \n-스트리밍을 사용하면 에이전트 실행이 진행되는 동안 업데이트를 구독할 수 있습니다. 이는 최종 사용자에게 진행 상황과 부분 응답을 보여주는 데 유용합니다.\n+스트리밍을 통해 에이전트 실행이 진행되는 동안 업데이트를 구독할 수 있습니다. 이는 최종 사용자에게 진행 상황 업데이트와 부분 응답을 보여줄 때 유용합니다.\n \n-스트리밍하려면 [`Runner.run_streamed()`][agents.run.Runner.run_streamed]를 호출하면 되며, 이때 [`RunResultStreaming`][agents.result.RunResultStreaming]이 반환됩니다. `result.stream_events()`를 호출하면 아래에 설명된 [`StreamEvent`][agents.stream_events.StreamEvent] 객체의 비동기 스트림을 받을 수 있습니다.\n+스트리밍하려면 [`Runner.run_streamed()`][agents.run.Runner.run_streamed]를 호출하여 [`RunResultStreaming`][agents.result.RunResultStreaming]을 받습니다. `result.stream_events()`를 호출하면 아래에 설명된 [`StreamEvent`][agents.stream_events.StreamEvent] 객체의 비동기 스트림을 얻을 수 있습니다.\n \n ## 원문 응답 이벤트\n \n-[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent]는 LLM에서 직접 전달되는 원문 이벤트입니다. OpenAI Responses API 형식이며, 각 이벤트에는 유형(예: `response.created`, `response.output_text.delta` 등)과 데이터가 있습니다. 이러한 이벤트는 생성 즉시 사용자에게 응답 메시지를 스트리밍하려는 경우 유용합니다.\n+[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent]는 LLM에서 직접 전달되는 원문 이벤트입니다. OpenAI Responses API 형식이며, 각 이벤트에는 타입(예: `response.created`, `response.output_text.delta` 등)과 데이터가 있습니다. 생성되는 즉시 사용자에게 응답 메시지를 스트리밍하고자 할 때 유용합니다.\n \n-예를 들어, 다음 코드는 LLM이 생성한 텍스트를 토큰 단위로 출력합니다.\n+예를 들어, 다음은 LLM이 생성한 텍스트를 토큰 단위로 출력합니다.\n \n ```python\n import asyncio\n@@ -37,9 +37,9 @@ if __name__ == \"__main__\":\n \n ## 실행 항목 이벤트와 에이전트 이벤트\n \n-[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent]는 상위 수준 이벤트입니다. 항목이 완전히 생성되었을 때 알려 줍니다. 이를 통해 각 토큰이 아닌 \"메시지 생성됨\", \"도구 실행됨\" 수준에서 진행 상황을 전달할 수 있습니다. 마찬가지로, [`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent]는 현재 에이전트가 변경될 때(예: 핸드오프의 결과로) 업데이트를 제공합니다.\n+[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent]는 더 높은 수준의 이벤트입니다. 항목이 완전히 생성되었을 때 알려줍니다. 이를 통해 각 토큰 대신 \"메시지 생성됨\", \"도구 실행됨\" 등의 수준에서 진행 상황 업데이트를 보낼 수 있습니다. 유사하게, [`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent]는 현재 에이전트가 변경될 때(예: 핸드오프의 결과로) 업데이트를 제공합니다.\n \n-예를 들어, 다음 코드는 원문 이벤트를 무시하고 사용자에게 업데이트를 스트리밍합니다.\n+예를 들어, 다음은 원문 이벤트를 무시하고 사용자에게 업데이트를 스트리밍합니다.\n \n ```python\n import asyncio",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fstreaming.md",
        "sha": "303c99b898582181e14ae76bb7fe3c53d7b9bbf0",
        "status": "modified"
      },
      {
        "additions": 70,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Ftools.md",
        "changes": 140,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Ftools.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 70,
        "filename": "docs/ko/tools.md",
        "patch": "@@ -4,22 +4,22 @@ search:\n ---\n # 도구\n \n-도구는 에이전트가 데이터를 가져오고, 코드를 실행하고, 외부 API 를 호출하고, 심지어 컴퓨터를 사용하는 등의 동작을 수행할 수 있게 합니다. SDK 는 네 가지 카테고리를 지원합니다:\n+도구는 에이전트가 동작을 수행하도록 합니다: 데이터 가져오기, 코드 실행, 외부 API 호출, 심지어 컴퓨터 사용과 같은 작업입니다. SDK는 네 가지 카테고리를 지원합니다:\n \n-- OpenAI 호스트하는 도구: OpenAI 서버에서 모델과 함께 실행\n-- 로컬 런타임 도구: 사용자의 환경에서 실행(컴퓨터 사용, 셸, 패치 적용)\n-- 함수 호출: 임의의 Python 함수를 도구로 래핑\n-- 도구로서의 에이전트: 전체 핸드오프 없이 호출 가능한 도구로 에이전트를 노출\n+-   호스팅된 OpenAI 도구: OpenAI 서버에서 모델과 함께 실행\n+-   로컬 런타임 도구: 사용자의 환경에서 실행 (컴퓨터 사용, 셸, 패치 적용)\n+-   함수 호출: 어떤 Python 함수를 도구로 래핑\n+-   도구로서의 에이전트: 전체 핸드오프 없이 에이전트를 호출 가능한 도구로 노출\n \n-## 호스티드 툴\n+## 호스팅된 도구\n \n-OpenAI 는 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] 사용 시 몇 가지 기본 제공 도구를 제공합니다:\n+[`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]을 사용할 때 OpenAI는 몇 가지 기본 제공 도구를 제공합니다:\n \n-- [`WebSearchTool`][agents.tool.WebSearchTool]: 에이전트가 웹 검색을 수행\n-- [`FileSearchTool`][agents.tool.FileSearchTool]: OpenAI 벡터 스토어에서 정보를 검색\n-- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool]: LLM 이 샌드박스 환경에서 코드를 실행\n-- [`HostedMCPTool`][agents.tool.HostedMCPTool]: 원격 MCP 서버의 도구를 모델에 노출\n-- [`ImageGenerationTool`][agents.tool.ImageGenerationTool]: 프롬프트로부터 이미지를 생성\n+-   [`WebSearchTool`][agents.tool.WebSearchTool]: 에이전트가 웹을 검색할 수 있게 합니다.\n+-   [`FileSearchTool`][agents.tool.FileSearchTool]: OpenAI 벡터 스토어에서 정보를 가져올 수 있게 합니다.\n+-   [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool]: LLM이 샌드박스 환경에서 코드를 실행할 수 있게 합니다.\n+-   [`HostedMCPTool`][agents.tool.HostedMCPTool]: 원격 MCP 서버의 도구를 모델에 노출합니다.\n+-   [`ImageGenerationTool`][agents.tool.ImageGenerationTool]: 프롬프트로부터 이미지를 생성합니다.\n \n ```python\n from agents import Agent, FileSearchTool, Runner, WebSearchTool\n@@ -42,11 +42,11 @@ async def main():\n \n ## 로컬 런타임 도구\n \n-로컬 런타임 도구는 사용자의 환경에서 실행되며, 구현을 제공해야 합니다:\n+로컬 런타임 도구는 사용자의 환경에서 실행되며 구현을 제공해야 합니다:\n \n-- [`ComputerTool`][agents.tool.ComputerTool]: GUI/브라우저 자동화를 위해 [`Computer`][agents.computer.Computer] 또는 [`AsyncComputer`][agents.computer.AsyncComputer] 인터페이스를 구현\n-- [`ShellTool`][agents.tool.ShellTool] 또는 [`LocalShellTool`][agents.tool.LocalShellTool]: 명령을 실행할 셸 실행기를 제공\n-- [`ApplyPatchTool`][agents.tool.ApplyPatchTool]: 로컬에 diff 를 적용하기 위해 [`ApplyPatchEditor`][agents.editor.ApplyPatchEditor] 구현\n+-   [`ComputerTool`][agents.tool.ComputerTool]: GUI/브라우저 자동화를 활성화하려면 [`Computer`][agents.computer.Computer] 또는 [`AsyncComputer`][agents.computer.AsyncComputer] 인터페이스를 구현하세요.\n+-   [`ShellTool`][agents.tool.ShellTool] 또는 [`LocalShellTool`][agents.tool.LocalShellTool]: 명령을 실행할 셸 실행기를 제공합니다.\n+-   [`ApplyPatchTool`][agents.tool.ApplyPatchTool]: 로컬로 diff를 적용하려면 [`ApplyPatchEditor`][agents.editor.ApplyPatchEditor]를 구현하세요.\n \n ```python\n from agents import Agent, ApplyPatchTool, ShellTool\n@@ -90,14 +90,14 @@ agent = Agent(\n \n ## 함수 도구\n \n-임의의 Python 함수를 도구로 사용할 수 있습니다. Agents SDK 가 도구 설정을 자동으로 수행합니다:\n+어떤 Python 함수든 도구로 사용할 수 있습니다. Agents SDK가 자동으로 도구를 설정합니다:\n \n-- 도구의 이름은 Python 함수 이름이 됩니다(직접 이름을 지정할 수도 있음)\n-- 도구 설명은 함수의 docstring 에서 가져옵니다(직접 설명을 지정할 수도 있음)\n-- 함수 입력의 스키마는 함수의 인수로부터 자동으로 생성됩니다\n-- 각 입력의 설명은 함수의 docstring 에서 가져오며, 비활성화할 수 있습니다\n+-   도구 이름은 Python 함수 이름이 됩니다 (또는 이름을 직접 제공할 수 있음)\n+-   도구 설명은 함수의 docstring에서 가져옵니다 (또는 설명을 직접 제공할 수 있음)\n+-   함수 입력의 스키마는 함수의 인자에서 자동으로 생성됩니다\n+-   각 입력의 설명은 비활성화하지 않는 한 함수의 docstring에서 가져옵니다\n \n-Python 의 `inspect` 모듈로 함수 시그니처를 추출하고, [`griffe`](https://mkdocstrings.github.io/griffe/) 로 docstring 을 파싱하며, 스키마 생성을 위해 `pydantic` 을 사용합니다.\n+Python의 `inspect` 모듈을 사용해 함수 시그니처를 추출하고, docstring 파싱에는 [`griffe`](https://mkdocstrings.github.io/griffe/), 스키마 생성에는 `pydantic`을 사용합니다.\n \n ```python\n import json\n@@ -149,10 +149,10 @@ for tool in agent.tools:\n \n ```\n \n-1. 함수 인수로 어떤 Python 타입이든 사용할 수 있으며, 함수는 동기 또는 비동기일 수 있습니다\n-2. docstring 이 있으면, 전체 설명과 인수 설명을 추출하는 데 사용됩니다\n-3. 함수는 선택적으로 `context` 를 받을 수 있습니다(첫 번째 인수여야 함). 또한 도구 이름, 설명, 사용할 docstring 스타일 등 오버라이드를 설정할 수 있습니다\n-4. 데코레이트된 함수를 tools 목록에 전달할 수 있습니다\n+1.  함수 인자로 어떤 Python 타입이든 사용할 수 있으며, 함수는 동기 또는 비동기일 수 있습니다.\n+2.  docstring이 있으면 설명과 인자 설명을 캡처하는 데 사용됩니다\n+3.  함수는 선택적으로 `context`를 받을 수 있습니다 (첫 번째 인자여야 함). 또한 도구 이름, 설명, 사용할 docstring 스타일 등 오버라이드를 설정할 수 있습니다.\n+4.  데코레이트된 함수를 도구 목록에 전달할 수 있습니다.\n \n ??? note \"출력을 보려면 펼치기\"\n \n@@ -226,20 +226,20 @@ for tool in agent.tools:\n \n ### 함수 도구에서 이미지 또는 파일 반환\n \n-텍스트 출력 외에도, 함수 도구의 출력으로 하나 이상의 이미지나 파일을 반환할 수 있습니다. 이를 위해 다음 중 하나를 반환할 수 있습니다:\n+텍스트 출력 외에도, 함수 도구의 출력으로 하나 이상의 이미지 또는 파일을 반환할 수 있습니다. 이를 위해 다음 중 아무거나 반환할 수 있습니다:\n \n-- 이미지: [`ToolOutputImage`][agents.tool.ToolOutputImage] (또는 TypedDict 버전, [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict])\n-- 파일: [`ToolOutputFileContent`][agents.tool.ToolOutputFileContent] (또는 TypedDict 버전, [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict])\n-- 텍스트: 문자열 또는 문자열로 변환 가능한 객체, 또는 [`ToolOutputText`][agents.tool.ToolOutputText] (또는 TypedDict 버전, [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict])\n+-   이미지: [`ToolOutputImage`][agents.tool.ToolOutputImage] (또는 TypedDict 버전, [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict])\n+-   파일: [`ToolOutputFileContent`][agents.tool.ToolOutputFileContent] (또는 TypedDict 버전, [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict])\n+-   텍스트: 문자열 또는 문자열로 변환 가능한 객체, 또는 [`ToolOutputText`][agents.tool.ToolOutputText] (또는 TypedDict 버전, [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict])\n \n-### 커스텀 함수 도구\n+### 사용자 지정 함수 도구\n \n-때로는 Python 함수를 도구로 사용하고 싶지 않을 수 있습니다. 이 경우 직접 [`FunctionTool`][agents.tool.FunctionTool] 을 생성할 수 있습니다. 다음을 제공해야 합니다:\n+때로는 Python 함수를 도구로 사용하고 싶지 않을 수 있습니다. 원한다면 직접 [`FunctionTool`][agents.tool.FunctionTool]을 생성할 수 있습니다. 다음을 제공해야 합니다:\n \n-- `name`\n-- `description`\n-- `params_json_schema`: 인수의 JSON 스키마\n-- `on_invoke_tool`: [`ToolContext`][agents.tool_context.ToolContext] 와 JSON 문자열 형태의 인수를 받아 비동기로 실행되며, 도구 출력 문자열을 반환해야 하는 함수\n+-   `name`\n+-   `description`\n+-   `params_json_schema` (인자를 위한 JSON 스키마)\n+-   `on_invoke_tool` (async 함수로서 [`ToolContext`][agents.tool_context.ToolContext]와 JSON 문자열 형태의 인자를 받아, 도구 출력을 문자열로 반환해야 함)\n \n ```python\n from typing import Any\n@@ -272,18 +272,18 @@ tool = FunctionTool(\n )\n ```\n \n-### 인수 및 docstring 자동 파싱\n+### 자동 인자 및 docstring 파싱\n \n-앞서 언급했듯이, 도구의 스키마를 추출하기 위해 함수 시그니처를 자동으로 파싱하고, 도구 및 개별 인수의 설명을 추출하기 위해 docstring 을 파싱합니다. 참고 사항:\n+앞서 언급했듯이, 도구의 스키마를 추출하기 위해 함수 시그니처를 자동으로 파싱하고, 도구 및 개별 인자의 설명을 추출하기 위해 docstring을 파싱합니다. 이에 대한 몇 가지 참고 사항:\n \n-1. 시그니처 파싱은 `inspect` 모듈을 통해 수행됩니다. 인수 타입을 이해하기 위해 타입 힌트를 사용하고, 전체 스키마를 나타내는 Pydantic 모델을 동적으로 생성합니다. Python 기본형, Pydantic 모델, TypedDict 등 대부분의 타입을 지원합니다\n-2. docstring 파싱에는 `griffe` 를 사용합니다. 지원되는 docstring 형식은 `google`, `sphinx`, `numpy` 입니다. docstring 형식은 자동 감지를 시도하지만, 최선의 노력 기준이며 `function_tool` 호출 시 명시적으로 설정할 수 있습니다. `use_docstring_info` 를 `False` 로 설정해 docstring 파싱을 비활성화할 수도 있습니다\n+1. 시그니처 파싱은 `inspect` 모듈을 통해 수행됩니다. 인자의 타입을 이해하기 위해 타입 어노테이션을 사용하고, 전체 스키마를 표현하기 위해 동적으로 Pydantic 모델을 구성합니다. Python 기본 타입, Pydantic 모델, TypedDict 등 대부분의 타입을 지원합니다.\n+2. docstring 파싱에는 `griffe`를 사용합니다. 지원되는 docstring 형식은 `google`, `sphinx`, `numpy`입니다. docstring 형식은 자동 감지를 시도하지만 최선의 노력일 뿐이며, `function_tool` 호출 시 명시적으로 설정할 수 있습니다. `use_docstring_info`를 `False`로 설정해 docstring 파싱을 비활성화할 수도 있습니다.\n \n-스키마 추출 코드는 [`agents.function_schema`][] 에 있습니다.\n+스키마 추출을 위한 코드는 [`agents.function_schema`][]에 있습니다.\n \n ## 도구로서의 에이전트\n \n-일부 워크플로에서는 제어를 넘기지 않고, 중앙 에이전트가 특화된 에이전트 네트워크를 오케스트레이션하도록 하고 싶을 수 있습니다. 에이전트를 도구로 모델링하여 이를 수행할 수 있습니다.\n+일부 워크플로에서는 제어를 핸드오프하는 대신, 중앙 에이전트가 특화된 에이전트 네트워크를 멀티 에이전트 오케스트레이션하도록 하고 싶을 수 있습니다. 에이전트를 도구로 모델링하여 이를 수행할 수 있습니다.\n \n ```python\n from agents import Agent, Runner\n@@ -322,9 +322,9 @@ async def main():\n     print(result.final_output)\n ```\n \n-### 툴 에이전트 커스터마이징\n+### 도구-에이전트 커스터마이징\n \n-`agent.as_tool` 함수는 에이전트를 도구로 쉽게 전환하기 위한 편의 메서드입니다. 그러나 모든 구성을 지원하지는 않습니다. 예를 들어 `max_turns` 를 설정할 수 없습니다. 고급 사용 사례의 경우, 도구 구현에서 `Runner.run` 을 직접 사용하세요:\n+`agent.as_tool` 함수는 에이전트를 도구로 손쉽게 전환하기 위한 편의 메서드입니다. 그러나 모든 구성을 지원하지는 않습니다. 예를 들어 `max_turns`를 설정할 수 없습니다. 고급 사용 사례에서는 도구 구현 내에서 `Runner.run`을 직접 사용하세요:\n \n ```python\n @function_tool\n@@ -343,15 +343,15 @@ async def run_my_agent() -> str:\n     return str(result.final_output)\n ```\n \n-### 맞춤 출력 추출\n+### 사용자 지정 출력 추출\n \n-일부 경우, 중앙 에이전트에 반환하기 전에 툴 에이전트의 출력을 수정하고 싶을 수 있습니다. 예를 들어 다음과 같은 상황에서 유용합니다:\n+특정 경우, 중앙 에이전트에 반환하기 전에 도구-에이전트의 출력을 수정하고 싶을 수 있습니다. 이는 다음과 같은 경우에 유용합니다:\n \n-- 하위 에이전트의 대화 내역에서 특정 정보(예: JSON 페이로드)만 추출\n-- 에이전트의 최종 답변을 변환 또는 재포맷(예: Markdown 을 일반 텍스트나 CSV 로 변환)\n-- 에이전트 응답이 없거나 잘못된 경우 출력을 검증하거나 폴백 값을 제공\n+-   하위 에이전트의 채팅 기록에서 특정 정보(예: JSON 페이로드)를 추출\n+-   에이전트의 최종 답변을 변환 또는 재포맷 (예: Markdown을 일반 텍스트 또는 CSV로 변환)\n+-   출력 검증 또는 에이전트의 응답이 없거나 잘못된 경우 대체 값 제공\n \n-`as_tool` 메서드에 `custom_output_extractor` 인수를 제공하여 이를 수행할 수 있습니다:\n+이를 위해 `as_tool` 메서드에 `custom_output_extractor` 인자를 제공할 수 있습니다:\n \n ```python\n async def extract_json_payload(run_result: RunResult) -> str:\n@@ -372,7 +372,7 @@ json_tool = data_agent.as_tool(\n \n ### 중첩 에이전트 실행 스트리밍\n \n-중첩 에이전트가 내보내는 스트리밍 이벤트를 수신하면서, 스트림 완료 후 최종 출력도 반환받을 수 있도록 `as_tool` 에 `on_stream` 콜백을 전달하세요.\n+중첩된 에이전트가 스트리밍 중에 내보내는 이벤트를 수신하면서 스트림 완료 후 최종 출력도 반환받으려면, `as_tool`에 `on_stream` 콜백을 전달하세요.\n \n ```python\n from agents import AgentToolStreamEvent\n@@ -390,17 +390,17 @@ billing_agent_tool = billing_agent.as_tool(\n )\n ```\n \n-예상 동작:\n+예상되는 사항:\n \n-- 이벤트 타입은 `StreamEvent[\"type\"]` 을 반영합니다: `raw_response_event`, `run_item_stream_event`, `agent_updated_stream_event`\n-- `on_stream` 을 제공하면 중첩 에이전트가 자동으로 스트리밍 모드로 실행되고, 최종 출력을 반환하기 전에 스트림을 모두 소모합니다\n+- 이벤트 타입은 `StreamEvent[\"type\"]`과 동일합니다: `raw_response_event`, `run_item_stream_event`, `agent_updated_stream_event`\n+- `on_stream`을 제공하면 자동으로 중첩 에이전트가 스트리밍 모드로 실행되며, 최종 출력을 반환하기 전에 스트림을 모두 소비합니다\n - 핸들러는 동기 또는 비동기일 수 있으며, 각 이벤트는 도착 순서대로 전달됩니다\n-- 도구가 모델의 tool call 을 통해 호출될 때는 `tool_call_id` 가 존재합니다. 직접 호출의 경우 `None` 일 수 있습니다\n-- 완전한 실행 가능한 예시는 `examples/agent_patterns/agents_as_tools_streaming.py` 를 참고하세요\n+- 도구가 모델의 tool call을 통해 호출되는 경우 `tool_call_id`가 존재합니다. 직접 호출인 경우 `None`일 수 있습니다\n+- 완전한 실행 가능한 샘플은 `examples/agent_patterns/agents_as_tools_streaming.py`를 참조하세요\n \n ### 조건부 도구 활성화\n \n-런타임에 `is_enabled` 매개변수를 사용하여 에이전트 도구를 조건부로 활성화하거나 비활성화할 수 있습니다. 이를 통해 컨텍스트, 사용자 선호도, 런타임 조건에 따라 LLM 에 제공되는 도구를 동적으로 필터링할 수 있습니다.\n+런타임에 `is_enabled` 매개변수를 사용해 에이전트 도구를 조건부로 활성화 또는 비활성화할 수 있습니다. 이를 통해 컨텍스트, 사용자 선호도, 런타임 조건에 따라 LLM에서 사용 가능한 도구를 동적으로 필터링할 수 있습니다.\n \n ```python\n import asyncio\n@@ -457,24 +457,24 @@ asyncio.run(main())\n \n `is_enabled` 매개변수는 다음을 허용합니다:\n \n-- **Boolean 값**: `True`(항상 활성), `False`(항상 비활성)\n-- **호출 가능한 함수**: `(context, agent)` 를 받아 boolean 을 반환하는 함수\n-- **비동기 함수**: 복잡한 조건 로직을 위한 async 함수\n+-   **불리언 값**: `True`(항상 활성) 또는 `False`(항상 비활성)\n+-   **호출 가능한 함수**: `(context, agent)`를 받아 불리언을 반환하는 함수\n+-   **비동기 함수**: 복잡한 조건부 로직을 위한 async 함수\n \n-비활성화된 도구는 런타임에 LLM 에 완전히 숨겨지므로 다음과 같은 용도에 유용합니다:\n+비활성화된 도구는 런타임에 LLM에 완전히 숨겨지므로 다음에 유용합니다:\n \n-- 사용자 권한 기반 기능 게이팅\n-- 환경별 도구 가용성(dev vs prod)\n-- 서로 다른 도구 구성을 A/B 테스트\n-- 런타임 상태 기반 동적 도구 필터링\n+-   사용자 권한에 따른 기능 게이팅\n+-   환경별 도구 가용성 (개발 vs 운영)\n+-   다양한 도구 구성을 A/B 테스트\n+-   런타임 상태에 따른 동적 도구 필터링\n \n-## 함수 도구의 오류 처리\n+## 함수 도구에서의 오류 처리\n \n-`@function_tool` 로 함수 도구를 만들 때 `failure_error_function` 을 전달할 수 있습니다. 이는 도구 호출이 크래시할 경우 LLM 에 오류 응답을 제공하는 함수입니다.\n+`@function_tool`로 함수 도구를 만들 때, `failure_error_function`을 전달할 수 있습니다. 이는 도구 호출이 크래시한 경우 LLM에 오류 응답을 제공하는 함수입니다.\n \n-- 기본적으로(아무 것도 전달하지 않은 경우) 오류가 발생했음을 LLM 에 알리는 `default_tool_error_function` 이 실행됩니다\n-- 사용자 지정 오류 함수를 전달하면, 해당 함수가 대신 실행되어 응답이 LLM 으로 전송됩니다\n-- 명시적으로 `None` 을 전달하면, 도구 호출 오류가 다시 발생하여 사용자가 처리해야 합니다. 모델이 잘못된 JSON 을 생성한 경우 `ModelBehaviorError`, 사용자 코드가 크래시한 경우 `UserError` 등이 될 수 있습니다\n+-   기본적으로 (아무 것도 전달하지 않으면) 오류가 발생했음을 LLM에 알리는 `default_tool_error_function`을 실행합니다.\n+-   자체 오류 함수를 전달하면, 그 함수를 대신 실행하고 해당 응답을 LLM에 보냅니다.\n+-   명시적으로 `None`을 전달하면, 도구 호출 오류가 다시 발생하여 호출 측에서 처리해야 합니다. 모델이 잘못된 JSON을 생성한 경우 `ModelBehaviorError`, 사용자 코드가 크래시한 경우 `UserError` 등이 될 수 있습니다.\n \n ```python\n from agents import function_tool, RunContextWrapper",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Ftools.md",
        "sha": "7395c862bc6e8326bd2ee54c49569c1a2815ac8b",
        "status": "modified"
      },
      {
        "additions": 46,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Ftracing.md",
        "changes": 91,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Ftracing.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 45,
        "filename": "docs/ko/tracing.md",
        "patch": "@@ -4,52 +4,52 @@ search:\n ---\n # 트레이싱\n \n-Agents SDK에는 빌트인 트레이싱이 포함되어 있어 에이전트 실행 중 발생하는 이벤트에 대한 포괄적인 기록을 수집합니다: LLM 생성, 도구 호출, 핸드오프, 가드레일, 그리고 커스텀 이벤트까지 포함합니다. [Traces 대시보드](https://platform.openai.com/traces)를 사용하면 개발 및 프로덕션 환경에서 워크플로를 디버그하고, 시각화하며, 모니터링할 수 있습니다.\n+Agents SDK에는 빌트인 트레이싱이 포함되어 있어 에이전트 실행 중 발생하는 이벤트의 포괄적인 기록을 수집합니다: LLM 생성, 도구 호출, 핸드오프, 가드레일, 그리고 커스텀 이벤트까지. [Traces 대시보드](https://platform.openai.com/traces)를 사용해 개발 및 운영 환경에서 워크플로를 디버그하고, 시각화하며, 모니터링할 수 있습니다.\n \n !!!note\n \n-    트레이싱은 기본적으로 활성화되어 있습니다. 트레이싱을 비활성화하는 방법은 두 가지가 있습니다:\n+    트레이싱은 기본적으로 활성화되어 있습니다. 트레이싱을 비활성화하는 방법은 두 가지입니다:\n \n-    1. 환경 변수 `OPENAI_AGENTS_DISABLE_TRACING=1` 를 설정하여 전역적으로 비활성화할 수 있습니다\n-    2. [`agents.run.RunConfig.tracing_disabled`][] 를 `True` 로 설정하여 단일 실행에 대해서만 비활성화할 수 있습니다\n+    1. 환경 변수 `OPENAI_AGENTS_DISABLE_TRACING=1` 을 설정해 전역으로 비활성화할 수 있습니다\n+    2. 단일 실행에 대해서만 비활성화하려면 [`agents.run.RunConfig.tracing_disabled`][] 를 `True` 로 설정하세요\n \n-***OpenAI 의 API 를 사용하는 Zero Data Retention (ZDR) 정책 하의 조직에서는 트레이싱을 사용할 수 없습니다.***\n+***OpenAI API 를 사용하면서 Zero Data Retention(ZDR) 정책 하에 운영되는 조직의 경우, 트레이싱을 사용할 수 없습니다.***\n \n ## 트레이스와 스팬\n \n--   **트레이스(Traces)** 는 \"워크플로\"의 단일 엔드투엔드 작업을 나타냅니다. 스팬으로 구성됩니다. 트레이스에는 다음 속성이 있습니다:\n-    -   `workflow_name`: 논리적 워크플로 또는 앱 이름입니다. 예: \"Code generation\" 또는 \"Customer service\"\n-    -   `trace_id`: 트레이스의 고유 ID 입니다. 전달하지 않으면 자동 생성됩니다. 형식은 `trace_<32_alphanumeric>` 여야 합니다\n-    -   `group_id`: 동일한 대화에서 여러 트레이스를 연결하기 위한 선택적 그룹 ID 입니다. 예를 들어 채팅 스레드 ID 를 사용할 수 있습니다\n-    -   `disabled`: True 이면 트레이스가 기록되지 않습니다\n-    -   `metadata`: 트레이스의 선택적 메타데이터\n--   **스팬(Spans)** 은 시작 및 종료 시간이 있는 작업을 나타냅니다. 스팬에는 다음이 있습니다:\n+-   **트레이스(Traces)** 는 하나의 \"워크플로\"에 대한 단일 엔드 투 엔드 작업을 나타냅니다. 여러 스팬으로 구성됩니다. 트레이스는 다음 속성을 가집니다:\n+    -   `workflow_name`: 논리적 워크플로나 앱 이름입니다. 예: \"Code generation\" 또는 \"Customer service\"\n+    -   `trace_id`: 트레이스의 고유 ID 입니다. 전달하지 않으면 자동 생성됩니다. 형식은 `trace_<32_alphanumeric>` 이어야 합니다\n+    -   `group_id`: 선택적 그룹 ID 로, 동일한 대화에서 나온 여러 트레이스를 연결합니다. 예를 들어 채팅 스레드 ID 를 사용할 수 있습니다\n+    -   `disabled`: True 인 경우 트레이스가 기록되지 않습니다\n+    -   `metadata`: 트레이스에 대한 선택적 메타데이터\n+-   **스팬(Spans)** 은 시작 및 종료 시간이 있는 작업을 나타냅니다. 스팬은 다음을 가집니다:\n     -   `started_at` 및 `ended_at` 타임스탬프\n-    -   `trace_id`: 소속된 트레이스를 나타냄\n-    -   `parent_id`: 이 스팬의 상위 스팬을 가리킴(있는 경우)\n-    -   `span_data`: 스팬에 대한 정보입니다. 예를 들어, `AgentSpanData` 는 에이전트에 대한 정보를, `GenerationSpanData` 는 LLM 생성에 대한 정보를 포함합니다\n+    -   `trace_id`, 해당 스팬이 속한 트레이스를 나타냅니다\n+    -   `parent_id`, 이 스팬의 부모 스팬(있는 경우)을 가리킵니다\n+    -   `span_data`, 스팬에 대한 정보입니다. 예를 들어 `AgentSpanData` 는 에이전트에 대한 정보를, `GenerationSpanData` 는 LLM 생성에 대한 정보를 포함합니다\n \n ## 기본 트레이싱\n \n-기본적으로, SDK 는 다음을 트레이싱합니다:\n+기본적으로 SDK 는 다음을 트레이싱합니다:\n \n--   전체 `Runner.{run, run_sync, run_streamed}()` 이 `trace()` 로 래핑됩니다\n--   에이전트가 실행될 때마다 `agent_span()` 으로 래핑됩니다\n--   LLM 생성은 `generation_span()` 으로 래핑됩니다\n--   함수 도구 호출은 각각 `function_span()` 으로 래핑됩니다\n--   가드레일은 `guardrail_span()` 으로 래핑됩니다\n--   핸드오프는 `handoff_span()` 으로 래핑됩니다\n--   오디오 입력(음성-텍스트)은 `transcription_span()` 으로 래핑됩니다\n--   오디오 출력(텍스트-음성)은 `speech_span()` 으로 래핑됩니다\n--   관련 오디오 스팬은 `speech_group_span()` 아래에 부모-자식 관계로 구성될 수 있습니다\n+-   전체 `Runner.{run, run_sync, run_streamed}()` 가 `trace()` 로 래핑됨\n+-   에이전트가 실행될 때마다 `agent_span()` 으로 래핑됨\n+-   LLM 생성은 `generation_span()` 으로 래핑됨\n+-   함수 도구 호출은 각각 `function_span()` 으로 래핑됨\n+-   가드레일은 `guardrail_span()` 으로 래핑됨\n+-   핸드오프는 `handoff_span()` 으로 래핑됨\n+-   오디오 입력(음성-텍스트)은 `transcription_span()` 으로 래핑됨\n+-   오디오 출력(텍스트-음성)은 `speech_span()` 으로 래핑됨\n+-   관련 오디오 스팬은 `speech_group_span()` 아래에 부모-자식 관계로 묶일 수 있음\n \n-기본적으로 트레이스 이름은 \"Agent workflow\" 입니다. `trace` 를 사용해 이 이름을 설정할 수 있으며, 또는 [`RunConfig`][agents.run.RunConfig] 로 이름 및 기타 속성을 구성할 수 있습니다.\n+기본적으로 트레이스 이름은 \"Agent workflow\" 입니다. `trace` 를 사용할 경우 이 이름을 설정할 수 있으며, 또는 [`RunConfig`][agents.run.RunConfig] 로 이름 및 기타 속성을 구성할 수 있습니다.\n \n-또한, [사용자 지정 트레이스 프로세서](#custom-tracing-processors)를 설정하여 트레이스를 다른 목적지로 푸시할 수 있습니다(대체 또는 보조 목적지로).\n+또한 [커스텀 트레이스 프로세서](#custom-tracing-processors)를 설정하여 트레이스를 다른 대상에 전송할 수 있습니다(대체 또는 보조 대상).\n \n ## 상위 수준 트레이스\n \n-때때로 여러 번의 `run()` 호출을 단일 트레이스의 일부로 만들고 싶을 수 있습니다. 이 경우 전체 코드를 `trace()` 로 래핑하면 됩니다.\n+때로는 여러 `run()` 호출을 하나의 트레이스에 포함시키고 싶을 수 있습니다. 이 경우 전체 코드를 `trace()` 로 래핑하면 됩니다.\n \n ```python\n from agents import Agent, Runner, trace\n@@ -64,48 +64,49 @@ async def main():\n         print(f\"Rating: {second_result.final_output}\")\n ```\n \n-1. `Runner.run` 에 대한 두 번의 호출이 `with trace()` 로 래핑되어 있으므로, 개별 실행은 두 개의 트레이스를 만드는 대신 전체 트레이스의 일부가 됩니다.\n+1. `Runner.run` 에 대한 두 번의 호출이 `with trace()` 로 래핑되어 있으므로, 개별 실행이 두 개의 트레이스를 만들지 않고 전체 트레이스의 일부가 됩니다.\n \n ## 트레이스 생성\n \n [`trace()`][agents.tracing.trace] 함수를 사용해 트레이스를 생성할 수 있습니다. 트레이스는 시작과 종료가 필요합니다. 다음 두 가지 방법이 있습니다:\n \n-1. 권장: 컨텍스트 매니저로 사용합니다. 예: `with trace(...) as my_trace`. 이렇게 하면 적절한 시점에 트레이스가 자동으로 시작되고 종료됩니다\n+1. **권장**: 컨텍스트 매니저로 사용합니다. 즉, `with trace(...) as my_trace`. 적절한 시점에 트레이스를 자동으로 시작/종료합니다\n 2. 수동으로 [`trace.start()`][agents.tracing.Trace.start] 와 [`trace.finish()`][agents.tracing.Trace.finish] 를 호출할 수도 있습니다\n \n-현재 트레이스는 Python 의 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 를 통해 추적됩니다. 따라서 자동으로 동시성에 대응합니다. 트레이스를 수동으로 시작/종료하는 경우, 현재 트레이스를 업데이트하기 위해 `start()`/`finish()` 에 `mark_as_current` 및 `reset_current` 를 전달해야 합니다.\n+현재 트레이스는 Python 의 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 를 통해 추적됩니다. 이는 자동으로 동시성에서 동작함을 의미합니다. 트레이스를 수동으로 시작/종료하는 경우, 현재 트레이스를 업데이트하기 위해 `start()`/`finish()` 에 `mark_as_current` 및 `reset_current` 를 전달해야 합니다.\n \n ## 스팬 생성\n \n-다양한 [`*_span()`][agents.tracing.create] 메서드를 사용해 스팬을 생성할 수 있습니다. 일반적으로 스팬을 수동으로 생성할 필요는 없습니다. 커스텀 스팬 정보를 추적하기 위해 [`custom_span()`][agents.tracing.custom_span] 함수가 제공됩니다.\n+여러 [`*_span()`][agents.tracing.create] 메서드를 사용해 스팬을 생성할 수 있습니다. 일반적으로 스팬을 수동으로 만들 필요는 없습니다. 커스텀 스팬 정보를 추적하기 위해 [`custom_span()`][agents.tracing.custom_span] 함수가 제공됩니다.\n \n-스팬은 자동으로 현재 트레이스의 일부가 되며, Python 의 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 로 추적되는 가장 가까운 현재 스팬 아래에 중첩됩니다.\n+스팬은 자동으로 현재 트레이스의 일부가 되며, Python 의 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 를 통해 추적되는 가장 가까운 현재 스팬 아래에 중첩됩니다.\n \n ## 민감한 데이터\n \n 일부 스팬은 잠재적으로 민감한 데이터를 캡처할 수 있습니다.\n \n-`generation_span()` 은 LLM 생성의 입력/출력을 저장하며, `function_span()` 은 함수 호출의 입력/출력을 저장합니다. 민감한 데이터가 포함될 수 있으므로, [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] 를 통해 해당 데이터 캡처를 비활성화할 수 있습니다.\n+`generation_span()` 은 LLM 생성의 입력/출력을 저장하고, `function_span()` 은 함수 호출의 입력/출력을 저장합니다. 민감한 데이터가 포함될 수 있으므로, [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] 를 통해 해당 데이터 캡처를 비활성화할 수 있습니다.\n \n-마찬가지로, 오디오 스팬은 기본적으로 입력 및 출력 오디오에 대해 base64 로 인코딩된 PCM 데이터를 포함합니다. [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] 를 구성해 이 오디오 데이터 캡처를 비활성화할 수 있습니다.\n+마찬가지로, 오디오 스팬은 기본적으로 입력 및 출력 오디오에 대해 base64 로 인코딩된 PCM 데이터를 포함합니다. [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] 를 구성하여 이 오디오 데이터 캡처를 비활성화할 수 있습니다.\n \n-기본적으로 `trace_include_sensitive_data` 는 `True` 입니다. 앱을 실행하기 전에 `OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA` 환경 변수를 `true/1` 또는 `false/0` 로 설정하여 코드 없이 기본값을 구성할 수 있습니다.\n+기본적으로 `trace_include_sensitive_data` 는 `True` 입니다. 코드 변경 없이 기본값을 설정하려면 앱을 실행하기 전에 환경 변수 `OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA` 를 `true/1` 또는 `false/0` 으로 내보내세요.\n \n-## 사용자 지정 트레이싱 프로세서\n+## 커스텀 트레이싱 프로세서\n \n 트레이싱의 상위 수준 아키텍처는 다음과 같습니다:\n \n -   초기화 시, 트레이스를 생성하는 역할을 하는 전역 [`TraceProvider`][agents.tracing.setup.TraceProvider] 를 생성합니다\n--   `TraceProvider` 에 [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] 를 구성하여 스팬과 트레이스를 배치로 [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter] 에 전송하며, 이는 OpenAI 백엔드로 스팬과 트레이스를 배치 전송합니다\n+-   트레이스/스팬을 배치로 [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter] 에 전송하는 [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor] 로 `TraceProvider` 를 구성하며, 이 Exporter 는 스팬과 트레이스를 OpenAI 백엔드로 배치 전송합니다\n \n-이 기본 설정을 사용자 지정하여, 대체 또는 추가 백엔드로 트레이스를 전송하거나 익스포터 동작을 수정하려는 경우 두 가지 옵션이 있습니다:\n+기본 설정을 사용자 정의하여, 대체 또는 추가 백엔드로 트레이스를 전송하거나 Exporter 동작을 수정하려면 다음 두 가지 옵션이 있습니다:\n \n-1. [`add_trace_processor()`][agents.tracing.add_trace_processor] 를 사용하면 트레이스와 스팬이 준비되는 대로 수신할 **추가적인** 트레이스 프로세서를 추가할 수 있습니다. 이를 통해 OpenAI 백엔드로 트레이스를 전송하는 것에 더해 자체 처리를 수행할 수 있습니다\n-2. [`set_trace_processors()`][agents.tracing.set_trace_processors] 를 사용하면 기본 프로세서를 사용자 지정 트레이스 프로세서로 **교체** 할 수 있습니다. 이 경우 OpenAI 백엔드로 트레이스가 전송되지 않으며, 이를 수행하는 `TracingProcessor` 를 포함한 경우에만 전송됩니다\n+1. [`add_trace_processor()`][agents.tracing.add_trace_processor] 는 트레이스와 스팬이 준비되는 대로 수신할 **추가** 트레이스 프로세서를 추가할 수 있습니다. 이를 통해 트레이스를 OpenAI 백엔드로 전송하는 것에 더해 자체 처리도 수행할 수 있습니다\n+2. [`set_trace_processors()`][agents.tracing.set_trace_processors] 는 기본 프로세서를 사용자 정의 트레이스 프로세서로 **교체** 할 수 있습니다. 즉, 해당 작업을 수행하는 `TracingProcessor` 를 포함하지 않으면 트레이스가 OpenAI 백엔드로 전송되지 않습니다\n \n-## Non-OpenAI 모델과의 트레이싱\n \n-OpenAI 의 API 키를 Non-OpenAI 모델과 함께 사용하여 트레이싱을 비활성화할 필요 없이 OpenAI Traces 대시보드에서 무료 트레이싱을 활성화할 수 있습니다.\n+## OpenAI 이외 모델과의 트레이싱\n+\n+OpenAI 의 API 키를 비 OpenAI 모델과 함께 사용하여, 트레이싱을 비활성화할 필요 없이 OpenAI Traces 대시보드에서 무료 트레이싱을 활성화할 수 있습니다.\n \n ```python\n import os\n@@ -126,7 +127,7 @@ agent = Agent(\n )\n ```\n \n-단일 실행에 대해서만 다른 트레이싱 키가 필요하다면, 전역 익스포터를 변경하는 대신 `RunConfig` 를 통해 전달하세요.\n+단일 실행에 대해서만 다른 트레이싱 키가 필요하다면, 전역 Exporter 를 변경하는 대신 `RunConfig` 를 통해 전달하세요.\n \n ```python\n from agents import Runner, RunConfig\n@@ -139,7 +140,7 @@ await Runner.run(\n ```\n \n ## 참고\n-- OpenAI Traces 대시보드에서 무료 트레이스를 확인하세요\n+- OpenAI Traces 대시보드에서 무료 트레이스를 확인하세요.\n \n ## 외부 트레이싱 프로세서 목록\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Ftracing.md",
        "sha": "1df3614d454f4b099469070d91dc950c95e27510",
        "status": "modified"
      },
      {
        "additions": 16,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fusage.md",
        "changes": 32,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fusage.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 16,
        "filename": "docs/ko/usage.md",
        "patch": "@@ -4,22 +4,22 @@ search:\n ---\n # 사용량\n \n-Agents SDK는 모든 실행에 대해 토큰 사용량을 자동으로 추적합니다. 실행 컨텍스트에서 사용량에 접근하여 비용 모니터링, 한도 적용, 분석 기록에 활용할 수 있습니다.\n+Agents SDK는 실행마다 토큰 사용량을 자동으로 추적합니다. 실행 컨텍스트에서 이를 확인하여 비용을 모니터링하고, 제한을 적용하거나, 분석을 기록할 수 있습니다.\n \n ## 추적 항목\n \n - **requests**: 수행된 LLM API 호출 수\n-- **input_tokens**: 전송된 총 입력 토큰 수\n-- **output_tokens**: 수신된 총 출력 토큰 수\n+- **input_tokens**: 전송된 입력 토큰 총합\n+- **output_tokens**: 수신된 출력 토큰 총합\n - **total_tokens**: 입력 + 출력\n-- **request_usage_entries**: 요청별 사용량 분해 목록\n+- **request_usage_entries**: 요청별 사용량 내역 목록\n - **details**:\n   - `input_tokens_details.cached_tokens`\n   - `output_tokens_details.reasoning_tokens`\n \n-## 실행에서 사용량 조회\n+## 실행에서 사용량 접근\n \n-`Runner.run(...)` 이후, `result.context_wrapper.usage`를 통해 사용량에 접근합니다.\n+`Runner.run(...)` 이후, `result.context_wrapper.usage`를 통해 사용량을 확인합니다.\n \n ```python\n result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n@@ -31,11 +31,11 @@ print(\"Output tokens:\", usage.output_tokens)\n print(\"Total tokens:\", usage.total_tokens)\n ```\n \n-실행 중의 모든 모델 호출(도구 호출과 핸드오프 포함)에 걸쳐 사용량이 집계됩니다.\n+실행 중의 모든 모델 호출에 걸쳐 사용량이 집계됩니다(도구 호출 및 핸드오프 포함).\n \n ### LiteLLM 모델에서 사용량 활성화\n \n-LiteLLM 공급자는 기본적으로 사용량 메트릭을 보고하지 않습니다. [`LitellmModel`](models/litellm.md)을 사용할 때, 에이전트에 `ModelSettings(include_usage=True)`를 전달하면 LiteLLM 응답이 `result.context_wrapper.usage`에 채워집니다.\n+기본적으로 LiteLLM 공급자는 사용량 지표를 보고하지 않습니다. [`LitellmModel`](models/litellm.md)을 사용할 때, 에이전트에 `ModelSettings(include_usage=True)`를 전달하면 LiteLLM 응답이 `result.context_wrapper.usage`에 채워집니다.\n \n ```python\n from agents import Agent, ModelSettings, Runner\n@@ -53,7 +53,7 @@ print(result.context_wrapper.usage.total_tokens)\n \n ## 요청별 사용량 추적\n \n-SDK는 `request_usage_entries`에서 각 API 요청에 대한 사용량을 자동으로 추적하여, 상세 비용 계산과 컨텍스트 윈도 소비 모니터링에 유용합니다.\n+SDK는 `request_usage_entries`에서 각 API 요청의 사용량을 자동으로 추적하여, 상세 비용 계산과 컨텍스트 윈도우 소비 모니터링에 유용합니다.\n \n ```python\n result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n@@ -62,9 +62,9 @@ for i, request in enumerate(result.context_wrapper.usage.request_usage_entries):\n     print(f\"Request {i + 1}: {request.input_tokens} in, {request.output_tokens} out\")\n ```\n \n-## 세션에서 사용량 조회\n+## 세션에서 사용량 접근\n \n-`Session`(예: `SQLiteSession`)을 사용할 때는 `Runner.run(...)` 호출마다 해당 실행의 사용량이 반환됩니다. 세션은 컨텍스트를 위한 대화 기록을 유지하지만, 각 실행의 사용량은 독립적입니다.\n+`Session`(예: `SQLiteSession`)을 사용할 때, `Runner.run(...)` 호출마다 해당 실행의 사용량이 반환됩니다. 세션은 컨텍스트를 위한 대화 기록을 유지하지만, 각 실행의 사용량은 독립적입니다.\n \n ```python\n session = SQLiteSession(\"my_conversation\")\n@@ -76,11 +76,11 @@ second = await Runner.run(agent, \"Can you elaborate?\", session=session)\n print(second.context_wrapper.usage.total_tokens)  # Usage for second run\n ```\n \n-세션은 실행 간 대화 컨텍스트를 보존하지만, 각 `Runner.run()` 호출이 반환하는 사용량 메트릭은 그 실행만을 나타냅니다. 세션에서는 이전 메시지가 매번 입력으로 다시 제공될 수 있으며, 이는 이후 턴의 입력 토큰 수에 영향을 줍니다.\n+세션은 실행 간 대화 컨텍스트를 보존하지만, 각 `Runner.run()` 호출에서 반환되는 사용량 지표는 해당 실행에만 해당합니다. 세션에서는 이전 메시지가 각 실행의 입력으로 다시 제공될 수 있으며, 이는 이후 턴의 입력 토큰 수에 영향을 줍니다.\n \n-## 훅에서 사용량 활용\n+## 훅에서 사용량 사용\n \n-`RunHooks`를 사용하는 경우, 각 훅에 전달되는 `context` 객체에는 `usage`가 포함됩니다. 이를 통해 수명주기의 주요 지점에서 사용량을 로깅할 수 있습니다.\n+`RunHooks`를 사용하는 경우, 각 훅에 전달되는 `context` 객체에 `usage`가 포함됩니다. 이를 통해 수명 주기의 주요 시점에 사용량을 기록할 수 있습니다.\n \n ```python\n class MyHooks(RunHooks):\n@@ -89,11 +89,11 @@ class MyHooks(RunHooks):\n         print(f\"{agent.name} → {u.requests} requests, {u.total_tokens} total tokens\")\n ```\n \n-## API 레퍼런스\n+## API 참조\n \n 자세한 API 문서는 다음을 참조하세요:\n \n -   [`Usage`][agents.usage.Usage] - 사용량 추적 데이터 구조\n -   [`RequestUsage`][agents.usage.RequestUsage] - 요청별 사용량 상세\n -   [`RunContextWrapper`][agents.run.RunContextWrapper] - 실행 컨텍스트에서 사용량 접근\n--   [`RunHooks`][agents.run.RunHooks] - 사용량 추적 수명주기에 훅 연결\n\\ No newline at end of file\n+-   [`RunHooks`][agents.run.RunHooks] - 사용량 추적 수명 주기에 훅 연결\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fusage.md",
        "sha": "4be8b56dfdd4c1c72f2d058ab22def5cfc0164a3",
        "status": "modified"
      },
      {
        "additions": 18,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvisualization.md",
        "changes": 36,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fvisualization.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 18,
        "filename": "docs/ko/visualization.md",
        "patch": "@@ -4,24 +4,24 @@ search:\n ---\n # 에이전트 시각화\n \n-에이전트 시각화는 **Graphviz**를 사용해 에이전트와 그 관계를 구조화된 그래픽으로 생성합니다. 이는 애플리케이션 내에서 에이전트, 도구, 핸드오프가 어떻게 상호작용하는지 이해하는 데 유용합니다.\n+에이전트 시각화는 **Graphviz**를 사용해 에이전트와 그 관계를 구조화된 그래프 형태로 생성합니다. 이는 애플리케이션 내에서 에이전트, 도구, 핸드오프가 어떻게 상호작용하는지 이해하는 데 유용합니다.\n \n ## 설치\n \n-선택적 `viz` 종속성 그룹을 설치합니다:\n+선택적 `viz` 종속성 그룹을 설치하세요:\n \n ```bash\n pip install \"openai-agents[viz]\"\n ```\n \n ## 그래프 생성\n \n-`draw_graph` 함수를 사용해 에이전트 시각화를 생성할 수 있습니다. 이 함수는 다음과 같은 방향 그래프를 만듭니다:\n+`draw_graph` 함수를 사용하여 에이전트 시각화를 생성할 수 있습니다. 이 함수는 다음과 같이 구성된 방향 그래프를 생성합니다:\n \n-- **에이전트**는 노란색 상자로 표시됩니다\n-- **MCP 서버**는 회색 상자로 표시됩니다\n-- **도구**는 초록색 타원으로 표시됩니다\n-- **핸드오프**는 한 에이전트에서 다른 에이전트로 향하는 방향 간선입니다\n+- **에이전트**는 노란색 상자로 표시됨\n+- **MCP 서버**는 회색 상자로 표시됨\n+- **도구**는 초록색 타원으로 표시됨\n+- **핸드오프**는 한 에이전트에서 다른 에이전트로 향하는 간선으로 표시됨\n \n ### 사용 예시\n \n@@ -69,30 +69,30 @@ draw_graph(triage_agent)\n \n ![Agent Graph](../assets/images/graph.png)\n \n-이는 **triage agent**의 구조와 하위 에이전트 및 도구와의 연결을 시각적으로 표현하는 그래프를 생성합니다.\n+이는 **트리아지 에이전트**의 구조와 하위 에이전트 및 도구와의 연결을 시각적으로 나타내는 그래프를 생성합니다.\n \n \n ## 시각화 이해\n \n 생성된 그래프에는 다음이 포함됩니다:\n \n-- 진입점을 나타내는 **시작 노드** (`__start__`)\n-- 노란색 채우기의 **사각형**으로 표시된 에이전트\n-- 초록색 채우기의 **타원**으로 표시된 도구\n-- 회색 채우기의 **사각형**으로 표시된 MCP 서버\n+- 진입점을 나타내는 **시작 노드**(`__start__`)\n+- 노란색 채우기의 **직사각형**으로 표시되는 에이전트\n+- 초록색 채우기의 **타원**으로 표시되는 도구\n+- 회색 채우기의 **직사각형**으로 표시되는 MCP 서버\n - 상호작용을 나타내는 방향 간선:\n   - 에이전트 간 핸드오프는 **실선 화살표**\n   - 도구 호출은 **점선 화살표**\n   - MCP 서버 호출은 **파선 화살표**\n-- 실행 종료 지점을 나타내는 **종료 노드** (`__end__`)\n+- 실행 종료 지점을 나타내는 **종료 노드**(`__end__`)\n \n-**참고:** MCP 서버는 최신 버전의\n-`agents` 패키지에서 렌더링됩니다 ( **v0.2.8**에서 확인됨). 시각화에서 MCP 상자가 보이지 않는 경우 최신 릴리스로 업그레이드하세요.\n+**참고:** MCP 서버는 최근 버전의\n+`agents` 패키지에서 렌더링됩니다(**v0.2.8**에서 확인됨). 시각화에 MCP 상자가 보이지 않는다면 최신 릴리스로 업그레이드하세요.\n \n-## 그래프 사용자 지정\n+## 그래프 커스터마이징\n \n ### 그래프 표시\n-기본적으로 `draw_graph`는 그래프를 인라인으로 표시합니다. 그래프를 별도 창에서 표시하려면 다음을 작성하세요:\n+기본적으로 `draw_graph`는 그래프를 인라인으로 표시합니다. 그래프를 별도 창에서 표시하려면 다음과 같이 작성하세요:\n \n ```python\n draw_graph(triage_agent).view()\n@@ -105,4 +105,4 @@ draw_graph(triage_agent).view()\n draw_graph(triage_agent, filename=\"agent_graph\")\n ```\n \n-그러면 작업 디렉터리에 `agent_graph.png`가 생성됩니다.\n\\ No newline at end of file\n+이 경우 작업 디렉터리에 `agent_graph.png`가 생성됩니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvisualization.md",
        "sha": "28bfceace98e30a8d1b5f565e4f566392772f6dd",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvoice%2Fpipeline.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fvoice%2Fpipeline.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/ko/voice/pipeline.md",
        "patch": "@@ -2,9 +2,9 @@\n search:\n   exclude: true\n ---\n-# 파이프라인과 워크플로우\n+# 파이프라인과 워크플로\n \n-[`VoicePipeline`][agents.voice.pipeline.VoicePipeline]은 에이전트형 워크플로우를 음성 앱으로 쉽게 전환할 수 있게 해 주는 클래스입니다. 실행할 워크플로우를 전달하면, 파이프라인이 입력 오디오를 음성 인식하고, 오디오 종료를 감지하며, 적절한 시점에 워크플로우를 호출하고, 워크플로우 출력을 다시 오디오로 변환하는 일을 처리합니다.\n+[`VoicePipeline`][agents.voice.pipeline.VoicePipeline]은 에이전트 기반 워크플로를 음성 앱으로 쉽게 전환할 수 있게 해 주는 클래스입니다. 실행할 워크플로를 전달하면, 파이프라인이 입력 오디오 전사, 오디오 종료 시점 감지, 적절한 타이밍에 워크플로 호출, 워크플로 출력을 다시 오디오로 변환하는 작업을 처리합니다.\n \n ```mermaid\n graph LR\n@@ -34,28 +34,28 @@ graph LR\n \n ## 파이프라인 구성\n \n-파이프라인을 생성할 때 다음 항목을 설정할 수 있습니다:\n+파이프라인을 생성할 때 다음을 설정할 수 있습니다:\n \n-1. 새로운 오디오가 문자로 변환될 때마다 실행되는 코드인 [`workflow`][agents.voice.workflow.VoiceWorkflowBase]\n-2. 사용하는 [`speech-to-text`][agents.voice.model.STTModel] 및 [`text-to-speech`][agents.voice.model.TTSModel] 모델\n+1. 새 오디오가 전사될 때마다 실행되는 코드인 [`workflow`][agents.voice.workflow.VoiceWorkflowBase]\n+2. 사용할 [`speech-to-text`][agents.voice.model.STTModel] 및 [`text-to-speech`][agents.voice.model.TTSModel] 모델\n 3. 다음과 같은 항목을 구성할 수 있는 [`config`][agents.voice.pipeline_config.VoicePipelineConfig]\n-    - 모델 이름을 모델로 매핑할 수 있는 모델 제공자\n-    - 트레이싱: 트레이싱 비활성화 여부, 오디오 파일 업로드 여부, 워크플로우 이름, 트레이스 ID 등\n-    - TTS 및 STT 모델의 설정: 프롬프트, 언어, 사용되는 데이터 타입 등\n+    - 모델 이름을 모델에 매핑하는 모델 제공자\n+    - 트레이싱: 트레이싱 비활성화 여부, 오디오 파일 업로드 여부, 워크플로 이름, 트레이스 ID 등\n+    - TTS 및 STT 모델의 설정(프롬프트, 언어, 사용되는 데이터 타입 등)\n \n ## 파이프라인 실행\n \n-파이프라인은 [`run()`][agents.voice.pipeline.VoicePipeline.run] 메서드로 실행할 수 있으며, 두 가지 형태의 오디오 입력을 전달할 수 있습니다:\n+[`run()`][agents.voice.pipeline.VoicePipeline.run] 메서드를 통해 파이프라인을 실행할 수 있으며, 오디오 입력을 두 가지 형태로 전달할 수 있습니다:\n \n-1. [`AudioInput`][agents.voice.input.AudioInput]은 전체 오디오 전사가 있을 때 사용하며, 해당 전사에 대한 결과만 생성하면 되는 경우에 적합합니다. 예를 들어, 사전 녹음된 오디오나 사용자가 말하기를 마친 시점이 명확한 푸시투토크 앱에서 스피커의 발화 종료를 감지할 필요가 없는 경우 유용합니다\n-2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]은 사용자의 발화 종료를 감지해야 할 수 있는 경우에 사용합니다. 오디오 청크를 감지되는 대로 푸시할 수 있으며, 보이스 파이프라인은 \"활동 감지(activity detection)\"라는 프로세스를 통해 적절한 시점에 에이전트 워크플로우를 자동으로 실행합니다\n+1. [`AudioInput`][agents.voice.input.AudioInput]은 전체 오디오 전사가 이미 있고 그에 대한 결과만 생성하면 될 때 사용합니다. 이는 화자가 말을 마치는 시점을 감지할 필요가 없는 경우에 유용합니다. 예를 들어, 사전 녹음된 오디오가 있거나, 사용자가 말을 마치는 시점이 명확한 푸시투토크 앱에서 사용합니다.\n+2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]은 사용자가 말을 마치는 시점을 감지해야 할 수 있는 경우에 사용합니다. 감지되는 대로 오디오 청크를 푸시할 수 있으며, 음성 파이프라인은 \"activity detection\"(활동 감지)이라는 과정을 통해 적절한 시점에 에이전트 워크플로를 자동으로 실행합니다.\n \n ## 결과\n \n-보이스 파이프라인 실행 결과는 [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]입니다. 이는 이벤트가 발생하는 대로 스트리밍할 수 있게 해 주는 객체입니다. 다음과 같은 몇 가지 [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent]가 있습니다:\n+음성 파이프라인 실행의 결과는 [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]입니다. 이는 발생하는 대로 이벤트를 스트리밍할 수 있게 해 주는 객체입니다. [`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent]에는 다음과 같은 여러 종류가 있습니다:\n \n 1. 오디오 청크를 포함하는 [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]\n-2. 턴 시작/종료와 같은 라이프사이클 이벤트를 알려 주는 [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]\n+2. 턴 시작/종료와 같은 라이프사이클 이벤트를 알려주는 [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]\n 3. 오류 이벤트인 [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]\n \n ```python\n@@ -74,6 +74,6 @@ async for event in result.stream():\n \n ## 모범 사례\n \n-### 인터럽션\n+### 인터럽션(중단 처리)\n \n-Agents SDK는 현재 [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]에 대한 기본 제공 인터럽션(중단 처리) 지원을 제공하지 않습니다. 대신 탐지된 각 턴마다 워크플로우의 개별 실행을 트리거합니다. 애플리케이션 내부에서 인터럽션을 처리하려면 [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] 이벤트를 구독할 수 있습니다. `turn_started`는 새 턴이 전사되었고 처리가 시작되었음을 나타냅니다. `turn_ended`는 해당 턴의 모든 오디오가 디스패치된 후 트리거됩니다. 이러한 이벤트를 사용하여 모델이 턴을 시작할 때 스피커의 마이크를 음소거하고, 해당 턴과 관련된 모든 오디오를 플러시한 후 음소거를 해제할 수 있습니다.\n\\ No newline at end of file\n+Agents SDK는 현재 [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]에 대해 내장된 인터럽션(중단 처리) 기능을 지원하지 않습니다. 대신 감지된 각 턴마다 워크플로의 별도 실행이 트리거됩니다. 애플리케이션 내부에서 인터럽션을 처리하려면 [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] 이벤트를 구독할 수 있습니다. `turn_started`는 새 턴이 전사되어 처리가 시작되었음을 나타냅니다. `turn_ended`는 해당 턴의 모든 오디오가 디스패치된 후에 트리거됩니다. 모델이 턴을 시작할 때 화자의 마이크를 음소거하고, 턴과 관련된 모든 오디오 전송을 마친 뒤 음소거를 해제하는 방식으로 이러한 이벤트를 활용할 수 있습니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvoice%2Fpipeline.md",
        "sha": "df92e91c56f6607b7621ae81024667fab114fb4f",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvoice%2Fquickstart.md",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fvoice%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 8,
        "filename": "docs/ko/voice/quickstart.md",
        "patch": "@@ -4,21 +4,21 @@ search:\n ---\n # 빠른 시작\n \n-## 사전 준비\n+## 준비 사항\n \n-Agents SDK의 기본 [빠른 시작 안내](../quickstart.md)를 따라 가상 환경을 설정했는지 확인하세요. 그런 다음, SDK에서 선택 사항인 음성 관련 의존성을 설치합니다:\n+Agents SDK의 기본 [빠른 시작 안내](../quickstart.md)를 따라 가상 환경을 설정했는지 확인하세요. 그런 다음 SDK에서 제공하는 선택적 음성 의존성을 설치하세요:\n \n ```bash\n pip install 'openai-agents[voice]'\n ```\n \n ## 개념\n \n-핵심 개념은 [`VoicePipeline`][agents.voice.pipeline.VoicePipeline]이며, 3단계 프로세스입니다:\n+알아두어야 할 주요 개념은 [`VoicePipeline`][agents.voice.pipeline.VoicePipeline]이며, 3단계로 구성됩니다:\n \n 1. 음성을 텍스트로 변환하기 위해 음성-텍스트 모델을 실행\n-2. 보통 에이전트형(Agentic) 워크플로인 코드를 실행하여 결과 생성\n-3. 결과 텍스트를 다시 음성으로 변환하기 위해 텍스트-음성 모델을 실행\n+2. 결과를 생성하기 위해 코드를 실행하며, 일반적으로 에이전트 기반 워크플로를 사용\n+3. 결과 텍스트를 다시 오디오로 변환하기 위해 텍스트-음성 모델을 실행\n \n ```mermaid\n graph LR\n@@ -48,7 +48,7 @@ graph LR\n \n ## 에이전트\n \n-먼저 에이전트를 몇 개 설정해 보겠습니다. 이 SDK로 에이전트를 만들어 본 적이 있다면 익숙하게 느껴지실 것입니다. 에이전트 몇 개와 핸드오프, 그리고 도구를 사용할 것입니다.\n+먼저 에이전트를 설정해 보겠습니다. 이 SDK로 에이전트를 만들어 본 적이 있다면 익숙할 것입니다. 에이전트 몇 개와 핸드오프, 그리고 도구 하나를 사용합니다.\n \n ```python\n import asyncio\n@@ -124,7 +124,7 @@ async for event in result.stream():\n \n ```\n \n-## 모두 합치기\n+## 전체 통합\n \n ```python\n import asyncio\n@@ -195,4 +195,4 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-이 예제를 실행하면 에이전트가 말합니다! 직접 에이전트와 대화할 수 있는 데모는 [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) 예제를 확인하세요.\n\\ No newline at end of file\n+이 예제를 실행하면 에이전트가 여러분에게 말을 겁니다! 직접 에이전트와 대화할 수 있는 데모는 [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static)에서 확인하세요.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvoice%2Fquickstart.md",
        "sha": "ca7059e71068a3bf5bfb76f27fe60ebc1c555cb4",
        "status": "modified"
      },
      {
        "additions": 9,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvoice%2Ftracing.md",
        "changes": 18,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fko%2Fvoice%2Ftracing.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 9,
        "filename": "docs/ko/voice/tracing.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # 트레이싱\n \n-[에이전트 트레이싱 방법](../tracing.md)과 마찬가지로, 음성 파이프라인도 자동으로 트레이싱됩니다.\n+[에이전트 트레이싱 방식](../tracing.md)과 마찬가지로, 음성 파이프라인도 자동으로 트레이싱됩니다.\n \n-기본 트레이싱 정보는 위 문서를 참고하시고, 추가로 [`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig]를 통해 파이프라인 트레이싱을 구성할 수 있습니다.\n+기본 트레이싱 정보는 위의 문서를 참고하시고, 파이프라인의 트레이싱은 [`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig]를 통해 추가로 구성할 수 있습니다.\n \n-트레이싱 관련 주요 필드는 다음과 같습니다:\n+주요 트레이싱 관련 필드는 다음과 같습니다:\n \n-- [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: 트레이싱 비활성화 여부를 제어합니다. 기본적으로 트레이싱은 활성화되어 있습니다.\n-- [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: 오디오 전사본과 같이 민감할 수 있는 데이터를 트레이스에 포함할지 제어합니다. 이는 음성 파이프라인에만 적용되며, 워크플로 내부에서 일어나는 작업에는 적용되지 않습니다.\n-- [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: 오디오 데이터를 트레이스에 포함할지 제어합니다.\n-- [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: 트레이스 워크플로의 이름입니다.\n-- [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: 여러 트레이스를 연결할 수 있게 하는 트레이스의 `group_id`입니다.\n-- [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: 트레이스에 포함할 추가 메타데이터입니다.\n\\ No newline at end of file\n+-   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: 트레이싱 비활성화 여부를 제어합니다. 기본적으로 트레이싱은 활성화되어 있습니다.\n+-   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]: 오디오 필사본 등 잠재적으로 민감한 데이터를 트레이스에 포함할지 여부를 제어합니다. 이는 음성 파이프라인에만 해당하며, 워크플로 내부에서 발생하는 작업에는 적용되지 않습니다.\n+-   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]: 트레이스에 오디오 데이터를 포함할지 여부를 제어합니다.\n+-   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]: 트레이스 워크플로의 이름입니다.\n+-   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]: 여러 트레이스를 연결할 수 있게 해주는 트레이스의 `group_id`입니다.\n+-   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]: 트레이스에 포함할 추가 메타데이터입니다.\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fko%2Fvoice%2Ftracing.md",
        "sha": "76284ef84516f3daba4e33aab129ac7852d8eaeb",
        "status": "modified"
      },
      {
        "additions": 30,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fagents.md",
        "changes": 60,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fagents.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 30,
        "filename": "docs/zh/agents.md",
        "patch": "@@ -4,16 +4,16 @@ search:\n ---\n # 智能体\n \n-智能体是你应用中的核心构建块。一个智能体是一个大型语言模型（LLM），通过 instructions 和工具进行配置。\n+智能体是你应用中的核心构建块。一个智能体是一个大型语言模型（LLM），并配置有 instructions 和 tools。\n \n-## 基本配置\n+## 基础配置\n \n-你最常配置的智能体属性包括：\n+你最常为智能体配置的属性包括：\n \n--   `name`: 用于标识智能体的必填字符串。\n--   `instructions`: 也称为开发者消息或系统提示词（system prompt）。\n--   `model`: 要使用的 LLM，以及可选的 `model_settings`，用于配置模型调优参数，如 temperature、top_p 等。\n--   `tools`: 智能体可用于完成任务的工具。\n+- `name`: 用于标识智能体的必填字符串。\n+- `instructions`: 也称为开发者消息或系统提示词。\n+- `model`: 使用哪个 LLM，可选的 `model_settings` 用于配置 temperature、top_p 等模型调参。\n+- `tools`: 智能体可用于完成任务的工具。\n \n ```python\n from agents import Agent, ModelSettings, function_tool\n@@ -33,7 +33,7 @@ agent = Agent(\n \n ## 上下文\n \n-智能体在其 `context` 类型上是通用的。Context 是一种依赖注入工具：这是你创建并传递给 `Runner.run()` 的对象，它会传递给每个智能体、工具、任务转移等，用作该次运行的依赖与状态集合。你可以提供任意 Python 对象作为 context。\n+智能体在其 `context` 类型上是通用的。Context 是一种依赖注入工具：它是你创建并传递给 `Runner.run()` 的对象，会传递给每个智能体、工具、任务转移等，作为本次运行所需依赖与状态的集合。你可以提供任意 Python 对象作为 context。\n \n ```python\n @dataclass\n@@ -52,7 +52,7 @@ agent = Agent[UserContext](\n \n ## 输出类型\n \n-默认情况下，智能体产生纯文本（即 `str`）输出。如果你希望智能体产生特定类型的输出，可以使用 `output_type` 参数。常见的做法是使用 [Pydantic](https://docs.pydantic.dev/) 对象，但我们支持任何可以被 Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) 包装的类型——dataclasses、lists、TypedDict 等。\n+默认情况下，智能体生成纯文本（即 `str`）输出。若你希望智能体生成特定类型的输出，可以使用 `output_type` 参数。一个常见选择是使用 [Pydantic](https://docs.pydantic.dev/) 对象，但我们支持任何可被 Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) 包装的类型——如数据类（dataclasses）、列表（lists）、TypedDict 等。\n \n ```python\n from pydantic import BaseModel\n@@ -73,20 +73,20 @@ agent = Agent(\n \n !!! note\n \n-    当你传入 `output_type` 时，这会指示模型使用 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) 而不是常规的纯文本响应。\n+    当你传入 `output_type` 时，这会指示模型使用 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) 而非普通纯文本响应。\n \n ## 多智能体系统设计模式\n \n-设计多智能体系统有很多方式，但我们常见到两类广泛适用的模式：\n+设计多智能体系统的方式有很多，但我们常见到两种广泛适用的模式：\n \n-1. 管理者（智能体作为工具）：一个中心管理者/编排者将专业子智能体作为工具调用，并保持对话控制权。\n-2. 任务转移：对等智能体将控制权转移给一个专业化智能体，由其接管对话。这是去中心化的。\n+1. 管理者（将智能体作为工具）：一个集中式的管理者/编排者将专用子智能体作为工具调用，并保持对话控制权。\n+2. 任务转移：对等的智能体将控制权移交给一个专门化智能体，由其接管对话。这是去中心化的。\n \n-详见[构建智能体的实用指南](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf)。\n+更多细节参见[构建智能体的实用指南](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf)。\n \n-### 管理者（智能体作为工具）\n+### 管理者（将智能体作为工具）\n \n-`customer_facing_agent` 处理所有用户交互，并调用以工具形式暴露的专业子智能体。详见 [工具](tools.md#agents-as-tools) 文档。\n+`customer_facing_agent` 处理所有用户交互，并调用作为工具暴露的专用子智能体。更多内容见 [tools](tools.md#agents-as-tools) 文档。\n \n ```python\n from agents import Agent\n@@ -115,7 +115,7 @@ customer_facing_agent = Agent(\n \n ### 任务转移\n \n-任务转移是指智能体可以委派给的子智能体。当发生任务转移时，被委派的智能体会收到对话历史并接管对话。该模式可实现模块化、专业化的智能体，使其在单一任务上表现出色。详见[任务转移](handoffs.md)文档。\n+任务转移是指智能体可委派的子智能体。当发生任务转移时，被委派的智能体会接收对话历史并接管对话。该模式支持模块化、专门化、擅长单一任务的智能体。更多内容见 [任务转移](handoffs.md) 文档。\n \n ```python\n from agents import Agent\n@@ -136,7 +136,7 @@ triage_agent = Agent(\n \n ## 动态 instructions\n \n-在大多数情况下，你可以在创建智能体时提供 instructions。但你也可以通过函数提供动态 instructions。该函数会接收智能体和 context，并且必须返回提示词。支持常规函数和 `async` 函数。\n+在大多数情况下，你可以在创建智能体时提供 instructions。不过，你也可以通过函数提供动态 instructions。该函数会接收智能体与 context，并且必须返回提示词。常规与 `async` 函数都被接受。\n \n ```python\n def dynamic_instructions(\n@@ -153,15 +153,15 @@ agent = Agent[UserContext](\n \n ## 生命周期事件（hooks）\n \n-有时你需要观察智能体的生命周期。例如，你可能希望记录事件，或在某些事件发生时预取数据。你可以通过 `hooks` 属性挂接到智能体生命周期。继承 [`AgentHooks`][agents.lifecycle.AgentHooks] 类，并重写你感兴趣的方法。\n+有时你希望观察智能体的生命周期。例如，你可能希望记录事件，或在特定事件发生时预取数据。你可以通过 `hooks` 属性挂接到智能体生命周期。子类化 [`AgentHooks`][agents.lifecycle.AgentHooks] 并重写你关心的方法。\n \n ## 安全防护措施\n \n-安全防护措施允许你在智能体运行的同时对用户输入进行并行的检查/验证，并在智能体生成输出后对其结果进行检查。例如，你可以筛查用户输入和智能体输出的相关性。详见[安全防护措施](guardrails.md)文档。\n+安全防护措施允许你在智能体运行的同时对用户输入进行检查/验证，并在智能体产出结果后对其输出进行检查。例如，你可以筛查用户输入和智能体输出的相关性。更多内容见[安全防护措施](guardrails.md)文档。\n \n ## 克隆/复制智能体\n \n-通过在智能体上使用 `clone()` 方法，你可以复制一个智能体，并可选择性地更改任意属性。\n+通过在智能体上使用 `clone()` 方法，你可以复制一个智能体，并可选地修改任意你想要的属性。\n \n ```python\n pirate_agent = Agent(\n@@ -178,12 +178,12 @@ robot_agent = pirate_agent.clone(\n \n ## 强制使用工具\n \n-提供工具列表并不总能保证 LLM 会使用工具。你可以通过设置 [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] 来强制使用工具。可选值为：\n+提供工具列表并不总是意味着 LLM 会使用工具。你可以通过设置 [`ModelSettings.tool_choice`][agents.model_settings.ModelSettings.tool_choice] 来强制使用工具。可选值为：\n \n-1. `auto`，允许 LLM 决定是否使用工具。\n-2. `required`，要求 LLM 使用工具（但可以智能选择具体哪个工具）。\n+1. `auto`，允许 LLM 自主决定是否使用工具。\n+2. `required`，要求 LLM 使用工具（但可以智能选择使用哪个工具）。\n 3. `none`，要求 LLM 不使用工具。\n-4. 设置特定字符串，例如 `my_tool`，要求 LLM 使用该特定工具。\n+4. 指定某个字符串，例如 `my_tool`，要求 LLM 使用该特定工具。\n \n ```python\n from agents import Agent, Runner, function_tool, ModelSettings\n@@ -203,9 +203,9 @@ agent = Agent(\n \n ## 工具使用行为\n \n-`Agent` 配置中的 `tool_use_behavior` 参数控制工具输出的处理方式：\n+在 `Agent` 配置中的 `tool_use_behavior` 参数控制如何处理工具输出：\n \n-- `\"run_llm_again\"`：默认值。工具运行后，由 LLM 处理结果以生成最终响应。\n+- `\"run_llm_again\"`：默认值。运行工具后，LLM 会处理其结果以生成最终响应。\n - `\"stop_on_first_tool\"`：第一次工具调用的输出将作为最终响应，不再进行后续 LLM 处理。\n \n ```python\n@@ -224,7 +224,7 @@ agent = Agent(\n )\n ```\n \n-- `StopAtTools(stop_at_tool_names=[...])`：当调用到任一指定工具时停止，并将其输出作为最终响应。\n+- `StopAtTools(stop_at_tool_names=[...])`：若调用了任一指定工具，则停止，并使用其输出作为最终响应。\n \n ```python\n from agents import Agent, Runner, function_tool\n@@ -248,7 +248,7 @@ agent = Agent(\n )\n ```\n \n-- `ToolsToFinalOutputFunction`：自定义函数，用于处理工具结果，并决定是停止还是继续交给 LLM。\n+- `ToolsToFinalOutputFunction`：自定义函数，用于处理工具结果并决定是停止还是继续交由 LLM。\n \n ```python\n from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper\n@@ -286,4 +286,4 @@ agent = Agent(\n \n !!! note\n \n-    为防止无限循环，框架会在一次工具调用后自动将 `tool_choice` 重置为 \"auto\"。可通过 [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] 配置此行为。出现无限循环的原因是工具结果会发送给 LLM，而由于 `tool_choice` 的设置，LLM 又会生成新的工具调用，如此往复。\n\\ No newline at end of file\n+    为防止无限循环，框架会在一次工具调用后自动将 `tool_choice` 重置为 \"auto\"。该行为可通过 [`agent.reset_tool_choice`][agents.agent.Agent.reset_tool_choice] 配置。出现无限循环的原因是工具结果会被发送给 LLM，而由于 `tool_choice` 的缘故，LLM 会再次生成工具调用，如此往复。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fagents.md",
        "sha": "71d42f1f6d216b38de373924464eef7ca2ad24f3",
        "status": "modified"
      },
      {
        "additions": 10,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fconfig.md",
        "changes": 20,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fconfig.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 10,
        "filename": "docs/zh/config.md",
        "patch": "@@ -6,15 +6,15 @@ search:\n \n ## API 密钥与客户端\n \n-默认情况下，SDK 在被导入时会立即从环境变量 `OPENAI_API_KEY` 中读取用于 LLM 请求和追踪的密钥。如果你无法在应用启动前设置该环境变量，可以使用 [set_default_openai_key()][agents.set_default_openai_key] 函数来设置密钥。\n+默认情况下，SDK 在被导入后会立即从环境变量 `OPENAI_API_KEY` 中读取 LLM 请求与追踪所需的密钥。如果在应用启动前无法设置该环境变量，可以使用 [set_default_openai_key()][agents.set_default_openai_key] 函数来设置密钥。\n \n ```python\n from agents import set_default_openai_key\n \n set_default_openai_key(\"sk-...\")\n ```\n \n-或者，你也可以配置要使用的 OpenAI 客户端。默认情况下，SDK 会创建一个 `AsyncOpenAI` 实例，并使用环境变量中的 API 密钥或上面设置的默认密钥。你可以使用 [set_default_openai_client()][agents.set_default_openai_client] 函数进行更改。\n+或者，你也可以配置要使用的 OpenAI 客户端。默认情况下，SDK 会创建一个 `AsyncOpenAI` 实例，并使用来自环境变量或上述默认密钥的 API key。你可以通过 [set_default_openai_client()][agents.set_default_openai_client] 函数进行更改。\n \n ```python\n from openai import AsyncOpenAI\n@@ -24,7 +24,7 @@ custom_client = AsyncOpenAI(base_url=\"...\", api_key=\"...\")\n set_default_openai_client(custom_client)\n ```\n \n-最后，你也可以自定义所使用的 OpenAI API。默认使用的是 OpenAI Responses API。你可以通过 [set_default_openai_api()][agents.set_default_openai_api] 函数改为使用 Chat Completions API。\n+最后，你还可以自定义所使用的 OpenAI API。默认情况下，我们使用 OpenAI Responses API。你可以通过 [set_default_openai_api()][agents.set_default_openai_api] 函数将其改为使用 Chat Completions API。\n \n ```python\n from agents import set_default_openai_api\n@@ -34,15 +34,15 @@ set_default_openai_api(\"chat_completions\")\n \n ## 追踪\n \n-追踪默认启用。它默认使用上文中的 OpenAI API 密钥（即环境变量或你设置的默认密钥）。你可以使用 [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] 函数专门为追踪设置 API 密钥。\n+追踪默认启用。默认情况下，它使用上一节中的 OpenAI API key（即环境变量或你设置的默认密钥）。你可以使用 [`set_tracing_export_api_key`][agents.set_tracing_export_api_key] 函数专门为追踪设置 API key。\n \n ```python\n from agents import set_tracing_export_api_key\n \n set_tracing_export_api_key(\"sk-...\")\n ```\n \n-你也可以在不更改全局导出器的情况下，为单次运行设置追踪 API 密钥。\n+你也可以在不修改全局导出器的情况下，为每次运行设置一个追踪 API key。\n \n ```python\n from agents import Runner, RunConfig\n@@ -64,7 +64,7 @@ set_tracing_disabled(True)\n \n ## 调试日志\n \n-该 SDK 提供两个未设置任何处理器的 Python 日志记录器。默认情况下，这意味着警告和错误会发送到 `stdout`，其他日志将被抑制。\n+该 SDK 提供了两个未配置任何处理器的 Python 日志记录器。默认情况下，这意味着警告和错误会发送到 `stdout`，而其他日志会被抑制。\n \n 要启用详细日志，请使用 [`enable_verbose_stdout_logging()`][agents.enable_verbose_stdout_logging] 函数。\n \n@@ -74,7 +74,7 @@ from agents import enable_verbose_stdout_logging\n enable_verbose_stdout_logging()\n ```\n \n-或者，你可以通过添加处理器、过滤器、格式化器等自定义日志。可阅读 [Python logging 指南](https://docs.python.org/3/howto/logging.html)了解更多信息。\n+或者，你可以通过添加处理器、过滤器、格式化器等自定义日志。更多信息参见 [Python logging 指南](https://docs.python.org/3/howto/logging.html)。\n \n ```python\n import logging\n@@ -95,15 +95,15 @@ logger.addHandler(logging.StreamHandler())\n \n ### 日志中的敏感数据\n \n-某些日志可能包含敏感数据（例如用户数据）。如果你希望禁用这些数据的记录，请设置以下环境变量。\n+某些日志可能包含敏感数据（例如，用户数据）。如果你想禁止记录这些数据，请设置以下环境变量。\n \n-要禁用记录 LLM 的输入和输出：\n+要禁用记录 LLM 的输入与输出：\n \n ```bash\n export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1\n ```\n \n-要禁用记录工具的输入和输出：\n+要禁用记录工具的输入与输出：\n \n ```bash\n export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fconfig.md",
        "sha": "a39949a15e223a1ce4102b135f8a4cb38e66a366",
        "status": "modified"
      },
      {
        "additions": 26,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fcontext.md",
        "changes": 52,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fcontext.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 26,
        "filename": "docs/zh/context.md",
        "patch": "@@ -4,30 +4,30 @@ search:\n ---\n # 上下文管理\n \n-“上下文”是一个含义较多的术语。通常你会关心两类上下文：\n+“上下文”是个多义词。通常你会关心两大类上下文：\n \n-1. 代码本地可用的上下文：当工具函数运行、在 `on_handoff` 这类回调、生命周期钩子等期间可能需要的数据与依赖。\n-2. LLM 可用的上下文：LLM 在生成响应时可见的数据。\n+1. 代码本地可用的上下文：当工具函数运行、`on_handoff` 等回调、生命周期钩子等执行时，可能需要的数据与依赖。\n+2. LLM 可用的上下文：LLM 在生成回复时能看到的数据。\n \n ## 本地上下文\n \n-这由 [`RunContextWrapper`][agents.run_context.RunContextWrapper] 类以及其中的 [`context`][agents.run_context.RunContextWrapper.context] 属性来表示。其工作方式是：\n+这由 [`RunContextWrapper`][agents.run_context.RunContextWrapper] 类及其内部的 [`context`][agents.run_context.RunContextWrapper.context] 属性表示。其工作方式为：\n \n-1. 你创建任意 Python 对象。常见模式是使用数据类（dataclass）或 Pydantic 对象。\n-2. 将该对象传给各类运行方法（例如 `Runner.run(..., **context=whatever**)`）。\n-3. 你的所有工具调用、生命周期钩子等都会接收一个包装对象 `RunContextWrapper[T]`，其中 `T` 表示你的上下文对象类型，你可通过 `wrapper.context` 访问。\n+1. 你创建任意 Python 对象。常见做法是使用 dataclass 或 Pydantic 对象。\n+2. 将该对象传给各种运行方法（如 `Runner.run(..., **context=whatever**)`）。\n+3. 你的所有工具调用、生命周期钩子等都会收到一个包装对象 `RunContextWrapper[T]`，其中 `T` 表示你的上下文对象类型，你可通过 `wrapper.context` 访问。\n \n-需要特别注意的**最重要**一点：给定一次智能体运行，所有智能体、工具函数、生命周期等都必须使用相同_类型_的上下文。\n+需要特别注意的**最重要**一点：对一次给定的智能体运行，所有智能体、工具函数、生命周期等必须使用相同类型的上下文。\n \n 你可以将上下文用于：\n \n-- 运行的情境数据（例如用户名/uid 或关于该用户的其他信息）\n-- 依赖（例如日志记录器对象、数据获取器等）\n-- 帮助函数\n+- 运行的情境数据（如用户名/uid 或其他关于用户的信息）\n+- 依赖（如 logger 对象、数据获取器等）\n+- 辅助函数\n \n !!! danger \"注意\"\n \n-    上下文对象**不会**发送给 LLM。它纯粹是一个本地对象，你可以读取、写入并在其上调用方法。\n+    上下文对象**不会**被发送给 LLM。它纯粹是一个本地对象，你可以读取、写入并调用其方法。\n \n ```python\n import asyncio\n@@ -66,10 +66,10 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-1. 这是上下文对象。这里使用了数据类，但你可以使用任意类型。\n-2. 这是一个工具。它接收 `RunContextWrapper[UserInfo]`。工具实现会从上下文中读取数据。\n-3. 我们用泛型 `UserInfo` 标注该智能体，以便类型检查器能捕获错误（例如，如果我们尝试传入一个接收不同上下文类型的工具）。\n-4. 上下文被传入 `run` 函数。\n+1. 这是上下文对象。这里用了 dataclass，但你可以使用任意类型。\n+2. 这是一个工具。可以看到它接收 `RunContextWrapper[UserInfo]`。工具实现会从上下文中读取数据。\n+3. 我们在智能体上标注了泛型 `UserInfo`，这样类型检查器可以捕获错误（例如，如果我们尝试传入一个使用不同上下文类型的工具）。\n+4. 通过 `run` 函数传入上下文。\n 5. 智能体正确调用工具并获取年龄。\n \n ---\n@@ -108,20 +108,20 @@ agent = Agent(\n `ToolContext` 提供与 `RunContextWrapper` 相同的 `.context` 属性，  \n 并额外包含当前工具调用特有的字段：\n \n-- `tool_name` – 正在调用的工具名称  \n-- `tool_call_id` – 此次工具调用的唯一标识符  \n-- `tool_arguments` – 传递给工具的原始参数字符串  \n+- `tool_name` – 被调用工具的名称  \n+- `tool_call_id` – 本次工具调用的唯一标识符  \n+- `tool_arguments` – 传给工具的原始参数字符串  \n \n-当你在执行期间需要工具层面的元数据时，请使用 `ToolContext`。  \n-对于在智能体与工具之间共享的一般上下文，`RunContextWrapper` 已经足够。\n+当你在执行期间需要工具层面的元数据时，使用 `ToolContext`。  \n+而对于在智能体与工具之间共享通用上下文，`RunContextWrapper` 已足够。\n \n ---\n \n ## 智能体/LLM 上下文\n \n-当调用 LLM 时，它能看到的**唯一**数据来自对话历史。也就是说，如果你想让 LLM 获取新的数据，必须以能让该数据进入对话历史的方式提供它。常见做法包括：\n+当调用 LLM 时，它能看到的**唯一**数据来自会话历史。这意味着如果你想让 LLM 获取新的数据，必须以一种能将其放入会话历史的方式提供。常见方法包括：\n \n-1. 将其添加到智能体的 `instructions`。这也被称为“system prompt”或“developer message”。System prompts 可以是静态字符串，也可以是接收上下文并输出字符串的动态函数。对于总是有用的信息（例如用户名或当前日期），这是常用策略。\n-2. 在调用 `Runner.run` 时将其添加到 `input`。这与 `instructions` 的策略类似，但允许消息位于[指挥链](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command)的更低位置。\n-3. 通过工具调用（function tools）暴露它。适用于_按需_上下文——LLM 决定何时需要某些数据，并可调用工具以获取该数据。\n-4. 使用检索或网络检索（web search）。这些是特殊工具，能够从文件或数据库中获取相关数据（检索），或从网络中获取（网络检索）。这有助于使响应基于相关的上下文数据。\n\\ No newline at end of file\n+1. 将其添加到智能体的 `instructions`。这也被称为“系统提示词（system prompt）”或“开发者消息”。系统提示词可以是静态字符串，也可以是动态函数，接收上下文并输出字符串。这常用于总是有用的信息（例如用户名或当前日期）。\n+2. 在调用 `Runner.run` 函数时将其添加到 `input`。这与 `instructions` 策略类似，但允许你将消息放在更低的[指令链](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command)中。\n+3. 通过 工具调用 暴露。适用于按需上下文——LLM 决定何时需要某些数据，并可调用工具获取这些数据。\n+4. 使用检索（retrieval）或 网络检索。这些是特殊工具，能够从文件或数据库（检索）或从网络（网络检索）中获取相关数据。这有助于用相关的上下文数据对回复进行“grounding（落地）”。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fcontext.md",
        "sha": "4fa101f53725ef96b784a46d307030cb37f38ffb",
        "status": "modified"
      },
      {
        "additions": 27,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fexamples.md",
        "changes": 54,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fexamples.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 27,
        "filename": "docs/zh/examples.md",
        "patch": "@@ -4,58 +4,58 @@ search:\n ---\n # 代码示例\n \n-在[repo](https://github.com/openai/openai-agents-python/tree/main/examples)的 examples 部分查看多种 SDK 的示例实现。这些示例按多个类别组织，展示不同的模式与能力。\n+在 [repo](https://github.com/openai/openai-agents-python/tree/main/examples) 的 examples 部分可以查看该 SDK 的各类示例实现。这些代码示例按多个目录组织，展示不同的模式与能力。\n \n ## 目录\n \n -   **[agent_patterns](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns):**\n-    本目录中的示例展示常见的智能体设计模式，例如\n+    本目录的示例展示常见的智能体设计模式，例如：\n \n     -   确定性工作流\n-    -   将智能体作为工具\n-    -   智能体并行执行\n-    -   条件式工具使用\n+    -   作为工具的智能体\n+    -   并行智能体执行\n+    -   条件性工具使用\n     -   输入/输出安全防护措施\n-    -   LLM 作为评审\n+    -   由 LLM 充当评审\n     -   路由\n-    -   流式传输安全防护措施\n+    -   流式传输 安全防护措施\n \n -   **[basic](https://github.com/openai/openai-agents-python/tree/main/examples/basic):**\n-    这些示例展示 SDK 的基础能力，例如\n+    这些示例展示 SDK 的基础能力，例如：\n \n-    -   Hello world examples（默认模型、GPT-5、开源权重模型）\n+    -   Hello World 示例（默认模型、GPT-5、开放权重模型）\n     -   智能体生命周期管理\n     -   动态系统提示词\n-    -   流式传输输出（文本、项目、函数调用参数）\n+    -   流式传输 输出（文本、条目、函数调用参数）\n     -   提示词模板\n     -   文件处理（本地与远程、图像与 PDF）\n     -   使用情况追踪\n     -   非严格输出类型\n-    -   先前响应 ID 的用法\n+    -   先前响应 ID 的使用\n \n -   **[customer_service](https://github.com/openai/openai-agents-python/tree/main/examples/customer_service):**\n     航空公司客服系统示例。\n \n -   **[financial_research_agent](https://github.com/openai/openai-agents-python/tree/main/examples/financial_research_agent):**\n-    一个金融研究智能体，展示结合智能体与工具进行金融数据分析的结构化研究工作流。\n+    金融研究智能体，展示使用智能体与工具进行金融数据分析的结构化研究工作流。\n \n -   **[handoffs](https://github.com/openai/openai-agents-python/tree/main/examples/handoffs):**\n-    了解带消息过滤的智能体任务转移的实用示例。\n+    查看带消息过滤的智能体任务转移的实用示例。\n \n -   **[hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp):**\n     展示如何使用托管的 MCP (Model Context Protocol) 连接器与审批的示例。\n \n -   **[mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp):**\n     学习如何使用 MCP (Model Context Protocol) 构建智能体，包括：\n \n-    -   文件系统代码示例\n-    -   Git 代码示例\n-    -   MCP 提示词服务代码示例\n-    -   SSE（服务器发送事件）代码示例\n-    -   可流式传输 HTTP 代码示例\n+    -   文件系统示例\n+    -   Git 示例\n+    -   MCP 提示词服务示例\n+    -   SSE（Server-Sent Events，服务器发送事件）示例\n+    -   可流式传输 的 HTTP 示例\n \n -   **[memory](https://github.com/openai/openai-agents-python/tree/main/examples/memory):**\n-    不同的智能体记忆实现示例，包括：\n+    智能体的不同记忆实现示例，包括：\n \n     -   SQLite 会话存储\n     -   高级 SQLite 会话存储\n@@ -65,29 +65,29 @@ search:\n     -   OpenAI 会话存储\n \n -   **[model_providers](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers):**\n-    探索如何在 SDK 中使用非 OpenAI 模型，包括自定义提供方与 LiteLLM 集成。\n+    了解如何在 SDK 中使用非 OpenAI 模型，包括自定义提供方与 LiteLLM 集成。\n \n -   **[realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime):**\n-    展示如何使用 SDK 构建实时体验的示例，包括：\n+    展示如何用 SDK 构建实时体验的示例，包括：\n \n     -   Web 应用\n     -   命令行界面\n-    -   Twilio 集成\n+    -   与 Twilio 集成\n \n -   **[reasoning_content](https://github.com/openai/openai-agents-python/tree/main/examples/reasoning_content):**\n-    展示如何处理推理内容与 structured outputs 的示例。\n+    演示如何处理推理内容与 格式良好的数据 的示例。\n \n -   **[research_bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot):**\n-    简单的深度研究克隆，展示复杂的多智能体研究工作流。\n+    简单的深度研究 克隆，展示复杂的多智能体研究工作流。\n \n -   **[tools](https://github.com/openai/openai-agents-python/tree/main/examples/tools):**\n     学习如何实现 由OpenAI托管的工具，例如：\n \n-    -   网络检索与带筛选条件的网络检索\n+    -   网络检索 与 带筛选的 网络检索\n     -   文件检索\n-    -   Code interpreter\n+    -   Code Interpreter\n     -   计算机操作\n     -   图像生成\n \n -   **[voice](https://github.com/openai/openai-agents-python/tree/main/examples/voice):**\n-    了解语音智能体示例，使用我们的 TTS 与 STT 模型，包括流式语音示例。\n\\ No newline at end of file\n+    查看语音智能体示例，使用我们的 TTS 和 STT 模型，包括流式语音示例。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fexamples.md",
        "sha": "4b41195a13680f872f5bc52ad79da67bf8233d02",
        "status": "modified"
      },
      {
        "additions": 71,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fguardrails.md",
        "changes": 138,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fguardrails.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 67,
        "filename": "docs/zh/guardrails.md",
        "patch": "@@ -4,105 +4,64 @@ search:\n ---\n # 安全防护措施\n \n-安全防护措施允许你对用户输入和智能体输出进行检查与验证。举例来说，假设你有一个使用非常智能（因此也更慢/更昂贵）模型来处理客户请求的智能体。你不希望恶意用户让模型帮他们做数学作业。于是，你可以用一个更快/更便宜的模型来运行安全防护措施。如果安全防护措施检测到恶意使用，它可以立即抛出错误并阻止昂贵模型的运行，从而节省时间和成本（**在使用阻塞式安全防护措施时；对于并行安全防护措施，可能在安全防护措施完成之前，昂贵模型就已经开始运行。详见下文“执行模式”**）。\n+安全防护措施可用于检查与验证用户输入和智能体输出。举例来说，设想你有一个使用非常智能（因此也很慢/昂贵）的模型来帮助处理客户请求的智能体。你不希望恶意用户要求模型帮他们做数学作业。所以，你可以用一个快速/廉价的模型运行安全防护措施。如果安全防护措施检测到恶意使用，它可以立即抛出错误并阻止昂贵模型运行，从而节省时间和金钱（**在使用阻塞式安全防护措施时；对于并行安全防护措施，可能在安全防护措施完成之前昂贵模型就已经开始运行。详见下文“执行模式”**）。\n \n-安全防护措施有两种类型：\n+安全防护措施分为两类：\n \n-1. 输入安全防护措施运行于初始用户输入\n-2. 输出安全防护措施运行于最终智能体输出\n+1. 输入安全防护措施：运行于初始用户输入\n+2. 输出安全防护措施：运行于最终智能体输出\n \n ## 输入安全防护措施\n \n-输入安全防护措施分3步运行：\n+输入安全防护措施分三步运行：\n \n 1. 首先，安全防护措施接收与智能体相同的输入。\n-2. 接着，安全防护函数运行以生成一个[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]，随后被包装进一个[`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]\n-3. 最后，我们检查[`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]是否为 true。若为 true，则抛出[`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered]异常，以便你适当回复用户或处理异常。\n+2. 接着，安全防护函数运行以生成一个 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]，随后包装为一个 [`InputGuardrailResult`][agents.guardrail.InputGuardrailResult]\n+3. 最后，我们检查 [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] 是否为 true。若为 true，则会抛出 [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered] 异常，便于你适当地响应用户或处理该异常。\n \n !!! Note\n \n-    输入安全防护措施旨在对用户输入运行，因此只有当智能体是*第一个*智能体时，才会运行该智能体的安全防护措施。你可能会疑惑：为什么 `guardrails` 属性在智能体上，而不是通过 `Runner.run` 传入？这是因为安全防护措施通常与具体智能体相关——不同智能体会运行不同的安全防护措施，因此将代码与智能体放在一起有助于可读性。\n+    输入安全防护措施旨在运行于用户输入上，因此智能体的安全防护措施只会在该智能体是“第一个”智能体时运行。你可能会好奇，为何 `guardrails` 属性在智能体上，而不是通过 `Runner.run` 传入？这是因为安全防护措施通常与具体的智能体相关——你会为不同的智能体运行不同的安全防护措施，因此把代码放在一起有助于可读性。\n \n ### 执行模式\n \n 输入安全防护措施支持两种执行模式：\n \n-- **并行执行**（默认，`run_in_parallel=True`）：安全防护措施与智能体的执行并发运行。由于二者同时启动，这提供了最佳延迟表现。然而，如果安全防护失败，智能体可能在被取消前已经消耗了 tokens 并执行了工具。\n+- **并行执行**（默认，`run_in_parallel=True`）：安全防护措施与智能体执行并发运行。这能提供最佳时延，因为二者同时开始。然而，如果安全防护措施失败，智能体在被取消前可能已经消耗了 tokens 并执行了 tools。\n \n-- **阻塞执行**（`run_in_parallel=False`）：安全防护措施在智能体启动前运行并完成。如果绊线被触发，智能体将不会执行，从而避免 token 消耗与工具执行。这对于成本优化以及需要避免工具调用潜在副作用的场景非常理想。\n+- **阻塞执行**（`run_in_parallel=False`）：安全防护措施在智能体启动之前先行运行并完成。如果安全防护措施触发了触发线，智能体将不会执行，从而避免 token 消耗与 tool 执行。此模式适用于成本优化，以及你想避免工具调用可能带来的副作用时。\n \n ## 输出安全防护措施\n \n-输出安全防护措施分3步运行：\n+输出安全防护措施分三步运行：\n \n-1. 首先，安全防护措施接收由智能体生成的输出。\n-2. 接着，安全防护函数运行以生成一个[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]，随后被包装进一个[`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]\n-3. 最后，我们检查[`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered]是否为 true。若为 true，则抛出[`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]异常，以便你适当回复用户或处理异常。\n+1. 首先，安全防护措施接收由智能体产生的输出。\n+2. 接着，安全防护函数运行以生成一个 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]，随后包装为一个 [`OutputGuardrailResult`][agents.guardrail.OutputGuardrailResult]\n+3. 最后，我们检查 [`.tripwire_triggered`][agents.guardrail.GuardrailFunctionOutput.tripwire_triggered] 是否为 true。若为 true，则会抛出 [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered] 异常，便于你适当地响应用户或处理该异常。\n \n !!! Note\n \n-    输出安全防护措施旨在对最终的智能体输出运行，因此只有当智能体是*最后一个*智能体时，才会运行该智能体的安全防护措施。类似于输入安全防护措施，我们这样设计是因为安全防护措施往往与具体智能体相关——不同智能体会运行不同的安全防护措施，因此将代码与智能体放在一起有助于可读性。\n+    输出安全防护措施旨在运行于最终的智能体输出上，因此智能体的安全防护措施只会在该智能体是“最后一个”智能体时运行。与输入安全防护措施类似，我们这样做是因为安全防护措施通常与具体的智能体相关——你会为不同的智能体运行不同的安全防护措施，因此把代码放在一起有助于可读性。\n \n-    输出安全防护措施总是在智能体完成之后运行，因此不支持 `run_in_parallel` 参数。\n+    输出安全防护措施总是在智能体完成后运行，因此它们不支持 `run_in_parallel` 参数。\n \n ## 工具安全防护措施\n \n-工具安全防护措施包装**工具调用**，允许你在执行工具前后验证或阻止工具调用。它们在工具本身上进行配置，并在该工具每次被调用时运行。\n+工具安全防护措施包装 **工具调用**，并允许你在执行之前和之后验证或阻止工具调用。它们在工具本身上配置，每次调用该工具时都会运行。\n \n-- 输入工具安全防护措施在工具执行之前运行，可以跳过调用、用一条消息替换输出，或触发绊线。\n-- 输出工具安全防护措施在工具执行之后运行，可以替换输出或触发绊线。\n-- 工具安全防护措施仅适用于使用[`function_tool`][agents.function_tool]创建的工具调用；托管工具（`WebSearchTool`、`FileSearchTool`、`HostedMCPTool`、`CodeInterpreterTool`、`ImageGenerationTool`）和本地运行时工具（`ComputerTool`、`ShellTool`、`ApplyPatchTool`、`LocalShellTool`）不使用此安全防护流程。\n+- 工具输入安全防护措施在工具执行之前运行，可以跳过调用、用消息替换输出、或触发触发线。\n+- 工具输出安全防护措施在工具执行之后运行，可以替换输出或触发触发线。\n+- 工具安全防护措施仅适用于使用 [`function_tool`][agents.function_tool] 创建的工具调用；托管工具（`WebSearchTool`、`FileSearchTool`、`HostedMCPTool`、`CodeInterpreterTool`、`ImageGenerationTool`）和本地运行时工具（`ComputerTool`、`ShellTool`、`ApplyPatchTool`、`LocalShellTool`）不使用此安全防护流水线。\n \n-```python\n-import json\n-from agents import (\n-    Agent,\n-    Runner,\n-    ToolGuardrailFunctionOutput,\n-    function_tool,\n-    tool_input_guardrail,\n-    tool_output_guardrail,\n-)\n+详见下方代码片段。\n \n-@tool_input_guardrail\n-def block_secrets(data):\n-    args = json.loads(data.context.tool_arguments or \"{}\")\n-    if \"sk-\" in json.dumps(args):\n-        return ToolGuardrailFunctionOutput.reject_content(\n-            \"Remove secrets before calling this tool.\"\n-        )\n-    return ToolGuardrailFunctionOutput.allow()\n+## 触发线\n \n-\n-@tool_output_guardrail\n-def redact_output(data):\n-    text = str(data.output or \"\")\n-    if \"sk-\" in text:\n-        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n-    return ToolGuardrailFunctionOutput.allow()\n-\n-\n-@function_tool(\n-    tool_input_guardrails=[block_secrets],\n-    tool_output_guardrails=[redact_output],\n-)\n-def classify_text(text: str) -> str:\n-    \"\"\"Classify text for internal routing.\"\"\"\n-    return f\"length:{len(text)}\"\n-\n-\n-agent = Agent(name=\"Classifier\", tools=[classify_text])\n-result = Runner.run_sync(agent, \"hello world\")\n-print(result.final_output)\n-```\n-\n-## 绊线\n-\n-如果输入或输出未通过安全防护措施，安全防护措施可以通过绊线来发出信号。一旦我们发现某个安全防护措施触发了绊线，我们会立即抛出 `{Input,Output}GuardrailTripwireTriggered` 异常并停止智能体执行。\n+如果输入或输出未通过安全防护措施的检查，安全防护措施可通过触发线发出信号。一旦检测到某个安全防护措施触发了触发线，我们会立即抛出 `{Input,Output}GuardrailTripwireTriggered` 异常并停止智能体执行。\n \n ## 实现安全防护措施\n \n-你需要提供一个接收输入并返回[`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]的函数。在此示例中，我们将通过在底层运行一个智能体来实现。\n+你需要提供一个函数来接收输入，并返回一个 [`GuardrailFunctionOutput`][agents.guardrail.GuardrailFunctionOutput]。在此示例中，我们将通过在底层运行一个智能体来实现。\n \n ```python\n from pydantic import BaseModel\n@@ -160,7 +119,7 @@ async def main():\n 3. 我们可以在安全防护结果中包含额外信息。\n 4. 这是定义工作流的实际智能体。\n \n-输出安全防护措施与此类似。\n+输出安全防护措施类似。\n \n ```python\n from pydantic import BaseModel\n@@ -216,4 +175,49 @@ async def main():\n 1. 这是实际智能体的输出类型。\n 2. 这是安全防护措施的输出类型。\n 3. 这是接收智能体输出并返回结果的安全防护函数。\n-4. 这是定义工作流的实际智能体。\n\\ No newline at end of file\n+4. 这是定义工作流的实际智能体。\n+\n+最后，以下是工具安全防护措施的示例。\n+\n+```python\n+import json\n+from agents import (\n+    Agent,\n+    Runner,\n+    ToolGuardrailFunctionOutput,\n+    function_tool,\n+    tool_input_guardrail,\n+    tool_output_guardrail,\n+)\n+\n+@tool_input_guardrail\n+def block_secrets(data):\n+    args = json.loads(data.context.tool_arguments or \"{}\")\n+    if \"sk-\" in json.dumps(args):\n+        return ToolGuardrailFunctionOutput.reject_content(\n+            \"Remove secrets before calling this tool.\"\n+        )\n+    return ToolGuardrailFunctionOutput.allow()\n+\n+\n+@tool_output_guardrail\n+def redact_output(data):\n+    text = str(data.output or \"\")\n+    if \"sk-\" in text:\n+        return ToolGuardrailFunctionOutput.reject_content(\"Output contained sensitive data.\")\n+    return ToolGuardrailFunctionOutput.allow()\n+\n+\n+@function_tool(\n+    tool_input_guardrails=[block_secrets],\n+    tool_output_guardrails=[redact_output],\n+)\n+def classify_text(text: str) -> str:\n+    \"\"\"Classify text for internal routing.\"\"\"\n+    return f\"length:{len(text)}\"\n+\n+\n+agent = Agent(name=\"Classifier\", tools=[classify_text])\n+result = Runner.run_sync(agent, \"hello world\")\n+print(result.final_output)\n+```\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fguardrails.md",
        "sha": "cb714f19c7034ea4ef080688c05e21aba9b338d8",
        "status": "modified"
      },
      {
        "additions": 18,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fhandoffs.md",
        "changes": 36,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fhandoffs.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 18,
        "filename": "docs/zh/handoffs.md",
        "patch": "@@ -4,21 +4,21 @@ search:\n ---\n # 任务转移\n \n-任务转移允许一个智能体将任务委派给另一个智能体。这在不同智能体专注于不同领域的场景中尤为有用。例如，一个客服应用可能拥有分别处理订单状态、退款、常见问题等任务的智能体。\n+任务转移允许一个智能体将任务委派给另一个智能体。这在不同智能体各自专长不同领域的场景中特别有用。例如，一个客户支持应用可以让不同智能体分别处理订单状态、退款、常见问题等任务。\n \n-对于 LLM 而言，任务转移以工具的形式呈现。因此，如果有一个移交到名为 `Refund Agent` 的智能体的任务转移，那么该工具将被命名为 `transfer_to_refund_agent`。\n+对 LLM 而言，任务转移被表示为工具。因此，如果有一个转移到名为 `Refund Agent` 的智能体，对应的工具会被命名为 `transfer_to_refund_agent`。\n \n ## 创建任务转移\n \n-所有智能体都有一个 [`handoffs`][agents.agent.Agent.handoffs] 参数，它可以直接接收一个 `Agent`，或一个用于自定义任务转移的 `Handoff` 对象。\n+所有智能体都有一个 [`handoffs`][agents.agent.Agent.handoffs] 参数，它可以直接接收一个 `Agent`，或者一个用于自定义任务转移的 `Handoff` 对象。\n \n-如果传入的是普通的 `Agent` 实例，它们的 [`handoff_description`][agents.agent.Agent.handoff_description]（如果设置）会附加到默认的工具描述中。使用它可以提示模型在无需编写完整 `handoff()` 对象的情况下选择该任务转移。\n+如果你传入的是普通的 `Agent` 实例，那么其 [`handoff_description`][agents.agent.Agent.handoff_description]（若已设置）会被追加到默认的工具描述中。用它来提示模型在无需编写完整 `handoff()` 对象的情况下何时应选择该任务转移。\n \n-你可以使用 Agents SDK 提供的 [`handoff()`][agents.handoffs.handoff] 函数来创建任务转移。该函数允许你指定要移交到的智能体，并可选地提供覆盖项和输入过滤器。\n+你可以使用 Agents SDK 提供的 [`handoff()`][agents.handoffs.handoff] 函数创建任务转移。此函数允许你指定要转移到的智能体，以及可选的覆盖项和输入过滤器。\n \n ### 基本用法\n \n-以下是如何创建一个简单的任务转移：\n+如下是创建一个简单任务转移的方法：\n \n ```python\n from agents import Agent, handoff\n@@ -30,18 +30,18 @@ refund_agent = Agent(name=\"Refund agent\")\n triage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n ```\n \n-1. 你可以直接使用智能体（如 `billing_agent`），也可以使用 `handoff()` 函数。\n+1. 你可以直接使用智能体（如 `billing_agent`），或使用 `handoff()` 函数。\n \n ### 通过 `handoff()` 函数自定义任务转移\n \n [`handoff()`][agents.handoffs.handoff] 函数允许你进行自定义。\n \n-- `agent`: 要移交到的智能体。\n-- `tool_name_override`: 默认使用 `Handoff.default_tool_name()` 函数，其解析为 `transfer_to_<agent_name>`。你可以覆盖它。\n-- `tool_description_override`: 覆盖 `Handoff.default_tool_description()` 提供的默认工具描述。\n-- `on_handoff`: 当任务转移被调用时执行的回调函数。这对于在你知道将进行任务转移时立即启动一些数据获取等操作很有用。该函数会接收智能体上下文，并可选地接收 LLM 生成的输入。输入数据由 `input_type` 参数控制。\n-- `input_type`: 任务转移预期的输入类型（可选）。\n-- `input_filter`: 允许你过滤下一个智能体接收的输入。详见下文。\n+- `agent`: 要转移到的智能体。\n+- `tool_name_override`: 默认使用 `Handoff.default_tool_name()`，其解析为 `transfer_to_<agent_name>`。你可以覆盖它。\n+- `tool_description_override`: 覆盖来自 `Handoff.default_tool_description()` 的默认工具描述。\n+- `on_handoff`: 任务转移被调用时执行的回调函数。可用于在确定要进行任务转移时立即启动数据获取等操作。该函数接收智能体上下文，并可选接收 LLM 生成的输入。输入数据由 `input_type` 参数控制。\n+- `input_type`: 任务转移期望的输入类型（可选）。\n+- `input_filter`: 允许你过滤下一个智能体将接收的输入。详见下文。\n - `is_enabled`: 任务转移是否启用。可以是布尔值或返回布尔值的函数，允许你在运行时动态启用或禁用任务转移。\n \n ```python\n@@ -62,7 +62,7 @@ handoff_obj = handoff(\n \n ## 任务转移输入\n \n-在某些情况下，你希望 LLM 在调用任务转移时提供一些数据。比如，设想一个移交到“升级智能体（Escalation agent）”的场景。你可能希望提供一个原因，以便记录。\n+在某些情况下，你希望 LLM 在调用任务转移时提供一些数据。比如，想象一个转移到“升级处理（Escalation）智能体”的场景。你可能希望提供一个原因，以便记录。\n \n ```python\n from pydantic import BaseModel\n@@ -86,11 +86,11 @@ handoff_obj = handoff(\n \n ## 输入过滤器\n \n-当发生任务转移时，就好像新智能体接管了对话，并且可以看到整个先前的对话历史。如果你想改变这一点，可以设置一个 [`input_filter`][agents.handoffs.Handoff.input_filter]。输入过滤器是一个函数，它通过 [`HandoffInputData`][agents.handoffs.HandoffInputData] 接收现有输入，并且必须返回一个新的 `HandoffInputData`。\n+当发生任务转移时，就好像新的智能体接管了对话，并能够看到之前的完整对话历史。如果你想改变这一点，可以设置一个 [`input_filter`][agents.handoffs.Handoff.input_filter]。输入过滤器是一个函数，它通过 [`HandoffInputData`][agents.handoffs.HandoffInputData] 接收现有输入，并且必须返回新的 `HandoffInputData`。\n \n-默认情况下，运行器现在会将先前的对话记录折叠为一条助手摘要消息（参见 [`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]）。该摘要出现在一个 `<CONVERSATION HISTORY>` 块中，当同一次运行中发生多次任务转移时，此块会不断追加新的轮次。你可以通过 [`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper] 提供你自己的映射函数，以替换生成的消息，而无需编写完整的 `input_filter`。该默认行为仅在任务转移和运行都未提供显式 `input_filter` 时适用，因此已自定义载荷的现有代码（包括本仓库中的 code examples）无需更改即可保持当前行为。你可以通过向 [`handoff(...)`][agents.handoffs.handoff] 传入 `nest_handoff_history=True` 或 `False` 来覆盖单次任务转移的嵌套行为，这会设置 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history]。如果你只需要更改生成摘要的包裹文本，请在运行智能体之前调用 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers]（并可选调用 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]）。\n+默认情况下，runner 现在会将先前的对话转录折叠为一条助理摘要消息（参见 [`RunConfig.nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]）。该摘要出现在一个 `<CONVERSATION HISTORY>` 块中，当在同一次运行中发生多次任务转移时会不断追加新的对话轮次。你可以通过 [`RunConfig.handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper] 提供你自己的映射函数，以替换生成的消息，而无需编写完整的 `input_filter`。该默认行为仅在任务转移和运行都未提供显式 `input_filter` 时生效，因此已自定义负载的现有代码（包括本仓库中的 code examples）将无须变更即可保持当前行为。你可以在单次任务转移中通过向 [`handoff(...)`][agents.handoffs.handoff] 传入 `nest_handoff_history=True` 或 `False`，覆盖默认的嵌套行为，这会设置 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history]。如果你只需要修改生成摘要的包装文本，请在运行智能体前调用 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers]（以及可选的 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]）。\n \n-有一些常见模式（例如从历史中移除所有工具调用），已在 [`agents.extensions.handoff_filters`][] 中为你实现。\n+有一些常见模式（例如从历史中移除所有工具调用）已在 [`agents.extensions.handoff_filters`][] 中为你实现。\n \n ```python\n from agents import Agent, handoff\n@@ -108,7 +108,7 @@ handoff_obj = handoff(\n \n ## 推荐提示词\n \n-为确保 LLM 正确理解任务转移，我们建议在你的智能体中包含有关任务转移的信息。我们在 [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] 中提供了建议的前缀，或者你可以调用 [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] 将推荐数据自动添加到你的提示词中。\n+为确保 LLM 正确理解任务转移，我们建议在智能体中包含有关任务转移的信息。我们在 [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`][] 中提供了一个推荐前缀，或者你可以调用 [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`][] 将推荐的信息自动添加到你的提示词中。\n \n ```python\n from agents import Agent",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fhandoffs.md",
        "sha": "ee6a749f50ea4f04a089c2c309e0616f27723f0a",
        "status": "modified"
      },
      {
        "additions": 19,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Findex.md",
        "changes": 38,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 19,
        "filename": "docs/zh/index.md",
        "patch": "@@ -4,31 +4,31 @@ search:\n ---\n # OpenAI Agents SDK\n \n-[OpenAI Agents SDK](https://github.com/openai/openai-agents-python) 让你以轻量、易用、几乎不引入抽象的方式构建基于智能体的 AI 应用。它是我们此前面向智能体的实验项目 [Swarm](https://github.com/openai/swarm/tree/main) 的面向生产升级版。Agents SDK 仅包含一小组基本组件：\n+[OpenAI Agents SDK](https://github.com/openai/openai-agents-python) 使你能够以轻量、易用、抽象极少的方式构建智能体式 AI 应用。它是我们此前针对智能体的试验项目 [Swarm](https://github.com/openai/swarm/tree/main) 的可用于生产的升级版。Agents SDK 仅包含一小组基本组件：\n \n-- **智能体**：配备了 instructions 和 tools 的 LLM\n-- **任务转移**：允许智能体将特定任务委派给其他智能体\n-- **安全防护措施**：支持对智能体输入与输出进行验证\n-- **会话**：在多次智能体运行间自动维护对话历史\n+-   **智能体（Agents）**，即配备 instructions 和 tools 的 LLM\n+-   **任务转移（Handoffs）**，允许智能体将特定任务委派给其他智能体\n+-   **安全防护措施（Guardrails）**，用于验证智能体的输入与输出\n+-   **会话（Sessions）**，在多次智能体运行间自动维护对话历史\n \n-结合 Python，这些基本组件足以表达工具与智能体之间的复杂关系，使你无需陡峭学习曲线即可构建真实世界应用。此外，SDK 内置 **追踪**，可视化与调试你的智能体流程，并支持评估、以及针对你的应用微调模型，甚至进行蒸馏。\n+结合 Python，这些基本组件足以表达 tools 与智能体之间的复杂关系，让你无需陡峭学习曲线即可构建真实世界应用。此外，SDK 自带内置的 **追踪（tracing）**，可帮助你可视化与调试智能体流程，对其进行评估，甚至为你的应用微调模型。\n \n-## Why use the Agents SDK\n+## 为何使用 Agents SDK\n \n-该 SDK 的两条核心设计原则：\n+该 SDK 遵循两条核心设计原则：\n \n-1. 功能足够多，值得使用；基本组件足够少，易于上手。\n-2. 开箱即用，同时允许你精确自定义行为。\n+1. 功能足够多以值得使用，但基本组件足够少以便快速上手。\n+2. 开箱即用且效果良好，同时支持你精确定制执行过程。\n \n-主要特性如下：\n+SDK 的主要特性包括：\n \n-- 智能体循环：内置循环，负责调用工具、将结果回传给 LLM，并持续迭代直至 LLM 完成。\n-- Python 优先：使用语言内置特性编排与串联智能体，无需学习新的抽象。\n-- 任务转移：在多个智能体间进行协调与委派的强大能力。\n-- 安全防护措施：与智能体并行运行输入校验与检查，若检查失败可提前中断。\n-- 会话：跨多次智能体运行自动管理对话历史，免去手动状态处理。\n-- 工具调用：将任意 Python 函数变为工具，自动生成模式，并通过 Pydantic 提供校验。\n-- 追踪：内置追踪，便于可视化、调试与监控工作流，并可使用 OpenAI 的评估、微调与蒸馏工具套件。\n+-   智能体循环：内置循环，负责调用 tools、将结果发送给 LLM，并循环直至 LLM 完成。\n+-   Python 优先：使用内置语言特性来编排并串联智能体，而无需学习新的抽象概念。\n+-   任务转移：在多个智能体间进行协调与委派的强大能力。\n+-   安全防护措施：与智能体并行运行输入验证与检查，如失败则尽早中止。\n+-   会话：跨多次智能体运行的对话历史自动管理，省去手动状态处理。\n+-   工具调用：将任意 Python 函数变为 tool，自动生成 schema，并由 Pydantic 提供验证。\n+-   追踪：内置追踪，支持可视化、调试与监控工作流，并可使用 OpenAI 的评估、微调与蒸馏工具套件。\n \n ## 安装\n \n@@ -51,7 +51,7 @@ print(result.final_output)\n # Infinite loop's dance.\n ```\n \n-(_如果要运行，请确保已设置 `OPENAI_API_KEY` 环境变量_)\n+(_If running this, ensure you set the `OPENAI_API_KEY` environment variable_)\n \n ```bash\n export OPENAI_API_KEY=sk-...",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Findex.md",
        "sha": "b2fa57179beef8b0cb7424c20c64c4c9943621d0",
        "status": "modified"
      },
      {
        "additions": 50,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmcp.md",
        "changes": 105,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fmcp.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 55,
        "filename": "docs/zh/mcp.md",
        "patch": "@@ -4,34 +4,32 @@ search:\n ---\n # Model context protocol (MCP)\n \n-[Model context protocol](https://modelcontextprotocol.io/introduction)（MCP）标准化了应用如何向语言模型暴露工具与上下文。官方文档中指出：\n+[Model context protocol](https://modelcontextprotocol.io/introduction)（MCP）标准化了应用如何向语言模型暴露工具与上下文。以下摘自官方文档：\n \n-> MCP 是一个开放协议，用于标准化应用向 LLM 提供上下文的方式。可以把 MCP 想象成面向 AI 应用的 USB‑C 接口。正如 USB‑C 为你的设备连接各类外设与配件提供了标准化方式，MCP 也为将 AI 模型连接到不同数据源与工具提供了标准化方式。\n+> MCP 是一个开放协议，用于标准化应用如何向 LLM 提供上下文。可以把 MCP 想象成 AI 应用的 USB‑C 接口。就像 USB‑C 提供了将设备连接到各种外设与配件的标准化方式，MCP 为将 AI 模型连接到不同数据源与工具提供了标准化方式。\n \n-Agents Python SDK 支持多种 MCP 传输方式。这使你可以复用现有 MCP 服务器，或自行搭建以向智能体暴露文件系统、HTTP 或连接器驱动的工具。\n+Agents Python SDK 支持多种 MCP 传输方式。这让你可以复用现有 MCP 服务或自行构建，通过智能体暴露文件系统、HTTP 或基于连接器的工具。\n \n-## Choosing an MCP integration\n+## 选择 MCP 集成方式\n \n-在将 MCP 服务器接入智能体之前，先确定工具调用应在哪里执行，以及可达的传输方式。下表总结了 Python SDK 支持的选项。\n+在将 MCP 服务接入智能体前，先决定工具调用应在哪里执行，以及你能使用哪些传输方式。下表总结了 Python SDK 支持的选项。\n \n-| 你的需求                                                                            | 推荐选项                                              |\n+| 你的需求                                                                               | 推荐选项                                              |\n | ------------------------------------------------------------------------------------ | ----------------------------------------------------- |\n-| 让 OpenAI 的 Responses API 代表模型调用一个公网可达的 MCP 服务器                    | **Hosted MCP server tools**，通过 [`HostedMCPTool`][agents.tool.HostedMCPTool] |\n-| 连接你在本地或远端运行的可流式 HTTP 服务器                                           | **Streamable HTTP MCP servers**，通过 [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |\n-| 与实现了带 Server-Sent Events 的 HTTP 的服务器通信                                   | **HTTP with SSE MCP servers**，通过 [`MCPServerSse`][agents.mcp.server.MCPServerSse] |\n-| 启动本地进程并通过 stdin/stdout 通信                                                 | **stdio MCP servers**，通过 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |\n+| 让 OpenAI 的 Responses API 代表模型调用可公网访问的 MCP 服务                           | **托管 MCP 服务工具**，通过 [`HostedMCPTool`][agents.tool.HostedMCPTool] |\n+| 连接你在本地或远程运行的可流式传输的 HTTP 服务                                         | **可流式 HTTP MCP 服务**，通过 [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp] |\n+| 与实现了带 Server-Sent Events 的 HTTP 的服务通信                                       | **HTTP + SSE MCP 服务**，通过 [`MCPServerSse`][agents.mcp.server.MCPServerSse] |\n+| 启动本地进程并通过 stdin/stdout 通信                                                   | **stdio MCP 服务**，通过 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio] |\n \n-下面各小节将介绍每个选项、如何配置，以及在何种情况下优先选择某种传输方式。\n+下文将逐一介绍每种选项的配置方法，以及在何种情况下优先选择某种传输方式。\n \n-## 1. Hosted MCP server tools\n+## 1. 托管 MCP 服务工具\n \n-托管工具将整个工具调用的往返流程放在 OpenAI 的基础设施内完成。不是由你的代码列举并调用工具，而是通过\n-[`HostedMCPTool`][agents.tool.HostedMCPTool] 将服务器标签（以及可选的连接器元数据）转发给 Responses API。模型会列举远端服务器的工具并直接调用，而无需额外回调到你的 Python 进程。托管工具目前适用于支持 Responses API 的 hosted MCP 集成的 OpenAI 模型。\n+托管工具将整个工具调用往返放入 OpenAI 的基础设施中。你的代码无需列出和调用工具，[`HostedMCPTool`][agents.tool.HostedMCPTool] 会将服务器标签（以及可选的连接器元数据）转发给 Responses API。模型会列出远程服务器的工具并直接调用，而无需额外回调到你的 Python 进程。托管工具目前可用于支持 Responses API 托管 MCP 集成的 OpenAI 模型。\n \n-### Basic hosted MCP tool\n+### 基本托管 MCP 工具\n \n-通过在智能体的 `tools` 列表中添加一个 [`HostedMCPTool`][agents.tool.HostedMCPTool] 来创建托管工具。`tool_config`\n-字典与发送到 REST API 的 JSON 相对应：\n+通过在智能体的 `tools` 列表中添加 [`HostedMCPTool`][agents.tool.HostedMCPTool] 来创建托管工具。`tool_config` 字典与您发送到 REST API 的 JSON 保持一致：\n \n ```python\n import asyncio\n@@ -61,9 +59,9 @@ asyncio.run(main())\n \n 托管服务器会自动暴露其工具；你无需将其添加到 `mcp_servers`。\n \n-### Streaming hosted MCP results\n+### 托管 MCP 结果的流式传输\n \n-托管工具以与工具调用完全相同的方式支持流式传输结果。将 `stream=True` 传给 `Runner.run_streamed`，即可在模型仍在运行时消费增量 MCP 输出：\n+托管工具以与工具调用完全相同的方式支持流式结果。向 `Runner.run_streamed` 传入 `stream=True`，即可在模型仍在运行时消费增量 MCP 输出：\n \n ```python\n result = Runner.run_streamed(agent, \"Summarise this repository's top languages\")\n@@ -73,10 +71,9 @@ async for event in result.stream_events():\n print(result.final_output)\n ```\n \n-### Optional approval flows\n+### 可选审批流程\n \n-如果服务器可以执行敏感操作，你可以在每次工具执行前要求人工或程序化审批。在 `tool_config` 中配置\n-`require_approval`，可以是单一策略（`\"always\"`、`\"never\"`），也可以是将工具名映射到策略的字典。若要在 Python 内做出决定，请提供 `on_approval_request` 回调。\n+如果服务器可以执行敏感操作，你可以在每次工具执行前要求人工或程序化审批。在 `tool_config` 中配置 `require_approval`，可使用单一策略（`\"always\"`、`\"never\"`）或将工具名映射到策略的字典。若要在 Python 内部做决策，提供一个 `on_approval_request` 回调。\n \n ```python\n from agents import MCPToolApprovalFunctionResult, MCPToolApprovalRequest\n@@ -104,11 +101,11 @@ agent = Agent(\n )\n ```\n \n-当模型需要审批数据以继续运行时，将调用该回调；回调可以是同步或异步的。\n+回调可以是同步或异步的，并会在模型需要审批数据以继续运行时被调用。\n \n-### Connector-backed hosted servers\n+### 基于连接器的托管服务\n \n-托管 MCP 还支持 OpenAI 连接器。无需指定 `server_url`，只需提供 `connector_id` 和访问令牌。Responses API 会处理身份验证，托管服务器将暴露该连接器的工具。\n+托管 MCP 也支持 OpenAI connectors。无需指定 `server_url`，而是提供 `connector_id` 和访问令牌。Responses API 负责认证，托管服务器会暴露该连接器的工具。\n \n ```python\n import os\n@@ -124,13 +121,12 @@ HostedMCPTool(\n )\n ```\n \n-完整可运行的托管工具示例——包括流式传输、审批与连接器——位于\n+完整可用的托管工具示例——包括流式传输、审批与连接器——参见\n [`examples/hosted_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp)。\n \n-## 2. Streamable HTTP MCP servers\n+## 2. 可流式 HTTP MCP 服务\n \n-当你希望自行管理网络连接时，请使用\n-[`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]。当你可控传输方式，或希望在自有基础设施内运行服务器并保持低延迟时，可流式 HTTP 服务器是理想选择。\n+当你希望自行管理网络连接时，使用 [`MCPServerStreamableHttp`][agents.mcp.server.MCPServerStreamableHttp]。当你可控传输方式、或想在自有基础设施中运行服务器并保持低延迟时，可流式 HTTP 服务是理想选择。\n \n ```python\n import asyncio\n@@ -165,21 +161,20 @@ async def main() -> None:\n asyncio.run(main())\n ```\n \n-构造函数还接受其他选项：\n+构造函数接受以下附加选项：\n \n - `client_session_timeout_seconds` 控制 HTTP 读取超时。\n - `use_structured_content` 切换是否优先使用 `tool_result.structured_content` 而非文本输出。\n-- `max_retry_attempts` 与 `retry_backoff_seconds_base` 为 `list_tools()` 与 `call_tool()` 添加自动重试。\n-- `tool_filter` 允许你仅暴露部分工具（参见 [Tool filtering](#tool-filtering)）。\n+- `max_retry_attempts` 和 `retry_backoff_seconds_base` 为 `list_tools()` 与 `call_tool()` 添加自动重试。\n+- `tool_filter` 允许只暴露工具子集（参见 [工具过滤](#tool-filtering)）。\n \n-## 3. HTTP with SSE MCP servers\n+## 3. HTTP + SSE MCP 服务\n \n !!! warning\n \n-    MCP 项目已弃用 Server-Sent Events 传输。新集成请优先使用 Streamable HTTP 或 stdio，将 SSE 仅用于遗留服务器。\n+    MCP 项目已弃用 Server‑Sent Events 传输。新集成应优先选择可流式 HTTP 或 stdio，仅在遗留服务中保留 SSE。\n \n-如果 MCP 服务器实现了基于 SSE 的 HTTP 传输，请实例化\n-[`MCPServerSse`][agents.mcp.server.MCPServerSse]。除传输方式不同外，其 API 与可流式 HTTP 服务器相同。\n+如果 MCP 服务实现了带 Server‑Sent Events 的 HTTP 传输，实例化 [`MCPServerSse`][agents.mcp.server.MCPServerSse]。除传输方式外，API 与可流式 HTTP 服务相同。\n \n ```python\n \n@@ -206,9 +201,9 @@ async with MCPServerSse(\n     print(result.final_output)\n ```\n \n-## 4. stdio MCP servers\n+## 4. stdio MCP 服务\n \n-对于以本地子进程运行的 MCP 服务器，使用 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]。SDK 会启动进程、保持管道打开，并在上下文管理器退出时自动关闭。此选项适用于快速原型，或当服务器仅暴露命令行入口时。\n+对于作为本地子进程运行的 MCP 服务，使用 [`MCPServerStdio`][agents.mcp.server.MCPServerStdio]。SDK 会启动进程、保持管道打开，并在上下文管理器退出时自动关闭。该选项适用于快速原型验证，或当服务器仅暴露命令行入口时。\n \n ```python\n from pathlib import Path\n@@ -234,11 +229,11 @@ async with MCPServerStdio(\n     print(result.final_output)\n ```\n \n-## Tool filtering\n+## 工具过滤\n \n-每个 MCP 服务器都支持工具过滤，以便你只暴露智能体所需的函数。过滤可以在构造时进行，也可以在每次运行时动态指定。\n+每个 MCP 服务都支持工具过滤，以便你仅暴露智能体所需的函数。过滤可在构造时或按运行动态进行。\n \n-### Static tool filtering\n+### 静态工具过滤\n \n 使用 [`create_static_tool_filter`][agents.mcp.create_static_tool_filter] 配置简单的允许/阻止列表：\n \n@@ -258,11 +253,11 @@ filesystem_server = MCPServerStdio(\n )\n ```\n \n-当同时提供 `allowed_tool_names` 与 `blocked_tool_names` 时，SDK 会先应用允许列表，再从剩余集合中移除被阻止的工具。\n+当同时提供 `allowed_tool_names` 与 `blocked_tool_names` 时，SDK 会先应用允许列表，然后再从剩余集合中移除被阻止的工具。\n \n-### Dynamic tool filtering\n+### 动态工具过滤\n \n-对于更复杂的逻辑，传入接收 [`ToolFilterContext`][agents.mcp.ToolFilterContext] 的可调用对象。该可调用对象可为同步或异步，并在应暴露工具时返回 `True`。\n+对于更复杂的逻辑，传入一个可调用对象，该对象接收一个 [`ToolFilterContext`][agents.mcp.ToolFilterContext]。该可调用对象可为同步或异步，并在应暴露工具时返回 `True`。\n \n ```python\n from pathlib import Path\n@@ -286,14 +281,14 @@ async with MCPServerStdio(\n     ...\n ```\n \n-过滤上下文会暴露当前的 `run_context`、请求工具的 `agent`，以及 `server_name`。\n+过滤上下文会提供当前的 `run_context`、请求工具的 `agent`，以及 `server_name`。\n \n-## Prompts\n+## 提示词\n \n-MCP 服务器还可以提供提示，用于动态生成智能体指令。支持提示的服务器会暴露两个方法：\n+MCP 服务也可以提供动态生成智能体指令的提示词。支持提示词的服务会暴露两个方法：\n \n-- `list_prompts()` 枚举可用的提示模板。\n-- `get_prompt(name, arguments)` 获取具体提示，可选传入参数。\n+- `list_prompts()` 列出可用的提示模板。\n+- `get_prompt(name, arguments)` 获取具体提示词，可选携带参数。\n \n ```python\n from agents import Agent\n@@ -311,21 +306,21 @@ agent = Agent(\n )\n ```\n \n-## Caching\n+## 缓存\n \n-每次智能体运行都会在每个 MCP 服务器上调用 `list_tools()`。远程服务器可能引入显著延迟，因此所有 MCP 服务器类都暴露了 `cache_tools_list` 选项。仅当你确认工具定义不频繁变化时才将其设为 `True`。若之后需要强制刷新列表，请在服务器实例上调用 `invalidate_tools_cache()`。\n+每次智能体运行都会对每个 MCP 服务调用 `list_tools()`。远程服务可能带来明显延迟，因此所有 MCP 服务类都暴露了 `cache_tools_list` 选项。仅当你确信工具定义不频繁变化时才将其设为 `True`。如需稍后强制刷新列表，可在服务实例上调用 `invalidate_tools_cache()`。\n \n-## Tracing\n+## 追踪\n \n [追踪](./tracing.md)会自动捕获 MCP 活动，包括：\n \n-1. 调用 MCP 服务器列举工具。\n+1. 调用 MCP 服务以列出工具。\n 2. 工具调用中的 MCP 相关信息。\n \n ![MCP 追踪截图](../assets/images/mcp-tracing.jpg)\n \n-## Further reading\n+## 延伸阅读\n \n - [Model Context Protocol](https://modelcontextprotocol.io/) – 规范与设计指南。\n - [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp) – 可运行的 stdio、SSE 与可流式 HTTP 示例。\n-- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) – 完整的托管 MCP 演示，包括审批与连接器。\n\\ No newline at end of file\n+- [examples/hosted_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp) – 完整的托管 MCP 演示，包含审批与连接器。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmcp.md",
        "sha": "e21f20bbd6ba060802c239d47d7b5d10f6b22963",
        "status": "modified"
      },
      {
        "additions": 33,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmodels%2Findex.md",
        "changes": 66,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fmodels%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 33,
        "filename": "docs/zh/models/index.md",
        "patch": "@@ -11,13 +11,13 @@ Agents SDK 开箱即用地支持两种 OpenAI 模型用法：\n \n ## OpenAI 模型\n \n-当你在初始化一个 `Agent` 时未指定模型，将使用默认模型。目前默认是出于兼容性和低延迟考虑的 [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1)。如果你有权限，我们建议将你的智能体设置为 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) 以获得更高质量，同时保留显式的 `model_settings`。\n+当你在初始化 `Agent` 时未指定模型，将使用默认模型。当前默认是为了兼容性和低延迟而设置的 [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1)。如果你有访问权限，我们建议将你的智能体设置为 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) 以获得更高质量，同时保持显式的 `model_settings`。\n \n 如果你想切换到其他模型（如 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2)），请按照下一节的步骤操作。\n \n ### 默认 OpenAI 模型\n \n-如果你希望在所有未设置自定义模型的智能体中一致地使用某个特定模型，请在运行智能体之前设置环境变量 `OPENAI_DEFAULT_MODEL`。\n+如果你希望对所有未设置自定义模型的智能体始终使用某个特定模型，请在运行你的智能体之前设置 `OPENAI_DEFAULT_MODEL` 环境变量。\n \n ```bash\n export OPENAI_DEFAULT_MODEL=gpt-5\n@@ -26,9 +26,9 @@ python3 my_awesome_agent.py\n \n #### GPT-5 模型\n \n-当你以这种方式使用任何 GPT-5 推理模型（[`gpt-5`](https://platform.openai.com/docs/models/gpt-5)、[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) 或 [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano)）时，SDK 会默认应用合理的 `ModelSettings`。具体而言，它会将 `reasoning.effort` 和 `verbosity` 均设置为 `\"low\"`。如果你希望自行构建这些设置，请调用 `agents.models.get_default_model_settings(\"gpt-5\")`。\n+当你以这种方式使用任意 GPT-5 推理模型（[`gpt-5`](https://platform.openai.com/docs/models/gpt-5)、[`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) 或 [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano)）时，SDK 会默认应用合理的 `ModelSettings`。具体而言，它会将 `reasoning.effort` 和 `verbosity` 都设为 `\"low\"`。如果你希望自行构建这些设置，请调用 `agents.models.get_default_model_settings(\"gpt-5\")`。\n \n-为获得更低延迟或满足特定需求，你可以选择不同的模型和设置。要为默认模型调整推理强度，请传入你自己的 `ModelSettings`：\n+为了更低的延迟或满足特定需求，你可以选择不同的模型和设置。要为默认模型调整推理力度，请传入你自己的 `ModelSettings`：\n \n ```python\n from openai.types.shared import Reasoning\n@@ -44,21 +44,21 @@ my_agent = Agent(\n )\n ```\n \n-特别是为了降低延迟，使用 [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) 或 [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) 并设置 `reasoning.effort=\"minimal\"`，通常会比默认设置更快返回响应。但需要注意，Responses API 中的一些内置工具（例如 文件检索 和 图像生成）不支持 `\"minimal\"` 的推理强度，这也是该 Agents SDK 默认选择 `\"low\"` 的原因。\n+尤其是为了更低延迟，使用 [`gpt-5-mini`](https://platform.openai.com/docs/models/gpt-5-mini) 或 [`gpt-5-nano`](https://platform.openai.com/docs/models/gpt-5-nano) 并设置 `reasoning.effort=\"minimal\"`，通常会比默认设置更快返回结果。但需要注意，Responses API 中的一些内置工具（例如 文件检索 和 图像生成）不支持 `\"minimal\"` 推理力度，这也是本 Agents SDK 默认使用 `\"low\"` 的原因。\n \n #### 非 GPT-5 模型\n \n-如果你传入的是非 GPT-5 的模型名称，且未提供自定义 `model_settings`，SDK 将回退到与任意模型兼容的通用 `ModelSettings`。\n+如果你在未提供自定义 `model_settings` 的情况下传入非 GPT-5 的模型名称，SDK 将回退到与任意模型兼容的通用 `ModelSettings`。\n \n ## 非 OpenAI 模型\n \n-你可以通过 [LiteLLM 集成](./litellm.md) 来使用大多数其他非 OpenAI 模型。首先，安装 litellm 依赖分组：\n+你可以通过 [LiteLLM 集成](./litellm.md) 使用大多数其他非 OpenAI 模型。首先，安装 litellm 依赖分组：\n \n ```bash\n pip install \"openai-agents[litellm]\"\n ```\n \n-然后，使用带有 `litellm/` 前缀的任意[受支持的模型](https://docs.litellm.ai/docs/providers)：\n+然后，使用带有 `litellm/` 前缀的任意[受支持模型](https://docs.litellm.ai/docs/providers)：\n \n ```python\n claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\n@@ -67,29 +67,29 @@ gemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)\n \n ### 使用非 OpenAI 模型的其他方式\n \n-你还可以通过另外 3 种方式集成其他 LLM 提供商（示例见[这里](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)）：\n+你还可以通过另外 3 种方式集成其他 LLM 提供方（示例见[此处](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)）：\n \n-1. [`set_default_openai_client`][agents.set_default_openai_client] 适用于你希望在全局使用 `AsyncOpenAI` 实例作为 LLM 客户端的情况。适用于 LLM 提供商具有 OpenAI 兼容 API 端点、并且你可以设置 `base_url` 和 `api_key` 的情形。可参见 [examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py) 中的可配置示例。\n-2. [`ModelProvider`][agents.models.interface.ModelProvider] 作用于 `Runner.run` 层级。它允许你声明“在本次运行中为所有智能体使用自定义模型提供商”。可参见 [examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py) 中的可配置示例。\n-3. [`Agent.model`][agents.agent.Agent.model] 允许你在特定的 Agent 实例上指定模型。这样你就可以为不同智能体混用不同的提供商。可参见 [examples/model_providers/custom_example_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py) 中的可配置示例。使用大多数可用模型的简便方式是通过 [LiteLLM 集成](./litellm.md)。\n+1. [`set_default_openai_client`][agents.set_default_openai_client] 适用于你希望全局使用一个 `AsyncOpenAI` 实例作为 LLM client 的场景。即该 LLM 提供方有 OpenAI 兼容的 API 端点，你可以设置 `base_url` 和 `api_key`。参见可配置示例：[examples/model_providers/custom_example_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py)。\n+2. [`ModelProvider`][agents.models.interface.ModelProvider] 位于 `Runner.run` 层级。它允许你声明“在本次运行中为所有智能体使用自定义模型提供方”。参见可配置示例：[examples/model_providers/custom_example_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py)。\n+3. [`Agent.model`][agents.agent.Agent.model] 允许你在特定 Agent 实例上指定模型。这样你可以为不同智能体混搭不同提供方。一个使用大多数可用模型的简便方法是通过 [LiteLLM 集成](./litellm.md)。\n \n-在你没有来自 `platform.openai.com` 的 API key 的情况下，我们建议通过 `set_tracing_disabled()` 禁用追踪，或设置[不同的追踪进程](../tracing.md)。\n+如果你没有来自 `platform.openai.com` 的 API key，我们建议通过 `set_tracing_disabled()` 禁用 追踪，或设置[不同的追踪 进程](../tracing.md)。\n \n !!! note\n \n-    在这些示例中，我们使用 Chat Completions API/模型，因为大多数 LLM 提供商尚未支持 Responses API。如果你的 LLM 提供商支持，我们建议使用 Responses。\n+    在这些示例中，我们使用 Chat Completions API/模型，因为大多数 LLM 提供方尚不支持 Responses API。如果你的 LLM 提供方支持，我们推荐使用 Responses。\n \n-## 模型混搭\n+## 模型的混合搭配\n \n-在单个工作流中，你可能希望为每个智能体使用不同的模型。例如，你可以使用更小、更快的模型做分诊，同时用更大、更强的模型处理复杂任务。在配置 [`Agent`][agents.Agent] 时，你可以通过以下方式选择特定模型：\n+在单个工作流中，你可能希望为每个智能体使用不同的模型。例如，你可以为分诊使用更小更快的模型，而为复杂任务使用更大更强的模型。在配置 [`Agent`][agents.Agent] 时，你可以通过以下任一方式选择特定模型：\n \n 1. 传入模型名称。\n-2. 传入任意模型名称 + 一个能够将该名称映射为 Model 实例的 [`ModelProvider`][agents.models.interface.ModelProvider]。\n+2. 传入任意模型名称 + 一个可以将该名称映射到 Model 实例的 [`ModelProvider`][agents.models.interface.ModelProvider]。\n 3. 直接提供一个 [`Model`][agents.models.interface.Model] 实现。\n \n !!!note\n \n-    虽然我们的 SDK 同时支持 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] 和 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] 两种形态，但我们建议在每个工作流中使用单一模型形态，因为这两种形态支持的功能和工具集合不同。如果你的工作流需要混用不同的模型形态，请确保你使用的所有功能在二者中均可用。\n+    虽然我们的 SDK 同时支持 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] 和 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel] 两种形态，但我们建议在每个工作流中使用单一模型形态，因为这两种形态支持的功能和工具不同。如果你的工作流确实需要混合不同的模型形态，请确保你使用的所有功能在两种形态上都可用。\n \n ```python\n from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n@@ -125,7 +125,7 @@ async def main():\n 1.  直接设置一个 OpenAI 模型的名称。\n 2.  提供一个 [`Model`][agents.models.interface.Model] 实现。\n \n-当你希望进一步配置智能体所用模型时，可以传入 [`ModelSettings`][agents.models.interface.ModelSettings]，它提供诸如 temperature 等可选的模型配置参数。\n+当你希望进一步配置某个智能体所使用的模型时，你可以传入 [`ModelSettings`][agents.models.interface.ModelSettings]，它提供诸如 temperature 等可选模型配置参数。\n \n ```python\n from agents import Agent, ModelSettings\n@@ -138,7 +138,7 @@ english_agent = Agent(\n )\n ```\n \n-此外，当你使用 OpenAI 的 Responses API 时，[还有一些其他可选参数](https://platform.openai.com/docs/api-reference/responses/create)（例如 `user`、`service_tier` 等）。如果这些参数在顶层不可用，你也可以通过 `extra_args` 传递它们。\n+此外，当你使用 OpenAI 的 Responses API 时，[还有一些其他可选参数](https://platform.openai.com/docs/api-reference/responses/create)（例如 `user`、`service_tier` 等）。如果它们未在顶层提供，你可以使用 `extra_args` 传入。\n \n ```python\n from agents import Agent, ModelSettings\n@@ -154,39 +154,39 @@ english_agent = Agent(\n )\n ```\n \n-## 使用其他 LLM 提供商的常见问题\n+## 使用其他 LLM 提供方的常见问题\n \n-### Tracing 客户端错误 401\n+### 追踪 client 错误 401\n \n-如果你遇到与追踪相关的错误，这是因为追踪数据会上传到 OpenAI 服务，而你没有 OpenAI API key。你有三种方式解决：\n+如果你遇到与 追踪 相关的错误，这是因为追踪数据会上传到 OpenAI 服务，而你没有 OpenAI API key。你有三种解决方案：\n \n 1. 完全禁用追踪：[`set_tracing_disabled(True)`][agents.set_tracing_disabled]。\n 2. 为追踪设置一个 OpenAI key：[`set_tracing_export_api_key(...)`][agents.set_tracing_export_api_key]。该 API key 仅用于上传追踪数据，且必须来自 [platform.openai.com](https://platform.openai.com/)。\n-3. 使用非 OpenAI 的追踪进程。参见[追踪文档](../tracing.md#custom-tracing-processors)。\n+3. 使用非 OpenAI 的追踪 进程。参见[追踪文档](../tracing.md#custom-tracing-processors)。\n \n ### Responses API 支持\n \n-SDK 默认使用 Responses API，但大多数其他 LLM 提供商尚未支持。因此你可能会看到 404 或类似问题。为解决此问题，你有两个选项：\n+SDK 默认使用 Responses API，但大多数其他 LLM 提供方尚未支持。因此你可能会看到 404 或类似问题。为解决该问题，你有两个选项：\n \n 1. 调用 [`set_default_openai_api(\"chat_completions\")`][agents.set_default_openai_api]。如果你通过环境变量设置了 `OPENAI_API_KEY` 和 `OPENAI_BASE_URL`，这将有效。\n-2. 使用 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]。示例见[这里](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)。\n+2. 使用 [`OpenAIChatCompletionsModel`][agents.models.openai_chatcompletions.OpenAIChatCompletionsModel]。示例见[此处](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)。\n \n-### Structured outputs 支持\n+### structured outputs 支持\n \n-一些模型提供商不支持 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)。这有时会导致类似如下的错误：\n+一些模型提供方不支持 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs)。这有时会导致如下类似错误：\n \n ```\n \n BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n \n ```\n \n-这是某些模型提供商的不足之处——它们支持 JSON 输出，但不允许你为输出指定要使用的 `json_schema`。我们正在努力修复这一点，但我们建议依赖那些支持 JSON schema 输出的提供商，否则你的应用常会因为 JSON 格式不正确而出错。\n+这是部分模型提供方的不足——它们支持 JSON 输出，但不允许你为输出指定 `json_schema`。我们正在着手修复此问题，但我们建议依赖支持 JSON schema 输出的提供方，否则你的应用可能会因为不合规的 JSON 而经常出错。\n \n-## 跨提供商混用模型\n+## 不同提供方间的模型混用\n \n-你需要注意不同模型提供商之间的功能差异，否则可能会遇到错误。例如，OpenAI 支持 structured outputs、多模态输入，以及托管的文件检索和网络检索，但许多其他提供商并不支持这些功能。请注意以下限制：\n+你需要了解不同模型提供方之间的功能差异，否则可能会遇到错误。例如，OpenAI 支持 structured outputs、多模态输入，以及托管的 文件检索 和 网络检索，但许多其他提供方不支持这些功能。请注意以下限制：\n \n--   不要向不支持的提供商发送其不理解的 `tools`\n+-   不要向不理解的提供方发送不受支持的 `tools`\n -   在调用仅支持文本的模型之前，过滤掉多模态输入\n--   注意不支持结构化 JSON 输出的提供商可能会偶尔生成无效的 JSON。\n\\ No newline at end of file\n+-   注意不支持结构化 JSON 输出的提供方有时会生成无效的 JSON。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmodels%2Findex.md",
        "sha": "278c16578077aa04d54e971170c9412419a13eb5",
        "status": "modified"
      },
      {
        "additions": 12,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmodels%2Flitellm.md",
        "changes": 24,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fmodels%2Flitellm.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 12,
        "filename": "docs/zh/models/litellm.md",
        "patch": "@@ -2,17 +2,17 @@\n search:\n   exclude: true\n ---\n-# 通过 LiteLLM 使用任意模型\n+# 基于 LiteLLM 的通用模型使用\n \n !!! note\n \n-    LiteLLM 集成目前为测试版。你可能会在一些模型提供商（尤其是较小的提供商）上遇到问题。请通过[GitHub 问题](https://github.com/openai/openai-agents-python/issues)反馈，我们会尽快修复。\n+    LiteLLM 集成处于测试阶段。你在使用部分模型提供方（尤其是规模较小的）时可能会遇到问题。请通过 [Github issues](https://github.com/openai/openai-agents-python/issues) 报告问题，我们会尽快修复。\n \n [LiteLLM](https://docs.litellm.ai/docs/) 是一个库，可通过统一接口使用 100+ 模型。我们在 Agents SDK 中加入了 LiteLLM 集成，让你可以使用任意 AI 模型。\n \n ## 设置\n \n-你需要确保可用 `litellm`。可以通过安装可选的 `litellm` 依赖组来实现：\n+你需要确保可用的 `litellm`。可通过安装可选的 `litellm` 依赖组来完成：\n \n ```bash\n pip install \"openai-agents[litellm]\"\n@@ -22,11 +22,11 @@ pip install \"openai-agents[litellm]\"\n \n ## 示例\n \n-这是一个可直接运行的示例。运行后会提示你输入模型名称和 API key。比如可以输入：\n+这是一个可直接运行的示例。运行后会提示你输入模型名称和 API Key。例如，你可以输入：\n \n--   `openai/gpt-4.1` 作为模型，并提供你的 OpenAI API key\n--   `anthropic/claude-3-5-sonnet-20240620` 作为模型，并提供你的 Anthropic API key\n--   等等\n+- `openai/gpt-4.1` 作为模型，并提供你的 OpenAI API Key\n+- `anthropic/claude-3-5-sonnet-20240620` 作为模型，并提供你的 Anthropic API Key\n+- 等等\n \n LiteLLM 支持的完整模型列表见 [litellm providers docs](https://docs.litellm.ai/docs/providers)。\n \n@@ -76,9 +76,9 @@ if __name__ == \"__main__\":\n     asyncio.run(main(model, api_key))\n ```\n \n-## 使用数据追踪\n+## 用量数据追踪\n \n-如果希望 LiteLLM 的响应填充到 Agents SDK 的使用指标中，在创建智能体时传入 `ModelSettings(include_usage=True)`。\n+如果你希望将 LiteLLM 的响应纳入 Agents SDK 的用量指标，在创建智能体时传入 `ModelSettings(include_usage=True)`。\n \n ```python\n from agents import Agent, ModelSettings\n@@ -91,14 +91,14 @@ agent = Agent(\n )\n ```\n \n-启用 `include_usage=True` 后，LiteLLM 请求会通过 `result.context_wrapper.usage` 报告 token 与请求计数，就像内置的 OpenAI 模型一样。\n+在 `include_usage=True` 的情况下，LiteLLM 请求会通过 `result.context_wrapper.usage` 报告 token 与请求计数，与内置的 OpenAI 模型一致。\n \n ## 故障排查\n \n-如果你看到来自 LiteLLM 响应的 Pydantic 序列化器警告，请通过设置以下项启用小型兼容性补丁：\n+如果你在 LiteLLM 响应中看到来自 Pydantic 的序列化警告，可通过设置以下选项启用一项小的兼容性补丁：\n \n ```bash\n export OPENAI_AGENTS_ENABLE_LITELLM_SERIALIZER_PATCH=true\n ```\n \n-此可选标志会在保持正常行为的同时抑制已知的 LiteLLM 序列化器警告。如果不需要，可关闭（未设置或设为 `false`）。\n\\ No newline at end of file\n+该可选标志会抑制已知的 LiteLLM 序列化警告，同时保持正常行为。如果不需要，可将其关闭（不设置或设为 `false`）。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmodels%2Flitellm.md",
        "sha": "4ca5ab45e8c6f257aaff5f4ec61bf3e5a5815c21",
        "status": "modified"
      },
      {
        "additions": 23,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmulti_agent.md",
        "changes": 46,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fmulti_agent.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 23,
        "filename": "docs/zh/multi_agent.md",
        "patch": "@@ -2,40 +2,40 @@\n search:\n   exclude: true\n ---\n-# 编排多个智能体\n+# 协调多个智能体\n \n-编排指的是你在应用中如何安排智能体的流程：哪些智能体运行、以何种顺序运行，以及它们如何决定下一步该做什么。编排智能体主要有两种方式：\n+协调是指应用中智能体的流程：哪些智能体运行、以何种顺序运行，以及它们如何决定下一步该做什么。协调智能体有两种主要方式：\n \n-1. 让 LLM 做决策：利用 LLM 的智能来规划、推理，并据此决定下一步操作。\n-2. 通过代码编排：用你的代码来确定智能体的流程。\n+1. 允许 LLM 做出决策：利用 LLM 的智能进行规划、推理，并据此决定采取哪些步骤。\n+2. 通过代码进行协调：用代码确定智能体的流程。\n \n-你可以混合使用这些方式。每种方式都有取舍，见下文。\n+你可以混合使用这些模式。每种方式都有各自的权衡，见下文。\n \n-## 通过 LLM 编排\n+## 通过 LLM 协调\n \n-一个智能体是配备了 instructions、tools 和 任务转移 的 LLM。这意味着面对开放式任务时，LLM 可以自主规划如何处理任务，使用 tools 采取行动并获取数据，并通过 任务转移 将子任务委派给子智能体。比如，一个研究型智能体可以配备如下工具：\n+智能体是配备了指令（instructions）、工具（tools）和任务转移（handoffs）的 LLM。这意味着在给定一个开放式任务时，LLM 可以自主规划如何处理该任务，使用工具采取行动并获取数据，并使用任务转移将工作委派给子智能体。例如，一个研究智能体可以配备以下工具：\n \n-- 网络检索，用于在线查找信息\n-- 文件检索与提取，用于搜索专有数据和连接\n+- 网络检索，用于在网上查找信息\n+- 文件检索与获取，用于搜索专有数据和连接\n - 计算机操作，用于在计算机上执行操作\n - 代码执行，用于进行数据分析\n-- 任务转移，交给擅长规划、报告撰写等的专门智能体\n+- 任务转移到擅长规划、撰写报告等的专业智能体\n \n-当任务是开放式的且你希望依赖 LLM 的智能时，这种模式非常适合。这里最重要的策略包括：\n+当任务是开放式且你希望依赖 LLM 的智能时，这种模式非常适合。这里最重要的做法是：\n \n-1. 投入于优质提示词。明确可用的 tools、如何使用它们，以及必须遵循的参数与边界。\n-2. 监控你的应用并持续迭代。找出问题所在，并迭代你的提示词。\n-3. 允许智能体自省与改进。例如，将其置于循环中并让其自我批判；或者提供错误信息并让其改进。\n-4. 使用在单一任务上表现出色的专门智能体，而非期望一个通用智能体能面面俱到。\n-5. 投入于 [evals](https://platform.openai.com/docs/guides/evals)。这能帮助你训练智能体以改进并在任务上表现更好。\n+1. 打磨高质量的提示词。明确说明有哪些工具、如何使用，以及必须遵守的参数范围。\n+2. 监控并迭代你的应用。观察问题出在哪里，并迭代优化提示词。\n+3. 允许智能体自省与改进。例如，将其置于循环中，让其自我批判；或提供错误信息并让其改进。\n+4. 使用在某一任务上表现突出的专业智能体，而不是期望一个通用智能体在所有方面都很强。\n+5. 投入到[评测 (evals)](https://platform.openai.com/docs/guides/evals)。这可以训练你的智能体不断改进，提升完成任务的能力。\n \n-## 通过代码编排\n+## 通过代码协调\n \n-虽然通过 LLM 编排很强大，但通过代码编排能在速度、成本和性能方面让任务更具确定性和可预测性。常见模式包括：\n+虽然通过 LLM 协调很强大，但通过代码协调可以在速度、成本和性能方面使任务更确定、更可预测。常见模式包括：\n \n-- 使用 [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) 生成你可以用代码检查的 格式良好的数据。比如，你可以让一个智能体将任务分类到几个目录中，然后基于该目录选择下一个智能体。\n-- 将一个智能体的输出转换为下一个智能体的输入来串联多个智能体。你可以将撰写博文这样的任务分解为一系列步骤——做研究、写提纲、写正文、批判性审阅、再改进。\n-- 将执行任务的智能体与负责评估和反馈的智能体一起放在 `while` 循环中运行，直到评估者认为输出满足某些标准为止。\n-- 并行运行多个智能体，例如通过 Python 基本组件如 `asyncio.gather`。当你有彼此不依赖的多个任务时，这对提升速度很有用。\n+- 使用[structured outputs](https://platform.openai.com/docs/guides/structured-outputs)生成你可以用代码检查的格式良好的数据。例如，你可以让智能体将任务分类到若干目录中，然后依据该目录选择下一个智能体。\n+- 将多个智能体串联起来，把一个的输出转换为下一个的输入。你可以将撰写博客这样的任务分解为一系列步骤——进行研究、写提纲、写正文、批判性审阅，然后改进。\n+- 让执行任务的智能体与负责评估和反馈的智能体在 `while` 循环中运行，直到评估者认为输出通过了某些标准。\n+- 并行运行多个智能体，例如通过 Python 的基础组件如 `asyncio.gather`。当你有多个彼此不依赖的任务时，这对提升速度很有帮助。\n \n-我们在 [`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns) 中提供了若干示例。\n\\ No newline at end of file\n+我们在[`examples/agent_patterns`](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)中提供了许多代码示例。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fmulti_agent.md",
        "sha": "9568f9c5299f809b30c456e7a97f4853f47019d3",
        "status": "modified"
      },
      {
        "additions": 10,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fquickstart.md",
        "changes": 20,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 10,
        "filename": "docs/zh/quickstart.md",
        "patch": "@@ -30,15 +30,15 @@ pip install openai-agents # or `uv add openai-agents`, etc\n \n ### 设置 OpenAI API 密钥\n \n-如果你还没有，请按照[这些说明](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)创建 OpenAI API 密钥。\n+如果你还没有，请按照[这些说明](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)创建一个 OpenAI API 密钥。\n \n ```bash\n export OPENAI_API_KEY=sk-...\n ```\n \n ## 创建你的第一个智能体\n \n-智能体由 instructions、名称以及可选的配置（如 `model_config`）定义。\n+智能体由 instructions、名称和可选配置（例如 `model_config`）定义。\n \n ```python\n from agents import Agent\n@@ -51,7 +51,7 @@ agent = Agent(\n \n ## 添加更多智能体\n \n-可以用相同方式定义更多智能体。`handoff_descriptions` 为确定任务转移路由提供额外上下文。\n+可以用同样的方式定义其他智能体。`handoff_descriptions` 提供了用于确定任务转移路由的额外上下文。\n \n ```python\n from agents import Agent\n@@ -71,7 +71,7 @@ math_tutor_agent = Agent(\n \n ## 定义你的任务转移\n \n-在每个智能体上，你可以定义一个可供选择的外发任务转移选项清单，以决定如何推进其任务。\n+在每个智能体上，你可以定义一个可供该智能体选择的外发任务转移选项清单，以决定如何推进其任务。\n \n ```python\n triage_agent = Agent(\n@@ -121,9 +121,9 @@ async def homework_guardrail(ctx, agent, input_data):\n     )\n ```\n \n-## 整体运行\n+## 集成运行\n \n-让我们把上述内容整合起来，运行整个工作流，使用任务转移和输入安全防护措施。\n+让我们把这些组合起来，使用任务转移和输入安全防护措施运行整个工作流。\n \n ```python\n from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\n@@ -190,14 +190,14 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-## 查看追踪\n+## 查看你的追踪\n \n-要回顾智能体运行期间发生的事情，请前往 [OpenAI 仪表盘中的 Trace viewer](https://platform.openai.com/traces) 查看你的运行追踪。\n+若要回顾智能体运行期间发生的情况，请前往 [OpenAI Dashboard 中的 Trace 查看器](https://platform.openai.com/traces)查看智能体运行的追踪。\n \n ## 后续步骤\n \n 了解如何构建更复杂的智能体流程：\n \n - 学习如何配置[智能体](agents.md)。\n-- 了解[运行智能体](running_agents.md)。\n-- 了解[tools](tools.md)、[安全防护措施](guardrails.md)和[模型](models/index.md)。\n\\ No newline at end of file\n+- 学习[运行智能体](running_agents.md)。\n+- 学习[工具](tools.md)、[安全防护措施](guardrails.md)和[模型](models/index.md)。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fquickstart.md",
        "sha": "0f08f786dfeb8c78ee01b07f70e1abe2940e69f7",
        "status": "modified"
      },
      {
        "additions": 42,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frealtime%2Fguide.md",
        "changes": 84,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Frealtime%2Fguide.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 42,
        "filename": "docs/zh/realtime/guide.md",
        "patch": "@@ -4,65 +4,65 @@ search:\n ---\n # 指南\n \n-本指南深入介绍如何使用 OpenAI Agents SDK 的 realtime 能力来构建语音驱动的 AI 智能体。\n+本指南深入介绍如何使用 OpenAI Agents SDK 的实时能力构建语音驱动的 AI 智能体。\n \n-!!! warning \"Beta 功能\"\n-Realtime 智能体处于测试阶段。随着实现的改进，可能会出现不兼容的变更。\n+!!! warning \"测试版功能\"\n+实时智能体处于测试阶段。随着实现不断改进，可能会出现不兼容变更。\n \n-## 概述\n+## 概览\n \n-Realtime 智能体支持对话式流程，能实时处理音频与文本输入，并以实时音频进行响应。它们与 OpenAI 的 Realtime API 保持持久连接，实现低延迟、可自然打断的语音对话。\n+实时智能体支持会话式流程，可实时处理音频与文本输入，并以实时音频进行响应。它们与 OpenAI 的 Realtime API 保持持久连接，实现低延迟、可自然打断的语音对话。\n \n ## 架构\n \n ### 核心组件\n \n-realtime 系统由以下关键组件组成：\n+实时系统由以下关键组件构成：\n \n--   **RealtimeAgent**: 一个智能体，通过 instructions、tools 和 任务转移 进行配置。\n--   **RealtimeRunner**: 管理配置。可调用 `runner.run()` 获取会话。\n--   **RealtimeSession**: 单次交互会话。通常在每次用户开始对话时创建一个，并保持其存活直到对话结束。\n+-   **RealtimeAgent**: 一个智能体，配置了 instructions、tools 和 任务转移。\n+-   **RealtimeRunner**: 管理配置。你可以调用 `runner.run()` 获取一个会话。\n+-   **RealtimeSession**: 单次交互会话。通常在每次用户开始对话时创建一个，并保持到对话结束。\n -   **RealtimeModel**: 底层模型接口（通常是 OpenAI 的 WebSocket 实现）\n \n ### 会话流程\n \n-典型的 realtime 会话流程如下：\n+典型的实时会话遵循以下流程：\n \n-1. **创建 RealtimeAgent**，并配置 instructions、tools 和 任务转移。\n-2. **设置 RealtimeRunner**，传入智能体与配置项\n-3. **启动会话**，使用 `await runner.run()` 返回一个 RealtimeSession。\n+1. **创建 RealtimeAgent**，包含 instructions、tools 和 任务转移。\n+2. **设置 RealtimeRunner**，提供智能体和配置选项\n+3. **启动会话**，使用 `await runner.run()`，返回一个 RealtimeSession。\n 4. **发送音频或文本消息** 到会话，使用 `send_audio()` 或 `send_message()`\n-5. **监听事件**，通过遍历会话对象获取事件——包括音频输出、转录、工具调用、任务转移和错误\n-6. **处理打断**，当用户打断说话时，自动停止当前音频生成\n+5. **监听事件**，通过迭代会话获取事件——包括音频输出、转写结果、工具调用、任务转移和错误\n+6. **处理打断**，当用户打断发言时，当前音频生成会自动停止\n \n-会话维护对话历史，并管理与 realtime 模型的持久连接。\n+会话会维护对话历史，并管理与实时模型的持久连接。\n \n ## 智能体配置\n \n-RealtimeAgent 的用法与常规 Agent 类似，但有一些关键差异。完整 API 详情参见 [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API 参考。\n+RealtimeAgent 的用法类似常规 Agent 类，但有一些关键差异。完整 API 详情见 [`RealtimeAgent`][agents.realtime.agent.RealtimeAgent] API 参考。\n \n 与常规智能体的主要差异：\n \n--   模型选择在会话级配置，而非智能体级。\n--   不支持 structured output（不支持 `outputType`）。\n--   可为每个智能体配置 Voice，但在第一个智能体发声后不可更改。\n--   其他功能如 tools、任务转移 和 instructions 与常规智能体一致。\n+-   模型选择在会话级配置，而不是智能体级。\n+-   不支持 structured outputs（不支持 `outputType`）。\n+-   可为每个智能体配置语音，但在第一个智能体开始说话后无法更改。\n+-   其他功能如 tools、任务转移和 instructions 的行为相同。\n \n ## 会话配置\n \n ### 模型设置\n \n-会话配置可控制底层 realtime 模型行为。你可以配置模型名称（如 `gpt-realtime`）、语音选择（alloy、echo、fable、onyx、nova、shimmer）以及支持的模态（文本和/或音频）。可分别设置输入与输出的音频格式，默认使用 PCM16。\n+会话配置允许你控制底层实时模型行为。你可以配置模型名称（例如 `gpt-realtime`）、语音选择（alloy、echo、fable、onyx、nova、shimmer）以及支持的模态（文本和/或音频）。输入与输出的音频格式都可设置，默认使用 PCM16。\n \n ### 音频配置\n \n-音频设置控制会话如何处理语音输入与输出。你可以使用诸如 Whisper 的模型进行输入音频转录、设置语言偏好，并提供转录提示以提升特定领域术语的准确性。回合检测（turn detection）设置控制智能体何时开始与停止回应，包括语音活动检测阈值、静音时长以及在检测到语音时的前后填充。\n+音频设置控制会话如何处理语音输入与输出。你可以使用如 Whisper 的模型进行输入音频转写，设置语言偏好，并提供转写提示以提升领域术语的准确性。轮次检测设置控制智能体何时开始与停止响应，可配置语音活动检测阈值、静音时长，以及检测语音前后的填充时长。\n \n ## 工具与函数\n \n ### 添加工具\n \n-与常规智能体一样，realtime 智能体支持在对话中执行的 工具调用（function tools）：\n+与常规智能体相同，实时智能体支持在对话中执行的 工具调用：\n \n ```python\n from agents import function_tool\n@@ -90,7 +90,7 @@ agent = RealtimeAgent(\n \n ### 创建任务转移\n \n-任务转移允许在专长不同的智能体之间转移对话。\n+任务转移允许在不同的专业化智能体之间转交对话。\n \n ```python\n from agents.realtime import realtime_handoff\n@@ -119,22 +119,22 @@ main_agent = RealtimeAgent(\n \n ## 事件处理\n \n-会话会流式发送事件，你可以通过遍历会话对象来监听。事件包括音频输出分片、转录结果、工具执行的开始与结束、智能体任务转移以及错误。需要处理的关键事件包括：\n+会话会流式传输事件，你可以通过迭代会话对象来监听。事件包括音频输出分片、转写结果、工具执行开始/结束、智能体任务转移以及错误。需要重点处理的事件包括：\n \n--   **audio**: 智能体响应的原始音频数据\n--   **audio_end**: 智能体完成发声\n+-   **audio**: 来自智能体响应的原始音频数据\n+-   **audio_end**: 智能体完成发言\n -   **audio_interrupted**: 用户打断了智能体\n--   **tool_start/tool_end**: 工具执行生命周期\n+-   **tool_start/tool_end**: 工具执行的生命周期\n -   **handoff**: 发生智能体任务转移\n--   **error**: 处理过程中出现错误\n+-   **error**: 处理过程中发生错误\n \n-完整事件详情参见 [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent]。\n+完整事件详情见 [`RealtimeSessionEvent`][agents.realtime.events.RealtimeSessionEvent]。\n \n ## 安全防护措施\n \n-Realtime 智能体仅支持输出安全防护措施。这些防护是去抖（debounced）的，并以周期性方式运行（而非每个词都运行），以避免实时生成时的性能问题。默认去抖长度为 100 个字符，可进行配置。\n+实时智能体仅支持输出侧的安全防护措施。这些防护是去抖动执行的，会定期运行（而不是逐字运行），以避免实时生成过程中的性能问题。默认去抖长度为 100 个字符，但可配置。\n \n-安全防护措施可直接附加到 `RealtimeAgent`，也可通过会话的 `run_config` 提供。两处来源的防护会同时生效。\n+可以将安全防护措施直接附加到 `RealtimeAgent`，或通过会话的 `run_config` 提供。两处配置的防护会共同生效。\n \n ```python\n from agents.guardrail import GuardrailFunctionOutput, OutputGuardrail\n@@ -152,19 +152,19 @@ agent = RealtimeAgent(\n )\n ```\n \n-当安全防护措施被触发时，会生成 `guardrail_tripped` 事件，并可中断智能体当前的响应。去抖行为有助于在安全与实时性能之间取得平衡。与文本智能体不同，realtime 智能体在防护被触发时**不会**抛出异常。\n+当安全防护措施被触发时，会生成 `guardrail_tripped` 事件，并可中断智能体当前的响应。去抖动机制有助于在安全性与实时性能之间取得平衡。与文本智能体不同，实时智能体在防护触发时不会抛出 Exception。\n \n ## 音频处理\n \n-使用 [`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio] 发送音频到会话，或使用 [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message] 发送文本。\n+通过 [`session.send_audio(audio_bytes)`][agents.realtime.session.RealtimeSession.send_audio] 发送音频到会话，或通过 [`session.send_message()`][agents.realtime.session.RealtimeSession.send_message] 发送文本。\n \n-对于音频输出，监听 `audio` 事件并通过你偏好的音频库播放音频数据。务必监听 `audio_interrupted` 事件，以便在用户打断智能体时立即停止播放并清空排队的音频。\n+对于音频输出，监听 `audio` 事件，并通过你选择的音频库播放音频数据。务必监听 `audio_interrupted` 事件，以在用户打断智能体时立即停止播放并清空任何已排队的音频。\n \n ## SIP 集成\n \n-你可以将 realtime 智能体接入通过 [Realtime Calls API](https://platform.openai.com/docs/guides/realtime-sip) 到来的电话。SDK 提供了 [`OpenAIRealtimeSIPModel`][agents.realtime.openai_realtime.OpenAIRealtimeSIPModel]，在通过 SIP 协商媒体的同时，复用相同的智能体流程。\n+你可以将实时智能体附加到通过 [Realtime Calls API](https://platform.openai.com/docs/guides/realtime-sip) 接入的来电。SDK 提供了 [`OpenAIRealtimeSIPModel`][agents.realtime.openai_realtime.OpenAIRealtimeSIPModel]，它在通过 SIP 协商媒体的同时复用相同的智能体流程。\n \n-使用方法：将该模型实例传给 runner，并在启动会话时提供 SIP 的 `call_id`。呼叫 ID 由指示来电的 webhook 传递。\n+使用时，将该模型实例传给 runner，并在启动会话时提供 SIP 的 `call_id`。来电的 Call ID 由指示来电的 webhook 传递。\n \n ```python\n from agents.realtime import RealtimeAgent, RealtimeRunner\n@@ -187,7 +187,7 @@ async with await runner.run(\n         ...\n ```\n \n-当来电方挂断时，SIP 会话结束，realtime 连接将自动关闭。完整电话集成示例参见 [`examples/realtime/twilio_sip`](https://github.com/openai/openai-agents-python/tree/main/examples/realtime/twilio_sip)。\n+当主叫挂断时，SIP 会话结束，实时连接会自动关闭。完整电话集成示例见 [`examples/realtime/twilio_sip`](https://github.com/openai/openai-agents-python/tree/main/examples/realtime/twilio_sip)。\n \n ## 直接访问模型\n \n@@ -198,8 +198,8 @@ async with await runner.run(\n session.model.add_listener(my_custom_listener)\n ```\n \n-这将为你提供对 [`RealtimeModel`][agents.realtime.model.RealtimeModel] 接口的直接访问，适用于需要更底层连接控制的高级用例。\n+这将为你提供对 [`RealtimeModel`][agents.realtime.model.RealtimeModel] 接口的直接访问，以便在需要更底层连接控制的高级用例中使用。\n \n-## 示例\n+## 代码示例\n \n-欲获取完整可运行的示例，请参阅 [examples/realtime 目录](https://github.com/openai/openai-agents-python/tree/main/examples/realtime)，其中包含带 UI 和不带 UI 的演示。\n\\ No newline at end of file\n+完整可运行的示例请参见 [examples/realtime 目录](https://github.com/openai/openai-agents-python/tree/main/examples/realtime)，其中包含带 UI 组件和不带 UI 组件的演示。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frealtime%2Fguide.md",
        "sha": "d16f63590dbd7774f6b647fcac96451e90cf87fe",
        "status": "modified"
      },
      {
        "additions": 14,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frealtime%2Fquickstart.md",
        "changes": 28,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Frealtime%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 14,
        "filename": "docs/zh/realtime/quickstart.md",
        "patch": "@@ -4,20 +4,20 @@ search:\n ---\n # 快速开始\n \n-实时智能体通过 OpenAI 的 Realtime API 让你的 AI 智能体支持语音对话。本指南将带你创建第一个实时语音智能体。\n+实时智能体通过 OpenAI 的 Realtime API 实现与你的 AI 智能体进行语音对话。本指南将引导你创建第一个实时语音智能体。\n \n !!! warning \"测试版功能\"\n-实时智能体目前为测试版。随着实现不断改进，可能会有不向后兼容的变更。\n+实时智能体处于测试阶段。随着实现改进，可能会有不兼容变更。\n \n ## 前提条件\n \n - Python 3.9 或更高版本\n-- OpenAI API key\n-- 对 OpenAI Agents SDK 有基本了解\n+- OpenAI API 密钥\n+- 对 OpenAI Agents SDK 的基本了解\n \n ## 安装\n \n-如果尚未安装，请先安装 OpenAI Agents SDK：\n+如果尚未安装，请安装 OpenAI Agents SDK：\n \n ```bash\n pip install openai-agents\n@@ -32,7 +32,7 @@ import asyncio\n from agents.realtime import RealtimeAgent, RealtimeRunner\n ```\n \n-### 2. 创建实时智能体\n+### 2. 创建一个实时智能体\n \n ```python\n agent = RealtimeAgent(\n@@ -41,7 +41,7 @@ agent = RealtimeAgent(\n )\n ```\n \n-### 3. 设置 runner\n+### 3. 设置运行器\n \n ```python\n runner = RealtimeRunner(\n@@ -111,7 +111,7 @@ def _truncate_str(s: str, max_length: int) -> str:\n \n ## 完整示例\n \n-以下是一个可运行的完整示例：\n+下面是一个可运行的完整示例：\n \n ```python\n import asyncio\n@@ -202,24 +202,24 @@ if __name__ == \"__main__\":\n - `output_audio_format`: 输出音频格式\n - `input_audio_transcription`: 转写配置\n \n-### 轮次检测\n+### 说话轮次检测\n \n - `type`: 检测方法（`server_vad`、`semantic_vad`）\n - `threshold`: 语音活动阈值（0.0-1.0）\n-- `silence_duration_ms`: 用于检测轮次结束的静音时长\n+- `silence_duration_ms`: 用于检测说话结束的静音时长\n - `prefix_padding_ms`: 语音前的音频填充\n \n ## 后续步骤\n \n - [了解更多关于实时智能体](guide.md)\n-- 在 [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) 文件夹中查看可用示例\n+- 在 [examples/realtime](https://github.com/openai/openai-agents-python/tree/main/examples/realtime) 文件夹查看可用示例\n - 为你的智能体添加工具\n-- 在智能体之间实现任务转移\n-- 设置安全防护措施以保证安全\n+- 实现智能体之间的任务转移\n+- 配置安全防护措施\n \n ## 身份验证\n \n-确保你的 OpenAI API key 已在环境中设置：\n+确保在环境中设置了 OpenAI API 密钥：\n \n ```bash\n export OPENAI_API_KEY=\"your-api-key-here\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frealtime%2Fquickstart.md",
        "sha": "7243ad848797ede1f1df8770daa5f5be580426e6",
        "status": "modified"
      },
      {
        "additions": 16,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frelease.md",
        "changes": 32,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Frelease.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 16,
        "filename": "docs/zh/release.md",
        "patch": "@@ -2,51 +2,51 @@\n search:\n   exclude: true\n ---\n-# 发布流程/变更日志\n+# 发布流程/更新日志\n \n-本项目遵循一种稍作修改的语义化版本规则，采用 `0.Y.Z` 形式。前导的 `0` 表示 SDK 仍在快速演进。版本号的递增规则如下：\n+本项目采用经过轻微修改的语义化版本，格式为 `0.Y.Z`。前导的 `0` 表示该 SDK 仍在快速演进中。版本号的递增规则如下：\n \n-## 次版本（`Y`）\n+## 次要 (`Y`) 版本\n \n-对于未标注为 beta 的任何公共接口出现的**破坏性变更**，我们会提升次版本号 `Y`。例如，从 `0.0.x` 升到 `0.1.x` 可能包含破坏性变更。\n+对于未标记为 beta 的任何公共接口发生的**破坏性变更**，我们将提升次要版本号 `Y`。例如，从 `0.0.x` 升级到 `0.1.x` 可能包含破坏性变更。\n \n-如果你不希望引入破坏性变更，建议在你的项目中固定到 `0.0.x` 版本。\n+如果你不希望引入破坏性变更，建议在项目中固定到 `0.0.x` 版本。\n \n-## 修订版本（`Z`）\n+## 修订（`Z`）版本\n \n-对于非破坏性变更，我们会提升 `Z`：\n+对于非破坏性变更，我们将递增 `Z`：\n \n - Bug 修复\n - 新功能\n - 私有接口的变更\n - beta 功能的更新\n \n-## 破坏性变更变更日志\n+## 破坏性变更日志\n \n ### 0.6.0\n \n-在该版本中，默认的任务转移历史现在被打包为单条 assistant 消息，而不是暴露原始的 user/assistant 轮次，从而为下游智能体提供简洁、可预测的摘要\n-- 现有的单消息任务转移记录现在默认在 `<CONVERSATION HISTORY>` 块之前以“For context, here is the conversation so far between the user and the previous agent:”开头，使下游智能体获得标注清晰的摘要\n+此版本中，默认的任务转移历史现在被打包为单条助手消息，而不是暴露原始的 用户/助手 轮次，为下游智能体提供简洁且可预测的回顾\n+- 现有的单消息任务转移记录默认以 \"For context, here is the conversation so far between the user and the previous agent:\" 开头，随后是 `<CONVERSATION HISTORY>` 块，从而让下游智能体获得清晰标注的回顾\n \n ### 0.5.0\n \n-此版本没有引入任何可见的破坏性变更，但包含新功能和一些重要的底层更新：\n+此版本未引入任何可见的破坏性变更，但包含新功能与多项底层重要更新：\n \n-- 为 `RealtimeRunner` 增加了处理 [SIP 协议连接](https://platform.openai.com/docs/guides/realtime-sip) 的支持\n+- 为 `RealtimeRunner` 增加了对 [SIP 协议连接](https://platform.openai.com/docs/guides/realtime-sip) 的支持\n - 为兼容 Python 3.14，显著修订了 `Runner#run_sync` 的内部逻辑\n \n ### 0.4.0\n \n-在该版本中，不再支持 [openai](https://pypi.org/project/openai/) 包的 v1.x 版本。请将 openai 升级至 v2.x 并配合本 SDK 使用。\n+在此版本中，不再支持 [openai](https://pypi.org/project/openai/) 包的 v1.x 版本。请配合本 SDK 使用 openai v2.x。\n \n ### 0.3.0\n \n-在该版本中，Realtime API 的支持迁移到了 gpt-realtime 模型及其 API 接口（GA 版本）。\n+在此版本中，Realtime API 支持迁移到 gpt-realtime 模型及其 API 接口（GA 版本）。\n \n ### 0.2.0\n \n-在该版本中，一些过去接收 `Agent` 作为参数的地方，现在改为接收 `AgentBase` 作为参数。例如，MCP 服务中的 `list_tools()` 调用。这纯属类型变更，你仍会接收 `Agent` 对象。要更新，只需将类型错误中的 `Agent` 替换为 `AgentBase` 即可。\n+在此版本中，部分原本接收 `Agent` 作为参数的位置，现在改为接收 `AgentBase` 作为参数。例如 MCP 服务 中的 `list_tools()` 调用。这仅是类型层面的变更，你仍会接收到 `Agent` 对象。更新方式：将类型错误中的 `Agent` 替换为 `AgentBase` 即可。\n \n ### 0.1.0\n \n-在该版本中，[`MCPServer.list_tools()`][agents.mcp.server.MCPServer] 新增两个参数：`run_context` 和 `agent`。你需要在任何继承 `MCPServer` 的类中加入这些参数。\n\\ No newline at end of file\n+在此版本中，[`MCPServer.list_tools()`][agents.mcp.server.MCPServer] 新增了两个参数：`run_context` 和 `agent`。你需要在继承 `MCPServer` 的任意类中添加这些参数。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frelease.md",
        "sha": "aa82dea000b6db4c0127d5e3ab716fa892fdccef",
        "status": "modified"
      },
      {
        "additions": 4,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frepl.md",
        "changes": 7,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Frepl.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 3,
        "filename": "docs/zh/repl.md",
        "patch": "@@ -4,7 +4,8 @@ search:\n ---\n # REPL 实用程序\n \n-该 SDK 提供 `run_demo_loop`，用于在终端中直接对智能体的行为进行快速、交互式测试。\n+该 SDK 提供 `run_demo_loop`，可在终端中快速、交互式地测试智能体的行为。\n+\n \n ```python\n import asyncio\n@@ -18,6 +19,6 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-`run_demo_loop` 会在循环中提示输入用户消息，并在回合之间保留对话历史。默认情况下，它会在模型生成输出时进行流式传输。运行上面的示例后，run_demo_loop 会启动一个交互式聊天会话。它会持续请求你的输入、在回合之间记住完整的对话历史（使你的智能体知道已讨论的内容），并在生成时将智能体的响应自动实时流式传输给你。\n+`run_demo_loop` 会在循环中提示用户输入，并在轮次之间保留对话历史。默认情况下，它会在模型生成输出时进行流式传输。当你运行上面的示例时，run_demo_loop 会启动一个交互式聊天会话。它会持续询问你的输入，在轮次之间记住完整的对话历史（因此你的智能体知道已讨论的内容），并在生成过程中实时自动将智能体的响应流式传输给你。\n \n-要结束该聊天会话，只需输入 `quit` 或 `exit`（然后按回车），或使用 `Ctrl-D` 键盘快捷键。\n\\ No newline at end of file\n+要结束此聊天会话，只需输入 `quit` 或 `exit`（然后按回车），或使用 `Ctrl-D` 键盘快捷键。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frepl.md",
        "sha": "6635715d32ca92263b9f1a674c812db006fc879b",
        "status": "modified"
      },
      {
        "additions": 22,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fresults.md",
        "changes": 44,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fresults.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 22,
        "filename": "docs/zh/results.md",
        "patch": "@@ -6,53 +6,53 @@ search:\n \n 当你调用 `Runner.run` 方法时，你会得到：\n \n--   如果调用 `run` 或 `run_sync`，则返回 [`RunResult`][agents.result.RunResult]\n--   如果调用 `run_streamed`，则返回 [`RunResultStreaming`][agents.result.RunResultStreaming]\n+-   如果调用 `run` 或 `run_sync`，则返回 [`运行结果 (RunResult)`][agents.result.RunResult]\n+-   如果调用 `run_streamed`，则返回 [`流式运行结果 (RunResultStreaming)`][agents.result.RunResultStreaming]\n \n-两者均继承自 [`RunResultBase`][agents.result.RunResultBase]，大多数有用信息位于其中。\n+二者都继承自 [`运行结果基类 (RunResultBase)`][agents.result.RunResultBase]，大多数有用信息都在这里。\n \n ## 最终输出\n \n-[`final_output`][agents.result.RunResultBase.final_output] 属性包含最后运行的智能体的最终输出。可能是：\n+[`final_output`][agents.result.RunResultBase.final_output] 属性包含最后一个运行的智能体的最终输出。它可能是：\n \n--   `str`，如果最后的智能体未定义 `output_type`\n--   类型为 `last_agent.output_type` 的对象，如果该智能体定义了输出类型。\n+-   `str`，如果最后一个智能体未定义 `output_type`\n+-   类型为 `last_agent.output_type` 的对象，如果该智能体定义了输出类型\n \n !!! note\n \n-    `final_output` 的类型为 `Any`。由于有 任务转移，我们无法进行静态类型标注。如果发生 任务转移，任何智能体都可能成为最后一个智能体，因此我们在静态上无法知道可能的输出类型集合。\n+    `final_output` 的类型为 `Any`。由于存在 任务转移，我们无法进行静态类型标注。如果发生 任务转移，任何智能体都可能成为最后一个智能体，因此我们无法静态地知道可能的输出类型集合。\n \n-## 下一轮的输入\n+## 下一轮输入\n \n-你可以使用 [`result.to_input_list()`][agents.result.RunResultBase.to_input_list] 将结果转换为一个输入列表，把你提供的原始输入与智能体运行期间生成的条目串联起来。这样便于将一次智能体运行的输出传递到另一次运行中，或在循环中运行并每次追加新的用户输入。\n+你可以使用 [`result.to_input_list()`][agents.result.RunResultBase.to_input_list] 将结果转换为一个输入列表，该列表把你提供的原始输入与智能体运行期间生成的项目按顺序连接起来。这样可以方便地将一次智能体运行的输出传入另一次运行，或在循环中运行并每次附加新的用户输入。\n \n ## 最后的智能体\n \n-[`last_agent`][agents.result.RunResultBase.last_agent] 属性包含最后运行的智能体。根据你的应用场景，这通常对用户下次输入时很有用。例如，如果你有一个前线分诊智能体，会将任务转移给特定语言的智能体，你可以存储最后的智能体，并在用户下次与智能体对话时复用它。\n+[`last_agent`][agents.result.RunResultBase.last_agent] 属性包含最后一个运行的智能体。根据你的应用，这通常对下一次用户输入很有用。例如，如果你有一个前线分诊智能体将任务转移给特定语言的智能体，你可以存储该最后智能体，并在用户下次与智能体对话时复用它。\n \n-## 新增项\n+## 新项目\n \n-[`new_items`][agents.result.RunResultBase.new_items] 属性包含本次运行期间生成的新条目。条目为 [`RunItem`][agents.items.RunItem]。运行条目封装了 LLM 生成的原始条目。\n+[`new_items`][agents.result.RunResultBase.new_items] 属性包含运行期间生成的新项目。这些项目是 [`运行条目 (RunItem)`][agents.items.RunItem]。运行条目封装了由 LLM 生成的原始项目。\n \n--   [`MessageOutputItem`][agents.items.MessageOutputItem] 表示来自 LLM 的消息。原始条目即生成的消息。\n--   [`HandoffCallItem`][agents.items.HandoffCallItem] 表示 LLM 调用了任务转移工具。原始条目是来自 LLM 的工具调用条目。\n--   [`HandoffOutputItem`][agents.items.HandoffOutputItem] 表示发生了任务转移。原始条目是对任务转移工具调用的工具响应。你还可以从该条目访问源/目标智能体。\n--   [`ToolCallItem`][agents.items.ToolCallItem] 表示 LLM 调用了某个工具。\n--   [`ToolCallOutputItem`][agents.items.ToolCallOutputItem] 表示某个工具被调用。原始条目是工具响应。你还可以从该条目访问工具输出。\n--   [`ReasoningItem`][agents.items.ReasoningItem] 表示来自 LLM 的推理条目。原始条目是生成的推理内容。\n+-   [`消息输出条目 (MessageOutputItem)`][agents.items.MessageOutputItem] 表示来自 LLM 的消息。原始项目是生成的消息。\n+-   [`任务转移调用条目 (HandoffCallItem)`][agents.items.HandoffCallItem] 表示 LLM 调用了任务转移工具。原始项目是来自 LLM 的工具调用项。\n+-   [`任务转移输出条目 (HandoffOutputItem)`][agents.items.HandoffOutputItem] 表示发生了任务转移。原始项目是对任务转移工具调用的工具响应。你也可以从该条目访问源/目标智能体。\n+-   [`工具调用条目 (ToolCallItem)`][agents.items.ToolCallItem] 表示 LLM 调用了某个工具。\n+-   [`工具调用输出条目 (ToolCallOutputItem)`][agents.items.ToolCallOutputItem] 表示某个工具被调用。原始项目是工具响应。你也可以从该条目访问工具输出。\n+-   [`推理条目 (ReasoningItem)`][agents.items.ReasoningItem] 表示来自 LLM 的推理项目。原始项目是生成的推理内容。\n \n ## 其他信息\n \n ### 安全防护措施结果\n \n-[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] 和 [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] 属性包含（如果存在）安全防护措施的结果。安全防护措施结果有时包含你可能希望记录或存储的有用信息，因此我们将其提供给你。\n+[`input_guardrail_results`][agents.result.RunResultBase.input_guardrail_results] 和 [`output_guardrail_results`][agents.result.RunResultBase.output_guardrail_results] 属性包含（若有）安全防护措施的结果。安全防护措施的结果有时包含你可能想记录或存储的有用信息，因此我们向你提供这些内容。\n \n-工具的安全防护措施结果单独提供为 [`tool_input_guardrail_results`][agents.result.RunResultBase.tool_input_guardrail_results] 和 [`tool_output_guardrail_results`][agents.result.RunResultBase.tool_output_guardrail_results]。这些安全防护措施可以附加到工具上，这些工具调用会在智能体工作流中执行安全防护措施。\n+工具的安全防护措施结果单独提供为 [`tool_input_guardrail_results`][agents.result.RunResultBase.tool_input_guardrail_results] 和 [`tool_output_guardrail_results`][agents.result.RunResultBase.tool_output_guardrail_results]。这些安全防护措施可以附加到工具上，并在智能体工作流中执行工具调用时运行。\n \n ### 原始响应\n \n-[`raw_responses`][agents.result.RunResultBase.raw_responses] 属性包含由 LLM 生成的 [`ModelResponse`][agents.items.ModelResponse]。\n+[`raw_responses`][agents.result.RunResultBase.raw_responses] 属性包含由 LLM 生成的 [`模型响应 (ModelResponse)`][agents.items.ModelResponse]。\n \n ### 原始输入\n \n-[`input`][agents.result.RunResultBase.input] 属性包含你提供给 `run` 方法的原始输入。在大多数情况下你不需要它，但需要时可用。\n\\ No newline at end of file\n+[`input`][agents.result.RunResultBase.input] 属性包含你传递给 `run` 方法的原始输入。大多数情况下你不需要它，但在需要时可以使用。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fresults.md",
        "sha": "03c258d2edac55cfc18f1924abfd85f0a4aafd2c",
        "status": "modified"
      },
      {
        "additions": 47,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frunning_agents.md",
        "changes": 94,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Frunning_agents.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 47,
        "filename": "docs/zh/running_agents.md",
        "patch": "@@ -4,11 +4,11 @@ search:\n ---\n # 运行智能体\n \n-你可以通过 [`Runner`][agents.run.Runner] 类来运行智能体。你有 3 种选项：\n+你可以通过 [`Runner`][agents.run.Runner] 类来运行智能体。你有 3 种选择：\n \n-1. [`Runner.run()`][agents.run.Runner.run]，异步运行并返回 [`RunResult`][agents.result.RunResult]。\n-2. [`Runner.run_sync()`][agents.run.Runner.run_sync]，同步方法，内部调用 `.run()`。\n-3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]，异步运行并返回 [`RunResultStreaming`][agents.result.RunResultStreaming]。它以流式模式调用 LLM，并在接收时将这些事件流式传给你。\n+1. [`Runner.run()`][agents.run.Runner.run]：异步运行并返回 [`RunResult`][agents.result.RunResult]。\n+2. [`Runner.run_sync()`][agents.run.Runner.run_sync]：同步方法，实际上在内部调用 `.run()`。\n+3. [`Runner.run_streamed()`][agents.run.Runner.run_streamed]：异步运行并返回 [`RunResultStreaming`][agents.result.RunResultStreaming]。它以流式模式调用 LLM，并在接收事件时将其流式传输给你。\n \n ```python\n from agents import Agent, Runner\n@@ -27,15 +27,15 @@ async def main():\n \n ## 智能体循环\n \n-当你在 `Runner` 中使用 run 方法时，你需要传入一个起始智能体和输入。输入可以是字符串（视为用户消息），也可以是输入项列表，即 OpenAI Responses API 中的条目。\n+当你在 `Runner` 中使用 run 方法时，你会传入一个起始智能体和输入。输入可以是字符串（被视为用户消息），也可以是输入项列表，这些输入项符合 OpenAI Responses API 的格式。\n \n-runner 随后运行一个循环：\n+运行器随后执行一个循环：\n \n-1. 我们用当前输入为当前智能体调用 LLM。\n+1. 我们使用当前输入调用当前智能体的 LLM。\n 2. LLM 生成输出。\n     1. 如果 LLM 返回 `final_output`，循环结束并返回结果。\n     2. 如果 LLM 进行任务转移，我们更新当前智能体和输入，并重新运行循环。\n-    3. 如果 LLM 产生工具调用，我们运行这些工具调用，追加结果，并重新运行循环。\n+    3. 如果 LLM 产生工具调用，我们运行这些工具调用、追加结果并重新运行循环。\n 3. 如果超过传入的 `max_turns`，我们会抛出 [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded] 异常。\n \n !!! note\n@@ -44,41 +44,41 @@ runner 随后运行一个循环：\n \n ## 流式传输\n \n-流式传输允许你在 LLM 运行时接收额外的流式事件。流结束后，[`RunResultStreaming`][agents.result.RunResultStreaming] 将包含有关此次运行的完整信息，包括所有新产生的输出。你可以调用 `.stream_events()` 获取流式事件。更多信息见[流式传输指南](streaming.md)。\n+流式传输允许你在 LLM 运行时接收流式事件。流结束后，[`RunResultStreaming`][agents.result.RunResultStreaming] 将包含关于此次运行的完整信息，包括所有新产生的输出。你可以调用 `.stream_events()` 获取流式事件。更多内容请阅读[流式传输指南](streaming.md)。\n \n ## 运行配置\n \n `run_config` 参数可让你为智能体运行配置一些全局设置：\n \n--   [`model`][agents.run.RunConfig.model]：允许设置一个全局 LLM 模型使用，而不管每个 Agent 的 `model` 是什么。\n+-   [`model`][agents.run.RunConfig.model]：允许设置一个全局 LLM 模型使用，而不受每个 Agent 的 `model` 限制。\n -   [`model_provider`][agents.run.RunConfig.model_provider]：用于查找模型名称的模型提供方，默认是 OpenAI。\n -   [`model_settings`][agents.run.RunConfig.model_settings]：覆盖智能体特定设置。例如，你可以设置全局的 `temperature` 或 `top_p`。\n--   [`input_guardrails`][agents.run.RunConfig.input_guardrails], [`output_guardrails`][agents.run.RunConfig.output_guardrails]：在所有运行中包含的输入或输出安全防护措施列表。\n--   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]：对所有任务转移应用的全局输入过滤器（如果该任务转移尚未定义过滤器）。输入过滤器允许你编辑发送给新智能体的输入。详情见 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 的文档。\n--   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]：当为 `True`（默认）时，runner 会在调用下一个智能体前，将先前的对话记录折叠为单条助手消息。工具会将内容放在一个 `<CONVERSATION HISTORY>` 块中，并在后续任务转移发生时持续追加新回合。如果你希望传递原始对话记录，请将其设置为 `False` 或提供自定义的任务转移过滤器。当你未显式传入时，所有 [`Runner` 方法](agents.run.Runner) 会自动创建一个 `RunConfig`，因此快速入门与 code examples 会自动采用该默认值，且任何显式的 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 回调仍会覆盖它。单个任务转移也可通过 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] 覆盖此设置。\n--   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]：可选的可调用对象，当 `nest_handoff_history` 为 `True` 时接收标准化后的对话记录（历史 + 任务转移项）。它必须返回要转发给下一个智能体的输入项精确列表，使你能够在无需编写完整任务转移过滤器的情况下替换内置摘要。\n+-   [`input_guardrails`][agents.run.RunConfig.input_guardrails]、[`output_guardrails`][agents.run.RunConfig.output_guardrails]：要在所有运行中包含的输入或输出安全防护措施列表。\n+-   [`handoff_input_filter`][agents.run.RunConfig.handoff_input_filter]：应用于所有任务转移的全局输入过滤器（如果该任务转移尚未设置）。输入过滤器允许你编辑发送给新智能体的输入。详见 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 的文档。\n+-   [`nest_handoff_history`][agents.run.RunConfig.nest_handoff_history]：当为 `True`（默认）时，运行器会在调用下一个智能体之前，将先前的对话记录折叠为单个 assistant 消息。辅助工具会将内容放入一个 `<CONVERSATION HISTORY>` 块中，随着后续任务转移不断追加新的回合。如果你希望传递原始对话记录，可将其设为 `False`，或提供自定义的任务转移过滤器。当你未显式传入时，所有 [`Runner` 方法](agents.run.Runner) 会自动创建一个 `RunConfig`，因此快速上手和 code examples 会自动采用该默认值，任何显式的 [`Handoff.input_filter`][agents.handoffs.Handoff.input_filter] 回调仍会覆盖该行为。单个任务转移也可以通过 [`Handoff.nest_handoff_history`][agents.handoffs.Handoff.nest_handoff_history] 覆盖此设置。\n+-   [`handoff_history_mapper`][agents.run.RunConfig.handoff_history_mapper]：可选的可调用对象，在 `nest_handoff_history` 为 `True` 时接收规范化的对话记录（历史 + 任务转移项）。它必须返回要转发给下一个智能体的精确输入项列表，使你无需编写完整的任务转移过滤器即可替换内置摘要。\n -   [`tracing_disabled`][agents.run.RunConfig.tracing_disabled]：允许为整个运行禁用[追踪](tracing.md)。\n--   [`tracing`][agents.run.RunConfig.tracing]：传入 [`TracingConfig`][agents.tracing.TracingConfig] 以覆盖此次运行的导出器、进程或追踪元数据。\n--   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]：配置追踪中是否包含潜在敏感数据，例如 LLM 与工具调用的输入/输出。\n--   [`workflow_name`][agents.run.RunConfig.workflow_name], [`trace_id`][agents.run.RunConfig.trace_id], [`group_id`][agents.run.RunConfig.group_id]：设置此次运行的追踪工作流名称、trace ID 和 trace group ID。我们建议至少设置 `workflow_name`。group ID 是可选字段，用于在多个运行之间关联追踪。\n+-   [`tracing`][agents.run.RunConfig.tracing]：传入 [`TracingConfig`][agents.tracing.TracingConfig]，以覆盖本次运行的导出器、进程或追踪元数据。\n+-   [`trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data]：配置追踪是否包含潜在敏感数据，如 LLM 和工具调用的输入/输出。\n+-   [`workflow_name`][agents.run.RunConfig.workflow_name]、[`trace_id`][agents.run.RunConfig.trace_id]、[`group_id`][agents.run.RunConfig.group_id]：为此次运行设置追踪的工作流名称、追踪 ID 和追踪分组 ID。我们建议至少设置 `workflow_name`。分组 ID 是可选字段，用于将多个运行的追踪关联起来。\n -   [`trace_metadata`][agents.run.RunConfig.trace_metadata]：要包含在所有追踪中的元数据。\n--   [`session_input_callback`][agents.run.RunConfig.session_input_callback]：当使用 Sessions 时，定制每回合前如何将新的用户输入与会话历史合并。\n--   [`call_model_input_filter`][agents.run.RunConfig.call_model_input_filter]：在模型调用前，编辑已完全准备好的模型输入（instructions 和输入项）的钩子，例如用于裁剪历史或注入系统提示词。\n+-   [`session_input_callback`][agents.run.RunConfig.session_input_callback]：使用 Sessions 时，自定义在每个回合前如何将新的用户输入与会话历史合并。\n+-   [`call_model_input_filter`][agents.run.RunConfig.call_model_input_filter]：在模型调用前立即编辑已完整准备好的模型输入（instructions 和输入项）的钩子，例如用于裁剪历史或注入 system prompt。\n \n-默认情况下，SDK 现在在智能体向另一个智能体进行任务转移时，会将先前的回合嵌套在一条助手摘要消息中。这可减少重复的助手消息，并将完整对话记录保存在单个块中，以便新智能体快速扫描。如果你希望回到旧有行为，请传入 `RunConfig(nest_handoff_history=False)`，或提供一个将对话按需原样转发的 `handoff_input_filter`（或 `handoff_history_mapper`）。你也可以对某个特定任务转移选择退出（或启用），通过设置 `handoff(..., nest_handoff_history=False)` 或 `True`。若要在不编写自定义 mapper 的情况下更改生成摘要中使用的包装文本，可调用 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers]（以及 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers] 以恢复默认值）。\n+默认情况下，SDK 现在会在智能体向另一个智能体进行任务转移时，将先前的回合嵌套在单个 assistant 摘要消息中。这减少了重复的 assistant 消息，并将完整对话记录置于一个新智能体可快速扫描的单独块中。如果你想恢复旧版行为，请传入 `RunConfig(nest_handoff_history=False)`，或提供一个按你需求精确转发对话的 `handoff_input_filter`（或 `handoff_history_mapper`）。你也可以为特定的任务转移选择退出（或加入），方式是设置 `handoff(..., nest_handoff_history=False)` 或 `True`。若希望在不编写自定义映射器的情况下更改生成摘要中使用的包装文本，请调用 [`set_conversation_history_wrappers`][agents.handoffs.set_conversation_history_wrappers]（以及用于恢复默认值的 [`reset_conversation_history_wrappers`][agents.handoffs.reset_conversation_history_wrappers]）。\n \n-## 对话/聊天线程\n+## 会话/聊天线程\n \n-调用任一运行方法可能会运行一个或多个智能体（因此可能会进行一次或多次 LLM 调用），但它表示一次聊天对话中的单个逻辑回合。例如：\n+调用任意运行方法都可能导致运行一个或多个智能体（因此也会有一次或多次 LLM 调用），但它表示聊天会话中的单个逻辑回合。例如：\n \n 1. 用户回合：用户输入文本\n-2. Runner 运行：第一个智能体调用 LLM，运行工具，进行一次到第二个智能体的任务转移；第二个智能体运行更多工具，然后生成输出。\n+2. Runner 运行：第一个智能体调用 LLM，运行工具，进行一次任务转移到第二个智能体，第二个智能体运行更多工具，然后产生输出。\n \n-在智能体运行结束时，你可以选择向用户展示什么内容。例如，你可以展示由智能体生成的每条新条目，或只展示最终输出。无论哪种方式，用户可能随后提出追问，此时你可以再次调用 run 方法。\n+在智能体运行结束时，你可以选择向用户展示什么。例如，你可以向用户展示智能体生成的每一条新内容，或仅展示最终输出。无论哪种方式，用户都可能接着提出后续问题，此时你可以再次调用运行方法。\n \n-### 手动对话管理\n+### 手动管理会话\n \n-你可以使用 [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] 手动管理对话历史，以获取下一回合的输入：\n+你可以使用 [`RunResultBase.to_input_list()`][agents.result.RunResultBase.to_input_list] 方法手动管理会话历史，从而获取下一回合的输入：\n \n ```python\n async def main():\n@@ -98,9 +98,9 @@ async def main():\n         # California\n ```\n \n-### 使用 Sessions 的自动对话管理\n+### 使用 Sessions 的自动会话管理\n \n-如果需要更简单的方法，你可以使用 [Sessions](sessions/index.md) 自动处理对话历史，而无需手动调用 `.to_input_list()`：\n+如果希望更简单的方式，你可以使用 [Sessions](sessions/index.md) 自动处理会话历史，而无需手动调用 `.to_input_list()`：\n \n ```python\n from agents import Agent, Runner, SQLiteSession\n@@ -126,21 +126,21 @@ async def main():\n \n Sessions 会自动：\n \n--   在每次运行前检索对话历史\n+-   在每次运行前检索会话历史\n -   在每次运行后存储新消息\n--   为不同的会话 ID 维护独立对话\n+-   为不同的会话 ID 维护独立的会话\n \n-更多详情参见 [Sessions 文档](sessions/index.md)。\n+更多详情请参阅 [Sessions 文档](sessions/index.md)。\n \n-### 服务端管理的对话\n+### 服务托管的会话\n \n-你也可以让 OpenAI 的对话状态功能在服务端管理对话状态，而不是在本地通过 `to_input_list()` 或 `Sessions` 进行处理。这样可以在无需手动重发所有历史消息的情况下保留对话历史。更多信息参见 [OpenAI Conversation state 指南](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)。\n+你也可以让 OpenAI 会话状态功能在服务上管理会话状态，而不是用 `to_input_list()` 或 `Sessions` 在本地处理。这样可以在无需手动重新发送所有历史消息的情况下保留会话历史。详情参见 [OpenAI Conversation state 指南](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)。\n \n-OpenAI 提供两种在回合间跟踪状态的方式：\n+OpenAI 提供两种跨回合跟踪状态的方式：\n \n #### 1. 使用 `conversation_id`\n \n-你首先使用 OpenAI Conversations API 创建一个对话，然后在每次后续调用中复用其 ID：\n+你首先使用 OpenAI Conversations API 创建一个会话，然后在后续每次调用中复用该 ID：\n \n ```python\n from agents import Agent, Runner\n@@ -163,7 +163,7 @@ async def main():\n \n #### 2. 使用 `previous_response_id`\n \n-另一种选择是**响应链（response chaining）**，即每个回合显式链接到上一个回合的响应 ID。\n+另一种选择是**响应串联（response chaining）**，其中每个回合都显式链接到上一回合的响应 ID。\n \n ```python\n from agents import Agent, Runner\n@@ -188,9 +188,9 @@ async def main():\n         print(f\"Assistant: {result.final_output}\")\n ```\n \n-## 调用模型输入过滤器\n+## 模型调用输入过滤器\n \n-使用 `call_model_input_filter` 在模型调用前编辑模型输入。该钩子会接收当前智能体、上下文，以及合并后的输入项（存在时包含会话历史），并返回新的 `ModelInputData`。\n+使用 `call_model_input_filter` 在模型调用前编辑模型输入。该钩子接收当前智能体、上下文以及合并后的输入项（在存在会话历史时包括会话历史），并返回新的 `ModelInputData`。\n \n ```python\n from agents import Agent, Runner, RunConfig\n@@ -209,20 +209,20 @@ result = Runner.run_sync(\n )\n ```\n \n-通过 `run_config` 为单次运行设置该钩子，或在你的 `Runner` 上将其设为默认，用于编辑敏感数据、裁剪过长历史或注入额外系统提示词。\n+通过 `run_config` 为每次运行设置该钩子，或将其作为 `Runner` 的默认设置，以实现敏感数据脱敏、裁剪过长历史或注入额外的系统指引。\n \n ## 长时运行智能体与人类参与\n \n-你可以使用 Agents SDK 与 [Temporal](https://temporal.io/) 的集成来运行持久的、长时运行的工作流，包括人类参与（human-in-the-loop）任务。查看 Temporal 与 Agents SDK 协同完成长时任务的演示[视频](https://www.youtube.com/watch?v=fFBZqzT4DD8)，以及[相关文档](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents)。\n+你可以使用 Agents SDK 与 [Temporal](https://temporal.io/) 的集成来运行可靠的长时工作流，包括有人类参与的任务。查看一个 Temporal 与 Agents SDK 协同完成长时任务的演示[视频](https://www.youtube.com/watch?v=fFBZqzT4DD8)，以及[相关文档](https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents)。\n \n ## 异常\n \n SDK 在某些情况下会抛出异常。完整列表见 [`agents.exceptions`][]。概览如下：\n \n--   [`AgentsException`][agents.exceptions.AgentsException]：SDK 内抛出的所有异常的基类。它是一个通用类型，其他特定异常均从其派生。\n--   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]：当智能体运行超过传给 `Runner.run`、`Runner.run_sync` 或 `Runner.run_streamed` 的 `max_turns` 限制时抛出。表示智能体未能在指定交互回合数内完成任务。\n--   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]：当底层模型（LLM）产生意外或无效输出时发生。包括：\n-    -   JSON 格式错误：当模型为工具调用或其直接输出提供了格式错误的 JSON，尤其是在定义了特定 `output_type` 时。\n-    -   与工具相关的意外失败：当模型未按预期方式使用工具时\n--   [`UserError`][agents.exceptions.UserError]：当你（使用 SDK 编写代码的人）在使用 SDK 时出错会抛出该异常。通常由不正确的代码实现、无效配置或对 SDK API 的误用引起。\n--   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered], [`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]：当输入安全防护措施或输出安全防护措施的触发条件分别被满足时抛出。输入安全防护措施在处理前检查传入消息，而输出安全防护措施在交付前检查智能体的最终响应。\n\\ No newline at end of file\n+-   [`AgentsException`][agents.exceptions.AgentsException]：SDK 内抛出的所有异常的基类。它作为通用类型，其他特定异常均从此派生。\n+-   [`MaxTurnsExceeded`][agents.exceptions.MaxTurnsExceeded]：当智能体运行超过传给 `Runner.run`、`Runner.run_sync` 或 `Runner.run_streamed` 方法的 `max_turns` 限制时抛出。表示智能体无法在指定的交互回合数内完成任务。\n+-   [`ModelBehaviorError`][agents.exceptions.ModelBehaviorError]：当底层模型（LLM）产生意外或无效输出时发生。这可能包括：\n+    -   JSON 格式错误：当模型为工具调用或直接输出提供了格式错误的 JSON，特别是在定义了特定 `output_type` 时。\n+    -   意外的工具相关失败：当模型未以预期方式使用工具时\n+-   [`UserError`][agents.exceptions.UserError]：当你（使用 SDK 编写代码的人）在使用 SDK 时发生错误会抛出该异常。通常由错误的代码实现、无效配置或对 SDK API 的误用导致。\n+-   [`InputGuardrailTripwireTriggered`][agents.exceptions.InputGuardrailTripwireTriggered]、[`OutputGuardrailTripwireTriggered`][agents.exceptions.OutputGuardrailTripwireTriggered]：当输入安全防护措施或输出安全防护措施的条件被满足时分别抛出。输入安全防护措施在处理前检查传入消息，而输出安全防护措施在交付前检查智能体的最终响应。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Frunning_agents.md",
        "sha": "646156ed396a88ef31bafebace3d70b00ba1fbf0",
        "status": "modified"
      },
      {
        "additions": 23,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Fadvanced_sqlite_session.md",
        "changes": 46,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fsessions%2Fadvanced_sqlite_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 23,
        "filename": "docs/zh/sessions/advanced_sqlite_session.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # 高级 SQLite 会话\n \n-`AdvancedSQLiteSession` 是基础 `SQLiteSession` 的增强版本，提供高级对话管理能力，包括对话分支、详细的使用分析以及结构化对话查询。\n+`AdvancedSQLiteSession` 是基础 `SQLiteSession` 的增强版本，提供包括会话分支、细粒度使用分析以及结构化会话查询在内的高级对话管理能力。\n \n ## 功能\n \n-- **对话分支**：从任意用户消息创建替代对话路径\n-- **使用追踪**：按轮次提供详细的 token 使用分析，包含完整的 JSON 明细\n-- **结构化查询**：按轮次获取对话、工具使用统计等\n-- **分支管理**：独立的分支切换与管理\n-- **消息结构元数据**：跟踪消息类型、工具使用和对话流程\n+- **会话分支**: 可从任意用户消息创建替代对话路径\n+- **使用情况追踪**: 按轮次记录详细的 Token 使用分析，并提供完整的 JSON 明细\n+- **结构化查询**: 按轮次获取会话、工具使用统计等\n+- **分支管理**: 独立的分支切换与管理\n+- **消息结构元数据**: 追踪消息类型、工具使用与会话流转\n \n ## 快速开始\n \n@@ -84,14 +84,14 @@ session = AdvancedSQLiteSession(\n \n ### 参数\n \n-- `session_id` (str): 对话会话的唯一标识符\n-- `db_path` (str | Path): SQLite 数据库文件路径。默认 `:memory:` 使用内存存储\n-- `create_tables` (bool): 是否自动创建高级表。默认 `False`\n-- `logger` (logging.Logger | None): 会话使用的自定义日志记录器。默认使用模块日志记录器\n+- `session_id` (str): 会话会话的唯一标识符\n+- `db_path` (str | Path): SQLite 数据库文件路径。默认为 `:memory:` 以使用内存存储\n+- `create_tables` (bool): 是否自动创建高级表。默认为 `False`\n+- `logger` (logging.Logger | None): 会话使用的自定义日志记录器。默认为模块自带记录器\n \n-## 使用追踪\n+## 使用情况追踪\n \n-AdvancedSQLiteSession 通过按对话轮次存储 token 使用数据，提供详细的使用分析。**这完全依赖于在每次智能体运行后调用 `store_run_usage` 方法。**\n+AdvancedSQLiteSession 通过按对话轮次存储 Token 使用数据来提供详细的使用分析。**这完全依赖于在每次智能体运行后调用 `store_run_usage` 方法。**\n \n ### 存储使用数据\n \n@@ -135,9 +135,9 @@ for turn_data in turn_usage:\n turn_2_usage = await session.get_turn_usage(user_turn_number=2)\n ```\n \n-## 对话分支\n+## 会话分支\n \n-AdvancedSQLiteSession 的一项关键功能是能够从任意用户消息创建对话分支，从而探索替代的对话路径。\n+AdvancedSQLiteSession 的关键特性之一是可以从任意用户消息创建会话分支，从而探索替代的对话路径。\n \n ### 创建分支\n \n@@ -217,9 +217,9 @@ await session.store_run_usage(result)\n \n ## 结构化查询\n \n-AdvancedSQLiteSession 提供多种方法来分析对话的结构与内容。\n+AdvancedSQLiteSession 提供多种方法来分析会话结构与内容。\n \n-### 对话分析\n+### 会话分析\n \n ```python\n # Get conversation organized by turns\n@@ -245,17 +245,17 @@ for turn in matching_turns:\n \n ### 消息结构\n \n-会话会自动跟踪消息结构，包括：\n+会话会自动追踪消息结构，包括：\n \n-- 消息类型（user、assistant、tool_call 等）\n-- 工具调用的工具名称\n-- 轮次编号与序号\n+- 消息类型（用户、助手、tool_call 等）\n+- 工具调用时的工具名称\n+- 轮次编号与序列号\n - 分支关联\n - 时间戳\n \n-## 数据库模式\n+## 数据库架构\n \n-AdvancedSQLiteSession 在基础的 SQLite 模式上扩展了两个额外的表：\n+AdvancedSQLiteSession 在基础 SQLite 架构上扩展了两个额外的数据表：\n \n ### message_structure 表\n \n@@ -298,7 +298,7 @@ CREATE TABLE turn_usage (\n \n ## 完整示例\n \n-查看[完整示例](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py)，全面演示所有功能。\n+查看[完整示例](https://github.com/openai/openai-agents-python/tree/main/examples/memory/advanced_sqlite_session_example.py)，了解所有功能的综合演示。\n \n \n ## API 参考",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Fadvanced_sqlite_session.md",
        "sha": "9c7740387bd095927f3189092f43f3da54bda3e8",
        "status": "modified"
      },
      {
        "additions": 15,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Fencrypted_session.md",
        "changes": 30,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fsessions%2Fencrypted_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 15,
        "filename": "docs/zh/sessions/encrypted_session.md",
        "patch": "@@ -4,14 +4,14 @@ search:\n ---\n # 加密会话\n \n-`EncryptedSession` 为任何会话实现提供透明加密，通过自动过期机制保护会话数据，并在项目过期后自动跳过。\n+`EncryptedSession` 为任意会话实现提供透明加密，通过自动过期机制保护会话数据并在到期后自动忽略旧项。\n \n ## 功能\n \n-- **透明加密**：使用 Fernet 加密包装任何会话\n-- **按会话生成密钥**：使用 HKDF 密钥派生，为每个会话生成唯一加密密钥\n-- **自动过期**：当 TTL 到期时，旧项目会被静默跳过\n-- **即插即用**：可与任何现有的会话实现配合使用\n+- **透明加密**：使用 Fernet 加密封装任意会话\n+- **每会话独立密钥**：使用 HKDF 为每个会话派生唯一加密密钥\n+- **自动过期**：到达 TTL 的旧项会在读取时被静默跳过\n+- **即插即用**：适用于任何现有会话实现\n \n ## 安装\n \n@@ -57,7 +57,7 @@ if __name__ == \"__main__\":\n \n ### 加密密钥\n \n-加密密钥可以是 Fernet 密钥或任意字符串：\n+加密密钥可以是一个 Fernet 密钥或任意字符串：\n \n ```python\n from agents.extensions.memory import EncryptedSession\n@@ -79,9 +79,9 @@ session = EncryptedSession(\n )\n ```\n \n-### TTL（生存时间）\n+### TTL（存活时间）\n \n-设置加密项目的有效时长：\n+设置加密项的有效时长：\n \n ```python\n # Items expire after 1 hour\n@@ -101,7 +101,7 @@ session = EncryptedSession(\n )\n ```\n \n-## 与不同会话类型的配合使用\n+## 不同会话类型的用法\n \n ### 搭配 SQLite 会话\n \n@@ -140,10 +140,10 @@ session = EncryptedSession(\n \n !!! warning \"高级会话特性\"\n \n-    将 `EncryptedSession` 用于诸如 `AdvancedSQLiteSession` 等高级会话实现时，请注意：\n+    当将 `EncryptedSession` 与诸如 `AdvancedSQLiteSession` 的高级会话实现一起使用时，请注意：\n \n-    - 由于消息内容被加密，`find_turns_by_content()` 等方法将无法有效工作\n-    - 基于内容的搜索将作用于加密数据，因而效果受限\n+    - 由于消息内容被加密，诸如 `find_turns_by_content()` 等方法将无法有效工作\n+    - 基于内容的搜索会作用在加密数据上，其效果会受到限制\n \n \n \n@@ -158,12 +158,12 @@ EncryptedSession 使用 HKDF（基于 HMAC 的密钥派生函数）为每个会\n \n 这确保了：\n - 每个会话都有唯一的加密密钥\n-- 没有主密钥无法派生密钥\n-- 不同会话之间无法互相解密会话数据\n+- 没有主密钥无法推导出各会话密钥\n+- 不同会话之间的数据无法互相解密\n \n ## 自动过期\n \n-当项目超过 TTL 时，在检索时会被自动跳过：\n+当条目超过 TTL 后，在检索时会被自动跳过：\n \n ```python\n # Items older than TTL are silently ignored",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Fencrypted_session.md",
        "sha": "5d0dcefa6eeab3438382e72a5781ef32ac022ea8",
        "status": "modified"
      },
      {
        "additions": 34,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Findex.md",
        "changes": 65,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fsessions%2Findex.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 31,
        "filename": "docs/zh/sessions/index.md",
        "patch": "@@ -4,9 +4,9 @@ search:\n ---\n # 会话\n \n-Agents SDK 提供内置的会话记忆，用于在多个智能体运行之间自动维护对话历史，无需在轮次之间手动处理 `.to_input_list()`。\n+Agents SDK 提供了内置的会话记忆，以在多个智能体运行之间自动维护对话历史，从而无需在轮次之间手动处理 `.to_input_list()`。\n \n-会话为特定会话存储对话历史，使智能体无需显式的手动内存管理即可保持上下文。这在构建聊天应用或多轮对话时尤为有用，因为你希望智能体记住先前的交互。\n+Sessions 为特定会话存储对话历史，使智能体无需显式的手动内存管理即可保持上下文。这对于构建聊天应用或需要让智能体记住先前交互的多轮对话尤为有用。\n \n ## 快速开始\n \n@@ -52,16 +52,16 @@ print(result.final_output)  # \"Approximately 39 million\"\n 当启用会话记忆时：\n \n 1. **每次运行之前**：运行器会自动检索该会话的对话历史，并将其预置到输入项之前。\n-2. **每次运行之后**：在运行期间生成的所有新项（用户输入、助理响应、工具调用等）都会自动保存到会话中。\n-3. **上下文保留**：使用同一会话进行的每次后续运行都会包含完整的对话历史，使智能体能够保持上下文。\n+2. **每次运行之后**：在运行期间生成的所有新项（用户输入、助手回复、工具调用等）都会自动存储到会话中。\n+3. **上下文保留**：每次使用相同会话进行的后续运行都会包含完整的对话历史，使智能体能够保持上下文。\n \n 这消除了在运行之间手动调用 `.to_input_list()` 并管理对话状态的需要。\n \n ## 内存操作\n \n ### 基本操作\n \n-会话支持多种用于管理对话历史的操作：\n+Sessions 支持多种用于管理对话历史的操作：\n \n ```python\n from agents import SQLiteSession\n@@ -86,9 +86,9 @@ print(last_item)  # {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n await session.clear_session()\n ```\n \n-### 使用 pop_item 进行更正\n+### 使用 pop_item 进行纠正\n \n-当你希望撤销或修改对话中的最后一项时，`pop_item` 方法特别有用：\n+当你想撤销或修改对话中的最后一项时，`pop_item` 方法特别有用：\n \n ```python\n from agents import Agent, Runner, SQLiteSession\n@@ -119,7 +119,7 @@ print(f\"Agent: {result.final_output}\")\n \n ## 会话类型\n \n-SDK 为不同用例提供了多种会话实现：\n+SDK 提供了多种会话实现以适配不同用例：\n \n ### OpenAI Conversations API 会话\n \n@@ -180,7 +180,7 @@ result = await Runner.run(\n \n ### SQLAlchemy 会话\n \n-使用任意 SQLAlchemy 支持的数据库的生产级会话：\n+可用于生产的会话，实现于任何 SQLAlchemy 支持的数据库之上：\n \n ```python\n from agents.extensions.memory import SQLAlchemySession\n@@ -198,11 +198,13 @@ engine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\n session = SQLAlchemySession(\"user_123\", engine=engine, create_tables=True)\n ```\n \n-详见 [SQLAlchemy 会话](sqlalchemy_session.md) 获取详细文档。\n+参见 [SQLAlchemy 会话](sqlalchemy_session.md) 获取详细文档。\n+\n+\n \n ### 高级 SQLite 会话\n \n-增强的 SQLite 会话，支持会话分支、使用分析与结构化查询：\n+增强的 SQLite 会话，支持会话分支、用量分析和结构化查询：\n \n ```python\n from agents.extensions.memory import AdvancedSQLiteSession\n@@ -222,11 +224,11 @@ await session.store_run_usage(result)  # Track token usage\n await session.create_branch_from_turn(2)  # Branch from turn 2\n ```\n \n-详见 [高级 SQLite 会话](advanced_sqlite_session.md) 获取详细文档。\n+参见 [高级 SQLite 会话](advanced_sqlite_session.md) 获取详细文档。\n \n ### 加密会话\n \n-适用于任意会话实现的透明加密封装：\n+对任意会话实现进行透明加密的包装器：\n \n ```python\n from agents.extensions.memory import EncryptedSession, SQLAlchemySession\n@@ -249,30 +251,31 @@ session = EncryptedSession(\n result = await Runner.run(agent, \"Hello\", session=session)\n ```\n \n-详见 [加密会话](encrypted_session.md) 获取详细文档。\n+参见 [加密会话](encrypted_session.md) 获取详细文档。\n \n ### 其他会话类型\n \n-还有更多内置选项。请参考 `examples/memory/` 与 `extensions/memory/` 下的源代码。\n+还有更多内置选项。请参考 `examples/memory/` 以及 `extensions/memory/` 下的源代码。\n \n ## 会话管理\n \n ### 会话 ID 命名\n \n 使用有意义的会话 ID 来帮助你组织对话：\n \n-- 用户基于：`\"user_12345\"`\n-- 线程基于：`\"thread_abc123\"`\n-- 上下文基于：`\"support_ticket_456\"`\n+- 用户维度：`\"user_12345\"`\n+- 线程维度：`\"thread_abc123\"`\n+- 场景维度：`\"support_ticket_456\"`\n \n ### 内存持久化\n \n - 使用内存型 SQLite（`SQLiteSession(\"session_id\")`）用于临时对话\n-- 使用文件型 SQLite（`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`）用于持久化对话\n-- 使用基于 SQLAlchemy 的会话（`SQLAlchemySession(\"session_id\", engine=engine, create_tables=True)`) 适用于由 SQLAlchemy 支持的现有数据库的生产系统\n-- 使用 Dapr 状态存储会话（`DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")`）适用于生产级云原生部署，支持 30+ 个数据库后端，并内置遥测、追踪和数据隔离\n+- 使用基于文件的 SQLite（`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`）用于持久化对话\n+- 使用基于 SQLAlchemy 的会话（`SQLAlchemySession(\"session_id\", engine=engine, create_tables=True)）` 用于生产系统，适配 SQLAlchemy 支持的现有数据库\n+- 使用 Dapr 状态存储会话（`DaprSession.from_address(\"session_id\", state_store_name=\"statestore\", dapr_address=\"localhost:50001\")`）用于生产级云原生部署，支持\n+30+ 种数据库后端，并内置遥测、追踪和数据隔离\n - 当你希望将历史存储在 OpenAI Conversations API 中时，使用 OpenAI 托管的存储（`OpenAIConversationsSession()`）\n-- 使用加密会话（`EncryptedSession(session_id, underlying_session, encryption_key)`）为任意会话添加透明加密和基于 TTL 的过期\n+- 使用加密会话（`EncryptedSession(session_id, underlying_session, encryption_key)`）为任意会话提供透明加密和基于 TTL 的过期能力\n - 针对更高级的用例，考虑为其他生产系统（Redis、Django 等）实现自定义会话后端\n \n ### 多个会话\n@@ -321,7 +324,7 @@ result2 = await Runner.run(\n \n ## 完整示例\n \n-下面是一个展示会话记忆实际效果的完整示例：\n+下面给出一个完整示例，展示会话记忆的实际效果：\n \n ```python\n import asyncio\n@@ -385,7 +388,7 @@ if __name__ == \"__main__\":\n \n ## 自定义会话实现\n \n-你可以通过创建一个遵循 [`Session`][agents.memory.session.Session] 协议的类来实现自定义的会话记忆：\n+你可以通过创建一个遵循 [`Session`][agents.memory.session.Session] 协议的类来实现你自己的会话记忆：\n \n ```python\n from agents.memory.session import SessionABC\n@@ -430,22 +433,22 @@ result = await Runner.run(\n \n ## 社区会话实现\n \n-社区已经开发了其他会话实现：\n+社区已开发了其他会话实现：\n \n-| 包 | 描述 |\n+| Package | Description |\n |---------|-------------|\n-| [openai-django-sessions](https://pypi.org/project/openai-django-sessions/) | 基于 Django ORM 的会话，适用于任意 Django 支持的数据库（PostgreSQL、MySQL、SQLite 等） |\n+| [openai-django-sessions](https://pypi.org/project/openai-django-sessions/) | 基于 Django ORM 的会话，适配任意 Django 支持的数据库（PostgreSQL、MySQL、SQLite 等） |\n \n 如果你构建了会话实现，欢迎提交文档 PR 将其添加到此处！\n \n ## API 参考\n \n-有关详细 API 文档，请参阅：\n+详细的 API 文档参见：\n \n - [`Session`][agents.memory.session.Session] - 协议接口\n-- [`OpenAIConversationsSession`][agents.memory.OpenAIConversationsSession] - OpenAI Conversations API 的实现\n+- [`OpenAIConversationsSession`][agents.memory.OpenAIConversationsSession] - OpenAI Conversations API 实现\n - [`SQLiteSession`][agents.memory.sqlite_session.SQLiteSession] - 基础 SQLite 实现\n - [`SQLAlchemySession`][agents.extensions.memory.sqlalchemy_session.SQLAlchemySession] - 基于 SQLAlchemy 的实现\n - [`DaprSession`][agents.extensions.memory.dapr_session.DaprSession] - Dapr 状态存储实现\n-- [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - 具备分支和分析能力的增强型 SQLite\n-- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 适用于任意会话的加密封装\n\\ No newline at end of file\n+- [`AdvancedSQLiteSession`][agents.extensions.memory.advanced_sqlite_session.AdvancedSQLiteSession] - 增强型 SQLite，支持分支与分析\n+- [`EncryptedSession`][agents.extensions.memory.encrypt_session.EncryptedSession] - 任意会话的加密包装器\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Findex.md",
        "sha": "90dd74c7641deba8484a673b792f7b65253bf335",
        "status": "modified"
      },
      {
        "additions": 4,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Fsqlalchemy_session.md",
        "changes": 8,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fsessions%2Fsqlalchemy_session.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 4,
        "filename": "docs/zh/sessions/sqlalchemy_session.md",
        "patch": "@@ -4,11 +4,11 @@ search:\n ---\n # SQLAlchemy 会话\n \n-`SQLAlchemySession` 使用 SQLAlchemy 提供可用于生产的会话实现，允许你将 SQLAlchemy 支持的任意数据库（PostgreSQL、MySQL、SQLite 等）用于会话存储。\n+`SQLAlchemySession` 使用 SQLAlchemy 提供可用于生产环境的会话实现，使你可以将 SQLAlchemy 支持的任意数据库（PostgreSQL、MySQL、SQLite 等）用于会话存储。\n \n ## 安装\n \n-SQLAlchemy 会话需要 `sqlalchemy` 扩展：\n+SQLAlchemy 会话需要 `sqlalchemy` 额外依赖：\n \n ```bash\n pip install openai-agents[sqlalchemy]\n@@ -18,7 +18,7 @@ pip install openai-agents[sqlalchemy]\n \n ### 使用数据库 URL\n \n-最简单的用法：\n+最简单的入门方式：\n \n ```python\n import asyncio\n@@ -42,7 +42,7 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-### 使用现有引擎\n+### 使用已有引擎\n \n 适用于已存在 SQLAlchemy 引擎的应用：\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fsessions%2Fsqlalchemy_session.md",
        "sha": "bc01188cd8628647ff333b5ef8814c6dc33b1a6b",
        "status": "modified"
      },
      {
        "additions": 4,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fstreaming.md",
        "changes": 8,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fstreaming.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 4,
        "filename": "docs/zh/streaming.md",
        "patch": "@@ -6,11 +6,11 @@ search:\n \n 流式传输允许你在智能体运行过程中订阅其更新。这对于向最终用户展示进度更新和部分响应很有用。\n \n-要进行流式传输，你可以调用[`Runner.run_streamed()`][agents.run.Runner.run_streamed]，它会返回一个[`RunResultStreaming`][agents.result.RunResultStreaming]。调用`result.stream_events()`会给你一个异步的[`StreamEvent`][agents.stream_events.StreamEvent]对象流，详见下文。\n+要进行流式传输，你可以调用 [`Runner.run_streamed()`][agents.run.Runner.run_streamed]，它将返回一个 [`RunResultStreaming`][agents.result.RunResultStreaming]。调用 `result.stream_events()` 会得到一个由 [`StreamEvent`][agents.stream_events.StreamEvent] 对象组成的异步流，详见下文。\n \n ## 原始响应事件\n \n-[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent]是直接来自 LLM 的原始事件。它们采用 OpenAI Responses API 格式，这意味着每个事件都有一个类型（例如 `response.created`、`response.output_text.delta` 等）和数据。如果你希望在生成后立即将响应消息流式传输给用户，这些事件会很有用。\n+[`RawResponsesStreamEvent`][agents.stream_events.RawResponsesStreamEvent] 是直接从 LLM 传递的原始事件。它们采用 OpenAI Responses API 格式，这意味着每个事件都有一个类型（如 `response.created`、`response.output_text.delta` 等）以及数据。如果你希望在生成过程中立刻将响应消息流式传输给用户，这些事件很有用。\n \n 例如，以下将逐 token 输出由 LLM 生成的文本。\n \n@@ -37,9 +37,9 @@ if __name__ == \"__main__\":\n \n ## 运行项事件与智能体事件\n \n-[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent] 属于更高层级的事件。它们会在某个条目已完全生成时通知你。这样你可以在“消息已生成”、“工具已运行”等层级推送进度更新，而不是每个 token。同样，[`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] 会在当前智能体发生变化时（例如由于一次任务转移）向你提供更新。\n+[`RunItemStreamEvent`][agents.stream_events.RunItemStreamEvent] 是更高层的事件。它们会在某个条目完全生成后通知你。这样你就可以在“消息已生成”“工具已运行”等层级推送进度更新，而不是逐个 token。类似地，[`AgentUpdatedStreamEvent`][agents.stream_events.AgentUpdatedStreamEvent] 会在当前智能体发生变化时向你提供更新（例如由于一次 任务转移 的结果）。\n \n-例如，以下将忽略原始事件，并向用户流式传输更新。\n+例如，下面将忽略原始事件并向用户流式传输更新。\n \n ```python\n import asyncio",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fstreaming.md",
        "sha": "4412ea775489d83ad943a128bac08a5caa755a68",
        "status": "modified"
      },
      {
        "additions": 49,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Ftools.md",
        "changes": 98,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Ftools.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 49,
        "filename": "docs/zh/tools.md",
        "patch": "@@ -4,21 +4,21 @@ search:\n ---\n # 工具\n \n-工具让智能体能够采取行动：如获取数据、运行代码、调用外部 API，甚至进行计算机操作。该 SDK 支持四种目录：\n+工具让智能体能够采取行动：例如获取数据、运行代码、调用外部 API，甚至进行计算机操作。SDK 支持四种目录：\n \n-- 由 OpenAI 托管的工具：与模型一起在 OpenAI 服务 上运行。\n+- 由OpenAI托管的工具：与模型一起在 OpenAI 服务 上运行。\n - 本地运行时工具：在你的环境中运行（计算机操作、shell、apply patch）。\n - Function calling：将任意 Python 函数包装为工具。\n-- 作为工具的智能体：将智能体暴露为可调用的工具，而无需完整的任务转移。\n+- 将智能体作为工具：将智能体暴露为可调用的工具而无需完整的任务转移。\n \n ## 托管工具\n \n-在使用[`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel]时，OpenAI 提供了一些内置工具：\n+使用 [`OpenAIResponsesModel`][agents.models.openai_responses.OpenAIResponsesModel] 时，OpenAI 提供了一些内置工具：\n \n - [`WebSearchTool`][agents.tool.WebSearchTool] 让智能体进行网络检索。\n-- [`FileSearchTool`][agents.tool.FileSearchTool] 允许从你的 OpenAI 向量存储中检索信息。\n-- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool] 让 LLM 在沙盒环境中执行代码。\n-- [`HostedMCPTool`][agents.tool.HostedMCPTool] 将远程 MCP 服务 的工具暴露给模型。\n+- [`FileSearchTool`][agents.tool.FileSearchTool] 允许从你的 OpenAI 向量存储 中检索信息。\n+- [`CodeInterpreterTool`][agents.tool.CodeInterpreterTool] 让 LLM 在沙箱环境中执行代码。\n+- [`HostedMCPTool`][agents.tool.HostedMCPTool] 将远程 MCP 服务的工具暴露给模型。\n - [`ImageGenerationTool`][agents.tool.ImageGenerationTool] 根据提示生成图像。\n \n ```python\n@@ -45,8 +45,8 @@ async def main():\n 本地运行时工具在你的环境中执行，需要你提供实现：\n \n - [`ComputerTool`][agents.tool.ComputerTool]：实现 [`Computer`][agents.computer.Computer] 或 [`AsyncComputer`][agents.computer.AsyncComputer] 接口以启用 GUI/浏览器自动化。\n-- [`ShellTool`][agents.tool.ShellTool] 或 [`LocalShellTool`][agents.tool.LocalShellTool]：提供 shell 执行器以运行命令。\n-- [`ApplyPatchTool`][agents.tool.ApplyPatchTool]：实现 [`ApplyPatchEditor`][agents.editor.ApplyPatchEditor] 在本地应用 diff。\n+- [`ShellTool`][agents.tool.ShellTool] 或 [`LocalShellTool`][agents.tool.LocalShellTool]：提供一个 shell 执行器来运行命令。\n+- [`ApplyPatchTool`][agents.tool.ApplyPatchTool]：实现 [`ApplyPatchEditor`][agents.editor.ApplyPatchEditor] 以在本地应用 diff。\n \n ```python\n from agents import Agent, ApplyPatchTool, ShellTool\n@@ -92,12 +92,12 @@ agent = Agent(\n \n 你可以将任意 Python 函数用作工具。Agents SDK 会自动设置该工具：\n \n-- 工具名称将是 Python 函数名（或你可以自定义名称）\n-- 工具描述将取自函数的 docstring（或你可以自定义描述）\n-- 函数输入的模式会根据函数参数自动创建\n-- 每个输入的描述将取自函数的 docstring，除非禁用\n+- 工具名称将是 Python 函数名（也可以自定义名称）\n+- 工具描述将来自函数的 docstring（也可以自定义描述）\n+- 函数输入的 schema 会根据函数参数自动创建\n+- 每个输入的描述将来自函数的 docstring，除非禁用\n \n-我们使用 Python 的 `inspect` 模块提取函数签名，使用 [`griffe`](https://mkdocstrings.github.io/griffe/) 解析 docstring，并使用 `pydantic` 创建模式。\n+我们使用 Python 的 `inspect` 模块提取函数签名，使用 [`griffe`](https://mkdocstrings.github.io/griffe/) 解析 docstring，并使用 `pydantic` 创建 schema。\n \n ```python\n import json\n@@ -149,9 +149,9 @@ for tool in agent.tools:\n \n ```\n \n-1. 你可以使用任意 Python 类型作为函数参数，函数可以是同步或异步。\n-2. 若存在 docstring，则用于提取描述和参数说明。\n-3. 函数可选接收 `context`（必须为第一个参数）。你也可以设置覆盖项，如工具名称、描述、docstring 样式等。\n+1. 你可以在函数参数中使用任意 Python 类型，函数可以是同步或异步。\n+2. 如果存在 docstring，会用于提取函数和参数的描述。\n+3. 函数可选地接收 `context`（必须是第一个参数）。你也可以设置覆盖项，如工具名称、描述、docstring 风格等。\n 4. 你可以将装饰后的函数传入工具列表。\n \n ??? note \"展开以查看输出\"\n@@ -226,20 +226,20 @@ for tool in agent.tools:\n \n ### 从工具调用返回图像或文件\n \n-除了返回文本输出外，你还可以将一个或多个图像或文件作为函数工具的输出返回。可返回以下任意类型：\n+除了返回文本输出外，你还可以将一个或多个图像或文件作为工具调用的输出。为此，你可以返回以下任意一种：\n \n-- 图像：[`ToolOutputImage`][agents.tool.ToolOutputImage]（或其 TypedDict 版本 [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict]）\n-- 文件：[`ToolOutputFileContent`][agents.tool.ToolOutputFileContent]（或其 TypedDict 版本 [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict]）\n-- 文本：字符串或可转为字符串的对象，或 [`ToolOutputText`][agents.tool.ToolOutputText]（或其 TypedDict 版本 [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict]）\n+- 图像：[`ToolOutputImage`][agents.tool.ToolOutputImage]（或 TypedDict 版本 [`ToolOutputImageDict`][agents.tool.ToolOutputImageDict]）\n+- 文件：[`ToolOutputFileContent`][agents.tool.ToolOutputFileContent]（或 TypedDict 版本 [`ToolOutputFileContentDict`][agents.tool.ToolOutputFileContentDict]）\n+- 文本：字符串或可转为字符串的对象，或 [`ToolOutputText`][agents.tool.ToolOutputText]（或 TypedDict 版本 [`ToolOutputTextDict`][agents.tool.ToolOutputTextDict]）\n \n ### 自定义函数工具\n \n-有时你可能不希望使用 Python 函数作为工具。如果需要，你可以直接创建一个 [`FunctionTool`][agents.tool.FunctionTool]。你需要提供：\n+有时你可能不想将 Python 函数作为工具使用。你可以直接创建 [`FunctionTool`][agents.tool.FunctionTool]。你需要提供：\n \n - `name`\n - `description`\n-- `params_json_schema`，即参数的 JSON schema\n-- `on_invoke_tool`，这是一个异步函数，接收 [`ToolContext`][agents.tool_context.ToolContext] 和 JSON 字符串形式的参数，并且必须以字符串形式返回工具输出。\n+- `params_json_schema`，即参数的 JSON 架构\n+- `on_invoke_tool`，这是一个异步函数，接收 [`ToolContext`][agents.tool_context.ToolContext] 和 JSON 字符串形式的参数，并且必须返回字符串形式的工具输出。\n \n ```python\n from typing import Any\n@@ -272,18 +272,18 @@ tool = FunctionTool(\n )\n ```\n \n-### 参数与 docstring 的自动解析\n+### 自动解析参数与 docstring\n \n-如前所述，我们会自动解析函数签名以提取工具的 schema，并解析 docstring 以提取工具及各参数的描述。说明如下：\n+如前所述，我们会自动解析函数签名以提取工具的 schema，并解析 docstring 以提取工具和各个参数的描述。说明如下：\n \n-1. 通过 `inspect` 模块解析签名。我们使用类型注解理解参数类型，并动态构建 Pydantic 模型来表示整体 schema。支持大多数类型，包括 Python 基本组件、Pydantic 模型、TypedDict 等。\n+1. 使用 `inspect` 模块进行签名解析。我们利用类型注解理解参数类型，并动态构建一个 Pydantic 模型来表示整体 schema。支持大多数类型，包括 Python 基本组件、Pydantic 模型、TypedDict 等。\n 2. 我们使用 `griffe` 解析 docstring。支持的 docstring 格式包括 `google`、`sphinx` 和 `numpy`。我们会尝试自动检测 docstring 格式，但这只是尽力而为；你也可以在调用 `function_tool` 时显式设置。你还可以通过将 `use_docstring_info` 设为 `False` 来禁用 docstring 解析。\n \n 用于 schema 提取的代码位于 [`agents.function_schema`][]。\n \n-## 作为工具的智能体\n+## 将智能体作为工具\n \n-在某些工作流中，你可能希望由一个中心智能体来编排一组专业智能体的网络，而不是进行控制权的转移。你可以通过将智能体建模为工具来实现。\n+在某些工作流中，你可能希望由一个中心智能体来编排一个由多个专业化智能体组成的网络，而不是进行控制权的任务转移。你可以通过将智能体建模为工具来实现。\n \n ```python\n from agents import Agent, Runner\n@@ -324,7 +324,7 @@ async def main():\n \n ### 自定义工具化智能体\n \n-`agent.as_tool` 函数是一个便捷方法，便于将智能体转换为工具。但它不支持所有配置；例如，你无法设置 `max_turns`。对于高级用例，请在你的工具实现中直接使用 `Runner.run`：\n+`agent.as_tool` 函数是一个便捷方法，可轻松将智能体转换为工具。但它不支持所有配置；例如，你不能设置 `max_turns`。对于高级用例，请在你的工具实现中直接使用 `Runner.run`：\n \n ```python\n @function_tool\n@@ -347,9 +347,9 @@ async def run_my_agent() -> str:\n \n 在某些情况下，你可能希望在将工具化智能体的输出返回给中心智能体之前对其进行修改。如果你希望：\n \n-- 从子智能体的对话历史中提取特定信息（例如 JSON 负载）。\n-- 转换或重新格式化智能体的最终回答（例如将 Markdown 转换为纯文本或 CSV）。\n-- 验证输出，或在智能体响应缺失或格式错误时提供回退值。\n+- 从子智能体的对话历史中提取特定信息（例如 JSON 负载）\n+- 转换或重新格式化智能体的最终答案（例如将 Markdown 转为纯文本或 CSV）\n+- 验证输出，或在智能体响应缺失或格式错误时提供后备值\n \n 你可以通过向 `as_tool` 方法提供 `custom_output_extractor` 参数来实现：\n \n@@ -372,7 +372,7 @@ json_tool = data_agent.as_tool(\n \n ### 嵌套智能体运行的流式传输\n \n-向 `as_tool` 传入 `on_stream` 回调，以在仍按流完成后返回最终输出的同时，监听由嵌套智能体发出的流式事件。\n+向 `as_tool` 传入 `on_stream` 回调，以监听嵌套智能体发出的流式事件，同时在流结束后仍返回其最终输出。\n \n ```python\n from agents import AgentToolStreamEvent\n@@ -393,14 +393,14 @@ billing_agent_tool = billing_agent.as_tool(\n 预期行为：\n \n - 事件类型与 `StreamEvent[\"type\"]` 一致：`raw_response_event`、`run_item_stream_event`、`agent_updated_stream_event`。\n-- 提供 `on_stream` 会自动以流式模式运行嵌套智能体，并在返回最终输出前耗尽该流。\n+- 提供 `on_stream` 会自动以流式模式运行嵌套智能体，并在返回最终输出前消费完整个流。\n - 处理器可以是同步或异步的；每个事件会按到达顺序依次投递。\n-- 当通过模型的工具调用触发时会包含 `tool_call_id`；直接调用时可能为 `None`。\n-- 参见 `examples/agent_patterns/agents_as_tools_streaming.py` 获取完整可运行的 sample code。\n+- 当通过模型工具调用触发时会包含 `tool_call_id`；直接调用时可能为 `None`。\n+- 参见 `examples/agent_patterns/agents_as_tools_streaming.py` 获取完整可运行示例。\n \n-### 条件启用工具\n+### 条件性启用工具\n \n-你可以使用 `is_enabled` 参数在运行时有条件地启用或禁用智能体工具。这样可以基于上下文、用户偏好或运行时条件，动态筛选对 LLM 可用的工具。\n+你可以使用 `is_enabled` 参数在运行时有条件地启用或禁用智能体工具。这样可以根据上下文、用户偏好或运行时条件，动态筛选对 LLM 可用的工具。\n \n ```python\n import asyncio\n@@ -455,26 +455,26 @@ async def main():\n asyncio.run(main())\n ```\n \n-`is_enabled` 参数接受：\n+`is_enabled` 参数可接受：\n \n - **布尔值**：`True`（始终启用）或 `False`（始终禁用）\n - **可调用函数**：接收 `(context, agent)` 并返回布尔值的函数\n-- **异步函数**：用于复杂条件逻辑的异步函数\n+- **异步函数**：用于更复杂的条件逻辑\n \n 被禁用的工具在运行时对 LLM 完全不可见，适用于：\n \n-- 基于用户权限的特性门控\n-- 特定环境的工具可用性（开发 vs 生产）\n+- 基于用户权限的功能门控\n+- 环境特定的工具可用性（开发与生产环境）\n - 不同工具配置的 A/B 测试\n - 基于运行时状态的动态工具筛选\n \n-## 工具调用的错误处理\n+## 在工具调用中处理错误\n \n-当你通过 `@function_tool` 创建函数工具时，可以传入 `failure_error_function`。这是一个函数，用于在工具调用崩溃时向 LLM 提供错误响应。\n+当你通过 `@function_tool` 创建一个函数工具时，你可以传入 `failure_error_function`。这是一个在工具调用崩溃时向 LLM 提供错误响应的函数。\n \n-- 默认情况下（即未传入时），会运行 `default_tool_error_function`，告知 LLM 发生了错误。\n-- 如果传入你自己的错误函数，则会运行该函数，并将其响应发送给 LLM。\n-- 如果显式传入 `None`，则任何工具调用错误都会被重新抛出以供你处理。若模型生成了无效 JSON，则可能是 `ModelBehaviorError`；若你的代码崩溃，则可能是 `UserError`，等等。\n+- 默认情况下（即未传入任何内容），会运行 `default_tool_error_function`，告知 LLM 发生了错误。\n+- 如果你传入了自定义错误函数，则会运行该函数，并将其响应发送给 LLM。\n+- 如果你显式传入 `None`，则任何工具调用错误都会重新抛出供你处理。如果模型生成了无效 JSON，可能是 `ModelBehaviorError`；如果你的代码崩溃，可能是 `UserError`，等等。\n \n ```python\n from agents import function_tool, RunContextWrapper\n@@ -497,4 +497,4 @@ def get_user_profile(user_id: str) -> str:\n \n ```\n \n-如果你手动创建 `FunctionTool` 对象，则必须在 `on_invoke_tool` 函数内处理错误。\n\\ No newline at end of file\n+如果你是手动创建 `FunctionTool` 对象，则必须在 `on_invoke_tool` 函数内部处理错误。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Ftools.md",
        "sha": "48bdb2184633d0c61c783f4585ee28dad3be3d61",
        "status": "modified"
      },
      {
        "additions": 45,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Ftracing.md",
        "changes": 90,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Ftracing.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 45,
        "filename": "docs/zh/tracing.md",
        "patch": "@@ -4,52 +4,52 @@ search:\n ---\n # 追踪\n \n-Agents SDK 内置了追踪功能，会在智能体运行期间收集全面的事件记录：LLM 生成、工具调用、任务转移、安全防护措施，甚至自定义事件。使用 [Traces 仪表盘](https://platform.openai.com/traces)，你可以在开发和生产环境中调试、可视化并监控工作流。\n+Agents SDK 内置了追踪功能，会在智能体运行期间收集完整的事件记录：LLM 生成、工具调用、任务转移、安全防护措施，以及自定义事件。使用 [Traces 仪表板](https://platform.openai.com/traces)，你可以在开发和生产中调试、可视化并监控工作流。\n \n !!!note\n \n-    追踪默认启用。可通过以下两种方式禁用追踪：\n+    追踪默认启用。可以通过两种方式禁用追踪：\n \n     1. 通过设置环境变量 `OPENAI_AGENTS_DISABLE_TRACING=1` 全局禁用追踪\n-    2. 可通过将 [`agents.run.RunConfig.tracing_disabled`][] 设为 `True` 来在单次运行中禁用追踪\n+    2. 为单次运行禁用追踪：将 [`agents.run.RunConfig.tracing_disabled`][] 设为 `True`\n \n-***对于在使用 OpenAI API 的零数据保留（Zero Data Retention，ZDR）策略下运营的组织，追踪不可用。***\n+***对于在 OpenAI 的 API 下采用 Zero Data Retention (ZDR) 策略的组织，追踪不可用。***\n \n ## 追踪与 Span\n \n--   **追踪（Traces）** 代表一次“工作流”的端到端操作。它们由 Span 组成。追踪具有以下属性：\n+-   **Traces（追踪）** 表示一次“工作流”的端到端操作。它由多个 Span 组成。Trace 具有以下属性：\n     -   `workflow_name`：逻辑上的工作流或应用。例如 “Code generation” 或 “Customer service”。\n-    -   `trace_id`：追踪的唯一 ID。如果未传入则自动生成。必须符合 `trace_<32_alphanumeric>` 格式。\n-    -   `group_id`：可选的分组 ID，用于关联同一会话中的多个追踪。例如，你可以使用聊天线程 ID。\n+    -   `trace_id`：该追踪的唯一 ID。如果未传入将自动生成。必须符合 `trace_<32_alphanumeric>` 格式。\n+    -   `group_id`：可选的分组 ID，用于关联同一会话中的多个追踪。例如可使用聊天线程 ID。\n     -   `disabled`：若为 True，则不会记录该追踪。\n-    -   `metadata`：追踪的可选元数据。\n--   **Span** 表示一个有开始与结束时间的操作。Span 包含：\n+    -   `metadata`：该追踪的可选元数据。\n+-   **Spans（Span）** 表示具有开始和结束时间的操作。Span 具有：\n     -   `started_at` 和 `ended_at` 时间戳。\n     -   `trace_id`，表示其所属的追踪\n-    -   `parent_id`，指向该 Span 的父级 Span（如果有）\n-    -   `span_data`，包含关于 Span 的信息。例如，`AgentSpanData` 包含智能体信息，`GenerationSpanData` 包含 LLM 生成的信息，等等。\n+    -   `parent_id`，指向该 Span 的父 Span（如果有）\n+    -   `span_data`，包含关于该 Span 的信息。例如，`AgentSpanData` 包含关于智能体的信息，`GenerationSpanData` 包含关于 LLM 生成的信息，等等。\n \n ## 默认追踪\n \n 默认情况下，SDK 会追踪以下内容：\n \n--   整个 `Runner.{run, run_sync, run_streamed}()` 被包裹在 `trace()` 中。\n--   每次智能体运行都会被包裹在 `agent_span()`\n--   LLM 生成会被包裹在 `generation_span()`\n--   工具调用会分别被包裹在 `function_span()`\n--   安全防护措施会被包裹在 `guardrail_span()`\n--   任务转移会被包裹在 `handoff_span()`\n--   音频输入（语音转文本）会被包裹在 `transcription_span()`\n--   音频输出（文本转语音）会被包裹在 `speech_span()`\n--   相关音频 Span 可能被归为 `speech_group_span()` 的子级\n+-   整个 `Runner.{run, run_sync, run_streamed}()` 被 `trace()` 包裹。\n+-   每次智能体运行时，都会被 `agent_span()` 包裹\n+-   LLM 生成会被 `generation_span()` 包裹\n+-   Function 工具调用会分别被 `function_span()` 包裹\n+-   安全防护措施会被 `guardrail_span()` 包裹\n+-   任务转移会被 `handoff_span()` 包裹\n+-   音频输入（语音转文本）会被 `transcription_span()` 包裹\n+-   音频输出（文本转语音）会被 `speech_span()` 包裹\n+-   相关音频 span 可能会被归于 `speech_group_span()` 之下\n \n-默认情况下，追踪名称为 “Agent workflow”。如果你使用 `trace`，可以设置该名称；或者通过 [`RunConfig`][agents.run.RunConfig] 配置名称和其他属性。\n+默认情况下，追踪名称为 “Agent workflow”。如果使用 `trace`，可以设置该名称；或者通过 [`RunConfig`][agents.run.RunConfig] 配置名称及其他属性。\n \n-此外，你可以设置[自定义追踪进程](#custom-tracing-processors)，将追踪数据推送到其他目的地（作为替代或附加目的地）。\n+此外，你可以设置[自定义追踪进程](#custom-tracing-processors)，将追踪发送到其他目的地（作为替代或次要目的地）。\n \n ## 更高层级的追踪\n \n-有时，你可能希望多次调用 `run()` 都属于同一个追踪。你可以通过将整段代码包裹在 `trace()` 中来实现。\n+有时，你可能希望多次调用 `run()` 都属于同一个追踪。可以通过用 `trace()` 包裹整个代码来实现。\n \n ```python\n from agents import Agent, Runner, trace\n@@ -64,48 +64,48 @@ async def main():\n         print(f\"Rating: {second_result.final_output}\")\n ```\n \n-1. 因为两次对 `Runner.run` 的调用被包裹在 `with trace()` 中，这些单独的运行会成为同一整体追踪的一部分，而不是创建两个独立的追踪。\n+1. 因为两次对 `Runner.run` 的调用被包裹在 `with trace()` 中，单次运行会合并到同一个总体追踪中，而不是创建两个追踪。\n \n ## 创建追踪\n \n-你可以使用 [`trace()`][agents.tracing.trace] 函数创建一个追踪。追踪需要启动与结束。你有两种方式：\n+你可以使用 [`trace()`][agents.tracing.trace] 函数来创建追踪。追踪需要启动并结束。你有两种方式：\n \n-1. 推荐：将 trace 作为上下文管理器使用，即 `with trace(...) as my_trace`。这会在合适的时机自动开始和结束追踪。\n-2. 你也可以手动调用 [`trace.start()`][agents.tracing.Trace.start] 和 [`trace.finish()`][agents.tracing.Trace.finish]。\n+1. 【推荐】将追踪作为上下文管理器使用，即 `with trace(...) as my_trace`。这会在合适的时间自动开始和结束追踪。\n+2. 也可以手动调用 [`trace.start()`][agents.tracing.Trace.start] 和 [`trace.finish()`][agents.tracing.Trace.finish]。\n \n-当前追踪通过 Python 的 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 跟踪。这意味着它可自动适配并发场景。若你手动开始/结束追踪，需要在 `start()`/`finish()` 时传入 `mark_as_current` 和 `reset_current` 来更新当前追踪。\n+当前追踪通过 Python 的 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 跟踪。这意味着它能自动适配并发。如果你手动开始/结束追踪，需要在 `start()`/`finish()` 中传入 `mark_as_current` 和 `reset_current` 来更新当前追踪。\n \n ## 创建 Span\n \n-你可以使用各种 [`*_span()`][agents.tracing.create] 方法创建 Span。通常，你无需手动创建 Span。可使用 [`custom_span()`][agents.tracing.custom_span] 来记录自定义 Span 信息。\n+你可以使用各类 [`*_span()`][agents.tracing.create] 方法来创建 Span。通常你不需要手动创建 Span。提供了一个 [`custom_span()`][agents.tracing.custom_span] 函数用于追踪自定义 Span 信息。\n \n-Span 会自动归属到当前追踪，并嵌套在最近的当前 Span 之下，该状态通过 Python 的 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 进行跟踪。\n+Span 会自动归属于当前追踪，并嵌套在最近的当前 Span 之下，当前 Span 通过 Python 的 [`contextvar`](https://docs.python.org/3/library/contextvars.html) 跟踪。\n \n ## 敏感数据\n \n 某些 Span 可能会捕获潜在的敏感数据。\n \n-`generation_span()` 会存储 LLM 生成的输入/输出，而 `function_span()` 会存储函数调用的输入/输出。这些可能包含敏感数据，因此你可以通过 [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] 禁用对这些数据的采集。\n+`generation_span()` 会存储 LLM 生成的输入/输出，`function_span()` 会存储函数调用的输入/输出。这些可能包含敏感数据，因此你可以通过 [`RunConfig.trace_include_sensitive_data`][agents.run.RunConfig.trace_include_sensitive_data] 禁用相关数据的采集。\n \n-类似地，音频相关的 Span 默认会包含输入与输出音频的 base64 编码 PCM 数据。你可以通过配置 [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] 禁用音频数据采集。\n+同样地，音频相关的 Span 默认会包含输入和输出音频的 base64 编码 PCM 数据。你可以通过配置 [`VoicePipelineConfig.trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data] 来禁用音频数据的采集。\n \n-默认情况下，`trace_include_sensitive_data` 为 `True`。你也可以在不修改代码的情况下，通过在运行应用前导出环境变量 `OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA` 为 `true/1` 或 `false/0` 来设置默认值。\n+默认情况下，`trace_include_sensitive_data` 为 `True`。无需改动代码，你可以在运行应用前将环境变量 `OPENAI_AGENTS_TRACE_INCLUDE_SENSITIVE_DATA` 设为 `true/1` 或 `false/0` 来设置默认值。\n \n ## 自定义追踪进程\n \n 追踪的高层架构如下：\n \n--   在初始化时，我们会创建一个全局的 [`TraceProvider`][agents.tracing.setup.TraceProvider]，负责创建追踪。\n--   我们为 `TraceProvider` 配置一个 [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor]，它会将追踪/Span 批量发送给 [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter]，后者再将 Span 与追踪批量导出到 OpenAI 后端。\n+-   在初始化时，我们创建一个全局的 [`TraceProvider`][agents.tracing.setup.TraceProvider]，负责创建追踪。\n+-   我们为 `TraceProvider` 配置一个 [`BatchTraceProcessor`][agents.tracing.processors.BatchTraceProcessor]，它会将追踪/Span 批量发送到 [`BackendSpanExporter`][agents.tracing.processors.BackendSpanExporter]，由其将 Span 和追踪批量导出到 OpenAI 后端。\n \n-若要自定义该默认设置，以便将追踪发送到替代或附加的后端，或修改导出器行为，你有两种选择：\n+要自定义该默认设置，以将追踪发送到其他或额外的后端，或修改导出行为，你有两种选择：\n \n-1. [`add_trace_processor()`][agents.tracing.add_trace_processor] 允许你添加一个“额外”的追踪进程，当追踪和 Span 就绪时会接收它们。这样你可以在将追踪发送到 OpenAI 后端之外，执行自定义处理。\n-2. [`set_trace_processors()`][agents.tracing.set_trace_processors] 允许你“替换”默认的进程为你自己的追踪进程。这意味着除非你包含一个会将数据发送到 OpenAI 后端的 `TracingProcessor`，否则追踪将不会被发送至 OpenAI 后端。\n+1. [`add_trace_processor()`][agents.tracing.add_trace_processor] 允许添加**额外的**追踪进程，它会在追踪和 Span 就绪时接收它们。这样你可以在将追踪发送到 OpenAI 后端之外执行自有处理。\n+2. [`set_trace_processors()`][agents.tracing.set_trace_processors] 允许你**替换**默认的进程为自定义追踪进程。若未包含能将数据发送到 OpenAI 后端的 `TracingProcessor`，则追踪将不会发送到 OpenAI 后端。\n \n-## 在非 OpenAI 模型中的追踪\n+## 在非 OpenAI 模型上的追踪\n \n-你可以将 OpenAI API key 与非 OpenAI 模型配合使用，在 OpenAI Traces 仪表盘中启用免费的追踪，而无需禁用追踪。\n+你可以在非 OpenAI 模型上使用 OpenAI API key 来启用 OpenAI Traces 仪表板中的免费追踪，无需禁用追踪。\n \n ```python\n import os\n@@ -126,7 +126,7 @@ agent = Agent(\n )\n ```\n \n-如果你只需要在单次运行中使用不同的追踪 key，可通过 `RunConfig` 传入，而不是修改全局导出器。\n+如果只需在单次运行中使用不同的追踪 key，请通过 `RunConfig` 传入，而不是修改全局导出器。\n \n ```python\n from agents import Runner, RunConfig\n@@ -139,15 +139,15 @@ await Runner.run(\n ```\n \n ## 备注\n-- 在 OpenAI Traces 仪表盘查看免费追踪。\n+- 可在 OpenAI Traces 仪表板查看免费的追踪。\n \n-## 外部追踪进程清单\n+## 外部追踪进程列表\n \n -   [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)\n -   [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)\n -   [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)\n--   [MLflow（自托管/OSS）](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)\n--   [MLflow（Databricks 托管）](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)\n+-   [MLflow (self-hosted/OSS)](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)\n+-   [MLflow (Databricks hosted)](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)\n -   [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)\n -   [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)\n -   [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Ftracing.md",
        "sha": "3a8424799db2e5cbb1c880d7382727d3b8af5c62",
        "status": "modified"
      },
      {
        "additions": 18,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fusage.md",
        "changes": 36,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fusage.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 18,
        "filename": "docs/zh/usage.md",
        "patch": "@@ -4,22 +4,22 @@ search:\n ---\n # 用量\n \n-Agents SDK 会自动为每次运行跟踪 token 用量。你可以从运行上下文中访问它，用于监控成本、实施限制或记录分析数据。\n+Agents SDK 会自动跟踪每次运行的令牌用量。你可以从运行上下文中获取它，用于监控成本、执行限制或记录分析数据。\n \n ## 跟踪内容\n \n - **requests**: 发起的 LLM API 调用次数\n-- **input_tokens**: 发送的输入 token 总数\n-- **output_tokens**: 接收的输出 token 总数\n+- **input_tokens**: 发送的输入令牌总数\n+- **output_tokens**: 接收的输出令牌总数\n - **total_tokens**: 输入 + 输出\n - **request_usage_entries**: 每次请求的用量明细列表\n - **details**:\n   - `input_tokens_details.cached_tokens`\n   - `output_tokens_details.reasoning_tokens`\n \n-## 从一次运行访问用量\n+## 从一次运行中获取用量\n \n-在 `Runner.run(...)` 之后，通过 `result.context_wrapper.usage` 访问用量。\n+在执行 `Runner.run(...)` 之后，通过 `result.context_wrapper.usage` 获取用量。\n \n ```python\n result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n@@ -31,11 +31,11 @@ print(\"Output tokens:\", usage.output_tokens)\n print(\"Total tokens:\", usage.total_tokens)\n ```\n \n-用量会在运行期间的所有模型调用中聚合（包括工具调用和任务转移）。\n+用量会聚合此次运行中的所有模型调用（包括工具调用和任务转移）。\n \n-### 在 LiteLLM 模型中启用用量\n+### 在 LiteLLM 模型中启用用量统计\n \n-LiteLLM 提供商默认不报告用量指标。当你使用 [`LitellmModel`](models/litellm.md) 时，向你的智能体传入 `ModelSettings(include_usage=True)`，以便 LiteLLM 的响应填充 `result.context_wrapper.usage`。\n+LiteLLM 提供方默认不报告用量指标。使用 [`LitellmModel`](models/litellm.md) 时，向你的智能体传入 `ModelSettings(include_usage=True)`，以便 LiteLLM 的响应填充 `result.context_wrapper.usage`。\n \n ```python\n from agents import Agent, ModelSettings, Runner\n@@ -51,9 +51,9 @@ result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n print(result.context_wrapper.usage.total_tokens)\n ```\n \n-## 按请求的用量跟踪\n+## 按请求的用量追踪\n \n-SDK 会在 `request_usage_entries` 中自动跟踪每个 API 请求的用量，便于进行细粒度的成本计算和监控上下文窗口的消耗。\n+SDK 会自动在 `request_usage_entries` 中跟踪每个 API 请求的用量，便于精细的成本计算和监控上下文窗口消耗。\n \n ```python\n result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n@@ -62,7 +62,7 @@ for i, request in enumerate(result.context_wrapper.usage.request_usage_entries):\n     print(f\"Request {i + 1}: {request.input_tokens} in, {request.output_tokens} out\")\n ```\n \n-## 会话中的用量访问\n+## 在会话中获取用量\n \n 当你使用 `Session`（例如 `SQLiteSession`）时，每次调用 `Runner.run(...)` 都会返回该次运行的用量。会话会维护用于上下文的对话历史，但每次运行的用量彼此独立。\n \n@@ -76,11 +76,11 @@ second = await Runner.run(agent, \"Can you elaborate?\", session=session)\n print(second.context_wrapper.usage.total_tokens)  # Usage for second run\n ```\n \n-请注意，尽管会话会在运行之间保留对话上下文，但每次 `Runner.run()` 返回的用量指标仅代表该次执行。在会话中，先前的消息可能会在每次运行时被重新作为输入提供，这会影响后续轮次的输入 token 计数。\n+请注意，虽然会话会在运行之间保留对话上下文，但每次 `Runner.run()` 返回的用量指标仅代表该次执行。在会话中，先前消息可能会作为输入重新注入到每次运行中，这会影响后续轮次的输入令牌计数。\n \n ## 在钩子中使用用量\n \n-如果你使用 `RunHooks`，传递给每个钩子的 `context` 对象包含 `usage`。这使你能够在关键生命周期时刻记录用量。\n+如果你使用 `RunHooks`，传递给每个钩子的 `context` 对象包含 `usage`。这使你可以在关键生命周期时刻记录用量。\n \n ```python\n class MyHooks(RunHooks):\n@@ -91,9 +91,9 @@ class MyHooks(RunHooks):\n \n ## API 参考\n \n-更多 API 文档参见：\n+有关详细 API 文档，请参阅：\n \n--   [`Usage`][agents.usage.Usage] - 用量跟踪数据结构\n--   [`RequestUsage`][agents.usage.RequestUsage] - 按请求的用量详情\n--   [`RunContextWrapper`][agents.run.RunContextWrapper] - 从运行上下文访问用量\n--   [`RunHooks`][agents.run.RunHooks] - 钩入用量跟踪的生命周期\n\\ No newline at end of file\n+- [`Usage`][agents.usage.Usage] - 用量追踪数据结构\n+- [`RequestUsage`][agents.usage.RequestUsage] - 按请求的用量详情\n+- [`RunContextWrapper`][agents.run.RunContextWrapper] - 从运行上下文访问用量\n+- [`RunHooks`][agents.run.RunHooks] - 接入用量追踪生命周期\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fusage.md",
        "sha": "d4c73f3f65e91e62be1396f4c3921b1348905198",
        "status": "modified"
      },
      {
        "additions": 17,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvisualization.md",
        "changes": 34,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fvisualization.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 17,
        "filename": "docs/zh/visualization.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # 智能体可视化\n \n-智能体可视化允许你使用 **Graphviz** 生成智能体及其关系的结构化图形表示。这有助于理解在应用中智能体、工具和任务转移如何交互。\n+智能体可视化可使用 **Graphviz** 生成智能体及其关系的结构化图形表示。这有助于理解在应用中智能体、工具与任务转移如何交互。\n \n ## 安装\n \n@@ -14,14 +14,14 @@ search:\n pip install \"openai-agents[viz]\"\n ```\n \n-## 图形生成\n+## 生成图\n \n 你可以使用 `draw_graph` 函数生成智能体可视化。该函数会创建一个有向图，其中：\n \n-- **智能体** 表示为黄色方框。\n-- **MCP 服务** 表示为灰色方框。\n-- **工具** 表示为绿色椭圆。\n-- **任务转移** 用从一个智能体指向另一个智能体的有向边表示。\n+- **智能体** 用黄色方框表示。\n+- **MCP 服务** 用灰色方框表示。\n+- **工具** 用绿色椭圆表示。\n+- **任务转移** 是从一个智能体到另一个智能体的有向边。\n \n ### 示例用法\n \n@@ -69,30 +69,30 @@ draw_graph(triage_agent)\n \n ![Agent Graph](../assets/images/graph.png)\n \n-这将生成一个图，直观展示 **分诊智能体** 的结构以及与子智能体和工具的连接。\n+这将生成一个图，直观地表示 **分诊智能体** 的结构，以及它与子智能体和工具的连接。\n \n \n ## 可视化解读\n \n 生成的图包含：\n \n-- 一个表示入口点的 **起始节点**（`__start__`）。\n-- 用黄色填充的 **矩形** 表示智能体。\n-- 用绿色填充的 **椭圆** 表示工具。\n-- 用灰色填充的 **矩形** 表示 MCP 服务。\n+- 一个表示入口的 **起始节点**（`__start__`）。\n+- 智能体以带黄色填充的 **矩形** 表示。\n+- 工具以带绿色填充的 **椭圆** 表示。\n+- MCP 服务以带灰色填充的 **矩形** 表示。\n - 表示交互的有向边：\n   - **实线箭头** 表示智能体之间的任务转移。\n-  - **点线箭头** 表示工具调用。\n+  - **虚线点状箭头** 表示工具调用。\n   - **虚线箭头** 表示 MCP 服务调用。\n - 一个表示执行终止位置的 **结束节点**（`__end__`）。\n \n-**注意：** 在较新的\n-`agents` 包版本中会渲染 MCP 服务（已在 **v0.2.8** 中验证）。如果在你的可视化中未看到 MCP 方框，请升级到最新版本。\n+**注意：** 在最近版本的\n+`agents` 包中（已在 **v0.2.8** 验证）会渲染 MCP 服务。如果在你的可视化中没有看到 MCP 方框，请升级到最新版本。\n \n-## 图形自定义\n+## 自定义图形\n \n ### 显示图形\n-默认情况下，`draw_graph` 会内联显示图形。若要在单独窗口中显示，请编写以下内容：\n+默认情况下，`draw_graph` 会内联显示图形。若要在单独窗口中显示，请编写：\n \n ```python\n draw_graph(triage_agent).view()\n@@ -105,4 +105,4 @@ draw_graph(triage_agent).view()\n draw_graph(triage_agent, filename=\"agent_graph\")\n ```\n \n-这将在工作目录中生成 `agent_graph.png`。\n\\ No newline at end of file\n+这将在工作目录生成 `agent_graph.png`。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvisualization.md",
        "sha": "24d256d638ea474a5e270fa2a2239112916f421e",
        "status": "modified"
      },
      {
        "additions": 14,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvoice%2Fpipeline.md",
        "changes": 28,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fvoice%2Fpipeline.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 14,
        "filename": "docs/zh/voice/pipeline.md",
        "patch": "@@ -4,7 +4,7 @@ search:\n ---\n # 流水线与工作流\n \n-[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] 是一个类，可将你的智能体工作流轻松变成语音应用。你传入要运行的工作流，流水线会负责转写输入音频、检测音频何时结束、在合适的时机调用你的工作流，并将工作流输出转换回音频。\n+[`VoicePipeline`][agents.voice.pipeline.VoicePipeline] 是一个便于将你的智能体工作流变成语音应用的类。你传入要运行的工作流，流水线会负责转写输入音频、检测音频结束、在正确的时间调用你的工作流，并将工作流输出转换回音频。\n \n ```mermaid\n graph LR\n@@ -36,27 +36,27 @@ graph LR\n \n 创建流水线时，你可以设置以下内容：\n \n-1. [`workflow`][agents.voice.workflow.VoiceWorkflowBase]：每次有新音频被转写时运行的代码。\n+1. [`workflow`][agents.voice.workflow.VoiceWorkflowBase]，即每次有新音频被转写时运行的代码。\n 2. 使用的 [`speech-to-text`][agents.voice.model.STTModel] 和 [`text-to-speech`][agents.voice.model.TTSModel] 模型\n-3. [`config`][agents.voice.pipeline_config.VoicePipelineConfig]：用于配置以下内容：\n-    - 模型提供器，可将模型名称映射到具体模型\n+3. [`config`][agents.voice.pipeline_config.VoicePipelineConfig]，用于配置以下内容：\n+    - 模型提供方，可将模型名称映射到模型\n     - 追踪，包括是否禁用追踪、是否上传音频文件、工作流名称、追踪 ID 等\n-    - TTS 与 STT 模型的设置，如提示词、语言和所用数据类型\n+    - TTS 与 STT 模型的设置，例如提示词、语言和使用的数据类型\n \n ## 运行流水线\n \n-你可以通过 [`run()`][agents.voice.pipeline.VoicePipeline.run] 方法运行流水线，它允许你以两种形式传入音频输入：\n+你可以通过 [`run()`][agents.voice.pipeline.VoicePipeline.run] 方法运行流水线，并以两种形式传入音频输入：\n \n-1. 当你已有完整的音频转录文本，并只想为其生成结果时，使用 [`AudioInput`][agents.voice.input.AudioInput]。这在你不需要检测说话者何时说完的场景很有用；例如，预录音频，或在按住说话的应用中能够明确识别用户何时说完。\n-2. 当你可能需要检测用户何时说完时，使用 [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]。它允许你在检测到时不断推送音频片段，语音流水线会通过一种称为“语音活动检测”的过程，在合适的时机自动运行智能体工作流。\n+1. [`AudioInput`][agents.voice.input.AudioInput]：当你已有完整的音频转写文本，只想为其生成结果时使用。这适用于无需检测说话者何时说完的场景；例如，预先录制的音频，或在按键说话应用中能明确知道用户何时说完。\n+2. [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput]：当你可能需要检测用户何时说完时使用。它允许你在检测到音频时逐段推送音频块，语音流水线会通过“活动检测”在正确的时间自动运行智能体工作流。\n \n ## 结果\n \n-一次语音流水线运行的结果是一个 [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]。这是一个对象，允许你在事件发生时进行流式传输。[`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent] 有几种类型，包括：\n+一次语音流水线运行的结果是 [`StreamedAudioResult`][agents.voice.result.StreamedAudioResult]。这是一个允许你在事件发生时进行流式传输的对象。[`VoiceStreamEvent`][agents.voice.events.VoiceStreamEvent] 有几种类型，包括：\n \n-1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]：包含一段音频。\n-2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]：告知诸如轮次开始或结束等生命周期事件。\n-3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]：错误事件。\n+1. [`VoiceStreamEventAudio`][agents.voice.events.VoiceStreamEventAudio]，包含一段音频数据。\n+2. [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle]，通知诸如轮次开始或结束等生命周期事件。\n+3. [`VoiceStreamEventError`][agents.voice.events.VoiceStreamEventError]，错误事件。\n \n ```python\n \n@@ -74,6 +74,6 @@ async for event in result.stream():\n \n ## 最佳实践\n \n-### 中断\n+### 打断\n \n-Agents SDK 目前没有为 [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] 提供任何内置的中断支持。对每个检测到的轮次，它都会触发对你的工作流的单独一次运行。如果你希望在应用内部处理中断，你可以监听 [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] 事件。`turn_started` 表示一个新轮次已被转写，且处理即将开始。`turn_ended` 会在相应轮次的所有音频都已分发完毕后触发。你可以利用这些事件在模型开始一个轮次时将说话者的麦克风静音，并在你将该轮次的相关音频全部发送完后取消静音。\n\\ No newline at end of file\n+Agents SDK 目前不支持对 [`StreamedAudioInput`][agents.voice.input.StreamedAudioInput] 的任何内置打断功能。相反，每检测到一个轮次都会触发你的工作流单独运行一次。如果你想在应用内处理打断，可以监听 [`VoiceStreamEventLifecycle`][agents.voice.events.VoiceStreamEventLifecycle] 事件。`turn_started` 表示新的轮次已被转写且处理开始；`turn_ended` 会在相应轮次的所有音频派发完成后触发。你可以使用这些事件在模型开始一轮时将说话者麦克风静音，并在你将该轮次的相关音频全部发送完毕后取消静音。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvoice%2Fpipeline.md",
        "sha": "587dabb1ef49a4660953822b5b6acbab4abb3882",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvoice%2Fquickstart.md",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fvoice%2Fquickstart.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 8,
        "filename": "docs/zh/voice/quickstart.md",
        "patch": "@@ -2,22 +2,22 @@\n search:\n   exclude: true\n ---\n-# 快速开始\n+# 快速入门\n \n ## 先决条件\n \n-请确保你已按照 Agents SDK 的基础[快速开始指南](../quickstart.md)完成设置，并创建了虚拟环境。然后，从 SDK 安装可选的语音相关依赖：\n+请确保你已按照 [quickstart instructions](../quickstart.md)（适用于 Agents SDK）的基础说明完成设置，并已创建虚拟环境。然后，从该 SDK 安装可选的语音相关依赖：\n \n ```bash\n pip install 'openai-agents[voice]'\n ```\n \n ## 概念\n \n-这里的核心概念是一个 [`VoicePipeline`][agents.voice.pipeline.VoicePipeline]，它是一个包含 3 个步骤的流程：\n+需要了解的核心概念是 [`VoicePipeline`][agents.voice.pipeline.VoicePipeline]，它是一个包含 3 个步骤的流程：\n \n 1. 运行语音转文本模型，将音频转换为文本。\n-2. 运行你的代码（通常是一个智能体工作流）以生成结果。\n+2. 运行你的代码（通常是一个智能体工作流）以产出结果。\n 3. 运行文本转语音模型，将结果文本转换回音频。\n \n ```mermaid\n@@ -48,7 +48,7 @@ graph LR\n \n ## 智能体\n \n-首先，我们来设置一些智能体。如果你已经使用此 SDK 构建过智能体，这部分会很熟悉。我们将创建几个智能体、一个任务转移，以及一个工具。\n+首先，让我们设置一些智能体。如果你已经使用此 SDK 构建过智能体，这应该会让你感到熟悉。我们将创建几个智能体、一次任务转移，以及一个工具。\n \n ```python\n import asyncio\n@@ -99,7 +99,7 @@ from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline\n pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n ```\n \n-## 运行流水线\n+## 流水线运行\n \n ```python\n import numpy as np\n@@ -124,7 +124,7 @@ async for event in result.stream():\n \n ```\n \n-## 集成在一起\n+## 整体整合\n \n ```python\n import asyncio\n@@ -195,4 +195,4 @@ if __name__ == \"__main__\":\n     asyncio.run(main())\n ```\n \n-运行该示例后，智能体会对你“说话”！查看 [代码示例/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) 以体验一个你可以直接与智能体对话的演示。\n\\ No newline at end of file\n+如果你运行此示例，智能体会对你说话！前往 [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) 查看示例，体验一个你可以亲自与智能体对话的演示。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvoice%2Fquickstart.md",
        "sha": "dbcc5b17c6c5de54e6ade9871e4596c11a068565",
        "status": "modified"
      },
      {
        "additions": 8,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvoice%2Ftracing.md",
        "changes": 16,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fzh%2Fvoice%2Ftracing.md?ref=cc790a6c888a166b3b48dc90fa3c69e594f4dd18",
        "deletions": 8,
        "filename": "docs/zh/voice/tracing.md",
        "patch": "@@ -4,15 +4,15 @@ search:\n ---\n # 追踪\n \n-与[智能体的追踪方式](../tracing.md)相同，语音流水线也会被自动追踪。\n+就像[智能体的追踪方式](../tracing.md)一样，语音流水线也会被自动追踪。\n \n-你可以参考上面的追踪文档以获取基础信息，此外还可以通过[`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig]对流水线的追踪进行配置。\n+你可以参考上面的追踪文档了解基础信息，此外还可以通过[`VoicePipelineConfig`][agents.voice.pipeline_config.VoicePipelineConfig]对流水线的追踪进行额外配置。\n \n 与追踪相关的关键字段包括：\n \n--   [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]：控制是否禁用追踪。默认启用追踪。\n--   [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]：控制追踪是否包含可能敏感的数据，例如音频转写内容。这仅适用于语音流水线，不涉及你的 Workflow 内部发生的任何事情。\n--   [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]：控制追踪是否包含音频数据。\n--   [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]：追踪工作流的名称。\n--   [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]：追踪的`group_id`，用于关联多个追踪。\n--   [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]：要随追踪一起包含的附加元数据。\n\\ No newline at end of file\n+- [`tracing_disabled`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]：控制是否禁用追踪。默认情况下，追踪是启用的。\n+- [`trace_include_sensitive_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data]：控制追踪是否包含可能敏感的数据，如音频转写。此配置仅针对语音流水线，不适用于你的工作流内部发生的内容。\n+- [`trace_include_sensitive_audio_data`][agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data]：控制追踪是否包含音频数据。\n+- [`workflow_name`][agents.voice.pipeline_config.VoicePipelineConfig.workflow_name]：追踪工作流的名称。\n+- [`group_id`][agents.voice.pipeline_config.VoicePipelineConfig.group_id]：追踪的`group_id`，用于关联多个追踪。\n+- [`trace_metadata`][agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled]：随追踪一起包含的附加元数据。\n\\ No newline at end of file",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/cc790a6c888a166b3b48dc90fa3c69e594f4dd18/docs%2Fzh%2Fvoice%2Ftracing.md",
        "sha": "da47ad208d1695e6e2421922fb6f1b132e7ee63c",
        "status": "modified"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:57:20.709607Z"
}
