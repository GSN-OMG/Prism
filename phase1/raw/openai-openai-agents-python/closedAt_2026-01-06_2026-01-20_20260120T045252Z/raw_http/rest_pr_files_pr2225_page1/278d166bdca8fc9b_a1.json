{
  "finished_at": "2026-01-20T04:53:52.712215Z",
  "meta": {
    "attempt": 1,
    "request_fingerprint": "278d166bdca8fc9b",
    "tag": "rest_pr_files_pr2225_page1"
  },
  "request": {
    "body": null,
    "headers": {
      "Accept": "application/vnd.github+json",
      "X-GitHub-Api-Version": "2022-11-28"
    },
    "method": "GET",
    "url": "https://api.github.com/repos/openai/openai-agents-python/pulls/2225/files?per_page=100&page=1"
  },
  "response": {
    "headers": {
      "access-control-allow-origin": "*",
      "access-control-expose-headers": "ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset",
      "cache-control": "private, max-age=60, s-maxage=60",
      "content-length": "35763",
      "content-security-policy": "default-src 'none'",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 20 Jan 2026 04:53:52 GMT",
      "etag": "\"27eb950040a4c7bd263a70307bd1d349fa198e0605946e81867981a5d783c7f1\"",
      "github-authentication-token-expiration": "2026-02-19 04:41:20 UTC",
      "last-modified": "Tue, 06 Jan 2026 02:18:35 GMT",
      "referrer-policy": "origin-when-cross-origin, strict-origin-when-cross-origin",
      "server": "github.com",
      "strict-transport-security": "max-age=31536000; includeSubdomains; preload",
      "vary": "Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With",
      "x-accepted-oauth-scopes": "",
      "x-content-type-options": "nosniff",
      "x-frame-options": "deny",
      "x-github-api-version-selected": "2022-11-28",
      "x-github-media-type": "github.v3; format=json",
      "x-github-request-id": "DACB:3FF1FA:3A1BC8:51981F:696F0A60",
      "x-oauth-scopes": "repo",
      "x-ratelimit-limit": "5000",
      "x-ratelimit-remaining": "4992",
      "x-ratelimit-reset": "1768885989",
      "x-ratelimit-resource": "core",
      "x-ratelimit-used": "8",
      "x-xss-protection": "0"
    },
    "json": [
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/docs%2Fllms-full.txt",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fllms-full.txt?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 1,
        "filename": "docs/llms-full.txt",
        "patch": "@@ -33,7 +33,7 @@ The Agents SDK delivers a focused set of Python primitivesâ€”agents, tools, guar\n - [Voice quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/): Build an end-to-end voice assistant with streaming transcription, text-to-speech, and event-driven responses.\n - [Voice pipeline](https://openai.github.io/openai-agents-python/voice/pipeline/): Customize audio capture, buffering, model invocation, and playback in voice-first experiences.\n - [Voice tracing](https://openai.github.io/openai-agents-python/voice/tracing/): Inspect voice session traces, latency breakdowns, and audio event timelines.\n-- [Realtime quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/): Launch realtime agents over WebRTC or websockets, subscribe to events, and manage low-latency execution.\n+- [Realtime quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/): Launch realtime agents over websockets (WebRTC is not available in the Python SDK), subscribe to events, and manage low-latency execution.\n - [Realtime guide](https://openai.github.io/openai-agents-python/realtime/guide/): Deep dive into realtime session lifecycle, event schemas, concurrency, and backpressure handling.\n \n ## Models and Provider Integrations",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/docs%2Fllms-full.txt",
        "sha": "c4bac19bacfe053b00e719ce1f86d3c638ea208f",
        "status": "modified"
      },
      {
        "additions": 1,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/docs%2Fllms.txt",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/docs%2Fllms.txt?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 1,
        "filename": "docs/llms.txt",
        "patch": "@@ -36,7 +36,7 @@ The SDK focuses on a concise set of primitives so you can orchestrate multi-agen\n ## Modalities and Interfaces\n - [Voice quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/): Build speech-enabled agents with streaming transcription and TTS.\n - [Voice pipeline](https://openai.github.io/openai-agents-python/voice/pipeline/): Customize audio ingestion, tool execution, and response rendering.\n-- [Realtime quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/): Stand up low-latency realtime agents with WebRTC and websocket transports.\n+- [Realtime quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/): Stand up low-latency realtime agents with websocket transport (WebRTC is not available in the Python SDK).\n - [Realtime guide](https://openai.github.io/openai-agents-python/realtime/guide/): Deep dive into session lifecycle, event formats, and concurrency patterns.\n \n ## API Reference Highlights",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/docs%2Fllms.txt",
        "sha": "86c4058e17e321ce45dc2b8b844a9fa6fdc0e6f7",
        "status": "modified"
      },
      {
        "additions": 2,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2F__init__.py",
        "changes": 2,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frealtime%2F__init__.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 0,
        "filename": "src/agents/realtime/__init__.py",
        "patch": "@@ -84,6 +84,7 @@\n )\n from .openai_realtime import (\n     DEFAULT_MODEL_SETTINGS,\n+    OpenAIRealtimeSIPModel,\n     OpenAIRealtimeWebSocketModel,\n     get_api_key,\n )\n@@ -176,6 +177,7 @@\n     \"RealtimeModelUserInputMessage\",\n     # OpenAI Realtime\n     \"DEFAULT_MODEL_SETTINGS\",\n+    \"OpenAIRealtimeSIPModel\",\n     \"OpenAIRealtimeWebSocketModel\",\n     \"get_api_key\",\n     # Session",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2F__init__.py",
        "sha": "74937d15194f56e695bbef6d17dda33d4386f736",
        "status": "modified"
      },
      {
        "additions": 25,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2Faudio_formats.py",
        "changes": 26,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frealtime%2Faudio_formats.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 1,
        "filename": "src/agents/realtime/audio_formats.py",
        "patch": "@@ -1,5 +1,8 @@\n from __future__ import annotations\n \n+from collections.abc import Mapping\n+from typing import Any, Literal\n+\n from openai.types.realtime.realtime_audio_formats import (\n     AudioPCM,\n     AudioPCMA,\n@@ -11,7 +14,7 @@\n \n \n def to_realtime_audio_format(\n-    input_audio_format: str | RealtimeAudioFormats | None,\n+    input_audio_format: str | RealtimeAudioFormats | Mapping[str, Any] | None,\n ) -> RealtimeAudioFormats | None:\n     format: RealtimeAudioFormats | None = None\n     if input_audio_format is not None:\n@@ -24,6 +27,27 @@ def to_realtime_audio_format(\n                 format = AudioPCMA(type=\"audio/pcma\")\n             else:\n                 logger.debug(f\"Unknown input_audio_format: {input_audio_format}\")\n+        elif isinstance(input_audio_format, Mapping):\n+            fmt_type = input_audio_format.get(\"type\")\n+            rate = input_audio_format.get(\"rate\")\n+            if fmt_type == \"audio/pcm\":\n+                pcm_rate: Literal[24000] | None\n+                if isinstance(rate, (int, float)) and int(rate) == 24000:\n+                    pcm_rate = 24000\n+                elif rate is None:\n+                    pcm_rate = 24000\n+                else:\n+                    logger.debug(\n+                        f\"Unknown pcm rate in input_audio_format mapping: {input_audio_format}\"\n+                    )\n+                    pcm_rate = 24000\n+                format = AudioPCM(type=\"audio/pcm\", rate=pcm_rate)\n+            elif fmt_type == \"audio/pcmu\":\n+                format = AudioPCMU(type=\"audio/pcmu\")\n+            elif fmt_type == \"audio/pcma\":\n+                format = AudioPCMA(type=\"audio/pcma\")\n+            else:\n+                logger.debug(f\"Unknown input_audio_format mapping: {input_audio_format}\")\n         else:\n             format = input_audio_format\n     return format",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2Faudio_formats.py",
        "sha": "fdfe12304faff4a3a850b12b48ba633b8daced04",
        "status": "modified"
      },
      {
        "additions": 40,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2Fconfig.py",
        "changes": 46,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frealtime%2Fconfig.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 6,
        "filename": "src/agents/realtime/config.py",
        "patch": "@@ -1,10 +1,7 @@\n from __future__ import annotations\n \n-from typing import (\n-    Any,\n-    Literal,\n-    Union,\n-)\n+from collections.abc import Mapping\n+from typing import Any, Literal, Union\n \n from openai.types.realtime.realtime_audio_formats import (\n     RealtimeAudioFormats as OpenAIRealtimeAudioFormats,\n@@ -28,13 +25,20 @@\n         \"gpt-4o-realtime-preview-2024-12-17\",\n         \"gpt-4o-realtime-preview-2024-10-01\",\n         \"gpt-4o-mini-realtime-preview-2024-12-17\",\n+        \"gpt-realtime-mini\",\n+        \"gpt-realtime-mini-2025-10-06\",\n     ],\n     str,\n ]\n \"\"\"The name of a realtime model.\"\"\"\n \n \n-RealtimeAudioFormat: TypeAlias = Union[Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"], str]\n+RealtimeAudioFormat: TypeAlias = Union[\n+    Literal[\"pcm16\", \"g711_ulaw\", \"g711_alaw\"],\n+    str,\n+    Mapping[str, Any],\n+    OpenAIRealtimeAudioFormats,\n+]\n \"\"\"The audio format for realtime audio streams.\"\"\"\n \n \n@@ -96,6 +100,30 @@ class RealtimeTurnDetectionConfig(TypedDict):\n     \"\"\"Threshold for server-vad to trigger a response if the user is idle for this duration.\"\"\"\n \n \n+class RealtimeAudioInputConfig(TypedDict, total=False):\n+    \"\"\"Configuration for audio input in realtime sessions.\"\"\"\n+\n+    format: RealtimeAudioFormat | OpenAIRealtimeAudioFormats\n+    noise_reduction: RealtimeInputAudioNoiseReductionConfig | None\n+    transcription: RealtimeInputAudioTranscriptionConfig\n+    turn_detection: RealtimeTurnDetectionConfig\n+\n+\n+class RealtimeAudioOutputConfig(TypedDict, total=False):\n+    \"\"\"Configuration for audio output in realtime sessions.\"\"\"\n+\n+    format: RealtimeAudioFormat | OpenAIRealtimeAudioFormats\n+    voice: str\n+    speed: float\n+\n+\n+class RealtimeAudioConfig(TypedDict, total=False):\n+    \"\"\"Audio configuration for realtime sessions.\"\"\"\n+\n+    input: RealtimeAudioInputConfig\n+    output: RealtimeAudioOutputConfig\n+\n+\n class RealtimeSessionModelSettings(TypedDict):\n     \"\"\"Model settings for a realtime model session.\"\"\"\n \n@@ -111,6 +139,12 @@ class RealtimeSessionModelSettings(TypedDict):\n     modalities: NotRequired[list[Literal[\"text\", \"audio\"]]]\n     \"\"\"The modalities the model should support.\"\"\"\n \n+    output_modalities: NotRequired[list[Literal[\"text\", \"audio\"]]]\n+    \"\"\"The output modalities the model should support.\"\"\"\n+\n+    audio: NotRequired[RealtimeAudioConfig]\n+    \"\"\"The audio configuration for the session.\"\"\"\n+\n     voice: NotRequired[str]\n     \"\"\"The voice to use for audio output.\"\"\"\n ",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2Fconfig.py",
        "sha": "4ac9bcaa44f7ed81fbd3709f2bd0ebfadd034979",
        "status": "modified"
      },
      {
        "additions": 194,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2Fopenai_realtime.py",
        "changes": 230,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/src%2Fagents%2Frealtime%2Fopenai_realtime.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 36,
        "filename": "src/agents/realtime/openai_realtime.py",
        "patch": "@@ -79,7 +79,7 @@\n )\n from openai.types.responses.response_prompt import ResponsePrompt\n from pydantic import Field, TypeAdapter\n-from typing_extensions import assert_never\n+from typing_extensions import TypeAlias, assert_never\n from websockets.asyncio.client import ClientConnection\n \n from agents.handoffs import Handoff\n@@ -91,11 +91,15 @@\n \n from ..exceptions import UserError\n from ..logger import logger\n+from ..run_context import RunContextWrapper, TContext\n from ..version import __version__\n+from .agent import RealtimeAgent\n from .config import (\n     RealtimeModelTracingConfig,\n+    RealtimeRunConfig,\n     RealtimeSessionModelSettings,\n )\n+from .handoffs import realtime_handoff\n from .items import RealtimeMessageItem, RealtimeToolCallItem\n from .model import (\n     RealtimeModel,\n@@ -131,6 +135,16 @@\n     RealtimeModelSendUserInput,\n )\n \n+FormatInput: TypeAlias = Union[\n+    str,\n+    AudioPCM,\n+    AudioPCMU,\n+    AudioPCMA,\n+    Mapping[str, Any],\n+    None,\n+]\n+\n+\n # Avoid direct imports of non-exported names by referencing via module\n OpenAIRealtimeAudioConfig = _rt_audio_config.RealtimeAudioConfig\n OpenAIRealtimeAudioInput = _rt_audio_config.RealtimeAudioConfigInput  # type: ignore[attr-defined]\n@@ -178,6 +192,60 @@ def get_server_event_type_adapter() -> TypeAdapter[AllRealtimeServerEvents]:\n     return ServerEventTypeAdapter\n \n \n+async def _collect_enabled_handoffs(\n+    agent: RealtimeAgent[Any], context_wrapper: RunContextWrapper[Any]\n+) -> list[Handoff[Any, RealtimeAgent[Any]]]:\n+    handoffs: list[Handoff[Any, RealtimeAgent[Any]]] = []\n+    for handoff_item in agent.handoffs:\n+        if isinstance(handoff_item, Handoff):\n+            handoffs.append(handoff_item)\n+        elif isinstance(handoff_item, RealtimeAgent):\n+            handoffs.append(realtime_handoff(handoff_item))\n+\n+    async def _check_handoff_enabled(handoff_obj: Handoff[Any, RealtimeAgent[Any]]) -> bool:\n+        attr = handoff_obj.is_enabled\n+        if isinstance(attr, bool):\n+            return attr\n+        res = attr(context_wrapper, agent)\n+        if inspect.isawaitable(res):\n+            return await res\n+        return res\n+\n+    results = await asyncio.gather(*(_check_handoff_enabled(h) for h in handoffs))\n+    return [h for h, ok in zip(handoffs, results) if ok]\n+\n+\n+async def _build_model_settings_from_agent(\n+    *,\n+    agent: RealtimeAgent[Any],\n+    context_wrapper: RunContextWrapper[Any],\n+    base_settings: RealtimeSessionModelSettings,\n+    starting_settings: RealtimeSessionModelSettings | None,\n+    run_config: RealtimeRunConfig | None,\n+) -> RealtimeSessionModelSettings:\n+    updated_settings = base_settings.copy()\n+\n+    if agent.prompt is not None:\n+        updated_settings[\"prompt\"] = agent.prompt\n+\n+    instructions, tools, handoffs = await asyncio.gather(\n+        agent.get_system_prompt(context_wrapper),\n+        agent.get_all_tools(context_wrapper),\n+        _collect_enabled_handoffs(agent, context_wrapper),\n+    )\n+    updated_settings[\"instructions\"] = instructions or \"\"\n+    updated_settings[\"tools\"] = tools or []\n+    updated_settings[\"handoffs\"] = handoffs or []\n+\n+    if starting_settings:\n+        updated_settings.update(starting_settings)\n+\n+    if run_config and run_config.get(\"tracing_disabled\", False):\n+        updated_settings[\"tracing\"] = None\n+\n+    return updated_settings\n+\n+\n # Note: Avoid a module-level union alias for Python 3.9 compatibility.\n # Using a union at runtime (e.g., A | B) in a type alias triggers evaluation\n # during import on 3.9. We instead inline the union in annotations below.\n@@ -819,6 +887,27 @@ def _read_format_type(fmt: object) -> str | None:\n \n         return type_value if isinstance(type_value, str) else None\n \n+    @staticmethod\n+    def _normalize_turn_detection_config(config: object) -> object:\n+        \"\"\"Normalize camelCase turn detection keys to snake_case for API compatibility.\"\"\"\n+        if not isinstance(config, Mapping):\n+            return config\n+\n+        normalized = dict(config)\n+        key_map = {\n+            \"createResponse\": \"create_response\",\n+            \"interruptResponse\": \"interrupt_response\",\n+            \"prefixPaddingMs\": \"prefix_padding_ms\",\n+            \"silenceDurationMs\": \"silence_duration_ms\",\n+            \"idleTimeoutMs\": \"idle_timeout_ms\",\n+        }\n+        for camel_key, snake_key in key_map.items():\n+            if camel_key in normalized and snake_key not in normalized:\n+                normalized[snake_key] = normalized[camel_key]\n+            normalized.pop(camel_key, None)\n+\n+        return normalized\n+\n     async def _update_session_config(self, model_settings: RealtimeSessionModelSettings) -> None:\n         session_config = self._get_session_config(model_settings)\n         await self._send_raw_message(\n@@ -829,62 +918,95 @@ def _get_session_config(\n         self, model_settings: RealtimeSessionModelSettings\n     ) -> OpenAISessionCreateRequest:\n         \"\"\"Get the session config.\"\"\"\n-        audio_input_args = {}\n+        audio_input_args: dict[str, Any] = {}\n+        audio_output_args: dict[str, Any] = {}\n+\n+        audio_config = model_settings.get(\"audio\")\n+        audio_config_mapping = audio_config if isinstance(audio_config, Mapping) else None\n+        input_audio_config: Mapping[str, Any] = (\n+            cast(Mapping[str, Any], audio_config_mapping.get(\"input\", {}))\n+            if audio_config_mapping\n+            else {}\n+        )\n+        output_audio_config: Mapping[str, Any] = (\n+            cast(Mapping[str, Any], audio_config_mapping.get(\"output\", {}))\n+            if audio_config_mapping\n+            else {}\n+        )\n \n-        if self._call_id:\n-            audio_input_args[\"format\"] = to_realtime_audio_format(\n-                model_settings.get(\"input_audio_format\")\n-            )\n-        else:\n-            audio_input_args[\"format\"] = to_realtime_audio_format(\n-                model_settings.get(\n+        input_format_source: FormatInput = (\n+            input_audio_config.get(\"format\") if input_audio_config else None\n+        )\n+        if input_format_source is None:\n+            if self._call_id:\n+                input_format_source = model_settings.get(\"input_audio_format\")\n+            else:\n+                input_format_source = model_settings.get(\n                     \"input_audio_format\", DEFAULT_MODEL_SETTINGS.get(\"input_audio_format\")\n                 )\n-            )\n+        audio_input_args[\"format\"] = to_realtime_audio_format(input_format_source)\n \n-        if \"input_audio_noise_reduction\" in model_settings:\n-            audio_input_args[\"noise_reduction\"] = model_settings.get(\"input_audio_noise_reduction\")  # type: ignore[assignment]\n+        if \"noise_reduction\" in input_audio_config:\n+            audio_input_args[\"noise_reduction\"] = input_audio_config.get(\"noise_reduction\")\n+        elif \"input_audio_noise_reduction\" in model_settings:\n+            audio_input_args[\"noise_reduction\"] = model_settings.get(\"input_audio_noise_reduction\")\n \n-        if \"input_audio_transcription\" in model_settings:\n-            audio_input_args[\"transcription\"] = model_settings.get(\"input_audio_transcription\")  # type: ignore[assignment]\n+        if \"transcription\" in input_audio_config:\n+            audio_input_args[\"transcription\"] = input_audio_config.get(\"transcription\")\n+        elif \"input_audio_transcription\" in model_settings:\n+            audio_input_args[\"transcription\"] = model_settings.get(\"input_audio_transcription\")\n         else:\n-            audio_input_args[\"transcription\"] = DEFAULT_MODEL_SETTINGS.get(  # type: ignore[assignment]\n+            audio_input_args[\"transcription\"] = DEFAULT_MODEL_SETTINGS.get(\n                 \"input_audio_transcription\"\n             )\n \n-        if \"turn_detection\" in model_settings:\n-            audio_input_args[\"turn_detection\"] = model_settings.get(\"turn_detection\")  # type: ignore[assignment]\n+        if \"turn_detection\" in input_audio_config:\n+            audio_input_args[\"turn_detection\"] = self._normalize_turn_detection_config(\n+                input_audio_config.get(\"turn_detection\")\n+            )\n+        elif \"turn_detection\" in model_settings:\n+            audio_input_args[\"turn_detection\"] = self._normalize_turn_detection_config(\n+                model_settings.get(\"turn_detection\")\n+            )\n         else:\n-            audio_input_args[\"turn_detection\"] = DEFAULT_MODEL_SETTINGS.get(\"turn_detection\")  # type: ignore[assignment]\n+            audio_input_args[\"turn_detection\"] = DEFAULT_MODEL_SETTINGS.get(\"turn_detection\")\n \n-        audio_output_args = {\n-            \"voice\": model_settings.get(\"voice\", DEFAULT_MODEL_SETTINGS.get(\"voice\")),\n-        }\n+        requested_voice = output_audio_config.get(\"voice\") if output_audio_config else None\n+        audio_output_args[\"voice\"] = requested_voice or model_settings.get(\n+            \"voice\", DEFAULT_MODEL_SETTINGS.get(\"voice\")\n+        )\n \n-        if self._call_id:\n-            audio_output_args[\"format\"] = to_realtime_audio_format(  # type: ignore[assignment]\n-                model_settings.get(\"output_audio_format\")\n-            )\n-        else:\n-            audio_output_args[\"format\"] = to_realtime_audio_format(  # type: ignore[assignment]\n-                model_settings.get(\n+        output_format_source: FormatInput = (\n+            output_audio_config.get(\"format\") if output_audio_config else None\n+        )\n+        if output_format_source is None:\n+            if self._call_id:\n+                output_format_source = model_settings.get(\"output_audio_format\")\n+            else:\n+                output_format_source = model_settings.get(\n                     \"output_audio_format\", DEFAULT_MODEL_SETTINGS.get(\"output_audio_format\")\n                 )\n-            )\n+        audio_output_args[\"format\"] = to_realtime_audio_format(output_format_source)\n \n-        if \"speed\" in model_settings:\n-            audio_output_args[\"speed\"] = model_settings.get(\"speed\")  # type: ignore[assignment]\n+        if \"speed\" in output_audio_config:\n+            audio_output_args[\"speed\"] = output_audio_config.get(\"speed\")\n+        elif \"speed\" in model_settings:\n+            audio_output_args[\"speed\"] = model_settings.get(\"speed\")\n+\n+        output_modalities = (\n+            model_settings.get(\"output_modalities\")\n+            or model_settings.get(\"modalities\")\n+            or DEFAULT_MODEL_SETTINGS.get(\"modalities\")\n+        )\n \n         # Construct full session object. `type` will be excluded at serialization time for updates.\n         session_create_request = OpenAISessionCreateRequest(\n             type=\"realtime\",\n             model=(model_settings.get(\"model_name\") or self.model) or \"gpt-realtime\",\n-            output_modalities=model_settings.get(\n-                \"modalities\", DEFAULT_MODEL_SETTINGS.get(\"modalities\")\n-            ),\n+            output_modalities=output_modalities,\n             audio=OpenAIRealtimeAudioConfig(\n-                input=OpenAIRealtimeAudioInput(**audio_input_args),  # type: ignore[arg-type]\n-                output=OpenAIRealtimeAudioOutput(**audio_output_args),  # type: ignore[arg-type]\n+                input=OpenAIRealtimeAudioInput(**audio_input_args),\n+                output=OpenAIRealtimeAudioOutput(**audio_output_args),\n             ),\n             tools=cast(\n                 Any,\n@@ -949,6 +1071,42 @@ def _tools_to_session_tools(\n class OpenAIRealtimeSIPModel(OpenAIRealtimeWebSocketModel):\n     \"\"\"Realtime model that attaches to SIP-originated calls using a call ID.\"\"\"\n \n+    @staticmethod\n+    async def build_initial_session_payload(\n+        agent: RealtimeAgent[Any],\n+        *,\n+        context: TContext | None = None,\n+        model_config: RealtimeModelConfig | None = None,\n+        run_config: RealtimeRunConfig | None = None,\n+        overrides: RealtimeSessionModelSettings | None = None,\n+    ) -> OpenAISessionCreateRequest:\n+        \"\"\"Build a session payload that mirrors what a RealtimeSession would send on connect.\n+\n+        This helper can be used to accept SIP-originated calls by forwarding the returned payload to\n+        the Realtime Calls API without duplicating session setup logic.\n+        \"\"\"\n+        run_config_settings = (run_config or {}).get(\"model_settings\") or {}\n+        initial_model_settings = (model_config or {}).get(\"initial_model_settings\") or {}\n+        base_settings: RealtimeSessionModelSettings = {\n+            **run_config_settings,\n+            **initial_model_settings,\n+        }\n+\n+        context_wrapper = RunContextWrapper(context)\n+        merged_settings = await _build_model_settings_from_agent(\n+            agent=agent,\n+            context_wrapper=context_wrapper,\n+            base_settings=base_settings,\n+            starting_settings=initial_model_settings,\n+            run_config=run_config,\n+        )\n+\n+        if overrides:\n+            merged_settings.update(overrides)\n+\n+        model = OpenAIRealtimeWebSocketModel()\n+        return model._get_session_config(merged_settings)\n+\n     async def connect(self, options: RealtimeModelConfig) -> None:\n         call_id = options.get(\"call_id\")\n         if not call_id:",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/src%2Fagents%2Frealtime%2Fopenai_realtime.py",
        "sha": "af8625f09914a6319c5b29ad500d227b86607222",
        "status": "modified"
      },
      {
        "additions": 22,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/tests%2Frealtime%2Ftest_audio_formats_unit.py",
        "changes": 23,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Frealtime%2Ftest_audio_formats_unit.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 1,
        "filename": "tests/realtime/test_audio_formats_unit.py",
        "patch": "@@ -1,4 +1,4 @@\n-from openai.types.realtime.realtime_audio_formats import AudioPCM\n+from openai.types.realtime.realtime_audio_formats import AudioPCM, AudioPCMA, AudioPCMU\n \n from agents.realtime.audio_formats import to_realtime_audio_format\n \n@@ -26,3 +26,24 @@ def test_to_realtime_audio_format_passthrough_and_unknown_logs():\n \n def test_to_realtime_audio_format_none():\n     assert to_realtime_audio_format(None) is None\n+\n+\n+def test_to_realtime_audio_format_from_mapping():\n+    pcm = to_realtime_audio_format({\"type\": \"audio/pcm\", \"rate\": 16000})\n+    assert isinstance(pcm, AudioPCM)\n+    assert pcm.type == \"audio/pcm\"\n+    assert pcm.rate == 24000\n+\n+    pcm_default_rate = to_realtime_audio_format({\"type\": \"audio/pcm\"})\n+    assert isinstance(pcm_default_rate, AudioPCM)\n+    assert pcm_default_rate.rate == 24000\n+\n+    ulaw = to_realtime_audio_format({\"type\": \"audio/pcmu\"})\n+    assert isinstance(ulaw, AudioPCMU)\n+    assert ulaw.type == \"audio/pcmu\"\n+\n+    alaw = to_realtime_audio_format({\"type\": \"audio/pcma\"})\n+    assert isinstance(alaw, AudioPCMA)\n+    assert alaw.type == \"audio/pcma\"\n+\n+    assert to_realtime_audio_format({\"type\": \"audio/unknown\", \"rate\": 8000}) is None",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/tests%2Frealtime%2Ftest_audio_formats_unit.py",
        "sha": "3eaf135562394c8dd67bc29480e7e05a271314a3",
        "status": "modified"
      },
      {
        "additions": 42,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/tests%2Frealtime%2Ftest_openai_realtime.py",
        "changes": 42,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Frealtime%2Ftest_openai_realtime.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 0,
        "filename": "tests/realtime/test_openai_realtime.py",
        "patch": "@@ -669,6 +669,48 @@ def test_session_config_preserves_sip_audio_formats(self, model):\n         assert cfg.audio.output is not None\n         assert cfg.audio.output.format is None\n \n+    def test_session_config_respects_audio_block_and_output_modalities(self, model):\n+        settings = {\n+            \"input_audio_format\": \"pcm16\",\n+            \"output_audio_format\": \"pcm16\",\n+            \"modalities\": [\"audio\"],\n+            \"output_modalities\": [\"text\"],\n+            \"audio\": {\n+                \"input\": {\n+                    \"format\": {\"type\": \"audio/pcmu\"},\n+                    \"turn_detection\": {\n+                        \"type\": \"server_vad\",\n+                        \"createResponse\": True,\n+                        \"silenceDurationMs\": 450,\n+                    },\n+                },\n+                \"output\": {\n+                    \"format\": {\"type\": \"audio/pcma\"},\n+                    \"voice\": \"synth-1\",\n+                    \"speed\": 1.5,\n+                },\n+            },\n+        }\n+        cfg = model._get_session_config(settings)\n+\n+        assert cfg.output_modalities == [\"text\"]\n+        assert cfg.audio is not None\n+        assert cfg.audio.input.format is not None\n+        assert cfg.audio.input.format.type == \"audio/pcmu\"\n+        assert cfg.audio.output.format is not None\n+        assert cfg.audio.output.format.type == \"audio/pcma\"\n+        assert cfg.audio.output.voice == \"synth-1\"\n+        assert cfg.audio.output.speed == 1.5\n+        assert cfg.audio.input.transcription is not None\n+\n+        turn_detection = cfg.audio.input.turn_detection\n+        turn_detection_mapping = (\n+            turn_detection if isinstance(turn_detection, dict) else turn_detection.model_dump()\n+        )\n+        assert turn_detection_mapping[\"create_response\"] is True\n+        assert turn_detection_mapping[\"silence_duration_ms\"] == 450\n+        assert \"silenceDurationMs\" not in turn_detection_mapping\n+\n     @pytest.mark.asyncio\n     async def test_handle_error_event_success(self, model):\n         \"\"\"Test successful handling of error events.\"\"\"",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/tests%2Frealtime%2Ftest_openai_realtime.py",
        "sha": "7895989d6d3f6fa2b6eb0def3b2c403f9f41eba9",
        "status": "modified"
      },
      {
        "additions": 131,
        "blob_url": "https://github.com/openai/openai-agents-python/blob/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/tests%2Frealtime%2Ftest_realtime_model_settings.py",
        "changes": 131,
        "contents_url": "https://api.github.com/repos/openai/openai-agents-python/contents/tests%2Frealtime%2Ftest_realtime_model_settings.py?ref=5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e",
        "deletions": 0,
        "filename": "tests/realtime/test_realtime_model_settings.py",
        "patch": "@@ -0,0 +1,131 @@\n+from __future__ import annotations\n+\n+from unittest.mock import AsyncMock\n+\n+import pytest\n+from openai.types.realtime.realtime_session_create_request import (\n+    RealtimeSessionCreateRequest,\n+)\n+\n+from agents.handoffs import Handoff\n+from agents.realtime.agent import RealtimeAgent\n+from agents.realtime.config import RealtimeRunConfig, RealtimeSessionModelSettings\n+from agents.realtime.handoffs import realtime_handoff\n+from agents.realtime.model import RealtimeModelConfig\n+from agents.realtime.openai_realtime import (\n+    OpenAIRealtimeSIPModel,\n+    _build_model_settings_from_agent,\n+    _collect_enabled_handoffs,\n+)\n+from agents.run_context import RunContextWrapper\n+from agents.tool import function_tool\n+\n+\n+@pytest.mark.asyncio\n+async def test_collect_enabled_handoffs_filters_disabled() -> None:\n+    parent = RealtimeAgent(name=\"parent\")\n+    disabled = realtime_handoff(\n+        RealtimeAgent(name=\"child_disabled\"),\n+        is_enabled=lambda ctx, agent: False,\n+    )\n+    parent.handoffs = [disabled, RealtimeAgent(name=\"child_enabled\")]\n+\n+    enabled = await _collect_enabled_handoffs(parent, RunContextWrapper(None))\n+\n+    assert len(enabled) == 1\n+    assert isinstance(enabled[0], Handoff)\n+    assert enabled[0].agent_name == \"child_enabled\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_build_model_settings_from_agent_merges_agent_fields(monkeypatch: pytest.MonkeyPatch):\n+    agent = RealtimeAgent(name=\"root\", prompt={\"id\": \"prompt-id\"})\n+    monkeypatch.setattr(agent, \"get_system_prompt\", AsyncMock(return_value=\"sys\"))\n+\n+    @function_tool\n+    def helper() -> str:\n+        \"\"\"Helper tool for testing.\"\"\"\n+        return \"ok\"\n+\n+    monkeypatch.setattr(agent, \"get_all_tools\", AsyncMock(return_value=[helper]))\n+    agent.handoffs = [RealtimeAgent(name=\"handoff-child\")]\n+    base_settings: RealtimeSessionModelSettings = {\"model_name\": \"gpt-realtime\"}\n+    starting_settings: RealtimeSessionModelSettings = {\"voice\": \"verse\"}\n+    run_config: RealtimeRunConfig = {\"tracing_disabled\": True}\n+\n+    merged = await _build_model_settings_from_agent(\n+        agent=agent,\n+        context_wrapper=RunContextWrapper(None),\n+        base_settings=base_settings,\n+        starting_settings=starting_settings,\n+        run_config=run_config,\n+    )\n+\n+    assert merged[\"prompt\"] == {\"id\": \"prompt-id\"}\n+    assert merged[\"instructions\"] == \"sys\"\n+    assert merged[\"tools\"][0].name == helper.name\n+    assert merged[\"handoffs\"][0].agent_name == \"handoff-child\"\n+    assert merged[\"voice\"] == \"verse\"\n+    assert merged[\"model_name\"] == \"gpt-realtime\"\n+    assert merged[\"tracing\"] is None\n+    assert base_settings == {\"model_name\": \"gpt-realtime\"}\n+\n+\n+@pytest.mark.asyncio\n+async def test_sip_model_build_initial_session_payload(monkeypatch: pytest.MonkeyPatch):\n+    agent = RealtimeAgent(name=\"parent\", prompt={\"id\": \"prompt-99\"})\n+    child_agent = RealtimeAgent(name=\"child\")\n+    agent.handoffs = [child_agent]\n+\n+    @function_tool\n+    def ping() -> str:\n+        \"\"\"Ping tool used for session payload building.\"\"\"\n+        return \"pong\"\n+\n+    monkeypatch.setattr(agent, \"get_system_prompt\", AsyncMock(return_value=\"parent-system\"))\n+    monkeypatch.setattr(agent, \"get_all_tools\", AsyncMock(return_value=[ping]))\n+\n+    model_config: RealtimeModelConfig = {\n+        \"initial_model_settings\": {\n+            \"model_name\": \"gpt-realtime-mini\",\n+            \"voice\": \"verse\",\n+        }\n+    }\n+    run_config: RealtimeRunConfig = {\n+        \"model_settings\": {\"output_modalities\": [\"text\"]},\n+        \"tracing_disabled\": True,\n+    }\n+    overrides: RealtimeSessionModelSettings = {\n+        \"audio\": {\"input\": {\"format\": {\"type\": \"audio/pcmu\"}}},\n+        \"output_audio_format\": \"g711_ulaw\",\n+    }\n+\n+    payload = await OpenAIRealtimeSIPModel.build_initial_session_payload(\n+        agent,\n+        context={\"user\": \"abc\"},\n+        model_config=model_config,\n+        run_config=run_config,\n+        overrides=overrides,\n+    )\n+\n+    assert isinstance(payload, RealtimeSessionCreateRequest)\n+    assert payload.model == \"gpt-realtime-mini\"\n+    assert payload.output_modalities == [\"text\"]\n+    assert payload.audio is not None\n+    audio = payload.audio\n+    assert audio.input is not None\n+    assert audio.input.format is not None\n+    assert audio.input.format.type == \"audio/pcmu\"\n+    assert audio.output is not None\n+    assert audio.output.format is not None\n+    assert audio.output.format.type == \"audio/pcmu\"\n+    assert audio.output.voice == \"verse\"\n+    assert payload.instructions == \"parent-system\"\n+    assert payload.prompt is not None and payload.prompt.id == \"prompt-99\"\n+    tool_names: set[str] = set()\n+    for tool in payload.tools or []:\n+        name = getattr(tool, \"name\", None)\n+        if name:\n+            tool_names.add(name)\n+    assert ping.name in tool_names\n+    assert f\"transfer_to_{child_agent.name}\" in tool_names",
        "raw_url": "https://github.com/openai/openai-agents-python/raw/5b2a26a7d7616a52bfefaba59cd60281b6d5fc1e/tests%2Frealtime%2Ftest_realtime_model_settings.py",
        "sha": "a73a63414a01a25f33d0a813c4c6897f336f5c03",
        "status": "added"
      }
    ],
    "status": 200
  },
  "started_at": "2026-01-20T04:53:52.248366Z"
}
